 Network data-agrigator_default  Creating
 Network data-agrigator_default  Created
 Container data-agrigator-jobmanager-1  Creating
 Container postgres_db  Creating
 Container broker  Creating
 Container data-agrigator-crowler-producer-1  Creating
 Container data-agrigator-jobmanager-1  Created
 Container data-agrigator-taskmanager-1  Creating
 Container data-agrigator-crowler-producer-1  Created
 Container postgres_db  Created
 Container broker  Created
 Container schema-registry  Creating
 Container data-agrigator-taskmanager-1  Created
 Container schema-registry  Created
 Container connect  Creating
 Container rest-proxy  Creating
 Container rest-proxy  Created
 Container connect  Created
 Container ksqldb-server  Creating
 Container ksqldb-server  Created
 Container ksqldb-cli  Creating
 Container control-center  Creating
 Container ksql-datagen  Creating
 Container ksqldb-cli  Created
 Container ksql-datagen  Created
 Container control-center  Created
Attaching to broker, connect, control-center, data-agrigator-crowler-producer-1, data-agrigator-jobmanager-1, data-agrigator-taskmanager-1, ksql-datagen, ksqldb-cli, ksqldb-server, postgres_db, rest-proxy, schema-registry
broker                             | ===> User
broker                             | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
broker                             | ===> Configuring ...
broker                             | Running in KRaft mode...
postgres_db                        | 
postgres_db                        | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres_db                        | 
postgres_db                        | 
data-agrigator-jobmanager-1        | Starting Job Manager
postgres_db                        | 2023-08-04 10:08:35.378 UTC [1] LOG:  starting PostgreSQL 15.3 on x86_64-pc-linux-musl, compiled by gcc (Alpine 12.2.1_git20220924-r10) 12.2.1 20220924, 64-bit
postgres_db                        | 2023-08-04 10:08:35.430 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgres_db                        | 2023-08-04 10:08:35.430 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres_db                        | 2023-08-04 10:08:35.440 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres_db                        | 2023-08-04 10:08:35.458 UTC [24] LOG:  database system was shut down at 2023-08-04 10:07:42 UTC
postgres_db                        | 2023-08-04 10:08:35.526 UTC [1] LOG:  database system is ready to accept connections
schema-registry                    | ===> User
schema-registry                    | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
schema-registry                    | ===> Configuring ...
data-agrigator-taskmanager-1       | Starting Task Manager
connect                            | ===> User
connect                            | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
connect                            | ===> Configuring ...
rest-proxy                         | ===> User
rest-proxy                         | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
rest-proxy                         | ===> Configuring ...
ksqldb-server                      | ===> User
ksqldb-server                      | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
ksqldb-server                      | ===> Configuring ...
ksqldb-cli                         | sh-4.4$ 
ksql-datagen                       | Waiting for Kafka to be ready...
control-center                     | ===> User
control-center                     | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
control-center                     | ===> Configuring ...
data-agrigator-jobmanager-1        | Starting standalonejob as a console application on host 45393183a926.
data-agrigator-taskmanager-1       | Starting taskexecutor as a console application on host 70e59067e6a2.
broker                             | ===> Running preflight checks ... 
broker                             | ===> Check if /var/lib/kafka/data is writable ...
rest-proxy                         | ===> Running preflight checks ... 
rest-proxy                         | ===> Check if Kafka is healthy ...
schema-registry                    | ===> Running preflight checks ... 
schema-registry                    | ===> Check if Kafka is healthy ...
broker                             | ===> Running in KRaft mode, skipping Zookeeper health check...
broker                             | ===> Using provided cluster id MkU3OEVBNTcwNTJENDM2Qk ...
control-center                     | ===> Check if /etc/confluent-control-center is writable ...
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,800 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,839 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Preconfiguration: 
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,840 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - 
data-agrigator-jobmanager-1        | 
data-agrigator-jobmanager-1        | 
data-agrigator-jobmanager-1        | RESOURCE_PARAMS extraction logs:
data-agrigator-jobmanager-1        | jvm_params: -Xmx1073741824 -Xms1073741824 -XX:MaxMetaspaceSize=268435456
data-agrigator-jobmanager-1        | dynamic_configs: -D jobmanager.memory.off-heap.size=134217728b -D jobmanager.memory.jvm-overhead.min=201326592b -D jobmanager.memory.jvm-metaspace.size=268435456b -D jobmanager.memory.heap.size=1073741824b -D jobmanager.memory.jvm-overhead.max=201326592b
data-agrigator-jobmanager-1        | logs: WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: blob.server.port, 6124
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: taskmanager.memory.process.size, 1728m
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: taskmanager.bind-host, 0.0.0.0
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: jobmanager.execution.failover-strategy, region
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: jobmanager.rpc.address, jobmanager
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: jobmanager.memory.process.size, 1600m
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: jobmanager.rpc.port, 6123
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: query.server.port, 6125
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: rest.bind-address, 0.0.0.0
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: jobmanager.bind-host, 0.0.0.0
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: parallelism.default, 2
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: rest.address, 0.0.0.0
data-agrigator-jobmanager-1        | INFO  [] - The derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
data-agrigator-jobmanager-1        | INFO  [] - Final Master Memory configuration:
data-agrigator-jobmanager-1        | INFO  [] -   Total Process Memory: 1.563gb (1677721600 bytes)
data-agrigator-jobmanager-1        | INFO  [] -     Total Flink Memory: 1.125gb (1207959552 bytes)
data-agrigator-jobmanager-1        | INFO  [] -       JVM Heap:         1024.000mb (1073741824 bytes)
data-agrigator-jobmanager-1        | INFO  [] -       Off-heap:         128.000mb (134217728 bytes)
data-agrigator-jobmanager-1        | INFO  [] -     JVM Metaspace:      256.000mb (268435456 bytes)
data-agrigator-jobmanager-1        | INFO  [] -     JVM Overhead:       192.000mb (201326592 bytes)
data-agrigator-jobmanager-1        | 
data-agrigator-jobmanager-1        | 
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,843 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,856 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Starting StandaloneApplicationClusterEntryPoint (Version: 1.17.1, Scala: 2.12, Rev:2750d5c, Date:2023-05-19T10:45:46+02:00)
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,857 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  OS current user: flink
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,865 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Current Hadoop/Kerberos user: <no hadoop dependency found>
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,867 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM: OpenJDK 64-Bit Server VM - Eclipse Adoptium - 11/11.0.20+8
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,879 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Arch: amd64
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,895 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Maximum heap size: 1024 MiBytes
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,896 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JAVA_HOME: /opt/java/openjdk
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,898 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  No Hadoop Dependency available
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,905 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM Options:
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,911 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xmx1073741824
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,919 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xms1073741824
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,928 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:MaxMetaspaceSize=268435456
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,936 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog.file=/opt/flink/log/flink--standalonejob-0-45393183a926.log
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,937 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configuration=file:/opt/flink/conf/log4j-console.properties
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,947 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configurationFile=file:/opt/flink/conf/log4j-console.properties
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,949 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlogback.configurationFile=file:/opt/flink/conf/logback-console.xml
data-agrigator-jobmanager-1        | 2023-08-04 10:09:16,960 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Program Arguments:
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,000 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     --configDir
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,012 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     /opt/flink/conf
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,031 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,042 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.off-heap.size=134217728b
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,056 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,068 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.min=201326592b
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,121 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,124 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-metaspace.size=268435456b
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,128 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,137 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.heap.size=1073741824b
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,141 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,147 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.max=201326592b
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,169 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     --job-classname
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,181 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     com.example.sharov.anatoliy.DataStreamJob
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,232 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Classpath: /opt/flink/lib/flink-cep-1.17.1.jar:/opt/flink/lib/flink-connector-files-1.17.1.jar:/opt/flink/lib/flink-csv-1.17.1.jar:/opt/flink/lib/flink-json-1.17.1.jar:/opt/flink/lib/flink-scala_2.12-1.17.1.jar:/opt/flink/lib/flink-table-api-java-uber-1.17.1.jar:/opt/flink/lib/flink-table-planner-loader-1.17.1.jar:/opt/flink/lib/flink-table-runtime-1.17.1.jar:/opt/flink/lib/log4j-1.2-api-2.17.1.jar:/opt/flink/lib/log4j-api-2.17.1.jar:/opt/flink/lib/log4j-core-2.17.1.jar:/opt/flink/lib/log4j-slf4j-impl-2.17.1.jar:/opt/flink/lib/flink-dist-1.17.1.jar::::
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,240 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,287 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Registered UNIX signal handlers for [TERM, HUP, INT]
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,679 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: blob.server.port, 6124
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,681 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,684 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.bind-host, 0.0.0.0
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,687 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,689 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, jobmanager
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,692 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.process.size, 1600m
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,695 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,697 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: query.server.port, 6125
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,699 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.bind-address, 0.0.0.0
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,702 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.bind-host, 0.0.0.0
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,704 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 2
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,706 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,708 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.address, 0.0.0.0
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,709 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: jobmanager.memory.off-heap.size, 134217728b
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,714 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: jobmanager.memory.jvm-overhead.min, 201326592b
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: jobmanager.memory.jvm-metaspace.size, 268435456b
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: jobmanager.memory.heap.size, 1073741824b
data-agrigator-jobmanager-1        | 2023-08-04 10:09:17,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: jobmanager.memory.jvm-overhead.max, 201326592b
ksqldb-server                      | ===> Running preflight checks ... 
ksqldb-server                      | ===> Check if Kafka is healthy ...
data-agrigator-jobmanager-1        | 2023-08-04 10:09:18,942 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Starting StandaloneApplicationClusterEntryPoint.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:19,295 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install default filesystem.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:19,357 INFO  org.apache.flink.core.fs.FileSystem                          [] - Hadoop is not in the classpath/dependencies. The extended set of supported File Systems via Hadoop is not available.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:19,653 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-statsd
data-agrigator-jobmanager-1        | 2023-08-04 10:09:19,689 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-datadog
data-agrigator-jobmanager-1        | 2023-08-04 10:09:19,690 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-influx
data-agrigator-jobmanager-1        | 2023-08-04 10:09:19,692 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-prometheus
data-agrigator-jobmanager-1        | 2023-08-04 10:09:19,693 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-jmx
data-agrigator-jobmanager-1        | 2023-08-04 10:09:19,695 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-graphite
data-agrigator-jobmanager-1        | 2023-08-04 10:09:19,696 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: external-resource-gpu
data-agrigator-jobmanager-1        | 2023-08-04 10:09:19,697 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-slf4j
data-agrigator-jobmanager-1        | 2023-08-04 10:09:19,932 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install security context.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,402 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,427 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Preconfiguration: 
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,431 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - 
data-agrigator-taskmanager-1       | 
data-agrigator-taskmanager-1       | 
data-agrigator-taskmanager-1       | RESOURCE_PARAMS extraction logs:
data-agrigator-taskmanager-1       | jvm_params: -Xmx536870902 -Xms536870902 -XX:MaxDirectMemorySize=268435458 -XX:MaxMetaspaceSize=268435456
data-agrigator-taskmanager-1       | dynamic_configs: -D taskmanager.memory.network.min=134217730b -D taskmanager.cpu.cores=2.0 -D taskmanager.memory.task.off-heap.size=0b -D taskmanager.memory.jvm-metaspace.size=268435456b -D external-resources=none -D taskmanager.memory.jvm-overhead.min=201326592b -D taskmanager.memory.framework.off-heap.size=134217728b -D taskmanager.memory.network.max=134217730b -D taskmanager.memory.framework.heap.size=134217728b -D taskmanager.memory.managed.size=536870920b -D taskmanager.memory.task.heap.size=402653174b -D taskmanager.numberOfTaskSlots=2 -D taskmanager.memory.jvm-overhead.max=201326592b
data-agrigator-taskmanager-1       | logs: WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: blob.server.port, 6124
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: taskmanager.memory.process.size, 1728m
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: taskmanager.bind-host, 0.0.0.0
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: jobmanager.execution.failover-strategy, region
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: jobmanager.rpc.address, jobmanager
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: jobmanager.memory.process.size, 1600m
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: jobmanager.rpc.port, 6123
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: query.server.port, 6125
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: rest.bind-address, 0.0.0.0
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: jobmanager.bind-host, 0.0.0.0
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: parallelism.default, 2
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: taskmanager.numberOfTaskSlots, 2
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: rest.address, 0.0.0.0
data-agrigator-taskmanager-1       | INFO  [] - The derived from fraction jvm overhead memory (172.800mb (181193935 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
data-agrigator-taskmanager-1       | INFO  [] - Final TaskExecutor Memory configuration:
data-agrigator-taskmanager-1       | INFO  [] -   Total Process Memory:          1.688gb (1811939328 bytes)
data-agrigator-taskmanager-1       | INFO  [] -     Total Flink Memory:          1.250gb (1342177280 bytes)
data-agrigator-taskmanager-1       | INFO  [] -       Total JVM Heap Memory:     512.000mb (536870902 bytes)
data-agrigator-taskmanager-1       | INFO  [] -         Framework:               128.000mb (134217728 bytes)
data-agrigator-taskmanager-1       | INFO  [] -         Task:                    384.000mb (402653174 bytes)
data-agrigator-taskmanager-1       | INFO  [] -       Total Off-heap Memory:     768.000mb (805306378 bytes)
data-agrigator-taskmanager-1       | INFO  [] -         Managed:                 512.000mb (536870920 bytes)
data-agrigator-taskmanager-1       | INFO  [] -         Total JVM Direct Memory: 256.000mb (268435458 bytes)
data-agrigator-taskmanager-1       | INFO  [] -           Framework:             128.000mb (134217728 bytes)
data-agrigator-taskmanager-1       | INFO  [] -           Task:                  0 bytes
data-agrigator-taskmanager-1       | INFO  [] -           Network:               128.000mb (134217730 bytes)
data-agrigator-taskmanager-1       | INFO  [] -     JVM Metaspace:               256.000mb (268435456 bytes)
data-agrigator-taskmanager-1       | INFO  [] -     JVM Overhead:                192.000mb (201326592 bytes)
data-agrigator-taskmanager-1       | 
data-agrigator-taskmanager-1       | 
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,435 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,439 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Starting TaskManager (Version: 1.17.1, Scala: 2.12, Rev:2750d5c, Date:2023-05-19T10:45:46+02:00)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,451 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  OS current user: flink
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,468 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Current Hadoop/Kerberos user: <no hadoop dependency found>
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,468 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  JVM: OpenJDK 64-Bit Server VM - Eclipse Adoptium - 11/11.0.20+8
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,480 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Arch: amd64
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,481 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Maximum heap size: 512 MiBytes
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,482 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  JAVA_HOME: /opt/java/openjdk
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,483 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  No Hadoop Dependency available
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,484 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  JVM Options:
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,485 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -XX:+UseG1GC
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,488 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Xmx536870902
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,489 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Xms536870902
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,493 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -XX:MaxDirectMemorySize=268435458
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,496 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -XX:MaxMetaspaceSize=268435456
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,497 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlog.file=/opt/flink/log/flink--taskexecutor-0-70e59067e6a2.log
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,500 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlog4j.configuration=file:/opt/flink/conf/log4j-console.properties
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,501 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlog4j.configurationFile=file:/opt/flink/conf/log4j-console.properties
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,512 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlogback.configurationFile=file:/opt/flink/conf/logback-console.xml
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,513 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Program Arguments:
data-agrigator-jobmanager-1        | 2023-08-04 10:09:20,519 INFO  org.apache.flink.runtime.security.modules.HadoopModuleFactory [] - Cannot create Hadoop Security Module because Hadoop cannot be found in the Classpath.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,551 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --configDir
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,552 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     /opt/flink/conf
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,556 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,562 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.network.min=134217730b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,563 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,563 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.cpu.cores=2.0
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,564 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,565 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.task.off-heap.size=0b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,567 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,568 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.jvm-metaspace.size=268435456b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,569 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,577 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     external-resources=none
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,578 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,578 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.jvm-overhead.min=201326592b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,579 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,580 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.framework.off-heap.size=134217728b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,580 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,629 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.network.max=134217730b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,632 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,632 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.framework.heap.size=134217728b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,633 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,634 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.managed.size=536870920b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,654 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,676 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.task.heap.size=402653174b
data-agrigator-jobmanager-1        | 2023-08-04 10:09:20,723 INFO  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file will be created as /tmp/jaas-210370981918708948.conf.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,725 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,733 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.numberOfTaskSlots=2
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,734 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,751 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.jvm-overhead.max=201326592b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,762 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Classpath: /opt/flink/lib/flink-cep-1.17.1.jar:/opt/flink/lib/flink-connector-files-1.17.1.jar:/opt/flink/lib/flink-csv-1.17.1.jar:/opt/flink/lib/flink-json-1.17.1.jar:/opt/flink/lib/flink-scala_2.12-1.17.1.jar:/opt/flink/lib/flink-table-api-java-uber-1.17.1.jar:/opt/flink/lib/flink-table-planner-loader-1.17.1.jar:/opt/flink/lib/flink-table-runtime-1.17.1.jar:/opt/flink/lib/log4j-1.2-api-2.17.1.jar:/opt/flink/lib/log4j-api-2.17.1.jar:/opt/flink/lib/log4j-core-2.17.1.jar:/opt/flink/lib/log4j-slf4j-impl-2.17.1.jar:/opt/flink/lib/flink-dist-1.17.1.jar::::
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,772 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
data-agrigator-jobmanager-1        | 2023-08-04 10:09:20,796 INFO  org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory [] - Cannot install HadoopSecurityContext because Hadoop cannot be found in the Classpath.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:20,810 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Initializing cluster services.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:20,843 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Using working directory: WorkingDirectory(/tmp/jm_89827682c0886bbe770fc245a2a729a7).
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,814 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Registered UNIX signal handlers for [TERM, HUP, INT]
data-agrigator-taskmanager-1       | 2023-08-04 10:09:20,932 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Maximum number of open file descriptors is 1048576.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,168 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: blob.server.port, 6124
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,169 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,173 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.bind-host, 0.0.0.0
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,184 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,185 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, jobmanager
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,195 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.process.size, 1600m
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,208 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,210 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: query.server.port, 6125
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,212 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.bind-address, 0.0.0.0
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,219 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.bind-host, 0.0.0.0
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,223 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 2
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,236 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 2
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,237 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.address, 0.0.0.0
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,240 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.network.min, 134217730b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,243 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.jvm-metaspace.size, 268435456b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,245 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.task.off-heap.size, 0b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,258 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.cpu.cores, 2.0
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,271 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: external-resources, none
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,273 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.jvm-overhead.min, 201326592b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,280 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.framework.off-heap.size, 134217728b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,282 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.network.max, 134217730b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,296 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.framework.heap.size, 134217728b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,298 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.managed.size, 536870920b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,299 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.task.heap.size, 402653174b
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,308 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.numberOfTaskSlots, 2
data-agrigator-taskmanager-1       | 2023-08-04 10:09:21,323 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.jvm-overhead.max, 201326592b
control-center                     | ===> Check if /var/lib/confluent-control-center is writable ...
data-agrigator-taskmanager-1       | 2023-08-04 10:09:22,731 INFO  org.apache.flink.core.fs.FileSystem                          [] - Hadoop is not in the classpath/dependencies. The extended set of supported File Systems via Hadoop is not available.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:23,280 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-statsd
data-agrigator-taskmanager-1       | 2023-08-04 10:09:23,558 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-datadog
data-agrigator-taskmanager-1       | 2023-08-04 10:09:23,572 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-influx
data-agrigator-taskmanager-1       | 2023-08-04 10:09:23,598 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-prometheus
data-agrigator-taskmanager-1       | 2023-08-04 10:09:23,605 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-jmx
data-agrigator-taskmanager-1       | 2023-08-04 10:09:23,614 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-graphite
data-agrigator-taskmanager-1       | 2023-08-04 10:09:23,616 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: external-resource-gpu
data-agrigator-taskmanager-1       | 2023-08-04 10:09:23,636 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-slf4j
data-agrigator-taskmanager-1       | 2023-08-04 10:09:24,177 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - StateChangelogStorageLoader initialized with shortcut names {memory,filesystem}.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:24,178 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-statsd
data-agrigator-taskmanager-1       | 2023-08-04 10:09:24,200 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-datadog
data-agrigator-taskmanager-1       | 2023-08-04 10:09:24,211 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-influx
data-agrigator-taskmanager-1       | 2023-08-04 10:09:24,219 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-prometheus
data-agrigator-taskmanager-1       | 2023-08-04 10:09:24,239 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-jmx
data-agrigator-taskmanager-1       | 2023-08-04 10:09:24,240 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-graphite
data-agrigator-taskmanager-1       | 2023-08-04 10:09:24,241 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: external-resource-gpu
data-agrigator-taskmanager-1       | 2023-08-04 10:09:24,242 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-slf4j
data-agrigator-taskmanager-1       | 2023-08-04 10:09:24,251 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - StateChangelogStorageLoader initialized with shortcut names {memory,filesystem}.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:24,701 INFO  org.apache.flink.runtime.security.modules.HadoopModuleFactory [] - Cannot create Hadoop Security Module because Hadoop cannot be found in the Classpath.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:25,006 INFO  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file will be created as /tmp/jaas-12369742089247962922.conf.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:25,321 INFO  org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory [] - Cannot install HadoopSecurityContext because Hadoop cannot be found in the Classpath.
rest-proxy                         | [2023-08-04 10:09:26,113] INFO AdminClientConfig values: 
rest-proxy                         | 	auto.include.jmx.reporter = true
rest-proxy                         | 	bootstrap.servers = [broker:29092]
rest-proxy                         | 	client.dns.lookup = use_all_dns_ips
rest-proxy                         | 	client.id = 
rest-proxy                         | 	connections.max.idle.ms = 300000
rest-proxy                         | 	default.api.timeout.ms = 60000
rest-proxy                         | 	metadata.max.age.ms = 300000
rest-proxy                         | 	metric.reporters = []
rest-proxy                         | 	metrics.num.samples = 2
rest-proxy                         | 	metrics.recording.level = INFO
rest-proxy                         | 	metrics.sample.window.ms = 30000
rest-proxy                         | 	receive.buffer.bytes = 65536
rest-proxy                         | 	reconnect.backoff.max.ms = 1000
rest-proxy                         | 	reconnect.backoff.ms = 50
rest-proxy                         | 	request.timeout.ms = 30000
rest-proxy                         | 	retries = 2147483647
rest-proxy                         | 	retry.backoff.ms = 100
rest-proxy                         | 	sasl.client.callback.handler.class = null
rest-proxy                         | 	sasl.jaas.config = null
rest-proxy                         | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
rest-proxy                         | 	sasl.kerberos.min.time.before.relogin = 60000
rest-proxy                         | 	sasl.kerberos.service.name = null
rest-proxy                         | 	sasl.kerberos.ticket.renew.jitter = 0.05
rest-proxy                         | 	sasl.kerberos.ticket.renew.window.factor = 0.8
rest-proxy                         | 	sasl.login.callback.handler.class = null
rest-proxy                         | 	sasl.login.class = null
rest-proxy                         | 	sasl.login.connect.timeout.ms = null
rest-proxy                         | 	sasl.login.read.timeout.ms = null
rest-proxy                         | 	sasl.login.refresh.buffer.seconds = 300
rest-proxy                         | 	sasl.login.refresh.min.period.seconds = 60
rest-proxy                         | 	sasl.login.refresh.window.factor = 0.8
rest-proxy                         | 	sasl.login.refresh.window.jitter = 0.05
rest-proxy                         | 	sasl.login.retry.backoff.max.ms = 10000
rest-proxy                         | 	sasl.login.retry.backoff.ms = 100
rest-proxy                         | 	sasl.mechanism = GSSAPI
rest-proxy                         | 	sasl.oauthbearer.clock.skew.seconds = 30
rest-proxy                         | 	sasl.oauthbearer.expected.audience = null
rest-proxy                         | 	sasl.oauthbearer.expected.issuer = null
rest-proxy                         | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
rest-proxy                         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
rest-proxy                         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
rest-proxy                         | 	sasl.oauthbearer.jwks.endpoint.url = null
rest-proxy                         | 	sasl.oauthbearer.scope.claim.name = scope
rest-proxy                         | 	sasl.oauthbearer.sub.claim.name = sub
rest-proxy                         | 	sasl.oauthbearer.token.endpoint.url = null
rest-proxy                         | 	security.protocol = PLAINTEXT
rest-proxy                         | 	security.providers = null
rest-proxy                         | 	send.buffer.bytes = 131072
rest-proxy                         | 	socket.connection.setup.timeout.max.ms = 30000
rest-proxy                         | 	socket.connection.setup.timeout.ms = 10000
rest-proxy                         | 	ssl.cipher.suites = null
rest-proxy                         | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
rest-proxy                         | 	ssl.endpoint.identification.algorithm = https
rest-proxy                         | 	ssl.engine.factory.class = null
rest-proxy                         | 	ssl.key.password = null
rest-proxy                         | 	ssl.keymanager.algorithm = SunX509
rest-proxy                         | 	ssl.keystore.certificate.chain = null
rest-proxy                         | 	ssl.keystore.key = null
rest-proxy                         | 	ssl.keystore.location = null
rest-proxy                         | 	ssl.keystore.password = null
rest-proxy                         | 	ssl.keystore.type = JKS
rest-proxy                         | 	ssl.protocol = TLSv1.3
rest-proxy                         | 	ssl.provider = null
rest-proxy                         | 	ssl.secure.random.implementation = null
rest-proxy                         | 	ssl.trustmanager.algorithm = PKIX
rest-proxy                         | 	ssl.truststore.certificates = null
rest-proxy                         | 	ssl.truststore.location = null
rest-proxy                         | 	ssl.truststore.password = null
rest-proxy                         | 	ssl.truststore.type = JKS
rest-proxy                         |  (org.apache.kafka.clients.admin.AdminClientConfig)
schema-registry                    | [2023-08-04 10:09:28,267] INFO AdminClientConfig values: 
schema-registry                    | 	auto.include.jmx.reporter = true
schema-registry                    | 	bootstrap.servers = [broker:29092]
schema-registry                    | 	client.dns.lookup = use_all_dns_ips
schema-registry                    | 	client.id = 
schema-registry                    | 	connections.max.idle.ms = 300000
schema-registry                    | 	default.api.timeout.ms = 60000
schema-registry                    | 	metadata.max.age.ms = 300000
schema-registry                    | 	metric.reporters = []
schema-registry                    | 	metrics.num.samples = 2
schema-registry                    | 	metrics.recording.level = INFO
schema-registry                    | 	metrics.sample.window.ms = 30000
schema-registry                    | 	receive.buffer.bytes = 65536
schema-registry                    | 	reconnect.backoff.max.ms = 1000
schema-registry                    | 	reconnect.backoff.ms = 50
schema-registry                    | 	request.timeout.ms = 30000
schema-registry                    | 	retries = 2147483647
schema-registry                    | 	retry.backoff.ms = 100
schema-registry                    | 	sasl.client.callback.handler.class = null
schema-registry                    | 	sasl.jaas.config = null
schema-registry                    | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
schema-registry                    | 	sasl.kerberos.min.time.before.relogin = 60000
schema-registry                    | 	sasl.kerberos.service.name = null
schema-registry                    | 	sasl.kerberos.ticket.renew.jitter = 0.05
schema-registry                    | 	sasl.kerberos.ticket.renew.window.factor = 0.8
schema-registry                    | 	sasl.login.callback.handler.class = null
schema-registry                    | 	sasl.login.class = null
schema-registry                    | 	sasl.login.connect.timeout.ms = null
schema-registry                    | 	sasl.login.read.timeout.ms = null
schema-registry                    | 	sasl.login.refresh.buffer.seconds = 300
schema-registry                    | 	sasl.login.refresh.min.period.seconds = 60
schema-registry                    | 	sasl.login.refresh.window.factor = 0.8
schema-registry                    | 	sasl.login.refresh.window.jitter = 0.05
schema-registry                    | 	sasl.login.retry.backoff.max.ms = 10000
schema-registry                    | 	sasl.login.retry.backoff.ms = 100
schema-registry                    | 	sasl.mechanism = GSSAPI
schema-registry                    | 	sasl.oauthbearer.clock.skew.seconds = 30
schema-registry                    | 	sasl.oauthbearer.expected.audience = null
schema-registry                    | 	sasl.oauthbearer.expected.issuer = null
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.url = null
schema-registry                    | 	sasl.oauthbearer.scope.claim.name = scope
schema-registry                    | 	sasl.oauthbearer.sub.claim.name = sub
schema-registry                    | 	sasl.oauthbearer.token.endpoint.url = null
schema-registry                    | 	security.protocol = PLAINTEXT
schema-registry                    | 	security.providers = null
schema-registry                    | 	send.buffer.bytes = 131072
schema-registry                    | 	socket.connection.setup.timeout.max.ms = 30000
schema-registry                    | 	socket.connection.setup.timeout.ms = 10000
schema-registry                    | 	ssl.cipher.suites = null
schema-registry                    | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
schema-registry                    | 	ssl.endpoint.identification.algorithm = https
schema-registry                    | 	ssl.engine.factory.class = null
schema-registry                    | 	ssl.key.password = null
schema-registry                    | 	ssl.keymanager.algorithm = SunX509
schema-registry                    | 	ssl.keystore.certificate.chain = null
schema-registry                    | 	ssl.keystore.key = null
schema-registry                    | 	ssl.keystore.location = null
schema-registry                    | 	ssl.keystore.password = null
schema-registry                    | 	ssl.keystore.type = JKS
schema-registry                    | 	ssl.protocol = TLSv1.3
schema-registry                    | 	ssl.provider = null
schema-registry                    | 	ssl.secure.random.implementation = null
schema-registry                    | 	ssl.trustmanager.algorithm = PKIX
schema-registry                    | 	ssl.truststore.certificates = null
schema-registry                    | 	ssl.truststore.location = null
schema-registry                    | 	ssl.truststore.password = null
schema-registry                    | 	ssl.truststore.type = JKS
schema-registry                    |  (org.apache.kafka.clients.admin.AdminClientConfig)
data-agrigator-jobmanager-1        | 2023-08-04 10:09:29,634 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address jobmanager:6123, bind address 0.0.0.0:6123.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:31,914 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils           [] - Trying to select the network interface and address to use by connecting to the leading JobManager.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:31,919 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils           [] - TaskManager will try to connect for PT10S before falling back to heuristics
ksqldb-server                      | [2023-08-04 10:09:32,034] INFO AdminClientConfig values: 
ksqldb-server                      | 	auto.include.jmx.reporter = true
ksqldb-server                      | 	bootstrap.servers = [broker:29092]
ksqldb-server                      | 	client.dns.lookup = use_all_dns_ips
ksqldb-server                      | 	client.id = 
ksqldb-server                      | 	connections.max.idle.ms = 300000
ksqldb-server                      | 	default.api.timeout.ms = 60000
ksqldb-server                      | 	metadata.max.age.ms = 300000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	metrics.num.samples = 2
ksqldb-server                      | 	metrics.recording.level = INFO
ksqldb-server                      | 	metrics.sample.window.ms = 30000
ksqldb-server                      | 	receive.buffer.bytes = 65536
ksqldb-server                      | 	reconnect.backoff.max.ms = 1000
ksqldb-server                      | 	reconnect.backoff.ms = 50
ksqldb-server                      | 	request.timeout.ms = 30000
ksqldb-server                      | 	retries = 2147483647
ksqldb-server                      | 	retry.backoff.ms = 100
ksqldb-server                      | 	sasl.client.callback.handler.class = null
ksqldb-server                      | 	sasl.jaas.config = null
ksqldb-server                      | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
ksqldb-server                      | 	sasl.kerberos.min.time.before.relogin = 60000
ksqldb-server                      | 	sasl.kerberos.service.name = null
ksqldb-server                      | 	sasl.kerberos.ticket.renew.jitter = 0.05
ksqldb-server                      | 	sasl.kerberos.ticket.renew.window.factor = 0.8
ksqldb-server                      | 	sasl.login.callback.handler.class = null
ksqldb-server                      | 	sasl.login.class = null
ksqldb-server                      | 	sasl.login.connect.timeout.ms = null
ksqldb-server                      | 	sasl.login.read.timeout.ms = null
ksqldb-server                      | 	sasl.login.refresh.buffer.seconds = 300
ksqldb-server                      | 	sasl.login.refresh.min.period.seconds = 60
ksqldb-server                      | 	sasl.login.refresh.window.factor = 0.8
ksqldb-server                      | 	sasl.login.refresh.window.jitter = 0.05
ksqldb-server                      | 	sasl.login.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.login.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.mechanism = GSSAPI
ksqldb-server                      | 	sasl.oauthbearer.clock.skew.seconds = 30
ksqldb-server                      | 	sasl.oauthbearer.expected.audience = null
ksqldb-server                      | 	sasl.oauthbearer.expected.issuer = null
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.url = null
ksqldb-server                      | 	sasl.oauthbearer.scope.claim.name = scope
ksqldb-server                      | 	sasl.oauthbearer.sub.claim.name = sub
ksqldb-server                      | 	sasl.oauthbearer.token.endpoint.url = null
ksqldb-server                      | 	security.protocol = PLAINTEXT
ksqldb-server                      | 	security.providers = null
ksqldb-server                      | 	send.buffer.bytes = 131072
ksqldb-server                      | 	socket.connection.setup.timeout.max.ms = 30000
ksqldb-server                      | 	socket.connection.setup.timeout.ms = 10000
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (org.apache.kafka.clients.admin.AdminClientConfig)
rest-proxy                         | [2023-08-04 10:09:32,334] INFO Kafka version: 7.4.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
rest-proxy                         | [2023-08-04 10:09:32,347] INFO Kafka commitId: fed9c006bfc7ba5b (org.apache.kafka.common.utils.AppInfoParser)
rest-proxy                         | [2023-08-04 10:09:32,348] INFO Kafka startTimeMs: 1691143772261 (org.apache.kafka.common.utils.AppInfoParser)
rest-proxy                         | [2023-08-04 10:09:33,065] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:33,143] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:33,204] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:33,205] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:33,311] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:33,313] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:33,619] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:33,620] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:33,878] INFO Kafka version: 7.4.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:09:33,884] INFO Kafka commitId: fed9c006bfc7ba5b (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:09:33,884] INFO Kafka startTimeMs: 1691143773819 (org.apache.kafka.common.utils.AppInfoParser)
rest-proxy                         | [2023-08-04 10:09:34,026] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:34,027] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:34,472] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:34,517] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:34,579] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:34,579] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:34,682] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:34,683] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:34,845] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:34,846] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:34,888] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:34,892] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:35,304] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:35,305] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:35,658] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:35,659] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:35,668 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to address jobmanager/192.168.160.4:6123
data-agrigator-taskmanager-1       | 2023-08-04 10:09:35,670 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [70e59067e6a2/192.168.160.7] with timeout [200] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:35,672 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:35,675 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:35,678 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/127.0.0.1] with timeout [50] due to: Invalid argument (connect failed)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:35,683 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [1000] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:35,689 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/127.0.0.1] with timeout [1000] due to: Invalid argument (connect failed)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:35,796 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to address jobmanager/192.168.160.4:6123
data-agrigator-taskmanager-1       | 2023-08-04 10:09:35,798 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [70e59067e6a2/192.168.160.7] with timeout [200] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:35,800 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:35,802 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:35,804 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/127.0.0.1] with timeout [50] due to: Invalid argument (connect failed)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:35,811 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [1000] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:35,813 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/127.0.0.1] with timeout [1000] due to: Invalid argument (connect failed)
ksqldb-server                      | [2023-08-04 10:09:35,853] INFO Kafka version: 7.4.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:09:35,854] INFO Kafka commitId: fed9c006bfc7ba5b (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:09:35,855] INFO Kafka startTimeMs: 1691143775829 (org.apache.kafka.common.utils.AppInfoParser)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:36,014 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to address jobmanager/192.168.160.4:6123
data-agrigator-taskmanager-1       | 2023-08-04 10:09:36,016 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [70e59067e6a2/192.168.160.7] with timeout [200] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:36,018 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:36,021 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:36,022 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/127.0.0.1] with timeout [50] due to: Invalid argument (connect failed)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:36,025 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [1000] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:36,033 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/127.0.0.1] with timeout [1000] due to: Invalid argument (connect failed)
ksqldb-server                      | [2023-08-04 10:09:36,101] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:36,115] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:36,119] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:36,150] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:36,187] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:36,207] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:36,312] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:36,313] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:36,440 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to address jobmanager/192.168.160.4:6123
data-agrigator-taskmanager-1       | 2023-08-04 10:09:36,444 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [70e59067e6a2/192.168.160.7] with timeout [200] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:36,450 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:36,466 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:36,472 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/127.0.0.1] with timeout [50] due to: Invalid argument (connect failed)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:36,508 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [1000] due to: Connection refused (Connection refused)
ksqldb-server                      | [2023-08-04 10:09:36,518] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:36,519] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:36,522 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/127.0.0.1] with timeout [1000] due to: Invalid argument (connect failed)
rest-proxy                         | [2023-08-04 10:09:36,881] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:36,882] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:36,930] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:36,931] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:37,130] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:37,132] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:37,372 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to address jobmanager/192.168.160.4:6123
data-agrigator-taskmanager-1       | 2023-08-04 10:09:37,374 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [70e59067e6a2/192.168.160.7] with timeout [200] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:37,378 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:37,391 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:37,396 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/127.0.0.1] with timeout [50] due to: Invalid argument (connect failed)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:37,401 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [1000] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:37,406 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/127.0.0.1] with timeout [1000] due to: Invalid argument (connect failed)
ksqldb-server                      | [2023-08-04 10:09:37,841] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:37,841] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:38,009] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:38,010] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:38,352] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:38,353] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:38,754] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:38,756] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:39,015 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to address jobmanager/192.168.160.4:6123
data-agrigator-taskmanager-1       | 2023-08-04 10:09:39,017 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [70e59067e6a2/192.168.160.7] with timeout [200] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:39,019 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:39,021 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:39,024 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/127.0.0.1] with timeout [50] due to: Invalid argument (connect failed)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:39,027 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/192.168.160.7] with timeout [1000] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:39,037 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/192.168.160.4:6123] from local address [/127.0.0.1] with timeout [1000] due to: Invalid argument (connect failed)
rest-proxy                         | [2023-08-04 10:09:39,138] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:39,139] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker                             | ===> Launching ... 
broker                             | ===> Launching kafka ... 
schema-registry                    | [2023-08-04 10:09:39,266] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:39,268] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:39,687] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:39,688] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:09:39,713 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
data-agrigator-jobmanager-1        | 2023-08-04 10:09:39,939 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:39,944 INFO  akka.remote.Remoting                                         [] - Starting remoting
rest-proxy                         | [2023-08-04 10:09:40,370] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:40,373] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:40,393] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:40,394] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:40,805] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:40,806] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
control-center                     | ===> Running preflight checks ... 
control-center                     | ===> Check if Kafka is healthy ...
data-agrigator-jobmanager-1        | 2023-08-04 10:09:41,211 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink@jobmanager:6123]
rest-proxy                         | [2023-08-04 10:09:41,392] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:41,395] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:41,518] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:41,519] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:41,924 WARN  org.apache.flink.runtime.net.ConnectionUtils                 [] - Could not connect to jobmanager/192.168.160.4:6123. Selecting a local address using heuristics.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:41,926 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - TaskManager will use hostname/address '70e59067e6a2' (192.168.160.7) for communication.
ksqldb-server                      | [2023-08-04 10:09:41,927] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:41,928] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:42,182 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address 192.168.160.7:0, bind address 0.0.0.0:0.
rest-proxy                         | [2023-08-04 10:09:42,517] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:42,519] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:42,659] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:42,660] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:09:42,693 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink@jobmanager:6123
data-agrigator-jobmanager-1        | 2023-08-04 10:09:42,836 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Loading delegation token providers
ksqldb-server                      | [2023-08-04 10:09:42,848] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:42,849] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:09:42,882 INFO  org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider [] - Hadoop FS is not available (not packaged with this application): NoClassDefFoundError : "org/apache/hadoop/conf/Configuration".
data-agrigator-jobmanager-1        | 2023-08-04 10:09:42,884 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Delegation token provider hadoopfs loaded and initialized
data-agrigator-jobmanager-1        | 2023-08-04 10:09:42,890 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Delegation token provider hbase loaded and initialized
data-agrigator-jobmanager-1        | 2023-08-04 10:09:42,900 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-statsd
data-agrigator-jobmanager-1        | 2023-08-04 10:09:42,915 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-datadog
data-agrigator-jobmanager-1        | 2023-08-04 10:09:42,917 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-influx
data-agrigator-jobmanager-1        | 2023-08-04 10:09:42,920 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-prometheus
data-agrigator-jobmanager-1        | 2023-08-04 10:09:42,921 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-jmx
data-agrigator-jobmanager-1        | 2023-08-04 10:09:42,924 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-graphite
data-agrigator-jobmanager-1        | 2023-08-04 10:09:42,925 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: external-resource-gpu
data-agrigator-jobmanager-1        | 2023-08-04 10:09:42,940 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-slf4j
data-agrigator-jobmanager-1        | 2023-08-04 10:09:42,947 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Delegation token providers loaded successfully
data-agrigator-jobmanager-1        | 2023-08-04 10:09:42,950 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Loading delegation token receivers
data-agrigator-jobmanager-1        | 2023-08-04 10:09:42,990 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receiver hadoopfs loaded and initialized
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,009 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receiver hbase loaded and initialized
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,021 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-statsd
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,022 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-datadog
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,022 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-influx
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,023 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-prometheus
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,024 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-jmx
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,024 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-graphite
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,025 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: external-resource-gpu
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,025 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-slf4j
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,050 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receivers loaded successfully
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,051 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Checking provider and receiver instances consistency
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,051 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Provider and receiver instances are consistent
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,052 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Obtaining delegation tokens
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,106 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Delegation tokens obtained successfully
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,124 WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - No tokens obtained so skipping notifications
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,359 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Created BLOB server storage directory /tmp/jm_89827682c0886bbe770fc245a2a729a7/blobStorage
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,392 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Started BLOB server at 0.0.0.0:6124 - max concurrent requests: 50 - max backlog: 1000
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,569 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:43,631 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address jobmanager:0, bind address 0.0.0.0:0.
rest-proxy                         | [2023-08-04 10:09:43,642] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:43,646] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:43,669] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:43,670] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:44,082] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:44,087] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:09:44,276 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
data-agrigator-jobmanager-1        | 2023-08-04 10:09:44,456 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:44,464 INFO  akka.remote.Remoting                                         [] - Starting remoting
rest-proxy                         | [2023-08-04 10:09:44,570] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:44,572] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:44,679] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:44,680] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:09:44,816 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink-metrics@jobmanager:44783]
data-agrigator-jobmanager-1        | 2023-08-04 10:09:45,072 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink-metrics@jobmanager:44783
data-agrigator-jobmanager-1        | 2023-08-04 10:09:45,310 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
ksqldb-server                      | [2023-08-04 10:09:45,310] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:45,311] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:45,494] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:45,494] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:45,792] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:45,792] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:46,325] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:46,329] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:46,520] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:46,522] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:09:46,612 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Upload directory /tmp/flink-web-b90ce9bc-1d70-4593-af62-d2f315724559/flink-web-upload does not exist. 
data-agrigator-jobmanager-1        | 2023-08-04 10:09:46,616 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Created directory /tmp/flink-web-b90ce9bc-1d70-4593-af62-d2f315724559/flink-web-upload for file uploads.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:46,661 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Starting rest endpoint.
schema-registry                    | [2023-08-04 10:09:46,701] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:46,702] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:47,449] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:47,451] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:47,650] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:47,653] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:47,911] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:47,912] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:48,476] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:48,477] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:48,673] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:48,674] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:48,839] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:48,840] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker                             | [2023-08-04 10:09:49,062] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
ksqldb-server                      | [2023-08-04 10:09:49,688] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:49,690] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:49,813] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:49,813] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:49,849] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:49,849] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:50,019 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
data-agrigator-jobmanager-1        | 2023-08-04 10:09:50,029 INFO  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Determined location of main cluster component log file: /opt/flink/log/flink--standalonejob-0-45393183a926.log
data-agrigator-jobmanager-1        | 2023-08-04 10:09:50,030 INFO  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Determined location of main cluster component stdout file: /opt/flink/log/flink--standalonejob-0-45393183a926.out
data-agrigator-taskmanager-1       | 2023-08-04 10:09:50,430 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:50,461 INFO  akka.remote.Remoting                                         [] - Starting remoting
rest-proxy                         | [2023-08-04 10:09:50,722] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:50,723] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
ksql-datagen                       | Error while getting broker list.
ksql-datagen                       | java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: listNodes
ksql-datagen                       | 	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
ksql-datagen                       | 	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
ksql-datagen                       | 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
ksql-datagen                       | 	at io.confluent.admin.utils.ClusterStatus.isKafkaReady(ClusterStatus.java:147)
ksql-datagen                       | 	at io.confluent.admin.utils.cli.KafkaReadyCommand.main(KafkaReadyCommand.java:149)
ksql-datagen                       | Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: listNodes
ksqldb-server                      | [2023-08-04 10:09:50,922] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:50,925] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:51,059] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:51,060] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
connect                            | ===> Running preflight checks ... 
connect                            | ===> Check if Kafka is healthy ...
data-agrigator-jobmanager-1        | 2023-08-04 10:09:51,588 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Rest endpoint listening at 0.0.0.0:8081
data-agrigator-jobmanager-1        | 2023-08-04 10:09:51,603 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - http://0.0.0.0:8081 was granted leadership with leaderSessionID=00000000-0000-0000-0000-000000000000
data-agrigator-jobmanager-1        | 2023-08-04 10:09:51,609 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Web frontend listening at http://0.0.0.0:8081.
ksql-datagen                       | Expected 1 brokers but found only 0. Trying to query Kafka for metadata again ...
ksql-datagen                       | Expected 1 brokers but found only 0. Brokers found [].
ksqldb-server                      | [2023-08-04 10:09:51,835] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:51,836] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
ksql-datagen                       | Using log4j config /etc/cp-base-new/log4j.properties
data-agrigator-jobmanager-1        | 2023-08-04 10:09:51,871 INFO  org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner [] - DefaultDispatcherRunner was granted leadership with leader id 00000000-0000-0000-0000-000000000000. Creating new DispatcherLeaderProcess.
rest-proxy                         | [2023-08-04 10:09:51,933] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:51,934] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:51,968] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:51,968] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:09:52,012 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:52,018 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Starting resource manager service.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:52,088 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is granted leadership with session id 00000000-0000-0000-0000-000000000000.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:52,130 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs that are not finished, yet.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:52,148 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:52,423 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink@192.168.160.7:42073]
data-agrigator-jobmanager-1        | 2023-08-04 10:09:52,555 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_0 .
data-agrigator-jobmanager-1        | 2023-08-04 10:09:52,656 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .
ksqldb-server                      | [2023-08-04 10:09:52,748] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:52,755] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:52,986] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:52,987] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
ksql-datagen exited with code 1
schema-registry                    | [2023-08-04 10:09:53,086] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:53,087] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker                             | [2023-08-04 10:09:53,252] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
data-agrigator-jobmanager-1        | 2023-08-04 10:09:53,583 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Starting the resource manager.
ksqldb-server                      | [2023-08-04 10:09:53,667] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:53,668] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:09:53,802 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Starting tokens update task
data-agrigator-jobmanager-1        | 2023-08-04 10:09:53,805 WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - No tokens obtained so skipping notifications
data-agrigator-jobmanager-1        | 2023-08-04 10:09:53,806 WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Tokens update task not started because either no tokens obtained or none of the tokens specified its renewal date
rest-proxy                         | [2023-08-04 10:09:53,917] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:53,921] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:54,002] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:54,003] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:09:54,173 INFO  org.apache.flink.client.ClientUtils                          [] - Starting program (detached: true)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:54,701 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink@192.168.160.7:42073
data-agrigator-jobmanager-1        | 2023-08-04 10:09:54,724 WARN  org.apache.flink.connector.kafka.source.KafkaSourceBuilder   [] - Offset commit on checkpoint is disabled because group.id is not specified
ksqldb-server                      | [2023-08-04 10:09:54,781] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:54,781] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:54,844] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:54,844] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:54,918 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Using working directory: WorkingDirectory(/tmp/tm_192.168.160.7:42073-a96234)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:55,011 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:55,038 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address 192.168.160.7:0, bind address 0.0.0.0:0.
broker                             | [2023-08-04 10:09:55,129] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
broker                             | [2023-08-04 10:09:55,150] INFO Starting controller (kafka.server.ControllerServer)
schema-registry                    | [2023-08-04 10:09:55,215] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:55,215] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:55,336 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
data-agrigator-taskmanager-1       | 2023-08-04 10:09:55,364 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
data-agrigator-jobmanager-1        | WARNING: An illegal reflective access operation has occurred
data-agrigator-jobmanager-1        | WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/opt/flink/lib/flink-dist-1.17.1.jar) to field java.lang.String.value
data-agrigator-jobmanager-1        | WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner
data-agrigator-jobmanager-1        | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
data-agrigator-jobmanager-1        | WARNING: All illegal access operations will be denied in a future release
data-agrigator-taskmanager-1       | 2023-08-04 10:09:55,380 INFO  akka.remote.Remoting                                         [] - Starting remoting
data-agrigator-taskmanager-1       | 2023-08-04 10:09:55,533 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink-metrics@192.168.160.7:41557]
data-agrigator-taskmanager-1       | 2023-08-04 10:09:55,650 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink-metrics@192.168.160.7:41557
ksqldb-server                      | [2023-08-04 10:09:55,792] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:55,793] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:55,841 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService_192.168.160.7:42073-a96234 .
rest-proxy                         | [2023-08-04 10:09:55,854] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:55,854] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,054 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Created BLOB cache storage directory /tmp/tm_192.168.160.7:42073-a96234/blobStorage
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,099 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Created BLOB cache storage directory /tmp/tm_192.168.160.7:42073-a96234/blobStorage
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,141 INFO  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Enabled external resources: []
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,149 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Loading delegation token receivers
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,254 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receiver hadoopfs loaded and initialized
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,257 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receiver hbase loaded and initialized
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,260 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-statsd
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,270 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-datadog
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,273 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-influx
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,281 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-prometheus
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,282 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-jmx
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,283 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-graphite
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,289 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: external-resource-gpu
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,290 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-slf4j
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,294 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receivers loaded successfully
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,297 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Starting TaskManager with ResourceID: 192.168.160.7:42073-a96234
schema-registry                    | [2023-08-04 10:09:56,329] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:56,329] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,597 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices    [] - Temporary file directory '/tmp': total 56 GB, usable 4 GB (7.14% usable)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,639 INFO  org.apache.flink.runtime.io.disk.iomanager.IOManager         [] - Created a new FileChannelManager for spilling of task related data to disk (joins, sorting, ...). Used directories:
data-agrigator-taskmanager-1       | 	/tmp/flink-io-58a08308-98a5-45b6-80af-dc8791d35ee5
data-agrigator-taskmanager-1       | 2023-08-04 10:09:56,731 INFO  org.apache.flink.runtime.io.network.netty.NettyConfig        [] - NettyConfig [server address: /0.0.0.0, server port: 0, ssl enabled: false, memory segment size (bytes): 32768, transport type: AUTO, number of server threads: 2 (manual), number of client threads: 2 (manual), server connect backlog: 0 (use Netty's default), client connect timeout (sec): 120, send/receive buffer size (bytes): 0 (use Netty's default)]
ksqldb-server                      | [2023-08-04 10:09:56,807] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:56,809] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
connect                            | SLF4J: Class path contains multiple SLF4J bindings.
connect                            | SLF4J: Found binding in [jar:file:/usr/share/java/cp-base-new/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
connect                            | SLF4J: Found binding in [jar:file:/usr/share/java/cp-base-new/slf4j-simple-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
connect                            | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
rest-proxy                         | [2023-08-04 10:09:56,897] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:56,897] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
connect                            | SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
connect                            | 
connect                            | 
data-agrigator-taskmanager-1       | 2023-08-04 10:09:57,196 INFO  org.apache.flink.runtime.io.network.NettyShuffleServiceFactory [] - Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
data-agrigator-taskmanager-1       | 	/tmp/flink-netty-shuffle-aa5d3cac-0b38-4df2-a8b5-f4650ab95f41
schema-registry                    | [2023-08-04 10:09:57,343] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:57,345] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:09:57,387 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Job 53db330e339329e833955cb86275d944 is submitted.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:57,387 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Submitting Job with JobId=53db330e339329e833955cb86275d944.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:57,510 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'Flink Java API Skeleton' (53db330e339329e833955cb86275d944).
data-agrigator-jobmanager-1        | 2023-08-04 10:09:57,518 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'Flink Java API Skeleton' (53db330e339329e833955cb86275d944).
data-agrigator-jobmanager-1        | 2023-08-04 10:09:57,667 INFO  org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner [] - JobMasterServiceLeadershipRunner for job 53db330e339329e833955cb86275d944 was granted leadership with leader id 00000000-0000-0000-0000-000000000000. Creating new JobMasterServiceProcess.
connect                            | 0 [main] DEBUG io.confluent.admin.utils.cli.KafkaReadyCommand  - Arguments Namespace(zookeeper_connect=null, min_expected_brokers=1, security_protocol=PLAINTEXT, config=null, bootstrap_servers=broker:29092, timeout=40000). 
data-agrigator-jobmanager-1        | 2023-08-04 10:09:57,812 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_2 .
ksqldb-server                      | [2023-08-04 10:09:57,920] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:57,920] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:09:57,939 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'Flink Java API Skeleton' (53db330e339329e833955cb86275d944).
rest-proxy                         | [2023-08-04 10:09:58,009] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:58,009] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
connect                            | 432 [main] INFO org.apache.kafka.clients.admin.AdminClientConfig  - AdminClientConfig values: 
connect                            | 	bootstrap.servers = [broker:29092]
connect                            | 	client.dns.lookup = use_all_dns_ips
connect                            | 	client.id = 
connect                            | 	connections.max.idle.ms = 300000
connect                            | 	default.api.timeout.ms = 60000
connect                            | 	metadata.max.age.ms = 300000
connect                            | 	metric.reporters = []
connect                            | 	metrics.num.samples = 2
connect                            | 	metrics.recording.level = INFO
connect                            | 	metrics.sample.window.ms = 30000
connect                            | 	receive.buffer.bytes = 65536
connect                            | 	reconnect.backoff.max.ms = 1000
connect                            | 	reconnect.backoff.ms = 50
connect                            | 	request.timeout.ms = 30000
connect                            | 	retries = 2147483647
connect                            | 	retry.backoff.ms = 100
connect                            | 	sasl.client.callback.handler.class = null
connect                            | 	sasl.jaas.config = null
connect                            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	sasl.kerberos.service.name = null
connect                            | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	sasl.login.callback.handler.class = null
connect                            | 	sasl.login.class = null
connect                            | 	sasl.login.connect.timeout.ms = null
connect                            | 	sasl.login.read.timeout.ms = null
connect                            | 	sasl.login.refresh.buffer.seconds = 300
connect                            | 	sasl.login.refresh.min.period.seconds = 60
connect                            | 	sasl.login.refresh.window.factor = 0.8
connect                            | 	sasl.login.refresh.window.jitter = 0.05
connect                            | 	sasl.login.retry.backoff.max.ms = 10000
connect                            | 	sasl.login.retry.backoff.ms = 100
connect                            | 	sasl.mechanism = GSSAPI
connect                            | 	sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	sasl.oauthbearer.expected.audience = null
connect                            | 	sasl.oauthbearer.expected.issuer = null
connect                            | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	sasl.oauthbearer.scope.claim.name = scope
connect                            | 	sasl.oauthbearer.sub.claim.name = sub
connect                            | 	sasl.oauthbearer.token.endpoint.url = null
connect                            | 	security.protocol = PLAINTEXT
connect                            | 	security.providers = null
connect                            | 	send.buffer.bytes = 131072
connect                            | 	socket.connection.setup.timeout.max.ms = 30000
connect                            | 	socket.connection.setup.timeout.ms = 10000
connect                            | 	ssl.cipher.suites = null
connect                            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	ssl.endpoint.identification.algorithm = https
connect                            | 	ssl.engine.factory.class = null
connect                            | 	ssl.key.password = null
connect                            | 	ssl.keymanager.algorithm = SunX509
connect                            | 	ssl.keystore.certificate.chain = null
connect                            | 	ssl.keystore.key = null
connect                            | 	ssl.keystore.location = null
connect                            | 	ssl.keystore.password = null
connect                            | 	ssl.keystore.type = JKS
connect                            | 	ssl.protocol = TLSv1.3
connect                            | 	ssl.provider = null
connect                            | 	ssl.secure.random.implementation = null
connect                            | 	ssl.trustmanager.algorithm = PKIX
connect                            | 	ssl.truststore.certificates = null
connect                            | 	ssl.truststore.location = null
connect                            | 	ssl.truststore.password = null
connect                            | 	ssl.truststore.type = JKS
connect                            | 
connect                            | 
data-agrigator-taskmanager-1       | 2023-08-04 10:09:58,287 INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool [] - Allocated 128 MB for network buffer pool (number of memory segments: 4096, bytes per segment: 32768).
data-agrigator-jobmanager-1        | 2023-08-04 10:09:58,290 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy NoRestartBackoffTimeStrategy for Flink Java API Skeleton (53db330e339329e833955cb86275d944).
data-agrigator-taskmanager-1       | 2023-08-04 10:09:58,398 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Starting the network environment and its components.
schema-registry                    | [2023-08-04 10:09:58,458] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:58,459] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
connect                            | 861 [main] DEBUG org.apache.kafka.clients.admin.internals.AdminMetadataManager  - [AdminClient clientId=adminclient-1] Setting bootstrap cluster metadata Cluster(id = null, nodes = [broker:29092 (id: -1 rack: null)], partitions = [], controller = null).
data-agrigator-jobmanager-1        | 2023-08-04 10:09:58,747 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Created execution graph 5cbd3dcd053fa27fd34008101b1671be for job 53db330e339329e833955cb86275d944.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:58,877 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Transport type 'auto': using EPOLL.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:58,899 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful initialization (took 494 ms).
data-agrigator-taskmanager-1       | 2023-08-04 10:09:58,954 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Transport type 'auto': using EPOLL.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:59,076 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job Flink Java API Skeleton (53db330e339329e833955cb86275d944).
data-agrigator-jobmanager-1        | 2023-08-04 10:09:59,085 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 9 ms.
rest-proxy                         | [2023-08-04 10:09:59,117] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:59,118] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:59,134] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:09:59,134] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:59,253 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful initialization (took 337 ms). Listening on SocketAddress /0.0.0.0:42665.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:59,264 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Starting the kvState service and its components.
schema-registry                    | [2023-08-04 10:09:59,479] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:09:59,480] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:09:59,490 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
data-agrigator-taskmanager-1       | 2023-08-04 10:09:59,641 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.
data-agrigator-jobmanager-1        | 2023-08-04 10:09:59,653 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 2 new pipelined regions in 31 ms, total 2 pipelined regions currently.
data-agrigator-taskmanager-1       | 2023-08-04 10:09:59,696 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - User file cache uses directory /tmp/flink-dist-cache-17656963-a210-4786-949f-f819bf4cc4e7
data-agrigator-jobmanager-1        | 2023-08-04 10:09:59,706 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@30ffb70c
data-agrigator-taskmanager-1       | 2023-08-04 10:09:59,714 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000).
data-agrigator-jobmanager-1        | 2023-08-04 10:09:59,715 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
data-agrigator-jobmanager-1        | 2023-08-04 10:09:59,720 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Checkpoint storage is set to 'jobmanager'
rest-proxy                         | [2023-08-04 10:09:59,933] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:09:59,933] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:10:00,051] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:10:00,053] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker                             | [2023-08-04 10:10:00,077] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:00,218 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.
broker                             | [2023-08-04 10:10:00,231] INFO Awaiting socket connections on broker:29093. (kafka.network.DataPlaneAcceptor)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:00,338 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@384f1c48 for Flink Java API Skeleton (53db330e339329e833955cb86275d944).
schema-registry                    | [2023-08-04 10:10:00,388] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:10:00,388] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:00,618 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'Flink Java API Skeleton' (53db330e339329e833955cb86275d944) under job master id 00000000000000000000000000000000.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:00,771 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Starting split enumerator for source Source: Kafka Source.
broker                             | [2023-08-04 10:10:00,791] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
broker                             | [2023-08-04 10:10:00,816] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)
rest-proxy                         | [2023-08-04 10:10:00,941] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:10:00,942] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:00,958 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
data-agrigator-jobmanager-1        | 2023-08-04 10:10:00,965 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Java API Skeleton (53db330e339329e833955cb86275d944) switched from state CREATED to RUNNING.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:01,022 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2) (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from CREATED to SCHEDULED.
ksqldb-server                      | [2023-08-04 10:10:01,075] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:10:01,076] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:10:01,302] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:10:01,302] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:01,314 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from CREATED to SCHEDULED.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:01,334 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:01,382 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration
data-agrigator-jobmanager-1        | 2023-08-04 10:10:01,398 INFO  org.apache.kafka.clients.admin.AdminClientConfig             [] - AdminClientConfig values: 
data-agrigator-jobmanager-1        | 	bootstrap.servers = [localhost:9092]
data-agrigator-jobmanager-1        | 	client.dns.lookup = use_all_dns_ips
data-agrigator-jobmanager-1        | 	client.id = KafkaSource-6492951471566687241-enumerator-admin-client
data-agrigator-jobmanager-1        | 	connections.max.idle.ms = 300000
data-agrigator-jobmanager-1        | 	default.api.timeout.ms = 60000
data-agrigator-jobmanager-1        | 	metadata.max.age.ms = 300000
data-agrigator-jobmanager-1        | 	metric.reporters = []
data-agrigator-jobmanager-1        | 	metrics.num.samples = 2
data-agrigator-jobmanager-1        | 	metrics.recording.level = INFO
data-agrigator-jobmanager-1        | 	metrics.sample.window.ms = 30000
data-agrigator-jobmanager-1        | 	receive.buffer.bytes = 65536
data-agrigator-jobmanager-1        | 	reconnect.backoff.max.ms = 1000
data-agrigator-jobmanager-1        | 	reconnect.backoff.ms = 50
data-agrigator-jobmanager-1        | 	request.timeout.ms = 30000
data-agrigator-jobmanager-1        | 	retries = 2147483647
data-agrigator-jobmanager-1        | 	retry.backoff.ms = 100
data-agrigator-jobmanager-1        | 	sasl.client.callback.handler.class = null
data-agrigator-jobmanager-1        | 	sasl.jaas.config = null
data-agrigator-jobmanager-1        | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
data-agrigator-jobmanager-1        | 	sasl.kerberos.min.time.before.relogin = 60000
data-agrigator-jobmanager-1        | 	sasl.kerberos.service.name = null
data-agrigator-jobmanager-1        | 	sasl.kerberos.ticket.renew.jitter = 0.05
data-agrigator-jobmanager-1        | 	sasl.kerberos.ticket.renew.window.factor = 0.8
data-agrigator-jobmanager-1        | 	sasl.login.callback.handler.class = null
data-agrigator-jobmanager-1        | 	sasl.login.class = null
data-agrigator-jobmanager-1        | 	sasl.login.connect.timeout.ms = null
data-agrigator-jobmanager-1        | 	sasl.login.read.timeout.ms = null
data-agrigator-jobmanager-1        | 	sasl.login.refresh.buffer.seconds = 300
data-agrigator-jobmanager-1        | 	sasl.login.refresh.min.period.seconds = 60
data-agrigator-jobmanager-1        | 	sasl.login.refresh.window.factor = 0.8
data-agrigator-jobmanager-1        | 	sasl.login.refresh.window.jitter = 0.05
data-agrigator-jobmanager-1        | 	sasl.login.retry.backoff.max.ms = 10000
data-agrigator-jobmanager-1        | 	sasl.login.retry.backoff.ms = 100
data-agrigator-jobmanager-1        | 	sasl.mechanism = GSSAPI
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.clock.skew.seconds = 30
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.expected.audience = null
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.expected.issuer = null
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.jwks.endpoint.url = null
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.scope.claim.name = scope
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.sub.claim.name = sub
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.token.endpoint.url = null
data-agrigator-jobmanager-1        | 	security.protocol = PLAINTEXT
data-agrigator-jobmanager-1        | 	security.providers = null
data-agrigator-jobmanager-1        | 	send.buffer.bytes = 131072
data-agrigator-jobmanager-1        | 	socket.connection.setup.timeout.max.ms = 30000
data-agrigator-jobmanager-1        | 	socket.connection.setup.timeout.ms = 10000
data-agrigator-jobmanager-1        | 	ssl.cipher.suites = null
data-agrigator-jobmanager-1        | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
data-agrigator-jobmanager-1        | 	ssl.endpoint.identification.algorithm = https
data-agrigator-jobmanager-1        | 	ssl.engine.factory.class = null
data-agrigator-jobmanager-1        | 	ssl.key.password = null
data-agrigator-jobmanager-1        | 	ssl.keymanager.algorithm = SunX509
data-agrigator-jobmanager-1        | 	ssl.keystore.certificate.chain = null
data-agrigator-jobmanager-1        | 	ssl.keystore.key = null
data-agrigator-jobmanager-1        | 	ssl.keystore.location = null
data-agrigator-jobmanager-1        | 	ssl.keystore.password = null
data-agrigator-jobmanager-1        | 	ssl.keystore.type = JKS
data-agrigator-jobmanager-1        | 	ssl.protocol = TLSv1.3
data-agrigator-jobmanager-1        | 	ssl.provider = null
data-agrigator-jobmanager-1        | 	ssl.secure.random.implementation = null
data-agrigator-jobmanager-1        | 	ssl.trustmanager.algorithm = PKIX
data-agrigator-jobmanager-1        | 	ssl.truststore.certificates = null
data-agrigator-jobmanager-1        | 	ssl.truststore.location = null
data-agrigator-jobmanager-1        | 	ssl.truststore.password = null
data-agrigator-jobmanager-1        | 	ssl.truststore.type = JKS
data-agrigator-jobmanager-1        | 
data-agrigator-jobmanager-1        | 2023-08-04 10:10:01,420 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager 00000000000000000000000000000000@akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job 53db330e339329e833955cb86275d944.
data-agrigator-jobmanager-1        | 
broker                             | [2023-08-04 10:10:01,578] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:10:01,589] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:10:01,596] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Producer state recovery took 6ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:01,623 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 00000000000000000000000000000000@akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job 53db330e339329e833955cb86275d944.
connect                            | 3849 [main] INFO org.apache.kafka.common.utils.AppInfoParser  - Kafka version: 7.1.0-ccs
connect                            | 3850 [main] INFO org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId: c86722379ab997cc
connect                            | 3850 [main] INFO org.apache.kafka.common.utils.AppInfoParser  - Kafka startTimeMs: 1691143801601
connect                            | 3859 [main] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Kafka admin client initialized
data-agrigator-jobmanager-1        | 2023-08-04 10:10:01,650 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager 00000000000000000000000000000000@akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job 53db330e339329e833955cb86275d944.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:01,670 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 00000000000000000000000000000000@akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job 53db330e339329e833955cb86275d944.
connect                            | 3867 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Thread starting
data-agrigator-jobmanager-1        | 2023-08-04 10:10:01,695 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: 00000000000000000000000000000000.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:01,702 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 53db330e339329e833955cb86275d944: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
connect                            | 4017 [main] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Queueing Call(callName=listNodes, deadlineMs=1691143841746, tries=0, nextAllowedTryMs=0) with a timeout 30000 ms from now.
connect                            | 4042 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host broker as 192.168.160.3
connect                            | 4043 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node broker:29092 (id: -1 rack: null) using address broker/192.168.160.3
broker                             | [2023-08-04 10:10:01,938] INFO Initialized snapshots with IDs SortedSet() from /tmp/kraft-combined-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
connect                            | 4165 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with broker/192.168.160.3 disconnected
connect                            | java.net.ConnectException: Connection refused
connect                            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
connect                            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
connect                            | 	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
connect                            | 	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
connect                            | 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
connect                            | 	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
connect                            | 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
connect                            | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
connect                            | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
connect                            | 	at java.base/java.lang.Thread.run(Thread.java:829)
connect                            | 4182 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 disconnected.
connect                            | 4215 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available.
connect                            | 4255 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host broker as 192.168.160.3
connect                            | 4256 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node broker:29092 (id: -1 rack: null) using address broker/192.168.160.3
connect                            | 4258 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with broker/192.168.160.3 disconnected
connect                            | java.net.ConnectException: Connection refused
connect                            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
connect                            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
connect                            | 	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
connect                            | 	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
connect                            | 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
connect                            | 	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
connect                            | 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
connect                            | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
connect                            | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
connect                            | 	at java.base/java.lang.Thread.run(Thread.java:829)
connect                            | 4261 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 disconnected.
connect                            | 4261 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available.
rest-proxy                         | [2023-08-04 10:10:02,050] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:10:02,053] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker                             | [2023-08-04 10:10:02,095] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
connect                            | 4367 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host broker as 192.168.160.3
connect                            | 4367 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node broker:29092 (id: -1 rack: null) using address broker/192.168.160.3
connect                            | 4369 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with broker/192.168.160.3 disconnected
connect                            | java.net.ConnectException: Connection refused
connect                            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
connect                            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
connect                            | 	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
connect                            | 	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
connect                            | 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
connect                            | 	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
connect                            | 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
connect                            | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
connect                            | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
connect                            | 	at java.base/java.lang.Thread.run(Thread.java:829)
connect                            | 4371 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 disconnected.
connect                            | 4374 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available.
ksqldb-server                      | [2023-08-04 10:10:02,295] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:10:02,296] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
connect                            | 4578 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host broker as 192.168.160.3
connect                            | 4578 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node broker:29092 (id: -1 rack: null) using address broker/192.168.160.3
connect                            | 4580 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with broker/192.168.160.3 disconnected
connect                            | java.net.ConnectException: Connection refused
connect                            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
connect                            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
connect                            | 	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
connect                            | 	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
connect                            | 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
connect                            | 	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
connect                            | 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
connect                            | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
connect                            | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
connect                            | 	at java.base/java.lang.Thread.run(Thread.java:829)
connect                            | 4582 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 disconnected.
connect                            | 4583 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,385 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'key.deserializer' was supplied but isn't a known config.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,386 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'commit.offsets.on.checkpoint' was supplied but isn't a known config.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,388 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'value.deserializer' was supplied but isn't a known config.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,388 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'enable.auto.commit' was supplied but isn't a known config.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,389 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,389 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,390 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'auto.offset.reset' was supplied but isn't a known config.
schema-registry                    | [2023-08-04 10:10:02,419] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:10:02,420] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,423 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.2.3
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,423 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 50029d3ed8ba576f
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,424 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1691143802390
rest-proxy                         | [2023-08-04 10:10:02,433] INFO [AdminClient clientId=adminclient-1] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
rest-proxy                         | org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: fetchMetadata
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,504 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Starting the KafkaSourceEnumerator for consumer group null without periodic partition discovery.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,590 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,600 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:02,649 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Resolved ResourceManager address, beginning registration
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,715 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,715 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,825 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:02,827 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
connect                            | 5089 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host broker as 192.168.160.3
connect                            | 5089 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node broker:29092 (id: -1 rack: null) using address broker/192.168.160.3
connect                            | 5094 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with broker/192.168.160.3 disconnected
connect                            | java.net.ConnectException: Connection refused
connect                            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
connect                            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
connect                            | 	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
connect                            | 	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
connect                            | 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
connect                            | 	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
connect                            | 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
connect                            | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
connect                            | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
connect                            | 	at java.base/java.lang.Thread.run(Thread.java:829)
connect                            | 5096 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 disconnected.
connect                            | 5101 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available.
rest-proxy                         | [2023-08-04 10:10:02,965] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:10:02,968] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:03,037 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:03,038 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
ksqldb-server                      | [2023-08-04 10:10:03,226] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:10:03,227] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:03,332 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 192.168.160.7:42073-a96234 (akka.tcp://flink@192.168.160.7:42073/user/rpc/taskmanager_0) at ResourceManager
data-agrigator-taskmanager-1       | 2023-08-04 10:10:03,426 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_* under registration id c6069da9863be0307db8a92ee2463718.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:03,447 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:03,448 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1       | WARNING: An illegal reflective access operation has occurred
data-agrigator-taskmanager-1       | WARNING: Illegal reflective access by org.jboss.netty.util.internal.ByteBufferUtil (file:/tmp/flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar) to method java.nio.DirectByteBuffer.cleaner()
data-agrigator-taskmanager-1       | WARNING: Please consider reporting this to the maintainers of org.jboss.netty.util.internal.ByteBufferUtil
data-agrigator-taskmanager-1       | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
data-agrigator-taskmanager-1       | WARNING: All illegal access operations will be denied in a future release
data-agrigator-taskmanager-1       | 
data-agrigator-taskmanager-1       | 
schema-registry                    | [2023-08-04 10:10:03,558] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:10:03,561] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
connect                            | 5813 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host broker as 192.168.160.3
connect                            | 5814 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node broker:29092 (id: -1 rack: null) using address broker/192.168.160.3
connect                            | 5816 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with broker/192.168.160.3 disconnected
connect                            | java.net.ConnectException: Connection refused
connect                            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
connect                            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
connect                            | 	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
connect                            | 	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
connect                            | 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
connect                            | 	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
connect                            | 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
connect                            | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
connect                            | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
connect                            | 	at java.base/java.lang.Thread.run(Thread.java:829)
connect                            | 5838 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 disconnected.
connect                            | 5840 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:03,673 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 38979a1c87fd4f74552500881122aa9d for job 53db330e339329e833955cb86275d944 from resource manager with leader id 00000000000000000000000000000000.
broker                             | [2023-08-04 10:10:03,711] INFO [RaftManager nodeId=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1618) (org.apache.kafka.raft.QuorumState)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:03,720 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 38979a1c87fd4f74552500881122aa9d.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:03,725 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job 53db330e339329e833955cb86275d944 for job leader monitoring.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:03,731 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 with leader id 00000000-0000-0000-0000-000000000000.
broker                             | [2023-08-04 10:10:03,731] INFO [RaftManager nodeId=1] Completed transition to CandidateState(localId=1, epoch=1, retries=1, electionTimeoutMs=1905) (org.apache.kafka.raft.QuorumState)
broker                             | [2023-08-04 10:10:03,763] INFO [RaftManager nodeId=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) (org.apache.kafka.raft.QuorumState)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:03,788 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request cc5ae52e6ae73eb640cef73e64585b38 for job 53db330e339329e833955cb86275d944 from resource manager with leader id 00000000000000000000000000000000.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:03,794 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for cc5ae52e6ae73eb640cef73e64585b38.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:03,868 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
broker                             | [2023-08-04 10:10:03,982] INFO [kafka-raft-outbound-request-thread]: Starting (kafka.raft.RaftSendThread)
broker                             | [2023-08-04 10:10:03,988] INFO [kafka-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)
schema-registry                    | [2023-08-04 10:10:03,955] INFO [AdminClient clientId=adminclient-1] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
schema-registry                    | org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: fetchMetadata
rest-proxy                         | [2023-08-04 10:10:04,084] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:10:04,084] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:04,144 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job 53db330e339329e833955cb86275d944.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:04,155 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job 53db330e339329e833955cb86275d944.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:04,160 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:04,161 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
broker                             | [2023-08-04 10:10:04,182] INFO [MetadataLoader 1] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:04,189 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job 53db330e339329e833955cb86275d944.
broker                             | [2023-08-04 10:10:04,231] INFO [RaftManager nodeId=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)
broker                             | [2023-08-04 10:10:04,286] INFO [MetadataLoader 1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:04,297 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2) (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from SCHEDULED to DEPLOYING.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:04,308 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2) (attempt #0) with attempt id 5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_0_0 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_0 to 192.168.160.7:42073-a96234 @ data-agrigator-taskmanager-1.data-agrigator_default (dataPort=42665) with allocation id 38979a1c87fd4f74552500881122aa9d
ksqldb-server                      | [2023-08-04 10:10:04,356] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:10:04,357] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker                             | [2023-08-04 10:10:04,363] INFO [RaftManager nodeId=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1234459363 (org.apache.kafka.raft.KafkaRaftClient)
broker                             | [2023-08-04 10:10:04,388] INFO [MetadataLoader 1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:04,439 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from SCHEDULED to DEPLOYING.
broker                             | [2023-08-04 10:10:04,443] INFO [MetadataLoader 1] handleCommit: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:04,451 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (attempt #0) with attempt id 5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_1_0 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_1 to 192.168.160.7:42073-a96234 @ data-agrigator-taskmanager-1.data-agrigator_default (dataPort=42665) with allocation id cc5ae52e6ae73eb640cef73e64585b38
broker                             | [2023-08-04 10:10:04,483] INFO [Controller 1] Creating new QuorumController with clusterId MkU3OEVBNTcwNTJENDM2Qg, authorizer Optional.empty. (org.apache.kafka.controller.QuorumController)
broker                             | [2023-08-04 10:10:04,499] INFO [MetadataLoader 1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
broker                             | [2023-08-04 10:10:04,507] INFO [RaftManager nodeId=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@1608806715 (org.apache.kafka.raft.KafkaRaftClient)
broker                             | [2023-08-04 10:10:04,519] INFO [Controller 1] Becoming the active controller at epoch 1, committed offset -1, committed epoch -1 (org.apache.kafka.controller.QuorumController)
broker                             | [2023-08-04 10:10:04,542] INFO [Controller 1] The metadata log appears to be empty. Appending 1 bootstrap record(s) at metadata.version 3.4-IV0 from the binary bootstrap metadata file: /tmp/kraft-combined-logs/bootstrap.checkpoint. (org.apache.kafka.controller.QuorumController)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:04,549 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 38979a1c87fd4f74552500881122aa9d.
broker                             | [2023-08-04 10:10:04,566] INFO [Controller 1] Setting metadata.version to 8 (org.apache.kafka.controller.FeatureControlManager)
broker                             | [2023-08-04 10:10:04,592] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:10:04,602] INFO [MetadataLoader 1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
schema-registry                    | [2023-08-04 10:10:04,609] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:10:04,609] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker                             | [2023-08-04 10:10:04,613] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:10:04,627] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:10:04,649] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:04,660 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
broker                             | [2023-08-04 10:10:04,666] INFO [MetadataLoader 1] handleCommit: The loader finished catching up to the current high water mark of 2 (org.apache.kafka.image.loader.MetadataLoader)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:04,700 INFO  org.apache.flink.runtime.state.TaskExecutorChannelStateExecutorFactoryManager [] - Creating the channel state executor factory for job id 53db330e339329e833955cb86275d944
broker                             | [2023-08-04 10:10:04,715] INFO [MetadataLoader 1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 1 (org.apache.kafka.image.loader.MetadataLoader)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:04,733 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_0_0), deploy into slot with allocation id 38979a1c87fd4f74552500881122aa9d.
broker                             | [2023-08-04 10:10:04,733] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
broker                             | [2023-08-04 10:10:04,724] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:04,744 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from CREATED to DEPLOYING.
broker                             | [2023-08-04 10:10:04,772] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
broker                             | [2023-08-04 10:10:04,774] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:04,785 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_0_0) [DEPLOYING].
data-agrigator-taskmanager-1       | 2023-08-04 10:10:04,786 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot cc5ae52e6ae73eb640cef73e64585b38.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:04,797 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_1_0), deploy into slot with allocation id cc5ae52e6ae73eb640cef73e64585b38.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:04,802 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 38979a1c87fd4f74552500881122aa9d.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:04,802 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot cc5ae52e6ae73eb640cef73e64585b38.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:04,808 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from CREATED to DEPLOYING.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:04,833 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_1_0) [DEPLOYING].
connect                            | 7060 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host broker as 192.168.160.3
connect                            | 7061 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node broker:29092 (id: -1 rack: null) using address broker/192.168.160.3
connect                            | 7063 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with broker/192.168.160.3 disconnected
connect                            | java.net.ConnectException: Connection refused
connect                            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
connect                            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
connect                            | 	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
connect                            | 	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
connect                            | 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
connect                            | 	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
connect                            | 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
connect                            | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
connect                            | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
connect                            | 	at java.base/java.lang.Thread.run(Thread.java:829)
connect                            | 7074 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 disconnected.
connect                            | 7074 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available.
broker                             | [2023-08-04 10:10:04,905] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:10:04,918] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:10:04,930] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:10:04,939] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:05,041 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@30acb36b
data-agrigator-taskmanager-1       | 2023-08-04 10:10:05,043 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4d473521
data-agrigator-taskmanager-1       | 2023-08-04 10:10:05,044 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
data-agrigator-taskmanager-1       | 2023-08-04 10:10:05,047 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
data-agrigator-taskmanager-1       | 2023-08-04 10:10:05,068 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
data-agrigator-taskmanager-1       | 2023-08-04 10:10:05,068 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
broker                             | [2023-08-04 10:10:05,080] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)
broker                             | [2023-08-04 10:10:05,089] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Recorded new controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
rest-proxy                         | [2023-08-04 10:10:05,111] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy                         | [2023-08-04 10:10:05,112] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:05,137 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from DEPLOYING to INITIALIZING.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:05,137 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from DEPLOYING to INITIALIZING.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:05,142 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:05,143 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:05,195 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from DEPLOYING to INITIALIZING.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:05,221 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2) (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from DEPLOYING to INITIALIZING.
ksqldb-server                      | [2023-08-04 10:10:05,274] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:10:05,276] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.160.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker                             | [2023-08-04 10:10:05,556] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
broker                             | [2023-08-04 10:10:05,568] INFO Awaiting socket connections on broker:29092. (kafka.network.DataPlaneAcceptor)
broker                             | [2023-08-04 10:10:05,596] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
broker                             | [2023-08-04 10:10:05,599] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
broker                             | [2023-08-04 10:10:05,602] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
broker                             | [2023-08-04 10:10:05,643] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
broker                             | [2023-08-04 10:10:05,727] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Starting (kafka.server.BrokerToControllerRequestThread)
broker                             | [2023-08-04 10:10:05,728] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Recorded new controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
broker                             | [2023-08-04 10:10:05,818] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:10:05,845] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:10:05,846] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:10:05,880] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
ksqldb-server                      | [2023-08-04 10:10:05,892] INFO [AdminClient clientId=adminclient-1] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
ksqldb-server                      | org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: fetchMetadata
connect                            | 8287 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host broker as 192.168.160.3
connect                            | 8287 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node broker:29092 (id: -1 rack: null) using address broker/192.168.160.3
connect                            | 8291 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node -1
broker                             | [2023-08-04 10:10:06,098] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:10:06,116] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:06,253 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:06,254 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
broker                             | [2023-08-04 10:10:06,947] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Starting (kafka.server.BrokerToControllerRequestThread)
broker                             | [2023-08-04 10:10:06,947] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Recorded new controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
broker                             | [2023-08-04 10:10:06,956] INFO [BrokerLifecycleManager id=1] Incarnation 4pXZ5YrQSZKkJWXbAwwBBA of broker 1 in cluster MkU3OEVBNTcwNTJENDM2Qg is now STARTING. (kafka.server.BrokerLifecycleManager)
broker                             | [2023-08-04 10:10:06,960] INFO [RaftManager nodeId=1] Registered the listener kafka.server.metadata.BrokerMetadataListener@1585573225 (org.apache.kafka.raft.KafkaRaftClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:07,096 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:07,097 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:07,369 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:07,370 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
broker                             | [2023-08-04 10:10:07,782] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:10:07,829] INFO [BrokerServer id=1] Waiting for broker metadata to catch up. (kafka.server.BrokerServer)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:07,904 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:07,907 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:07,936 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from INITIALIZING to FAILED with failure cause:
data-agrigator-taskmanager-1       | java.io.IOException: unable to open JDBC writer
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at java.lang.Thread.run(Unknown Source) [?:?]
data-agrigator-taskmanager-1       | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-taskmanager-1       | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.Driver.connect(Driver.java:297) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	... 14 more
data-agrigator-taskmanager-1       | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.Driver.make
data-agrigator-taskmanager-1       | Connection(Driver.java:443) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.Driver.connect(Driver.java:297) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	... 14 more
data-agrigator-taskmanager-1       | 2023-08-04 10:10:07,937 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from INITIALIZING to FAILED with failure cause:
data-agrigator-taskmanager-1       | java.io.IOException: unable to open JDBC writer
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at java.lang.Thread.run(Unknown Source) [?:?]
data-agrigator-taskmanager-1       | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-taskmanager-1       | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.Driver.connect(Driver.java:297) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	... 14 more
data-agrigator-taskmanager-1       | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.Driver.connect(Driver.java:297) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	... 14 more
data-agrigator-taskmanager-1       | 2023-08-04 10:10:08,038 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_0_0).
data-agrigator-taskmanager-1       | 2023-08-04 10:10:08,037 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_1_0).
data-agrigator-jobmanager-1        | 2023-08-04 10:10:08,277 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:08,278 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:08,296 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_0_0.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:08,615 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_1_0.
connect                            | 11021 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Completed connection to node -1. Fetching API versions.
connect                            | 11021 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating API versions fetch from node -1.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:08,870 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2) (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from INITIALIZING to FAILED on 192.168.160.7:42073-a96234 @ data-agrigator-taskmanager-1.data-agrigator_default (dataPort=42665).
data-agrigator-jobmanager-1        | java.io.IOException: unable to open JDBC writer
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1        | 	... 14 more
data-agrigator-jobmanager-1        | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-jobmanager-1        | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1        | 	... 14 more
connect                            | 11246 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=adminclient-1, correlationId=0) and timeout 3600000 to node -1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='7.1.0-ccs')
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,102 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 53db330e339329e833955cb86275d944: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,136 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Removing registered reader after failure for subtask 0 (#0) of source Source: Kafka Source.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,155 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Java API Skeleton (53db330e339329e833955cb86275d944) switched from state RUNNING to FAILING.
data-agrigator-jobmanager-1        | org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:258) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:242) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:748) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | Caused by: java.io.IOException: unable to open JDBC writer
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.op
data-agrigator-jobmanager-1        | en(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-jobmanager-1        | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
broker                             | [2023-08-04 10:10:09,197] INFO [Controller 1] Registered new broker: RegisterBrokerRecord(brokerId=1, isMigratingZkBroker=false, incarnationId=4pXZ5YrQSZKkJWXbAwwBBA, brokerEpoch=10, endPoints=[BrokerEndpoint(name='PLAINTEXT', host='broker', port=29092, securityProtocol=0), BrokerEndpoint(name='PLAINTEXT_HOST', host='localhost', port=9092, securityProtocol=0)], features=[BrokerFeature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=8)], rack=null, fenced=true, inControlledShutdown=false) (org.apache.kafka.controller.ClusterControlManager)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,221 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from INITIALIZING to CANCELING.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,252 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from CANCELING to CANCELED.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,256 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 53db330e339329e833955cb86275d944
broker                             | [2023-08-04 10:10:09,268] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 10 (kafka.server.BrokerLifecycleManager)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,255 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Java API Skeleton (53db330e339329e833955cb86275d944) switched from state FAILING to FAILED.
data-agrigator-jobmanager-1        | org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:258) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:242) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:748) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | Caused by: java.io.IOException: unable to open JDBC writer
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.ope
data-agrigator-jobmanager-1        | n(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-jobmanager-1        | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,294 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job 53db330e339329e833955cb86275d944.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,328 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 5cbd3dcd053fa27fd34008101b1671be_cbc357ccb763df2852fee8c4fc7d55f2_1_0.
broker                             | [2023-08-04 10:10:09,356] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,359 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 53db330e339329e833955cb86275d944 reached terminal state FAILED.
data-agrigator-jobmanager-1        | org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:258)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:242)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:748)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
data-agrigator-jobmanager-1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
data-agrigator-jobmanager-1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
data-agrigator-jobmanager-1        | 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
data-agrigator-jobmanager-1        | 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
data-agrigator-jobmanager-1        | 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
data-agrigator-jobmanager-1        | 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
data-agrigator-jobmanager-1        | 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
data-agrigator-jobmanager-1        | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579)
data-agrigator-jobmanager-1        | 	at akka.actor.ActorCell.invoke(ActorCell.scala:547)
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
data-agrigator-jobmanager-1        | 	at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
data-agrigator-jobmanager-1        | Caused by: java.io.IOException: unable to open JDBC writer
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142)
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52)
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101)
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46)
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734)
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709)
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
data-agrigator-jobmanager-1        | 	at java.base/java.lang.Thread.run(Unknown Source)
data-agrigator-jobmanager-1        | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342)
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443)
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297)
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121)
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140)
data-agrigator-jobmanager-1        | 	... 14 more
data-agrigator-jobmanager-1        | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-jobmanager-1        | 	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
data-agrigator-jobmanager-1        | 	at java.base/java.net.AbstractPlainSocketImpl.doConnect(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.net.AbstractPlainSocketImpl.connect(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.net.SocksSocketImpl.connect(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.net.Socket.connect(Unknown Source)
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243)
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98)
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132)
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)
data-agrigator-jobmanager-1        | 	... 20 more
broker                             | [2023-08-04 10:10:09,379] INFO [BrokerLifecycleManager id=1] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
broker                             | [2023-08-04 10:10:09,386] INFO [BrokerMetadataListener id=1] Starting to publish metadata events at offset 11. (kafka.server.metadata.BrokerMetadataListener)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,390 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 53db330e339329e833955cb86275d944 has been registered for cleanup in the JobResultStore after reaching a terminal state.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,399 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Node -1 disconnected.
broker                             | [2023-08-04 10:10:09,409] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=11, epoch=1) with metadata.version 3.4-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
broker                             | [2023-08-04 10:10:09,414] INFO Loading logs from log dirs ArraySeq(/tmp/kraft-combined-logs) (kafka.log.LogManager)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,408 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job 'Flink Java API Skeleton' (53db330e339329e833955cb86275d944).
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,407 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
broker                             | [2023-08-04 10:10:09,426] INFO Attempting recovery for all logs in /tmp/kraft-combined-logs since no clean shutdown file was found (kafka.log.LogManager)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,451 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Closing SourceCoordinator for source Source: Kafka Source.
broker                             | [2023-08-04 10:10:09,473] INFO Loaded 0 logs in 59ms. (kafka.log.LogManager)
broker                             | [2023-08-04 10:10:09,476] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
broker                             | [2023-08-04 10:10:09,480] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,485 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,488 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Disconnect TaskExecutor 192.168.160.7:42073-a96234 because: Stopping JobMaster for job 'Flink Java API Skeleton' (53db330e339329e833955cb86275d944).
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,491 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [38979a1c87fd4f74552500881122aa9d].
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,515 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [cc5ae52e6ae73eb640cef73e64585b38].
data-agrigator-taskmanager-1       | 2023-08-04 10:10:09,535 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1, taskHeapMemory=192.000mb (201326587 bytes), taskOffHeapMemory=0 bytes, managedMemory=256.000mb (268435460 bytes), networkMemory=64.000mb (67108865 bytes)}, allocationId: 38979a1c87fd4f74552500881122aa9d, jobId: 53db330e339329e833955cb86275d944).
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,554 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 89827682c0886bbe770fc245a2a729a7: Stopping JobMaster for job 'Flink Java API Skeleton' (53db330e339329e833955cb86275d944).
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,558 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job 53db330e339329e833955cb86275d944 from the resource manager.
broker                             | [2023-08-04 10:10:09,561] INFO Starting the log cleaner (kafka.log.LogCleaner)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:09,609 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:1, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1, taskHeapMemory=192.000mb (201326587 bytes), taskOffHeapMemory=0 bytes, managedMemory=256.000mb (268435460 bytes), networkMemory=64.000mb (67108865 bytes)}, allocationId: cc5ae52e6ae73eb640cef73e64585b38, jobId: 53db330e339329e833955cb86275d944).
data-agrigator-taskmanager-1       | 2023-08-04 10:10:09,619 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 53db330e339329e833955cb86275d944 from job leader monitoring.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:09,630 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job 53db330e339329e833955cb86275d944.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,654 INFO  org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap [] - Application FAILED: 
data-agrigator-jobmanager-1        | java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: FAILED
data-agrigator-jobmanager-1        | 	at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$unwrapJobResultException$7(ApplicationDispatcherBootstrap.java:403) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.postComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.complete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.client.deployment.application.JobStatusPollingUtils.lambda$null$2(JobStatusPollingUtils.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.whenComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.client.deployment.application.JobStatusPollingUtils.lambda$pollJobResultAsync$4(JobStatusPollingUtils.java:96) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.postComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.complete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:267) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.postComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.complete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1300) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.postComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.complete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.OnComplete.internal(Future.scala:300) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.OnComplete.internal(Future.scala:297) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:622) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) [flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | Caused by: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: FAILED
data-agrigator-jobmanager-1        | 	at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:71) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	... 56 more
data-agrigator-jobmanager-1        | Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:60) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	... 56 more
data-agrigator-jobmanager-1        | Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.De
data-agrigator-jobmanager-1        | faultScheduler.recordTaskFailure(DefaultScheduler.java:258) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:242) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:748) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_59720b34-8b1e-41e9-b27c-27380bb0584e.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.actor.ActorCell.invoke(ActorCell.scala:547) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
data-agrigator-jobmanager-1        | 	... 5 more
data-agrigator-jobmanager-1        | Caused by: java.io.IOException: unable to open JDBC writer
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-jobmanager-1        | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.op
data-agrigator-jobmanager-1        | enConnectionImpl(ConnectionFactoryImpl.java:258) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,813 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting StandaloneApplicationClusterEntryPoint down with application status FAILED. Diagnostics null.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:09,846 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shutting down rest endpoint.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,067 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Removing cache directory /tmp/flink-web-b90ce9bc-1d70-4593-af62-d2f315724559/flink-web-ui
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,070 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - http://0.0.0.0:8081 lost leadership
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,072 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shut down complete.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,107 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Shut down cluster because application is in FAILED, diagnostics null.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,110 INFO  org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent [] - Closing components.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,113 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Stopping SessionDispatcherLeaderProcess.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,119 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping dispatcher akka.tcp://flink@jobmanager:6123/user/rpc/dispatcher_1.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,122 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping all currently running jobs of dispatcher akka.tcp://flink@jobmanager:6123/user/rpc/dispatcher_1.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,147 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka.tcp://flink@jobmanager:6123/user/rpc/dispatcher_1.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,157 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Stopping resource manager service.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,162 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Stopping credential renewal
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,166 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Stopped credential renewal
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,167 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Closing the slot manager.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,169 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Suspending the slot manager.
broker                             | [2023-08-04 10:10:10,171] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,176 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is not running. Ignore revoking leadership.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,186 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:6124
broker                             | [2023-08-04 10:10:10,204] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
broker                             | [2023-08-04 10:10:10,210] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:10:10,217] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
broker                             | [2023-08-04 10:10:10,219] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,224 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
broker                             | [2023-08-04 10:10:10,264] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,260 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
broker                             | [2023-08-04 10:10:10,299] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,305 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,308 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-6492951471566687241-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
broker                             | [2023-08-04 10:10:10,321] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 8 at offset OffsetAndEpoch(offset=11, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)
broker                             | [2023-08-04 10:10:10,442] INFO KafkaConfig values: 
broker                             | 	advertised.listeners = PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
broker                             | 	alter.config.policy.class.name = null
broker                             | 	alter.log.dirs.replication.quota.window.num = 11
broker                             | 	alter.log.dirs.replication.quota.window.size.seconds = 1
broker                             | 	authorizer.class.name = 
broker                             | 	auto.create.topics.enable = true
broker                             | 	auto.include.jmx.reporter = true
broker                             | 	auto.leader.rebalance.enable = true
broker                             | 	background.threads = 10
broker                             | 	broker.heartbeat.interval.ms = 2000
broker                             | 	broker.id = 1
broker                             | 	broker.id.generation.enable = true
broker                             | 	broker.rack = null
broker                             | 	broker.session.timeout.ms = 9000
broker                             | 	client.quota.callback.class = null
broker                             | 	compression.type = producer
broker                             | 	connection.failed.authentication.delay.ms = 100
broker                             | 	connections.max.idle.ms = 600000
broker                             | 	connections.max.reauth.ms = 0
broker                             | 	control.plane.listener.name = null
broker                             | 	controlled.shutdown.enable = true
broker                             | 	controlled.shutdown.max.retries = 3
broker                             | 	controlled.shutdown.retry.backoff.ms = 5000
broker                             | 	controller.listener.names = CONTROLLER
broker                             | 	controller.quorum.append.linger.ms = 25
broker                             | 	controller.quorum.election.backoff.max.ms = 1000
broker                             | 	controller.quorum.election.timeout.ms = 1000
broker                             | 	controller.quorum.fetch.timeout.ms = 2000
broker                             | 	controller.quorum.request.timeout.ms = 2000
broker                             | 	controller.quorum.retry.backoff.ms = 20
broker                             | 	controller.quorum.voters = [1@broker:29093]
broker                             | 	controller.quota.window.num = 11
broker                             | 	controller.quota.window.size.seconds = 1
broker                             | 	controller.socket.timeout.ms = 30000
broker                             | 	create.topic.policy.class.name = null
broker                             | 	default.replication.factor = 1
broker                             | 	delegation.token.expiry.check.interval.ms = 3600000
broker                             | 	delegation.token.expiry.time.ms = 86400000
broker                             | 	delegation.token.master.key = null
broker                             | 	delegation.token.max.lifetime.ms = 604800000
broker                             | 	delegation.token.secret.key = null
broker                             | 	delete.records.purgatory.purge.interval.requests = 1
broker                             | 	delete.topic.enable = true
broker                             | 	early.start.listeners = null
broker                             | 	fetch.max.bytes = 57671680
broker                             | 	fetch.purgatory.purge.interval.requests = 1000
broker                             | 	group.initial.rebalance.delay.ms = 0
broker                             | 	group.max.session.timeout.ms = 1800000
broker                             | 	group.max.size = 2147483647
broker                             | 	group.min.session.timeout.ms = 6000
broker                             | 	initial.broker.registration.timeout.ms = 60000
broker                             | 	inter.broker.listener.name = PLAINTEXT
broker                             | 	inter.broker.protocol.version = 3.4-IV0
broker                             | 	kafka.metrics.polling.interval.secs = 10
broker                             | 	kafka.metrics.reporters = []
broker                             | 	leader.imbalance.check.interval.seconds = 300
broker                             | 	leader.imbalance.per.broker.percentage = 10
broker                             | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
broker                             | 	listeners = PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092
broker                             | 	log.cleaner.backoff.ms = 15000
broker                             | 	log.cleaner.dedupe.buffer.size = 134217728
broker                             | 	log.cleaner.delete.retention.ms = 86400000
broker                             | 	log.cleaner.enable = true
broker                             | 	log.cleaner.io.buffer.load.factor = 0.9
broker                             | 	log.cleaner.io.buffer.size = 524288
broker                             | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
broker                             | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
broker                             | 	log.cleaner.min.cleanable.ratio = 0.5
broker                             | 	log.cleaner.min.compaction.lag.ms = 0
broker                             | 	log.cleaner.threads = 1
broker                             | 	log.cleanup.policy = [delete]
broker                             | 	log.dir = /tmp/kafka-logs
broker                             | 	log.dirs = /tmp/kraft-combined-logs
broker                             | 	log.flush.interval.messages = 9223372036854775807
broker                             | 	log.flush.interval.ms = null
broker                             | 	log.flush.offset.checkpoint.interval.ms = 60000
broker                             | 	log.flush.scheduler.interval.ms = 9223372036854775807
broker                             | 	log.flush.start.offset.checkpoint.interval.ms = 60000
broker                             | 	log.index.interval.bytes = 4096
broker                             | 	log.index.size.max.bytes = 10485760
broker                             | 	log.message.downconversion.enable = true
broker                             | 	log.message.format.version = 3.0-IV1
broker                             | 	log.message.timestamp.difference.max.ms = 9223372036854775807
broker                             | 	log.message.timestamp.type = CreateTime
broker                             | 	log.preallocate = false
broker                             | 	log.retention.bytes = -1
broker                             | 	log.retention.check.interval.ms = 300000
broker                             | 	log.retention.hours = 168
broker                             | 	log.retention.minutes = null
broker                             | 	log.retention.ms = null
broker                             | 	log.roll.hours = 168
broker                             | 	log.roll.jitter.hours = 0
broker                             | 	log.roll.jitter.ms = null
broker                             | 	log.roll.ms = null
broker                             | 	log.segment.bytes = 1073741824
broker                             | 	log.segment.delete.delay.ms = 60000
broker                             | 	max.connection.creation.rate = 2147483647
broker                             | 	max.connections = 2147483647
broker                             | 	max.connections.per.ip = 2147483647
broker                             | 	max.connections.per.ip.overrides = 
broker                             | 	max.incremental.fetch.session.cache.slots = 1000
broker                             | 	message.max.bytes = 1048588
broker                             | 	metadata.log.dir = null
broker                             | 	metadata.log.max.record.bytes.between.snapshots = 20971520
broker                             | 	metadata.log.max.snapshot.interval.ms = 3600000
broker                             | 	metadata.log.segment.bytes = 1073741824
broker                             | 	metadata.log.segment.min.bytes = 8388608
broker                             | 	metadata.log.segment.ms = 604800000
broker                             | 	metadata.max.idle.interval.ms = 500
broker                             | 	metadata.max.retention.bytes = 104857600
broker                             | 	metadata.max.retention.ms = 604800000
broker                             | 	metric.reporters = []
broker                             | 	metrics.num.samples = 2
broker                             | 	metrics.recording.level = INFO
broker                             | 	metrics.sample.window.ms = 30000
broker                             | 	min.insync.replicas = 1
broker                             | 	node.id = 1
broker                             | 	num.io.threads = 8
broker                             | 	num.network.threads = 3
broker                             | 	num.partitions = 1
broker                             | 	num.recovery.threads.per.data.dir = 1
broker                             | 	num.replica.alter.log.dirs.threads = null
broker                             | 	num.replica.fetchers = 1
broker                             | 	offset.metadata.max.bytes = 4096
broker                             | 	offsets.commit.required.acks = -1
broker                             | 	offsets.commit.timeout.ms = 5000
broker                             | 	offsets.load.buffer.size = 5242880
broker                             | 	offsets.retention.check.interval.ms = 600000
broker                             | 	offsets.retention.minutes = 10080
broker                             | 	offsets.topic.compression.codec = 0
broker                             | 	offsets.topic.num.partitions = 50
broker                             | 	offsets.topic.replication.factor = 1
broker                             | 	offsets.topic.segment.bytes = 104857600
broker                             | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
broker                             | 	password.encoder.iterations = 4096
broker                             | 	password.encoder.key.length = 128
broker                             | 	password.encoder.keyfactory.algorithm = null
broker                             | 	password.encoder.old.secret = null
broker                             | 	password.encoder.secret = null
broker                             | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
broker                             | 	process.roles = [broker, controller]
broker                             | 	producer.id.expiration.check.interval.ms = 600000
broker                             | 	producer.id.expiration.ms = 86400000
broker                             | 	producer.purgatory.purge.interval.requests = 1000
broker                             | 	queued.max.request.bytes = -1
broker                             | 	queued.max.requests = 500
broker                             | 	quota.window.num = 11
broker                             | 	quota.window.size.seconds = 1
broker                             | 	remote.log.index.file.cache.total.size.bytes = 1073741824
broker                             | 	remote.log.manager.task.interval.ms = 30000
broker                             | 	remote.log.manager.task.retry.backoff.max.ms = 30000
broker                             | 	remote.log.manager.task.retry.backoff.ms = 500
broker                             | 	remote.log.manager.task.retry.jitter = 0.2
broker                             | 	remote.log.manager.thread.pool.size = 10
broker                             | 	remote.log.metadata.manager.class.name = null
broker                             | 	remote.log.metadata.manager.class.path = null
broker                             | 	remote.log.metadata.manager.impl.prefix = null
broker                             | 	remote.log.metadata.manager.listener.name = null
broker                             | 	remote.log.reader.max.pending.tasks = 100
broker                             | 	remote.log.reader.threads = 10
broker                             | 	remote.log.storage.manager.class.name = null
broker                             | 	remote.log.storage.manager.class.path = null
broker                             | 	remote.log.storage.manager.impl.prefix = null
broker                             | 	remote.log.storage.system.enable = false
broker                             | 	replica.fetch.backoff.ms = 1000
broker                             | 	replica.fetch.max.bytes = 1048576
broker                             | 	replica.fetch.min.bytes = 1
broker                             | 	replica.fetch.response.max.bytes = 10485760
broker                             | 	replica.fetch.wait.max.ms = 500
broker                             | 	replica.high.watermark.checkpoint.interval.ms = 5000
broker                             | 	replica.lag.time.max.ms = 30000
broker                             | 	replica.selector.class = null
broker                             | 	replica.socket.receive.buffer.bytes = 65536
broker                             | 	replica.socket.timeout.ms = 30000
broker                             | 	replication.quota.window.num = 11
broker                             | 	replication.quota.window.size.seconds = 1
broker                             | 	request.timeout.ms = 30000
broker                             | 	reserved.broker.max.id = 1000
broker                             | 	sasl.client.callback.handler.class = null
broker                             | 	sasl.enabled.mechanisms = [GSSAPI]
broker                             | 	sasl.jaas.config = null
broker                             | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
broker                             | 	sasl.kerberos.min.time.before.relogin = 60000
broker                             | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
broker                             | 	sasl.kerberos.service.name = null
broker                             | 	sasl.kerberos.ticket.renew.jitter = 0.05
broker                             | 	sasl.kerberos.ticket.renew.window.factor = 0.8
broker                             | 	sasl.login.callback.handler.class = null
broker                             | 	sasl.login.class = null
broker                             | 	sasl.login.connect.timeout.ms = null
broker                             | 	sasl.login.read.timeout.ms = null
broker                             | 	sasl.login.refresh.buffer.seconds = 300
broker                             | 	sasl.login.refresh.min.period.seconds = 60
broker                             | 	sasl.login.refresh.window.factor = 0.8
broker                             | 	sasl.login.refresh.window.jitter = 0.05
broker                             | 	sasl.login.retry.backoff.max.ms = 10000
broker                             | 	sasl.login.retry.backoff.ms = 100
broker                             | 	sasl.mechanism.controller.protocol = GSSAPI
broker                             | 	sasl.mechanism.inter.broker.protocol = GSSAPI
broker                             | 	sasl.oauthbearer.clock.skew.seconds = 30
broker                             | 	sasl.oauthbearer.expected.audience = null
broker                             | 	sasl.oauthbearer.expected.issuer = null
broker                             | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
broker                             | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
broker                             | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
broker                             | 	sasl.oauth
broker                             | bearer.jwks.endpoint.url = null
broker                             | 	sasl.oauthbearer.scope.claim.name = scope
broker                             | 	sasl.oauthbearer.sub.claim.name = sub
broker                             | 	sasl.oauthbearer.token.endpoint.url = null
broker                             | 	sasl.server.callback.handler.class = null
broker                             | 	sasl.server.max.receive.size = 524288
broker                             | 	security.inter.broker.protocol = PLAINTEXT
broker                             | 	security.providers = null
broker                             | 	socket.connection.setup.timeout.max.ms = 30000
broker                             | 	socket.connection.setup.timeout.ms = 10000
broker                             | 	socket.listen.backlog.size = 50
broker                             | 	socket.receive.buffer.bytes = 102400
broker                             | 	socket.request.max.bytes = 104857600
broker                             | 	socket.send.buffer.bytes = 102400
broker                             | 	ssl.cipher.suites = []
broker                             | 	ssl.client.auth = none
broker                             | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
broker                             | 	ssl.endpoint.identification.algorithm = https
broker                             | 	ssl.engine.factory.class = null
broker                             | 	ssl.key.password = null
broker                             | 	ssl.keymanager.algorithm = SunX509
broker                             | 	ssl.keystore.certificate.chain = null
broker                             | 	ssl.keystore.key = null
broker                             | 	ssl.keystore.location = null
broker                             | 	ssl.keystore.password = null
broker                             | 	ssl.keystore.type = JKS
broker                             | 	ssl.principal.mapping.rules = DEFAULT
broker                             | 	ssl.protocol = TLSv1.3
broker                             | 	ssl.provider = null
broker                             | 	ssl.secure.random.implementation = null
broker                             | 	ssl.trustmanager.algorithm = PKIX
broker                             | 	ssl.truststore.certificates = null
broker                             | 	ssl.truststore.location = null
broker                             | 	ssl.truststore.password = null
broker                             | 	ssl.truststore.type = JKS
broker                             | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
broker                             | 	transaction.max.timeout.ms = 900000
broker                             | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
broker                             | 	transaction.state.log.load.buffer.size = 5242880
broker                             | 	transaction.state.log.min.isr = 1
broker                             | 	transaction.state.log.num.partitions = 50
broker                             | 	transaction.state.log.replication.factor = 1
broker                             | 	transaction.state.log.segment.bytes = 104857600
broker                             | 	transactional.id.expiration.ms = 604800000
broker                             | 	unclean.leader.election.enable = false
broker                             | 	zookeeper.clientCnxnSocket = null
broker                             | 	zookeeper.connect = 
broker                             | 	zookeeper.connection.timeout.ms = null
broker                             | 	zookeeper.max.in.flight.requests = 10
broker                             | 	zookeeper.metadata.migration.enable = false
broker                             | 	zookeeper.session.timeout.ms = 18000
broker                             | 	zookeeper.set.acl = false
broker                             | 	zookeeper.ssl.cipher.suites = null
broker                             | 	zookeeper.ssl.client.enable = false
broker                             | 	zookeeper.ssl.crl.enable = false
broker                             | 	zookeeper.ssl.enabled.protocols = null
broker                             | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
broker                             | 	zookeeper.ssl.keystore.location = null
broker                             | 	zookeeper.ssl.keystore.password = null
broker                             | 	zookeeper.ssl.keystore.type = null
broker                             | 	zookeeper.ssl.ocsp.enable = false
broker                             | 	zookeeper.ssl.protocol = TLSv1.2
broker                             | 	zookeeper.ssl.truststore.location = null
broker                             | 	zookeeper.ssl.truststore.password = null
broker                             | 	zookeeper.ssl.truststore.type = null
broker                             |  (kafka.server.KafkaConfig)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,458 INFO  akka.actor.CoordinatedShutdown                               [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,461 INFO  akka.actor.CoordinatedShutdown                               [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
broker                             | [2023-08-04 10:10:10,540] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,544 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,558 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,568 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,570 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
broker                             | [2023-08-04 10:10:10,614] INFO [Controller 1] The request from broker 1 to unfence has been granted because it has caught up with the offset of it's register broker record 10. (org.apache.kafka.controller.BrokerHeartbeatManager)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,679 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,701 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
data-agrigator-jobmanager-1        | 2023-08-04 10:10:10,769 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
broker                             | [2023-08-04 10:10:10,805] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
broker                             | [2023-08-04 10:10:10,807] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)
broker                             | [2023-08-04 10:10:10,811] INFO Kafka version: 7.4.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
broker                             | [2023-08-04 10:10:10,816] INFO Kafka commitId: fed9c006bfc7ba5bf7d2dee840e041d1a851d903 (org.apache.kafka.common.utils.AppInfoParser)
broker                             | [2023-08-04 10:10:10,817] INFO Kafka startTimeMs: 1691143810808 (org.apache.kafka.common.utils.AppInfoParser)
broker                             | [2023-08-04 10:10:10,879] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)
data-agrigator-jobmanager-1        | 2023-08-04 10:10:11,001 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
connect                            | 13269 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Received API_VERSIONS response from node -1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=adminclient-1, correlationId=0): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=55, minVersion=0, maxVersion=1), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=64, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[SupportedFeatureKey(name='metadata.version', minVersion=1, maxVersion=8)], finalizedFeaturesEpoch=13, finalizedFeatures=[FinalizedFeatureKey(name='metadata.version', maxVersionLevel=8, minVersionLevel=8)])
data-agrigator-jobmanager-1        | 2023-08-04 10:10:11,005 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Terminating cluster entrypoint process StandaloneApplicationClusterEntryPoint with exit code 1443.
connect                            | 13825 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 has finalized features epoch: 13, finalized features: [FinalizedFeatureKey(name='metadata.version', maxVersionLevel=8, minVersionLevel=8)], supported features: [SupportedFeatureKey(name='metadata.version', minVersion=1, maxVersion=8)], API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): UNSUPPORTED, StopReplica(5): UNSUPPORTED, UpdateMetadata(6): UNSUPPORTED, ControlledShutdown(7): UNSUPPORTED, OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 7], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 4], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 2], CreateAcls(30): 0 to 3 [usable: 2], DeleteAcls(31): 0 to 3 [usable: 2], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 2], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): UNSUPPORTED, RenewDelegationToken(39): UNSUPPORTED, ExpireDelegationToken(40): UNSUPPORTED, DescribeDelegationToken(41): UNSUPPORTED, DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): UNSUPPORTED, AlterUserScramCredentials(51): UNSUPPORTED, DescribeQuorum(55): 0 to 1 [usable: 0], AlterIsr(56): UNSUPPORTED, UpdateFeatures(57): 0 to 1 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): 0 [usable: 0], DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): UNSUPPORTED).
connect                            | 13830 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Sending MetadataRequestData(topics=[], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to broker:29092 (id: -1 rack: null). correlationId=1, timeoutMs=20068
connect                            | 13834 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=adminclient-1, correlationId=1) and timeout 20068 to node -1: MetadataRequestData(topics=[], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
connect                            | 14012 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Received METADATA response from node -1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=adminclient-1, correlationId=1): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='broker', port=29092, rack=null)], clusterId='MkU3OEVBNTcwNTJENDM2Qg', controllerId=1, topics=[], clusterAuthorizedOperations=-2147483648)
connect                            | 14046 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.admin.internals.AdminMetadataManager  - [AdminClient clientId=adminclient-1] Updating cluster metadata to Cluster(id = MkU3OEVBNTcwNTJENDM2Qg, nodes = [broker:29092 (id: 1 rack: null)], partitions = [], controller = broker:29092 (id: 1 rack: null))
connect                            | 14051 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host broker as 192.168.160.3
connect                            | 14052 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node broker:29092 (id: 1 rack: null) using address broker/192.168.160.3
connect                            | 14090 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node 1
connect                            | 14102 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Completed connection to node 1. Fetching API versions.
connect                            | 14102 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating API versions fetch from node 1.
connect                            | 14102 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=adminclient-1, correlationId=2) and timeout 3600000 to node 1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='7.1.0-ccs')
connect                            | 14132 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Received API_VERSIONS response from node 1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=adminclient-1, correlationId=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=55, minVersion=0, maxVersion=1), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=64, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[SupportedFeatureKey(name='metadata.version', minVersion=1, maxVersion=8)], finalizedFeaturesEpoch=17, finalizedFeatures=[FinalizedFeatureKey(name='metadata.version', maxVersionLevel=8, minVersionLevel=8)])
connect                            | 14138 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node 1 has finalized features epoch: 17, finalized features: [FinalizedFeatureKey(name='metadata.version', maxVersionLevel=8, minVersionLevel=8)], supported features: [SupportedFeatureKey(name='metadata.version', minVersion=1, maxVersion=8)], API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): UNSUPPORTED, StopReplica(5): UNSUPPORTED, UpdateMetadata(6): UNSUPPORTED, ControlledShutdown(7): UNSUPPORTED, OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 7], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 4], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 2], CreateAcls(30): 0 to 3 [usable: 2], DeleteAcls(31): 0 to 3 [usable: 2], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 2], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): UNSUPPORTED, RenewDelegationToken(39): UNSUPPORTED, ExpireDelegationToken(40): UNSUPPORTED, DescribeDelegationToken(41): UNSUPPORTED, DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): UNSUPPORTED, AlterUserScramCredentials(51): UNSUPPORTED, DescribeQuorum(55): 0 to 1 [usable: 0], AlterIsr(56): UNSUPPORTED, UpdateFeatures(57): 0 to 1 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): 0 [usable: 0], DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): UNSUPPORTED).
connect                            | 14140 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Sending DescribeClusterRequestData(includeClusterAuthorizedOperations=false) to broker:29092 (id: 1 rack: null). correlationId=3, timeoutMs=29826
connect                            | 14140 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Sending DESCRIBE_CLUSTER request with header RequestHeader(apiKey=DESCRIBE_CLUSTER, apiVersion=0, clientId=adminclient-1, correlationId=3) and timeout 29826 to node 1: DescribeClusterRequestData(includeClusterAuthorizedOperations=false)
connect                            | 14165 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Received DESCRIBE_CLUSTER response from node 1 for request with header RequestHeader(apiKey=DESCRIBE_CLUSTER, apiVersion=0, clientId=adminclient-1, correlationId=3): DescribeClusterResponseData(throttleTimeMs=0, errorCode=0, errorMessage=null, clusterId='MkU3OEVBNTcwNTJENDM2Qg', controllerId=1, brokers=[DescribeClusterBroker(brokerId=1, host='broker', port=29092, rack=null)], clusterAuthorizedOperations=-2147483648)
connect                            | 14178 [main] DEBUG io.confluent.admin.utils.ClusterStatus  - Broker list: [broker:29092 (id: 1 rack: null)]
data-agrigator-jobmanager-1 exited with code 163
control-center                     | Using log4j config /etc/cp-base-new/log4j.properties
ksqldb-server                      | Using log4j config /etc/ksqldb-server/log4j.properties
schema-registry                    | Using log4j config /etc/schema-registry/log4j.properties
rest-proxy                         | Using log4j config /etc/kafka-rest/log4j.properties
ksqldb-server                      | ===> Launching ... 
ksqldb-server                      | ===> Launching ksqldb-server ... 
connect                            | ===> Launching ... 
control-center                     | ===> Launching ... 
schema-registry                    | ===> Launching ... 
control-center                     | ===> Launching control-center ... 
schema-registry                    | ===> Launching schema-registry ... 
connect                            | ===> Launching kafka-connect ... 
rest-proxy                         | ===> Launching ... 
rest-proxy                         | ===> Launching kafka-rest ... 
control-center                     | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
rest-proxy                         | [2023-08-04 10:10:24,181] INFO KafkaRestConfig values: 
rest-proxy                         | 	access.control.allow.headers = 
rest-proxy                         | 	access.control.allow.methods = 
rest-proxy                         | 	access.control.allow.origin = 
rest-proxy                         | 	access.control.skip.options = true
rest-proxy                         | 	advertised.listeners = []
rest-proxy                         | 	api.endpoints.allowlist = []
rest-proxy                         | 	api.endpoints.blocklist = []
rest-proxy                         | 	api.v2.enable = true
rest-proxy                         | 	api.v3.enable = true
rest-proxy                         | 	api.v3.produce.rate.limit.cache.expiry.ms = 3600000
rest-proxy                         | 	api.v3.produce.rate.limit.enabled = false
rest-proxy                         | 	api.v3.produce.rate.limit.max.bytes.global.per.sec = 10000000
rest-proxy                         | 	api.v3.produce.rate.limit.max.bytes.per.sec = 10000000
rest-proxy                         | 	api.v3.produce.rate.limit.max.requests.global.per.sec = 10000
rest-proxy                         | 	api.v3.produce.rate.limit.max.requests.per.sec = 10000
rest-proxy                         | 	api.v3.produce.response.thread.pool.size = 4
rest-proxy                         | 	authentication.method = NONE
rest-proxy                         | 	authentication.realm = 
rest-proxy                         | 	authentication.roles = [*]
rest-proxy                         | 	authentication.skip.paths = []
rest-proxy                         | 	bootstrap.servers = broker:29092
rest-proxy                         | 	client.init.timeout.ms = 60000
rest-proxy                         | 	client.sasl.kerberos.kinit.cmd = /usr/bin/kinit
rest-proxy                         | 	client.sasl.kerberos.min.time.before.relogin = 60000
rest-proxy                         | 	client.sasl.kerberos.service.name = 
rest-proxy                         | 	client.sasl.kerberos.ticket.renew.jitter = 0.05
rest-proxy                         | 	client.sasl.kerberos.ticket.renew.window.factor = 0.8
rest-proxy                         | 	client.sasl.mechanism = GSSAPI
rest-proxy                         | 	client.security.protocol = PLAINTEXT
rest-proxy                         | 	client.ssl.cipher.suites = 
rest-proxy                         | 	client.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
rest-proxy                         | 	client.ssl.endpoint.identification.algorithm = 
rest-proxy                         | 	client.ssl.key.password = [hidden]
rest-proxy                         | 	client.ssl.keymanager.algorithm = SunX509
rest-proxy                         | 	client.ssl.keystore.location = 
rest-proxy                         | 	client.ssl.keystore.password = [hidden]
rest-proxy                         | 	client.ssl.keystore.type = JKS
rest-proxy                         | 	client.ssl.protocol = TLS
rest-proxy                         | 	client.ssl.provider = 
rest-proxy                         | 	client.ssl.trustmanager.algorithm = PKIX
rest-proxy                         | 	client.ssl.truststore.location = 
rest-proxy                         | 	client.ssl.truststore.password = [hidden]
rest-proxy                         | 	client.ssl.truststore.type = JKS
rest-proxy                         | 	client.timeout.ms = 500
rest-proxy                         | 	client.zk.session.timeout.ms = 30000
rest-proxy                         | 	compression.enable = true
rest-proxy                         | 	confluent.resource.name.authority = 
rest-proxy                         | 	connector.connection.limit = 0
rest-proxy                         | 	consumer.instance.timeout.ms = 300000
rest-proxy                         | 	consumer.iterator.backoff.ms = 50
rest-proxy                         | 	consumer.iterator.timeout.ms = 1
rest-proxy                         | 	consumer.request.max.bytes = 67108864
rest-proxy                         | 	consumer.request.timeout.ms = 1000
rest-proxy                         | 	consumer.threads = 50
rest-proxy                         | 	csrf.prevention.enable = false
rest-proxy                         | 	csrf.prevention.token.endpoint = /csrf
rest-proxy                         | 	csrf.prevention.token.expiration.minutes = 30
rest-proxy                         | 	csrf.prevention.token.max.entries = 10000
rest-proxy                         | 	debug = false
rest-proxy                         | 	dos.filter.delay.ms = 100
rest-proxy                         | 	dos.filter.enabled = false
rest-proxy                         | 	dos.filter.insert.headers = true
rest-proxy                         | 	dos.filter.ip.whitelist = []
rest-proxy                         | 	dos.filter.managed.attr = false
rest-proxy                         | 	dos.filter.max.idle.tracker.ms = 30000
rest-proxy                         | 	dos.filter.max.requests.ms = 30000
rest-proxy                         | 	dos.filter.max.requests.per.connection.per.sec = 25
rest-proxy                         | 	dos.filter.max.requests.per.sec = 25
rest-proxy                         | 	dos.filter.max.wait.ms = 50
rest-proxy                         | 	dos.filter.throttle.ms = 30000
rest-proxy                         | 	dos.filter.throttled.requests = 5
rest-proxy                         | 	fetch.min.bytes = -1
rest-proxy                         | 	host.name = rest-proxy
rest-proxy                         | 	http2.enabled = true
rest-proxy                         | 	id = 
rest-proxy                         | 	idle.timeout.ms = 30000
rest-proxy                         | 	kafka.rest.resource.extension.class = []
rest-proxy                         | 	listener.protocol.map = []
rest-proxy                         | 	listeners = [http://0.0.0.0:8082]
rest-proxy                         | 	metric.reporters = []
rest-proxy                         | 	metrics.jmx.prefix = kafka.rest
rest-proxy                         | 	metrics.num.samples = 2
rest-proxy                         | 	metrics.sample.window.ms = 30000
rest-proxy                         | 	metrics.tag.map = []
rest-proxy                         | 	nosniff.prevention.enable = false
rest-proxy                         | 	port = 8082
rest-proxy                         | 	producer.threads = 5
rest-proxy                         | 	proxy.protocol.enabled = false
rest-proxy                         | 	rate.limit.backend = guava
rest-proxy                         | 	rate.limit.costs = 
rest-proxy                         | 	rate.limit.default.cost = 1
rest-proxy                         | 	rate.limit.enable = false
rest-proxy                         | 	rate.limit.per.cluster.cache.expiry.ms = 3600000
rest-proxy                         | 	rate.limit.per.cluster.permits.per.sec = 50
rest-proxy                         | 	rate.limit.permits.per.sec = 50
rest-proxy                         | 	rate.limit.timeout.ms = 0
rest-proxy                         | 	reject.options.request = false
rest-proxy                         | 	request.logger.name = io.confluent.rest-utils.requests
rest-proxy                         | 	request.queue.capacity = 2147483647
rest-proxy                         | 	request.queue.capacity.growby = 64
rest-proxy                         | 	request.queue.capacity.init = 128
rest-proxy                         | 	resource.extension.classes = []
rest-proxy                         | 	response.http.headers.config = 
rest-proxy                         | 	response.mediatype.default = application/json
rest-proxy                         | 	response.mediatype.preferred = [application/json, application/vnd.kafka.v2+json]
rest-proxy                         | 	rest.servlet.initializor.classes = []
rest-proxy                         | 	schema.registry.url = http://schema-registry:8081
rest-proxy                         | 	server.connection.limit = 0
rest-proxy                         | 	shutdown.graceful.ms = 1000
rest-proxy                         | 	simpleconsumer.pool.size.max = 25
rest-proxy                         | 	simpleconsumer.pool.timeout.ms = 1000
rest-proxy                         | 	ssl.cipher.suites = []
rest-proxy                         | 	ssl.client.auth = false
rest-proxy                         | 	ssl.client.authentication = NONE
rest-proxy                         | 	ssl.enabled.protocols = []
rest-proxy                         | 	ssl.endpoint.identification.algorithm = null
rest-proxy                         | 	ssl.key.password = [hidden]
rest-proxy                         | 	ssl.keymanager.algorithm = 
rest-proxy                         | 	ssl.keystore.location = 
rest-proxy                         | 	ssl.keystore.password = [hidden]
rest-proxy                         | 	ssl.keystore.reload = false
rest-proxy                         | 	ssl.keystore.type = JKS
rest-proxy                         | 	ssl.keystore.watch.location = 
rest-proxy                         | 	ssl.protocol = TLS
rest-proxy                         | 	ssl.provider = 
rest-proxy                         | 	ssl.trustmanager.algorithm = 
rest-proxy                         | 	ssl.truststore.location = 
rest-proxy                         | 	ssl.truststore.password = [hidden]
rest-proxy                         | 	ssl.truststore.type = JKS
rest-proxy                         | 	streaming.connection.max.duration.grace.period.ms = 500
rest-proxy                         | 	streaming.connection.max.duration.ms = 86400000
rest-proxy                         | 	suppress.stack.trace.response = true
rest-proxy                         | 	thread.pool.max = 200
rest-proxy                         | 	thread.pool.min = 8
rest-proxy                         | 	websocket.path.prefix = /ws
rest-proxy                         | 	websocket.servlet.initializor.classes = []
rest-proxy                         | 	zookeeper.connect = 
rest-proxy                         |  (io.confluent.kafkarest.KafkaRestConfig)
rest-proxy                         | [2023-08-04 10:10:25,976] INFO Logging initialized @12644ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
schema-registry                    | [2023-08-04 10:10:26,843] INFO SchemaRegistryConfig values: 
schema-registry                    | 	access.control.allow.headers = 
schema-registry                    | 	access.control.allow.methods = 
schema-registry                    | 	access.control.allow.origin = 
schema-registry                    | 	access.control.skip.options = true
schema-registry                    | 	authentication.method = NONE
schema-registry                    | 	authentication.realm = 
schema-registry                    | 	authentication.roles = [*]
schema-registry                    | 	authentication.skip.paths = []
schema-registry                    | 	avro.compatibility.level = 
schema-registry                    | 	compression.enable = true
schema-registry                    | 	connector.connection.limit = 0
schema-registry                    | 	csrf.prevention.enable = false
schema-registry                    | 	csrf.prevention.token.endpoint = /csrf
schema-registry                    | 	csrf.prevention.token.expiration.minutes = 30
schema-registry                    | 	csrf.prevention.token.max.entries = 10000
schema-registry                    | 	debug = false
schema-registry                    | 	dos.filter.delay.ms = 100
schema-registry                    | 	dos.filter.enabled = false
schema-registry                    | 	dos.filter.insert.headers = true
schema-registry                    | 	dos.filter.ip.whitelist = []
schema-registry                    | 	dos.filter.managed.attr = false
schema-registry                    | 	dos.filter.max.idle.tracker.ms = 30000
schema-registry                    | 	dos.filter.max.requests.ms = 30000
schema-registry                    | 	dos.filter.max.requests.per.connection.per.sec = 25
schema-registry                    | 	dos.filter.max.requests.per.sec = 25
schema-registry                    | 	dos.filter.max.wait.ms = 50
schema-registry                    | 	dos.filter.throttle.ms = 30000
schema-registry                    | 	dos.filter.throttled.requests = 5
schema-registry                    | 	host.name = schema-registry
schema-registry                    | 	http2.enabled = true
schema-registry                    | 	idle.timeout.ms = 30000
schema-registry                    | 	inter.instance.headers.whitelist = []
schema-registry                    | 	inter.instance.listener.name = 
schema-registry                    | 	inter.instance.protocol = http
schema-registry                    | 	kafkagroup.heartbeat.interval.ms = 3000
schema-registry                    | 	kafkagroup.rebalance.timeout.ms = 300000
schema-registry                    | 	kafkagroup.session.timeout.ms = 10000
schema-registry                    | 	kafkastore.bootstrap.servers = [broker:29092]
schema-registry                    | 	kafkastore.checkpoint.dir = /tmp
schema-registry                    | 	kafkastore.checkpoint.version = 0
schema-registry                    | 	kafkastore.connection.url = 
schema-registry                    | 	kafkastore.group.id = 
schema-registry                    | 	kafkastore.init.timeout.ms = 60000
schema-registry                    | 	kafkastore.sasl.kerberos.kinit.cmd = /usr/bin/kinit
schema-registry                    | 	kafkastore.sasl.kerberos.min.time.before.relogin = 60000
schema-registry                    | 	kafkastore.sasl.kerberos.service.name = 
schema-registry                    | 	kafkastore.sasl.kerberos.ticket.renew.jitter = 0.05
schema-registry                    | 	kafkastore.sasl.kerberos.ticket.renew.window.factor = 0.8
schema-registry                    | 	kafkastore.sasl.mechanism = GSSAPI
schema-registry                    | 	kafkastore.security.protocol = PLAINTEXT
schema-registry                    | 	kafkastore.ssl.cipher.suites = 
schema-registry                    | 	kafkastore.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
schema-registry                    | 	kafkastore.ssl.endpoint.identification.algorithm = 
schema-registry                    | 	kafkastore.ssl.key.password = [hidden]
schema-registry                    | 	kafkastore.ssl.keymanager.algorithm = SunX509
schema-registry                    | 	kafkastore.ssl.keystore.location = 
schema-registry                    | 	kafkastore.ssl.keystore.password = [hidden]
schema-registry                    | 	kafkastore.ssl.keystore.type = JKS
schema-registry                    | 	kafkastore.ssl.protocol = TLS
schema-registry                    | 	kafkastore.ssl.provider = 
schema-registry                    | 	kafkastore.ssl.trustmanager.algorithm = PKIX
schema-registry                    | 	kafkastore.ssl.truststore.location = 
schema-registry                    | 	kafkastore.ssl.truststore.password = [hidden]
schema-registry                    | 	kafkastore.ssl.truststore.type = JKS
schema-registry                    | 	kafkastore.timeout.ms = 500
schema-registry                    | 	kafkastore.topic = _schemas
schema-registry                    | 	kafkastore.topic.replication.factor = 3
schema-registry                    | 	kafkastore.topic.skip.validation = false
schema-registry                    | 	kafkastore.update.handlers = []
schema-registry                    | 	kafkastore.write.max.retries = 5
schema-registry                    | 	leader.connect.timeout.ms = 60000
schema-registry                    | 	leader.election.delay = false
schema-registry                    | 	leader.eligibility = true
schema-registry                    | 	leader.read.timeout.ms = 60000
schema-registry                    | 	listener.protocol.map = []
schema-registry                    | 	listeners = [http://0.0.0.0:8081]
schema-registry                    | 	master.eligibility = null
schema-registry                    | 	metadata.encoder.old.secret = null
schema-registry                    | 	metadata.encoder.secret = null
schema-registry                    | 	metadata.encoder.topic = _schema_encoders
schema-registry                    | 	metric.reporters = []
schema-registry                    | 	metrics.jmx.prefix = kafka.schema.registry
schema-registry                    | 	metrics.num.samples = 2
schema-registry                    | 	metrics.sample.window.ms = 30000
schema-registry                    | 	metrics.tag.map = []
schema-registry                    | 	mode.mutability = true
schema-registry                    | 	nosniff.prevention.enable = false
schema-registry                    | 	port = 8081
schema-registry                    | 	proxy.protocol.enabled = false
schema-registry                    | 	reject.options.request = false
schema-registry                    | 	request.logger.name = io.confluent.rest-utils.requests
schema-registry                    | 	request.queue.capacity = 2147483647
schema-registry                    | 	request.queue.capacity.growby = 64
schema-registry                    | 	request.queue.capacity.init = 128
schema-registry                    | 	resource.extension.class = []
schema-registry                    | 	resource.extension.classes = []
schema-registry                    | 	resource.static.locations = []
schema-registry                    | 	response.http.headers.config = 
schema-registry                    | 	response.mediatype.default = application/vnd.schemaregistry.v1+json
schema-registry                    | 	response.mediatype.preferred = [application/vnd.schemaregistry.v1+json, application/vnd.schemaregistry+json, application/json]
schema-registry                    | 	rest.servlet.initializor.classes = []
schema-registry                    | 	schema.cache.expiry.secs = 300
schema-registry                    | 	schema.cache.size = 1000
schema-registry                    | 	schema.canonicalize.on.consume = []
schema-registry                    | 	schema.compatibility.level = backward
schema-registry                    | 	schema.providers = []
schema-registry                    | 	schema.registry.group.id = schema-registry
schema-registry                    | 	schema.registry.inter.instance.protocol = 
schema-registry                    | 	schema.registry.resource.extension.class = []
schema-registry                    | 	schema.search.default.limit = 1000
schema-registry                    | 	schema.search.max.limit = 1000
schema-registry                    | 	server.connection.limit = 0
schema-registry                    | 	shutdown.graceful.ms = 1000
schema-registry                    | 	ssl.cipher.suites = []
schema-registry                    | 	ssl.client.auth = false
schema-registry                    | 	ssl.client.authentication = NONE
schema-registry                    | 	ssl.enabled.protocols = []
schema-registry                    | 	ssl.endpoint.identification.algorithm = null
schema-registry                    | 	ssl.key.password = [hidden]
schema-registry                    | 	ssl.keymanager.algorithm = 
schema-registry                    | 	ssl.keystore.location = 
schema-registry                    | 	ssl.keystore.password = [hidden]
schema-registry                    | 	ssl.keystore.reload = false
schema-registry                    | 	ssl.keystore.type = JKS
schema-registry                    | 	ssl.keystore.watch.location = 
schema-registry                    | 	ssl.protocol = TLS
schema-registry                    | 	ssl.provider = 
schema-registry                    | 	ssl.trustmanager.algorithm = 
schema-registry                    | 	ssl.truststore.location = 
schema-registry                    | 	ssl.truststore.password = [hidden]
schema-registry                    | 	ssl.truststore.type = JKS
schema-registry                    | 	suppress.stack.trace.response = true
schema-registry                    | 	thread.pool.max = 200
schema-registry                    | 	thread.pool.min = 8
schema-registry                    | 	websocket.path.prefix = /ws
schema-registry                    | 	websocket.servlet.initializor.classes = []
schema-registry                    |  (io.confluent.kafka.schemaregistry.rest.SchemaRegistryConfig)
rest-proxy                         | [2023-08-04 10:10:26,989] INFO Initial capacity 128, increased by 64, maximum capacity 2147483647. (io.confluent.rest.ApplicationServer)
schema-registry                    | [2023-08-04 10:10:28,179] INFO Logging initialized @14888ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
schema-registry                    | [2023-08-04 10:10:28,566] INFO Initial capacity 128, increased by 64, maximum capacity 2147483647. (io.confluent.rest.ApplicationServer)
connect                            | [2023-08-04 10:10:29,495] INFO WorkerInfo values: 
connect                            | 	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/var/log/kafka, -Dlog4j.configuration=file:/etc/kafka/connect-log4j.properties, -Dlog4j.config.dir=/etc/kafka
connect                            | 	jvm.spec = Azul Systems, Inc., OpenJDK 64-Bit Server VM, 11.0.14.1, 11.0.14.1+1-LTS
connect                            | 	jvm.classpath = /usr/share/java/monitoring-interceptors/monitoring-interceptors-7.4.1.jar:/usr/share/java/confluent-security/connect/tink-1.6.0.jar:/usr/share/java/confluent-security/connect/snakeyaml-1.27.jar:/usr/share/java/confluent-security/connect/aws-java-sdk-s3-1.11.988.jar:/usr/share/java/confluent-security/connect/error_prone_annotations-2.5.1.jar:/usr/share/java/confluent-security/connect/httpclient-4.5.13.jar:/usr/share/java/confluent-security/connect/kotlin-stdlib-jdk8-1.5.31.jar:/usr/share/java/confluent-security/connect/confluent-connect-secret-registry-plugin-7.1.0.jar:/usr/share/java/confluent-security/connect/commons-logging-1.2.jar:/usr/share/java/confluent-security/connect/jetty-client-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/jetty-server-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/netty-transport-classes-epoll-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/google-api-services-storage-v1-rev20210127-1.32.1.jar:/usr/share/java/confluent-security/connect/kafka-connect-json-schema-converter-7.1.0.jar:/usr/share/java/confluent-security/connect/netty-transport-sctp-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/scala-reflect-2.13.5.jar:/usr/share/java/confluent-security/connect/netty-codec-mqtt-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/netty-transport-rxtx-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/google-cloud-core-http-2.1.3.jar:/usr/share/java/confluent-security/connect/zookeeper-3.6.3.jar:/usr/share/java/confluent-security/connect/jackson-datatype-jdk8-2.12.3.jar:/usr/share/java/confluent-security/connect/common-utils-7.1.0.jar:/usr/share/java/confluent-security/connect/google-http-client-apache-v2-1.40.0.jar:/usr/share/java/confluent-security/connect/kotlin-scripting-compiler-embeddable-1.3.50.jar:/usr/share/java/confluent-security/connect/bcpg-jdk15on-1.68.jar:/usr/share/java/confluent-security/connect/javax.ws.rs-api-2.1.1.jar:/usr/share/java/confluent-security/connect/asm-tree-9.2.jar:/usr/share/java/confluent-security/connect/asm-analysis-9.2.jar:/usr/share/java/confluent-security/connect/google-http-client-1.40.0.jar:/usr/share/java/confluent-security/connect/google-auth-library-credentials-1.1.0.jar:/usr/share/java/confluent-security/connect/security-extensions-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/netty-resolver-dns-native-macos-4.1.73.Final-osx-aarch_64.jar:/usr/share/java/confluent-security/connect/netty-handler-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/netty-codec-memcache-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/jersey-container-servlet-2.34.jar:/usr/share/java/confluent-security/connect/kotlin-scripting-jvm-1.4.21.jar:/usr/share/java/confluent-security/connect/netty-codec-http-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/common-config-7.1.0.jar:/usr/share/java/confluent-security/connect/checker-qual-3.8.0.jar:/usr/share/java/confluent-security/connect/netty-resolver-dns-classes-macos-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/jetty-alpn-java-server-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/netty-tcnative-boringssl-static-2.0.46.Final.jar:/usr/share/java/confluent-security/connect/jetty-jndi-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/annotations-13.0.jar:/usr/share/java/confluent-security/connect/jetty-servlets-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/kafka-avro-serializer-7.1.0.jar:/usr/share/java/confluent-security/connect/metrics-core-2.2.0.jar:/usr/share/java/confluent-security/connect/jetty-xml-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/netty-all-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/commons-codec-1.13.jar:/usr/share/java/confluent-security/connect/netty-handler-proxy-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/commons-collections-3.2.2.jar:/usr/share/java/confluent-security/connect/netty-common-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/auto-service-1.0-rc7.jar:/usr/share/java/confluent-security/connect/jackson-dataformat-cbor-2.12.3.jar:/usr/share/java/confluent-security/connect/netty-codec-dns-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/jetty-webapp-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/threetenbp-1.5.1.jar:/usr/share/java/confluent-security/connect/jersey-common-2.34.jar:/usr/share/java/confluent-security/connect/google-oauth-client-1.32.1.jar:/usr/share/java/confluent-security/connect/api-common-2.0.2.jar:/usr/share/java/confluent-security/connect/jakarta.el-api-4.0.0.jar:/usr/share/java/confluent-security/connect/proto-google-common-protos-2.5.1.jar:/usr/share/java/confluent-security/connect/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/share/java/confluent-security/connect/javax-websocket-client-impl-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/okio-jvm-3.0.0.jar:/usr/share/java/confluent-security/connect/kotlin-scripting-common-1.4.21.jar:/usr/share/java/confluent-security/connect/kafka-json-serializer-7.1.0.jar:/usr/share/java/confluent-security/connect/kafka-schema-serializer-7.1.0.jar:/usr/share/java/confluent-security/connect/internal-rest-server-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/kafka-raft-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/aopalliance-repackaged-2.6.1.jar:/usr/share/java/confluent-security/connect/scala-collection-compat_2.13-2.4.4.jar:/usr/share/java/confluent-security/connect/hk2-locator-2.6.1.jar:/usr/share/java/confluent-security/connect/scala-library-2.13.5.jar:/usr/share/java/confluent-security/connect/javax.json-1.0.4.jar:/usr/share/java/confluent-security/connect/google-auth-library-oauth2-http-1.1.0.jar:/usr/share/java/confluent-security/connect/jakarta.validation-api-2.0.2.jar:/usr/share/java/confluent-security/connect/bcprov-jdk15on-1.68.jar:/usr/share/java/confluent-security/connect/confluent-security-plugins-common-7.1.0.jar:/usr/share/java/confluent-security/connect/j2objc-annotations-1.3.jar:/usr/share/java/confluent-security/connect/jackson-jaxrs-base-2.12.3.jar:/usr/share/java/confluent-security/connect/netty-codec-stomp-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/json-20201115.jar:/usr/share/java/confluent-security/connect/ion-java-1.0.2.jar:/usr/share/java/confluent-security/connect/google-api-services-cloudkms-v1-rev108-1.25.0.jar:/usr/share/java/confluent-security/connect/jackson-dataformat-yaml-2.12.3.jar:/usr/share/java/confluent-security/connect/websocket-client-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/antlr-runtime-3.5.2.jar:/usr/share/java/confluent-security/connect/netty-codec-xml-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/jetty-alpn-server-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/activation-1.1.1.jar:/usr/share/java/confluent-security/connect/org.everit.json.schema-1.12.2.jar:/usr/share/java/confluent-security/connect/jakarta.ws.rs-api-2.1.6.jar:/usr/share/java/confluent-security/connect/netty-transport-udt-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/jetty-util-ajax-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/google-http-client-gson-1.40.0.jar:/usr/share/java/confluent-security/connect/paranamer-2.8.jar:/usr/share/java/confluent-security/connect/kafka-connect-avro-converter-7.1.0.jar:/usr/share/java/confluent-security/connect/asm-9.2.jar:/usr/share/java/confluent-security/connect/jmespath-java-1.11.988.jar:/usr/share/java/co
connect                            | nfluent-security/connect/scala-java8-compat_2.13-1.0.0.jar:/usr/share/java/confluent-security/connect/kafka-metadata-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/jboss-logging-3.3.2.Final.jar:/usr/share/java/confluent-security/connect/commons-validator-1.6.jar:/usr/share/java/confluent-security/connect/jose4j-0.7.8.jar:/usr/share/java/confluent-security/connect/netty-buffer-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/joda-time-2.10.8.jar:/usr/share/java/confluent-security/connect/tink-gcpkms-1.6.0.jar:/usr/share/java/confluent-security/connect/kafka-json-schema-provider-7.1.0.jar:/usr/share/java/confluent-security/connect/jbcrypt-0.4.jar:/usr/share/java/confluent-security/connect/hibernate-validator-6.1.7.Final.jar:/usr/share/java/confluent-security/connect/jackson-datatype-guava-2.12.3.jar:/usr/share/java/confluent-security/connect/wire-runtime-jvm-4.0.0.jar:/usr/share/java/confluent-security/connect/jetty-util-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/auto-common-0.10.jar:/usr/share/java/confluent-security/connect/hk2-utils-2.6.1.jar:/usr/share/java/confluent-security/connect/confluent-connect-security-plugin-7.1.0.jar:/usr/share/java/confluent-security/connect/metrics-core-4.1.12.1.jar:/usr/share/java/confluent-security/connect/authorizer-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/opencensus-api-0.28.0.jar:/usr/share/java/confluent-security/connect/netty-transport-native-kqueue-4.1.73.Final-osx-aarch_64.jar:/usr/share/java/confluent-security/connect/jackson-module-jaxb-annotations-2.12.3.jar:/usr/share/java/confluent-security/connect/websocket-servlet-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/netty-codec-haproxy-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/wire-schema-jvm-4.0.0.jar:/usr/share/java/confluent-security/connect/bcpkix-jdk15on-1.68.jar:/usr/share/java/confluent-security/connect/jackson-dataformat-csv-2.12.3.jar:/usr/share/java/confluent-security/connect/commons-digester-1.8.1.jar:/usr/share/java/confluent-security/connect/netty-tcnative-classes-2.0.46.Final.jar:/usr/share/java/confluent-security/connect/aws-java-sdk-sts-1.11.988.jar:/usr/share/java/confluent-security/connect/aws-java-sdk-core-1.11.988.jar:/usr/share/java/confluent-security/connect/commons-cli-1.4.jar:/usr/share/java/confluent-security/connect/swagger-annotations-2.1.10.jar:/usr/share/java/confluent-security/connect/kafka-protobuf-types-7.1.0.jar:/usr/share/java/confluent-security/connect/failureaccess-1.0.1.jar:/usr/share/java/confluent-security/connect/jakarta.inject-2.6.1.jar:/usr/share/java/confluent-security/connect/kafka-protobuf-serializer-7.1.0.jar:/usr/share/java/confluent-security/connect/javassist-3.25.0-GA.jar:/usr/share/java/confluent-security/connect/netty-transport-native-unix-common-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/kafka-connect-protobuf-converter-7.1.0.jar:/usr/share/java/confluent-security/connect/jersey-container-servlet-core-2.34.jar:/usr/share/java/confluent-security/connect/confluent-serializers-new-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/jetty-io-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/rest-utils-7.1.0.jar:/usr/share/java/confluent-security/connect/javax-websocket-server-impl-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/kafka-schema-registry-client-7.1.0.jar:/usr/share/java/confluent-security/connect/netty-transport-classes-kqueue-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/kafka-server-common-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/kotlin-stdlib-1.4.21.jar:/usr/share/java/confluent-security/connect/confluent-licensing-new-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/websocket-api-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/argparse4j-0.7.0.jar:/usr/share/java/confluent-security/connect/netty-resolver-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/jopt-simple-5.0.4.jar:/usr/share/java/confluent-security/connect/netty-transport-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/kafka-protobuf-provider-7.1.0.jar:/usr/share/java/confluent-security/connect/audience-annotations-0.5.0.jar:/usr/share/java/confluent-security/connect/jackson-jaxrs-json-provider-2.12.3.jar:/usr/share/java/confluent-security/connect/icu4j-61.1.jar:/usr/share/java/confluent-security/connect/zookeeper-jute-3.6.3.jar:/usr/share/java/confluent-security/connect/websocket-server-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/scala-logging_2.13-3.9.3.jar:/usr/share/java/confluent-security/connect/gax-2.4.1.jar:/usr/share/java/confluent-security/connect/netty-codec-redis-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/commons-lang3-3.12.0.jar:/usr/share/java/confluent-security/connect/kafka-json-schema-serializer-7.1.0.jar:/usr/share/java/confluent-security/connect/kafka-connect-avro-data-7.1.0.jar:/usr/share/java/confluent-security/connect/javax.servlet-api-4.0.1.jar:/usr/share/java/confluent-security/connect/jersey-server-2.34.jar:/usr/share/java/confluent-security/connect/netty-codec-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/proto-google-iam-v1-1.1.0.jar:/usr/share/java/confluent-security/connect/websocket-common-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/rest-authorizer-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/netty-codec-socks-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/hk2-api-2.6.1.jar:/usr/share/java/confluent-security/connect/protobuf-java-util-3.17.3.jar:/usr/share/java/confluent-security/connect/jersey-bean-validation-2.34.jar:/usr/share/java/confluent-security/connect/aws-java-sdk-kms-1.11.988.jar:/usr/share/java/confluent-security/connect/jsr305-3.0.2.jar:/usr/share/java/confluent-security/connect/jetty-jaas-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/auto-service-annotations-1.0-rc7.jar:/usr/share/java/confluent-security/connect/kafka-storage-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/jetty-servlet-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/validation-api-2.0.1.Final.jar:/usr/share/java/confluent-security/connect/avro-1.11.0.jar:/usr/share/java/confluent-security/connect/kafka-secret-registry-client-7.1.0.jar:/usr/share/java/confluent-security/connect/flatbuffers-java-1.9.0.jar:/usr/share/java/confluent-security/connect/protobuf-java-3.17.3.jar:/usr/share/java/confluent-security/connect/jersey-hk2-2.34.jar:/usr/share/java/confluent-security/connect/jakarta.activation-api-1.2.1.jar:/usr/share/java/confluent-security/connect/asm-commons-9.2.jar:/usr/share/java/confluent-security/connect/jetty-continuation-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/javax.websocket-client-api-1.0.jar:/usr/share/java/confluent-security/connect/jersey-client-2.34.jar:/usr/share/java/confluent-security/connect/opencensus-contrib-http-util-0.28.0.jar:/usr/share/java/confluent-security/connect/kotlin-stdlib-jdk7-1.5.31.jar:/usr/share/java/confluent-security/connect/antlr4-4.9.2.jar:/usr/share/java/confluent-security/connect/jackson-annotations-2.12.3.jar:/usr/share/java/confluent-security/connect/kafka-storage-api-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/classmate-1.3.4.jar:/usr/share/java/confluent-security/connect/gson-2.8.6.jar:/usr/share/java/confluent-security/connect/kotlinx-coroutines-core-1.3.7.jar:/usr/share/java/confluent-security/connect/jakarta.el-3.0.3.jar:/usr/share/java/confluent-security/connect/netty-codec-http2-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/google-cloud-core-2.1.3.jar:/usr/share/java/confluent-security/connect/gax-httpjson-0.89.1.jar:/usr/share/java/confluent-security/connect/netty-resolver-dns-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/jakarta.annotation-api-1.3.5.jar:/usr/share/java/confluent-security/connect/jetty-http-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/kafka-secret-registry-7.1.0.jar:/usr/share/java/confluent-security/connect/ST4-4.3.jar:/usr/share/java/confluent-security/connect/org.abego.treelayout.core-1.0.3.jar:/usr/share/java/con
connect                            | fluent-security/connect/re2j-1.6.jar:/usr/share/java/confluent-security/connect/http2-server-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/jackson-module-parameter-names-2.12.3.jar:/usr/share/java/confluent-security/connect/http2-hpack-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/httpcore-4.4.13.jar:/usr/share/java/confluent-security/connect/google-api-client-1.32.1.jar:/usr/share/java/confluent-security/connect/antlr4-runtime-4.9.2.jar:/usr/share/java/confluent-security/connect/netty-transport-native-kqueue-4.1.73.Final-osx-x86_64.jar:/usr/share/java/confluent-security/connect/handy-uri-templates-2.1.8.jar:/usr/share/java/confluent-security/connect/jetty-jmx-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/http2-common-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/google-http-client-jackson2-1.40.0.jar:/usr/share/java/confluent-security/connect/guava-30.1.1-jre.jar:/usr/share/java/confluent-security/connect/netty-codec-smtp-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/grpc-context-1.40.1.jar:/usr/share/java/confluent-security/connect/google-http-client-appengine-1.40.0.jar:/usr/share/java/confluent-security/connect/jackson-core-2.12.3.jar:/usr/share/java/confluent-security/connect/kotlin-script-runtime-1.4.21.jar:/usr/share/java/confluent-security/connect/jetty-annotations-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/kafka-client-plugins-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/osgi-resource-locator-1.0.3.jar:/usr/share/java/confluent-security/connect/jackson-module-scala_2.13-2.12.3.jar:/usr/share/java/confluent-security/connect/jackson-datatype-jsr310-2.12.3.jar:/usr/share/java/confluent-security/connect/javax.websocket-api-1.0.jar:/usr/share/java/confluent-security/connect/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/usr/share/java/confluent-security/connect/jetty-plus-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/kafka_2.13-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/netty-resolver-dns-native-macos-4.1.73.Final-osx-x86_64.jar:/usr/share/java/confluent-security/connect/google-cloud-storage-2.1.2.jar:/usr/share/java/confluent-security/connect/jaxb-api-2.3.0.jar:/usr/share/java/confluent-security/connect/classgraph-4.8.21.jar:/usr/share/java/confluent-security/connect/commons-compress-1.21.jar:/usr/share/java/confluent-security/connect/auto-value-annotations-1.8.2.jar:/usr/share/java/confluent-security/connect/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/usr/share/java/confluent-security/connect/kotlin-stdlib-common-1.5.31.jar:/usr/share/java/confluent-security/connect/jetty-security-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/jackson-datatype-joda-2.12.3.jar:/usr/share/java/confluent-security/connect/javax.annotation-api-1.3.2.jar:/usr/share/java/confluent-security/connect/broker-plugins-7.1.0-ce-test.jar:/usr/share/java/confluent-security/connect/netty-transport-native-epoll-4.1.73.Final-linux-x86_64.jar:/usr/share/java/confluent-security/connect/jakarta.xml.bind-api-2.3.2.jar:/usr/share/java/confluent-security/connect/netty-transport-native-epoll-4.1.73.Final-linux-aarch_64.jar:/usr/share/java/kafka/tink-1.6.0.jar:/usr/share/java/kafka/snakeyaml-1.27.jar:/usr/share/java/kafka/aws-java-sdk-s3-1.11.988.jar:/usr/share/java/kafka/rbac-7.1.0-ce.jar:/usr/share/java/kafka/azure-storage-blob-12.12.0.jar:/usr/share/java/kafka/cloudevents-kafka-2.0.0.jar:/usr/share/java/kafka/httpclient-4.5.13.jar:/usr/share/java/kafka/kotlin-stdlib-jdk8-1.5.31.jar:/usr/share/java/kafka/connect-runtime-7.1.0-ce.jar:/usr/share/java/kafka/trogdor-7.1.0-ce.jar:/usr/share/java/kafka/commons-logging-1.2.jar:/usr/share/java/kafka/jetty-client-9.4.44.v20210927.jar:/usr/share/java/kafka/jetty-server-9.4.44.v20210927.jar:/usr/share/java/kafka/netty-transport-classes-epoll-4.1.73.Final.jar:/usr/share/java/kafka/google-api-services-storage-v1-rev20210127-1.32.1.jar:/usr/share/java/kafka/kafka-streams-7.1.0-ce.jar:/usr/share/java/kafka/netty-transport-sctp-4.1.73.Final.jar:/usr/share/java/kafka/KeePassJava2-dom-2.1.4.jar:/usr/share/java/kafka/reactor-netty-http-1.0.7.jar:/usr/share/java/kafka/confluent-log4j-1.2.17-cp10.jar:/usr/share/java/kafka/netty-codec-mqtt-4.1.73.Final.jar:/usr/share/java/kafka/netty-transport-rxtx-4.1.73.Final.jar:/usr/share/java/kafka/google-cloud-core-http-2.1.3.jar:/usr/share/java/kafka/zookeeper-3.6.3.jar:/usr/share/java/kafka/jackson-datatype-jdk8-2.12.3.jar:/usr/share/java/kafka/zipkin-reporter-brave-2.16.3.jar:/usr/share/java/kafka/google-http-client-apache-v2-1.40.0.jar:/usr/share/java/kafka/simpleclient_tracer_otel-0.12.0.jar:/usr/share/java/kafka/lz4-java-1.8.0.jar:/usr/share/java/kafka/scala-reflect-2.13.6.jar:/usr/share/java/kafka/kafka.jar:/usr/share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/share/java/kafka/core-1.54.0.0.jar:/usr/share/java/kafka/confluent-audit-7.1.0-ce.jar:/usr/share/java/kafka/google-http-client-1.40.0.jar:/usr/share/java/kafka/telemetry-client-1.745.0.jar:/usr/share/java/kafka/brave-instrumentation-http-5.13.3.jar:/usr/share/java/kafka/google-auth-library-credentials-1.1.0.jar:/usr/share/java/kafka/security-extensions-7.1.0-ce.jar:/usr/share/java/kafka/netty-resolver-dns-native-macos-4.1.73.Final-osx-aarch_64.jar:/usr/share/java/kafka/netty-handler-4.1.73.Final.jar:/usr/share/java/kafka/netty-codec-memcache-4.1.73.Final.jar:/usr/share/java/kafka/jersey-container-servlet-2.34.jar:/usr/share/java/kafka/netty-codec-http-4.1.73.Final.jar:/usr/share/java/kafka/snappy-java-1.1.8.4.jar:/usr/share/java/kafka/kafka-clients-7.1.0-ce.jar:/usr/share/java/kafka/checker-qual-3.8.0.jar:/usr/share/java/kafka/netty-resolver-dns-classes-macos-4.1.73.Final.jar:/usr/share/java/kafka/netty-transport-native-epoll-4.1.73.Final.jar:/usr/share/java/kafka/lang-tag-1.5.jar:/usr/share/java/kafka/netty-tcnative-boringssl-static-2.0.46.Final.jar:/usr/share/java/kafka/simpleclient_tracer_common-0.12.0.jar:/usr/share/java/kafka/snakeyaml-1.29.jar:/usr/share/java/kafka/annotations-13.0.jar:/usr/share/java/kafka/msal4j-1.10.1.jar:/usr/share/java/kafka/azure-identity-1.3.3.jar:/usr/share/java/kafka/jetty-servlets-9.4.44.v20210927.jar:/usr/share/java/kafka/jcip-annotations-1.0.jar:/usr/share/java/kafka/metrics-core-2.2.0.jar:/usr/share/java/kafka/zipkin-reporter-2.16.3.jar:/usr/share/java/kafka/netty-all-4.1.73.Final.jar:/usr/share/java/kafka/maven-artifact-3.8.1.jar:/usr/share/java/kafka/telemetry-events-api-7.1.0-ce.jar:/usr/share/java/kafka/netty-handler-proxy-4.1.73.Final.jar:/usr/share/java/kafka/netty-common-4.1.73.Final.jar:/usr/share/java/kafka/auto-service-1.0-rc7.jar:/usr/share/java/kafka/jackson-dataformat-cbor-2.12.3.jar:/usr/share/java/kafka/netty-codec-dns-4.1.73.Final.jar:/usr/share/java/kafka/threetenbp-1.5.1.jar:/usr/share/java/kafka/jersey-common-2.34.jar:/usr/share/java/kafka/accessors-smart-2.4.7.jar:/usr/share/java/kafka/cloudevents-core-2.0.0.jar:/usr/share/java/kafka/google-oauth-client-1.32.1.jar:/usr/share/java/kafka/api-common-2.0.2.jar:/usr/share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/share/java/kafka/okio-jvm-3.0.0.jar:/usr/share/java/kafka/httpcore-4.4.14.jar:/usr/share/java/kafka/telemetry-events-7.1.0-ce.jar:/usr/share/java/kafka/kotlin-stdlib-1.5.31.jar:/usr/share/java/kafka/internal-rest-server-7.1.0-ce.jar:/usr/share/java/kafka/kafka-raft-7.1.0-ce.jar:/usr/share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/share/java/kafka/scala-collection-compat_2.13-2.4.4.jar:/usr/share/java/kafka/hk2-locator-2.6.1.jar:/usr/share/java/kafka/javax.json-1.0.4.jar:/usr/share/java/kafka/google-auth-library-oauth2-http-1.1.0.jar:/usr/share/java/kafka/simpleclient-0.12.0.jar:/usr/share/java/kafka/msal4j-persistence-extension-1.1.0.jar:/usr/share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/share/java/kafka/connect-json-7.1.0-ce.jar:/usr/share/java/kafka/scala-library-2.13.6.jar:/usr/share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/share/java/kafka/swagger-annotations-1.6.3.jar:/usr/share/java/kafka/reflections-0.9.12.jar:/usr/share/java/kafka/j2objc-annotations-1.3.jar:/usr/share/java/kafka/jackson-jaxrs-base-2.12.3.
connect                            | jar:/usr/share/java/kafka/slf4j-api-1.7.32.jar:/usr/share/java/kafka/netty-codec-stomp-4.1.73.Final.jar:/usr/share/java/kafka/ion-java-1.0.2.jar:/usr/share/java/kafka/google-api-services-cloudkms-v1-rev108-1.25.0.jar:/usr/share/java/kafka/jackson-dataformat-yaml-2.12.3.jar:/usr/share/java/kafka/kafka-streams-scala_2.13-7.1.0-ce.jar:/usr/share/java/kafka/antlr-runtime-3.5.2.jar:/usr/share/java/kafka/netty-codec-xml-4.1.73.Final.jar:/usr/share/java/kafka/jackson-databind-2.12.3.jar:/usr/share/java/kafka/azure-core-1.18.0.jar:/usr/share/java/kafka/activation-1.1.1.jar:/usr/share/java/kafka/connect-api-7.1.0-ce.jar:/usr/share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/share/java/kafka/database-2.1.4.jar:/usr/share/java/kafka/netty-transport-udt-4.1.73.Final.jar:/usr/share/java/kafka/cloudevents-api-2.0.0.jar:/usr/share/java/kafka/jetty-util-ajax-9.4.44.v20210927.jar:/usr/share/java/kafka/google-http-client-gson-1.40.0.jar:/usr/share/java/kafka/joda-time-2.9.9.jar:/usr/share/java/kafka/paranamer-2.8.jar:/usr/share/java/kafka/commons-codec-1.15.jar:/usr/share/java/kafka/jmespath-java-1.11.988.jar:/usr/share/java/kafka/scala-java8-compat_2.13-1.0.0.jar:/usr/share/java/kafka/reactor-core-3.4.6.jar:/usr/share/java/kafka/kafka-metadata-7.1.0-ce.jar:/usr/share/java/kafka/content-type-2.1.jar:/usr/share/java/kafka/jose4j-0.7.8.jar:/usr/share/java/kafka/netty-buffer-4.1.73.Final.jar:/usr/share/java/kafka/plexus-utils-3.2.1.jar:/usr/share/java/kafka/tink-gcpkms-1.6.0.jar:/usr/share/java/kafka/kafka-tools-7.1.0-ce.jar:/usr/share/java/kafka/KeePassJava2-kdbx-2.1.4.jar:/usr/share/java/kafka/client-java-api-14.0.0.jar:/usr/share/java/kafka/jbcrypt-0.4.jar:/usr/share/java/kafka/slf4j-api-1.7.21.jar:/usr/share/java/kafka/jsr305-3.0.1.jar:/usr/share/java/kafka/jetty-util-9.4.44.v20210927.jar:/usr/share/java/kafka/connector-datapreview-extension-7.1.0-ce.jar:/usr/share/java/kafka/auto-common-0.10.jar:/usr/share/java/kafka/hk2-utils-2.6.1.jar:/usr/share/java/kafka/KeePassJava2-2.1.4.jar:/usr/share/java/kafka/metrics-core-4.1.12.1.jar:/usr/share/java/kafka/error_prone_annotations-2.3.4.jar:/usr/share/java/kafka/authorizer-7.1.0-ce.jar:/usr/share/java/kafka/rocksdbjni-6.22.1.1.jar:/usr/share/java/kafka/simpleclient_httpserver-0.12.0.jar:/usr/share/java/kafka/opencensus-api-0.28.0.jar:/usr/share/java/kafka/netty-transport-native-kqueue-4.1.73.Final-osx-aarch_64.jar:/usr/share/java/kafka/jackson-module-jaxb-annotations-2.12.3.jar:/usr/share/java/kafka/gson-fire-1.8.5.jar:/usr/share/java/kafka/jcip-annotations-1.0-1.jar:/usr/share/java/kafka/netty-codec-haproxy-4.1.73.Final.jar:/usr/share/java/kafka/logging-interceptor-4.9.1.jar:/usr/share/java/kafka/cloudevents-json-jackson-2.0.0.jar:/usr/share/java/kafka/jackson-dataformat-csv-2.12.3.jar:/usr/share/java/kafka/netty-tcnative-classes-2.0.46.Final.jar:/usr/share/java/kafka/aws-java-sdk-sts-1.11.988.jar:/usr/share/java/kafka/auth-providers-7.1.0-ce.jar:/usr/share/java/kafka/aws-java-sdk-core-1.11.988.jar:/usr/share/java/kafka/simpleclient_common-0.12.0.jar:/usr/share/java/kafka/commons-cli-1.4.jar:/usr/share/java/kafka/ce-sbk_2.13-7.1.0-ce.jar:/usr/share/java/kafka/failureaccess-1.0.1.jar:/usr/share/java/kafka/brave-5.13.3.jar:/usr/share/java/kafka/stax2-api-4.2.1.jar:/usr/share/java/kafka/jakarta.inject-2.6.1.jar:/usr/share/java/kafka/netty-transport-native-unix-common-4.1.73.Final.jar:/usr/share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/share/java/kafka/confluent-serializers-new-7.1.0-ce.jar:/usr/share/java/kafka/jetty-io-9.4.44.v20210927.jar:/usr/share/java/kafka/client-java-14.0.0.jar:/usr/share/java/kafka/netty-transport-classes-kqueue-4.1.73.Final.jar:/usr/share/java/kafka/kafka-server-common-7.1.0-ce.jar:/usr/share/java/kafka/connect-transforms-7.1.0-ce.jar:/usr/share/java/kafka/connect-mirror-client-7.1.0-ce.jar:/usr/share/java/kafka/confluent-licensing-new-7.1.0-ce.jar:/usr/share/java/kafka/argparse4j-0.7.0.jar:/usr/share/java/kafka/netty-resolver-4.1.73.Final.jar:/usr/share/java/kafka/annotations-15.0.jar:/usr/share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/usr/share/java/kafka/jopt-simple-5.0.4.jar:/usr/share/java/kafka/netty-transport-4.1.73.Final.jar:/usr/share/java/kafka/commons-math3-3.6.1.jar:/usr/share/java/kafka/connect-basic-auth-extension-7.1.0-ce.jar:/usr/share/java/kafka/audience-annotations-0.5.0.jar:/usr/share/java/kafka/KeePassJava2-kdb-2.1.4.jar:/usr/share/java/kafka/jackson-jaxrs-json-provider-2.12.3.jar:/usr/share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/share/java/kafka/client-java-proto-14.0.0.jar:/usr/share/java/kafka/icu4j-61.1.jar:/usr/share/java/kafka/kafka-log4j-appender-7.1.0-ce.jar:/usr/share/java/kafka/zookeeper-jute-3.6.3.jar:/usr/share/java/kafka/scala-logging_2.13-3.9.3.jar:/usr/share/java/kafka/logredactor-metrics-1.0.10.jar:/usr/share/java/kafka/gax-2.4.1.jar:/usr/share/java/kafka/netty-codec-redis-4.1.73.Final.jar:/usr/share/java/kafka/aalto-xml-1.0.0.jar:/usr/share/java/kafka/reactor-netty-core-1.0.7.jar:/usr/share/java/kafka/jersey-server-2.34.jar:/usr/share/java/kafka/simpleclient_tracer_otel_agent-0.12.0.jar:/usr/share/java/kafka/netty-codec-4.1.73.Final.jar:/usr/share/java/kafka/proto-google-iam-v1-1.1.0.jar:/usr/share/java/kafka/rest-authorizer-7.1.0-ce.jar:/usr/share/java/kafka/netty-codec-socks-4.1.73.Final.jar:/usr/share/java/kafka/hk2-api-2.6.1.jar:/usr/share/java/kafka/protobuf-java-util-3.17.3.jar:/usr/share/java/kafka/bcpkix-fips-1.0.3.jar:/usr/share/java/kafka/aws-java-sdk-kms-1.11.988.jar:/usr/share/java/kafka/jsr305-3.0.2.jar:/usr/share/java/kafka/kafka-shell-7.1.0-ce.jar:/usr/share/java/kafka/azure-core-http-netty-1.10.1.jar:/usr/share/java/kafka/broker-plugins-7.1.0-ce.jar:/usr/share/java/kafka/connect-mirror-7.1.0-ce.jar:/usr/share/java/kafka/auto-service-annotations-1.0-rc7.jar:/usr/share/java/kafka/kafka-storage-7.1.0-ce.jar:/usr/share/java/kafka/jetty-servlet-9.4.44.v20210927.jar:/usr/share/java/kafka/confluent-resource-names-7.1.0-ce.jar:/usr/share/java/kafka/commons-io-2.11.0.jar:/usr/share/java/kafka/telemetry-api-1.745.0.jar:/usr/share/java/kafka/flatbuffers-java-1.9.0.jar:/usr/share/java/kafka/protobuf-java-3.17.3.jar:/usr/share/java/kafka/jersey-hk2-2.34.jar:/usr/share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/share/java/kafka/jetty-continuation-9.4.44.v20210927.jar:/usr/share/java/kafka/jersey-client-2.34.jar:/usr/share/java/kafka/kafka-streams-test-utils-7.1.0-ce.jar:/usr/share/java/kafka/opencensus-contrib-http-util-0.28.0.jar:/usr/share/java/kafka/kotlin-stdlib-jdk7-1.5.31.jar:/usr/share/java/kafka/antlr4-4.9.2.jar:/usr/share/java/kafka/jackson-annotations-2.12.3.jar:/usr/share/java/kafka/proto-google-common-protos-2.5.0.jar:/usr/share/java/kafka/kafka-storage-api-7.1.0-ce.jar:/usr/share/java/kafka/gson-2.8.6.jar:/usr/share/java/kafka/netty-codec-http2-4.1.73.Final.jar:/usr/share/java/kafka/commons-collections4-4.4.jar:/usr/share/java/kafka/google-cloud-core-2.1.3.jar:/usr/share/java/kafka/oauth2-oidc-sdk-9.7.jar:/usr/share/java/kafka/gax-httpjson-0.89.1.jar:/usr/share/java/kafka/netty-resolver-dns-4.1.73.Final.jar:/usr/share/java/kafka/connect-ce-logs-7.1.0-ce.jar:/usr/share/java/kafka/netty-handler-proxy-4.1.65.Final.jar:/usr/share/java/kafka/minimal-json-0.9.5.jar:/usr/share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/share/java/kafka/jetty-http-9.4.44.v20210927.jar:/usr/share/java/kafka/ST4-4.3.jar:/usr/share/java/kafka/okhttp-4.9.1.jar:/usr/share/java/kafka/org.abego.treelayout.core-1.0.3.jar:/usr/share/java/kafka/re2j-1.6.jar:/usr/share/java/kafka/slf4j-api-1.7.30.jar:/usr/share/java/kafka/json-smart-2.4.7.jar:/usr/share/java/kafka/httpcore-4.4.13.jar:/usr/share/java/kafka/jline-3.12.1.jar:/usr/share/java/kafka/woodstox-core-6.2.4.jar:/usr/share/java/kafka/KeePassJava2-jaxb-2.1.4.jar:/usr/share/java/kafka/bctls-fips-1.0.10.jar:/usr/share/java/kafka/google-api-client-1.32.1.jar:/usr/share/java/kafka/antlr4-runtime-4.9.2.jar:/usr/share/java/kafka/zstd-jni-1.5.0-4.jar:/usr/share/java/kafka/azure-storage-common-12.12.0.jar:/usr/share/java/kafka/netty-transport-native-kqueue-4.1.73.Final-osx-x86_64.jar:/usr/share/java/kafka/KeePassJava2-simple-2.1.4.jar:/usr/share/java/kafka/google-http-client-
connect                            | jackson2-1.40.0.jar:/usr/share/java/kafka/netty-codec-smtp-4.1.73.Final.jar:/usr/share/java/kafka/logredactor-1.0.10.jar:/usr/share/java/kafka/grpc-context-1.40.1.jar:/usr/share/java/kafka/google-http-client-appengine-1.40.0.jar:/usr/share/java/kafka/javassist-3.27.0-GA.jar:/usr/share/java/kafka/jackson-core-2.12.3.jar:/usr/share/java/kafka/kafka-streams-examples-7.1.0-ce.jar:/usr/share/java/kafka/kafka-client-plugins-7.1.0-ce.jar:/usr/share/java/kafka/annotations-3.0.1.jar:/usr/share/java/kafka/jackson-dataformat-xml-2.12.3.jar:/usr/share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/share/java/kafka/jackson-module-scala_2.13-2.12.3.jar:/usr/share/java/kafka/jackson-datatype-jsr310-2.12.3.jar:/usr/share/java/kafka/nimbus-jose-jwt-9.9.3.jar:/usr/share/java/kafka/kafka_2.13-7.1.0-ce.jar:/usr/share/java/kafka/netty-resolver-dns-native-macos-4.1.73.Final-osx-x86_64.jar:/usr/share/java/kafka/reactor-netty-http-brave-1.0.7.jar:/usr/share/java/kafka/asm-9.1.jar:/usr/share/java/kafka/zipkin-2.23.2.jar:/usr/share/java/kafka/google-cloud-storage-2.1.2.jar:/usr/share/java/kafka/jaxb-api-2.3.0.jar:/usr/share/java/kafka/reactor-netty-1.0.7.jar:/usr/share/java/kafka/reactive-streams-1.0.3.jar:/usr/share/java/kafka/jna-5.6.0.jar:/usr/share/java/kafka/commons-compress-1.21.jar:/usr/share/java/kafka/guava-30.0-jre.jar:/usr/share/java/kafka/jackson-dataformat-properties-2.12.3.jar:/usr/share/java/kafka/auto-value-annotations-1.8.2.jar:/usr/share/java/kafka/bc-fips-1.0.2.jar:/usr/share/java/kafka/kotlin-stdlib-common-1.5.31.jar:/usr/share/java/kafka/jetty-security-9.4.44.v20210927.jar:/usr/share/java/kafka/commons-lang3-3.11.jar:/usr/share/java/kafka/javax.annotation-api-1.3.2.jar:/usr/share/java/kafka/opencensus-proto-0.2.0.jar:/usr/share/java/kafka/azure-storage-internal-avro-12.0.5.jar:/usr/share/java/kafka/checker-qual-3.5.0.jar:/usr/share/java/kafka/netty-transport-native-epoll-4.1.73.Final-linux-x86_64.jar:/usr/share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/share/java/kafka/jna-platform-5.6.0.jar:/usr/share/java/kafka/netty-transport-native-epoll-4.1.73.Final-linux-aarch_64.jar:/usr/share/java/confluent-common/common-utils-7.1.0.jar:/usr/share/java/confluent-common/common-config-7.1.0.jar:/usr/share/java/confluent-common/build-tools-7.1.0.jar:/usr/share/java/confluent-common/common-metrics-7.1.0.jar:/usr/share/java/confluent-common/slf4j-api-1.7.30.jar:/usr/share/java/kafka-serde-tools/error_prone_annotations-2.5.1.jar:/usr/share/java/kafka-serde-tools/kotlin-stdlib-jdk8-1.5.31.jar:/usr/share/java/kafka-serde-tools/commons-logging-1.2.jar:/usr/share/java/kafka-serde-tools/kafka-connect-json-schema-converter-7.1.0.jar:/usr/share/java/kafka-serde-tools/jackson-datatype-jdk8-2.12.3.jar:/usr/share/java/kafka-serde-tools/kotlin-scripting-compiler-embeddable-1.3.50.jar:/usr/share/java/kafka-serde-tools/kafka-streams-protobuf-serde-7.1.0.jar:/usr/share/java/kafka-serde-tools/kotlin-scripting-jvm-1.4.21.jar:/usr/share/java/kafka-serde-tools/checker-qual-3.8.0.jar:/usr/share/java/kafka-serde-tools/annotations-13.0.jar:/usr/share/java/kafka-serde-tools/kafka-avro-serializer-7.1.0.jar:/usr/share/java/kafka-serde-tools/commons-collections-3.2.2.jar:/usr/share/java/kafka-serde-tools/proto-google-common-protos-2.5.1.jar:/usr/share/java/kafka-serde-tools/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/share/java/kafka-serde-tools/okio-jvm-3.0.0.jar:/usr/share/java/kafka-serde-tools/kotlin-scripting-common-1.4.21.jar:/usr/share/java/kafka-serde-tools/kafka-json-serializer-7.1.0.jar:/usr/share/java/kafka-serde-tools/kafka-schema-serializer-7.1.0.jar:/usr/share/java/kafka-serde-tools/scala-library-2.13.5.jar:/usr/share/java/kafka-serde-tools/kafka-streams-json-schema-serde-7.1.0.jar:/usr/share/java/kafka-serde-tools/j2objc-annotations-1.3.jar:/usr/share/java/kafka-serde-tools/json-20201115.jar:/usr/share/java/kafka-serde-tools/jackson-databind-2.12.3.jar:/usr/share/java/kafka-serde-tools/org.everit.json.schema-1.12.2.jar:/usr/share/java/kafka-serde-tools/kafka-connect-avro-converter-7.1.0.jar:/usr/share/java/kafka-serde-tools/kafka-streams-7.1.0-ccs.jar:/usr/share/java/kafka-serde-tools/commons-validator-1.6.jar:/usr/share/java/kafka-serde-tools/kafka-streams-avro-serde-7.1.0.jar:/usr/share/java/kafka-serde-tools/joda-time-2.10.8.jar:/usr/share/java/kafka-serde-tools/kafka-json-schema-provider-7.1.0.jar:/usr/share/java/kafka-serde-tools/jackson-datatype-guava-2.12.3.jar:/usr/share/java/kafka-serde-tools/wire-runtime-jvm-4.0.0.jar:/usr/share/java/kafka-serde-tools/rocksdbjni-6.22.1.1.jar:/usr/share/java/kafka-serde-tools/wire-schema-jvm-4.0.0.jar:/usr/share/java/kafka-serde-tools/commons-digester-1.8.1.jar:/usr/share/java/kafka-serde-tools/swagger-annotations-2.1.10.jar:/usr/share/java/kafka-serde-tools/kafka-protobuf-types-7.1.0.jar:/usr/share/java/kafka-serde-tools/failureaccess-1.0.1.jar:/usr/share/java/kafka-serde-tools/re2j-1.3.jar:/usr/share/java/kafka-serde-tools/kafka-protobuf-serializer-7.1.0.jar:/usr/share/java/kafka-serde-tools/kafka-connect-protobuf-converter-7.1.0.jar:/usr/share/java/kafka-serde-tools/kafka-schema-registry-client-7.1.0.jar:/usr/share/java/kafka-serde-tools/kotlin-stdlib-1.4.21.jar:/usr/share/java/kafka-serde-tools/kafka-protobuf-provider-7.1.0.jar:/usr/share/java/kafka-serde-tools/kafka-json-schema-serializer-7.1.0.jar:/usr/share/java/kafka-serde-tools/kafka-connect-avro-data-7.1.0.jar:/usr/share/java/kafka-serde-tools/protobuf-java-util-3.17.3.jar:/usr/share/java/kafka-serde-tools/jsr305-3.0.2.jar:/usr/share/java/kafka-serde-tools/validation-api-2.0.1.Final.jar:/usr/share/java/kafka-serde-tools/avro-1.11.0.jar:/usr/share/java/kafka-serde-tools/protobuf-java-3.17.3.jar:/usr/share/java/kafka-serde-tools/kotlin-stdlib-jdk7-1.5.31.jar:/usr/share/java/kafka-serde-tools/jackson-annotations-2.12.3.jar:/usr/share/java/kafka-serde-tools/gson-2.8.6.jar:/usr/share/java/kafka-serde-tools/kotlinx-coroutines-core-1.3.7.jar:/usr/share/java/kafka-serde-tools/slf4j-api-1.7.30.jar:/usr/share/java/kafka-serde-tools/jackson-module-parameter-names-2.12.3.jar:/usr/share/java/kafka-serde-tools/handy-uri-templates-2.1.8.jar:/usr/share/java/kafka-serde-tools/guava-30.1.1-jre.jar:/usr/share/java/kafka-serde-tools/jackson-core-2.12.3.jar:/usr/share/java/kafka-serde-tools/kotlin-script-runtime-1.4.21.jar:/usr/share/java/kafka-serde-tools/jackson-datatype-jsr310-2.12.3.jar:/usr/share/java/kafka-serde-tools/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/usr/share/java/kafka-serde-tools/classgraph-4.8.21.jar:/usr/share/java/kafka-serde-tools/commons-compress-1.21.jar:/usr/share/java/kafka-serde-tools/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/usr/share/java/kafka-serde-tools/kotlin-stdlib-common-1.5.31.jar:/usr/share/java/kafka-serde-tools/jackson-datatype-joda-2.12.3.jar:/usr/share/java/monitoring-interceptors/monitoring-interceptors-7.1.0.jar:/usr/bin/../ce-broker-plugins/build/libs/*:/usr/bin/../ce-broker-plugins/build/dependant-libs/*:/usr/bin/../ce-auth-providers/build/libs/*:/usr/bin/../ce-auth-providers/build/dependant-libs/*:/usr/bin/../ce-rest-server/build/libs/*:/usr/bin/../ce-rest-server/build/dependant-libs/*:/usr/bin/../ce-audit/build/libs/*:/usr/bin/../ce-audit/build/dependant-libs/*:/usr/bin/../share/java/kafka/tink-1.6.0.jar:/usr/bin/../share/java/kafka/snakeyaml-1.27.jar:/usr/bin/../share/java/kafka/aws-java-sdk-s3-1.11.988.jar:/usr/bin/../share/java/kafka/rbac-7.1.0-ce.jar:/usr/bin/../share/java/kafka/azure-storage-blob-12.12.0.jar:/usr/bin/../share/java/kafka/cloudevents-kafka-2.0.0.jar:/usr/bin/../share/java/kafka/httpclient-4.5.13.jar:/usr/bin/../share/java/kafka/kotlin-stdlib-jdk8-1.5.31.jar:/usr/bin/../share/java/kafka/connect-runtime-7.1.0-ce.jar:/usr/bin/../share/java/kafka/trogdor-7.1.0-ce.jar:/usr/bin/../share/java/kafka/commons-logging-1.2.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/netty-transport-classes-epoll-4.1.73.Final.jar:/usr/bin/../share/java/kafka/google-api-services-storage-v1-rev20210127-1.32.1.jar:/usr/bin/../share/java/kafka/kafka-streams-7.1.0-ce.jar:/usr/bin/../share/java/kafk
connect                            | a/netty-transport-sctp-4.1.73.Final.jar:/usr/bin/../share/java/kafka/KeePassJava2-dom-2.1.4.jar:/usr/bin/../share/java/kafka/reactor-netty-http-1.0.7.jar:/usr/bin/../share/java/kafka/confluent-log4j-1.2.17-cp10.jar:/usr/bin/../share/java/kafka/netty-codec-mqtt-4.1.73.Final.jar:/usr/bin/../share/java/kafka/netty-transport-rxtx-4.1.73.Final.jar:/usr/bin/../share/java/kafka/google-cloud-core-http-2.1.3.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/kafka/zipkin-reporter-brave-2.16.3.jar:/usr/bin/../share/java/kafka/google-http-client-apache-v2-1.40.0.jar:/usr/bin/../share/java/kafka/simpleclient_tracer_otel-0.12.0.jar:/usr/bin/../share/java/kafka/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.6.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/core-1.54.0.0.jar:/usr/bin/../share/java/kafka/confluent-audit-7.1.0-ce.jar:/usr/bin/../share/java/kafka/google-http-client-1.40.0.jar:/usr/bin/../share/java/kafka/telemetry-client-1.745.0.jar:/usr/bin/../share/java/kafka/brave-instrumentation-http-5.13.3.jar:/usr/bin/../share/java/kafka/google-auth-library-credentials-1.1.0.jar:/usr/bin/../share/java/kafka/security-extensions-7.1.0-ce.jar:/usr/bin/../share/java/kafka/netty-resolver-dns-native-macos-4.1.73.Final-osx-aarch_64.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.73.Final.jar:/usr/bin/../share/java/kafka/netty-codec-memcache-4.1.73.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/netty-codec-http-4.1.73.Final.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/kafka/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/kafka/checker-qual-3.8.0.jar:/usr/bin/../share/java/kafka/netty-resolver-dns-classes-macos-4.1.73.Final.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.73.Final.jar:/usr/bin/../share/java/kafka/lang-tag-1.5.jar:/usr/bin/../share/java/kafka/netty-tcnative-boringssl-static-2.0.46.Final.jar:/usr/bin/../share/java/kafka/simpleclient_tracer_common-0.12.0.jar:/usr/bin/../share/java/kafka/snakeyaml-1.29.jar:/usr/bin/../share/java/kafka/annotations-13.0.jar:/usr/bin/../share/java/kafka/msal4j-1.10.1.jar:/usr/bin/../share/java/kafka/azure-identity-1.3.3.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/jcip-annotations-1.0.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/zipkin-reporter-2.16.3.jar:/usr/bin/../share/java/kafka/netty-all-4.1.73.Final.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.1.jar:/usr/bin/../share/java/kafka/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/kafka/netty-handler-proxy-4.1.73.Final.jar:/usr/bin/../share/java/kafka/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/kafka/auto-service-1.0-rc7.jar:/usr/bin/../share/java/kafka/jackson-dataformat-cbor-2.12.3.jar:/usr/bin/../share/java/kafka/netty-codec-dns-4.1.73.Final.jar:/usr/bin/../share/java/kafka/threetenbp-1.5.1.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/accessors-smart-2.4.7.jar:/usr/bin/../share/java/kafka/cloudevents-core-2.0.0.jar:/usr/bin/../share/java/kafka/google-oauth-client-1.32.1.jar:/usr/bin/../share/java/kafka/api-common-2.0.2.jar:/usr/bin/../share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/kafka/okio-jvm-3.0.0.jar:/usr/bin/../share/java/kafka/httpcore-4.4.14.jar:/usr/bin/../share/java/kafka/telemetry-events-7.1.0-ce.jar:/usr/bin/../share/java/kafka/kotlin-stdlib-1.5.31.jar:/usr/bin/../share/java/kafka/internal-rest-server-7.1.0-ce.jar:/usr/bin/../share/java/kafka/kafka-raft-7.1.0-ce.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.4.4.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/javax.json-1.0.4.jar:/usr/bin/../share/java/kafka/google-auth-library-oauth2-http-1.1.0.jar:/usr/bin/../share/java/kafka/simpleclient-0.12.0.jar:/usr/bin/../share/java/kafka/msal4j-persistence-extension-1.1.0.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/connect-json-7.1.0-ce.jar:/usr/bin/../share/java/kafka/scala-library-2.13.6.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/swagger-annotations-1.6.3.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/j2objc-annotations-1.3.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.32.jar:/usr/bin/../share/java/kafka/netty-codec-stomp-4.1.73.Final.jar:/usr/bin/../share/java/kafka/ion-java-1.0.2.jar:/usr/bin/../share/java/kafka/google-api-services-cloudkms-v1-rev108-1.25.0.jar:/usr/bin/../share/java/kafka/jackson-dataformat-yaml-2.12.3.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.1.0-ce.jar:/usr/bin/../share/java/kafka/antlr-runtime-3.5.2.jar:/usr/bin/../share/java/kafka/netty-codec-xml-4.1.73.Final.jar:/usr/bin/../share/java/kafka/jackson-databind-2.12.3.jar:/usr/bin/../share/java/kafka/azure-core-1.18.0.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/connect-api-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/database-2.1.4.jar:/usr/bin/../share/java/kafka/netty-transport-udt-4.1.73.Final.jar:/usr/bin/../share/java/kafka/cloudevents-api-2.0.0.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/google-http-client-gson-1.40.0.jar:/usr/bin/../share/java/kafka/joda-time-2.9.9.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/commons-codec-1.15.jar:/usr/bin/../share/java/kafka/jmespath-java-1.11.988.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.0.jar:/usr/bin/../share/java/kafka/reactor-core-3.4.6.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.1.0-ce.jar:/usr/bin/../share/java/kafka/content-type-2.1.jar:/usr/bin/../share/java/kafka/jose4j-0.7.8.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/tink-gcpkms-1.6.0.jar:/usr/bin/../share/java/kafka/kafka-tools-7.1.0-ce.jar:/usr/bin/../share/java/kafka/KeePassJava2-kdbx-2.1.4.jar:/usr/bin/../share/java/kafka/client-java-api-14.0.0.jar:/usr/bin/../share/java/kafka/jbcrypt-0.4.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.21.jar:/usr/bin/../share/java/kafka/jsr305-3.0.1.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/connector-datapreview-extension-7.1.0-ce.jar:/usr/bin/../share/java/kafka/auto-common-0.10.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/KeePassJava2-2.1.4.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/error_prone_annotations-2.3.4.jar:/usr/bin/../share/java/kafka/authorizer-7.1.0-ce.jar:/usr/bin/../share/java/kafka/rocksdbjni-6.22.1.1.jar:/usr/bin/../share/java/kafka/simpleclient_httpserver-0.12.0.jar:/usr/bin/../share/java/kafka/opencensus-api-0.28.0.jar:/usr/bin/../share/java/kafka/netty-transport-native-kqueue-4.1.73.Final-osx-aarch_64.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/gson-fire-1.8.5.jar:/usr/bin/../share/java/kafka/jcip-annotations-1.0-1.jar:/usr/bin/../share/java/kafka/netty-codec-haproxy-4.1.73.Final.jar:/usr/bin/../share/java/kafka/logging-interceptor-4.9.1.jar:/usr/bin/../share/java/kafka/cloudevents-json-jackson-2.0.0.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.12.3.jar:/usr/bin/../share/java/kafka/netty-tcnative-classes-2.0.46.Final.jar:/usr/bin/../share/java/kafka/aws-java-sdk-sts-1.11.988.jar:/usr/bin/../share/java/kafka/auth-providers-7.1.0-ce.jar:/usr/bin/../share/java/kafka/aws-java-sdk-core-1.11.988.jar:/usr/bin/../share/java/kafka/simpleclient_common-0.12.0.jar:/usr/bin/../share/java/kafka/commons-cli-1
connect                            | .4.jar:/usr/bin/../share/java/kafka/ce-sbk_2.13-7.1.0-ce.jar:/usr/bin/../share/java/kafka/failureaccess-1.0.1.jar:/usr/bin/../share/java/kafka/brave-5.13.3.jar:/usr/bin/../share/java/kafka/stax2-api-4.2.1.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.73.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/confluent-serializers-new-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/client-java-14.0.0.jar:/usr/bin/../share/java/kafka/netty-transport-classes-kqueue-4.1.73.Final.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.1.0-ce.jar:/usr/bin/../share/java/kafka/connect-transforms-7.1.0-ce.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.1.0-ce.jar:/usr/bin/../share/java/kafka/confluent-licensing-new-7.1.0-ce.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/kafka/annotations-15.0.jar:/usr/bin/../share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/kafka/commons-math3-3.6.1.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.1.0-ce.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/KeePassJava2-kdb-2.1.4.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/client-java-proto-14.0.0.jar:/usr/bin/../share/java/kafka/icu4j-61.1.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.1.0-ce.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.3.jar:/usr/bin/../share/java/kafka/logredactor-metrics-1.0.10.jar:/usr/bin/../share/java/kafka/gax-2.4.1.jar:/usr/bin/../share/java/kafka/netty-codec-redis-4.1.73.Final.jar:/usr/bin/../share/java/kafka/aalto-xml-1.0.0.jar:/usr/bin/../share/java/kafka/reactor-netty-core-1.0.7.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/simpleclient_tracer_otel_agent-0.12.0.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/kafka/proto-google-iam-v1-1.1.0.jar:/usr/bin/../share/java/kafka/rest-authorizer-7.1.0-ce.jar:/usr/bin/../share/java/kafka/netty-codec-socks-4.1.73.Final.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/kafka/bcpkix-fips-1.0.3.jar:/usr/bin/../share/java/kafka/aws-java-sdk-kms-1.11.988.jar:/usr/bin/../share/java/kafka/jsr305-3.0.2.jar:/usr/bin/../share/java/kafka/kafka-shell-7.1.0-ce.jar:/usr/bin/../share/java/kafka/azure-core-http-netty-1.10.1.jar:/usr/bin/../share/java/kafka/broker-plugins-7.1.0-ce.jar:/usr/bin/../share/java/kafka/connect-mirror-7.1.0-ce.jar:/usr/bin/../share/java/kafka/auto-service-annotations-1.0-rc7.jar:/usr/bin/../share/java/kafka/kafka-storage-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/confluent-resource-names-7.1.0-ce.jar:/usr/bin/../share/java/kafka/commons-io-2.11.0.jar:/usr/bin/../share/java/kafka/telemetry-api-1.745.0.jar:/usr/bin/../share/java/kafka/flatbuffers-java-1.9.0.jar:/usr/bin/../share/java/kafka/protobuf-java-3.17.3.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.1.0-ce.jar:/usr/bin/../share/java/kafka/opencensus-contrib-http-util-0.28.0.jar:/usr/bin/../share/java/kafka/kotlin-stdlib-jdk7-1.5.31.jar:/usr/bin/../share/java/kafka/antlr4-4.9.2.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/proto-google-common-protos-2.5.0.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.1.0-ce.jar:/usr/bin/../share/java/kafka/gson-2.8.6.jar:/usr/bin/../share/java/kafka/netty-codec-http2-4.1.73.Final.jar:/usr/bin/../share/java/kafka/commons-collections4-4.4.jar:/usr/bin/../share/java/kafka/google-cloud-core-2.1.3.jar:/usr/bin/../share/java/kafka/oauth2-oidc-sdk-9.7.jar:/usr/bin/../share/java/kafka/gax-httpjson-0.89.1.jar:/usr/bin/../share/java/kafka/netty-resolver-dns-4.1.73.Final.jar:/usr/bin/../share/java/kafka/connect-ce-logs-7.1.0-ce.jar:/usr/bin/../share/java/kafka/netty-handler-proxy-4.1.65.Final.jar:/usr/bin/../share/java/kafka/minimal-json-0.9.5.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/ST4-4.3.jar:/usr/bin/../share/java/kafka/okhttp-4.9.1.jar:/usr/bin/../share/java/kafka/org.abego.treelayout.core-1.0.3.jar:/usr/bin/../share/java/kafka/re2j-1.6.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/json-smart-2.4.7.jar:/usr/bin/../share/java/kafka/httpcore-4.4.13.jar:/usr/bin/../share/java/kafka/jline-3.12.1.jar:/usr/bin/../share/java/kafka/woodstox-core-6.2.4.jar:/usr/bin/../share/java/kafka/KeePassJava2-jaxb-2.1.4.jar:/usr/bin/../share/java/kafka/bctls-fips-1.0.10.jar:/usr/bin/../share/java/kafka/google-api-client-1.32.1.jar:/usr/bin/../share/java/kafka/antlr4-runtime-4.9.2.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/kafka/azure-storage-common-12.12.0.jar:/usr/bin/../share/java/kafka/netty-transport-native-kqueue-4.1.73.Final-osx-x86_64.jar:/usr/bin/../share/java/kafka/KeePassJava2-simple-2.1.4.jar:/usr/bin/../share/java/kafka/google-http-client-jackson2-1.40.0.jar:/usr/bin/../share/java/kafka/netty-codec-smtp-4.1.73.Final.jar:/usr/bin/../share/java/kafka/logredactor-1.0.10.jar:/usr/bin/../share/java/kafka/grpc-context-1.40.1.jar:/usr/bin/../share/java/kafka/google-http-client-appengine-1.40.0.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/jackson-core-2.12.3.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.1.0-ce.jar:/usr/bin/../share/java/kafka/kafka-client-plugins-7.1.0-ce.jar:/usr/bin/../share/java/kafka/annotations-3.0.1.jar:/usr/bin/../share/java/kafka/jackson-dataformat-xml-2.12.3.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.12.3.jar:/usr/bin/../share/java/kafka/jackson-datatype-jsr310-2.12.3.jar:/usr/bin/../share/java/kafka/nimbus-jose-jwt-9.9.3.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.1.0-ce.jar:/usr/bin/../share/java/kafka/netty-resolver-dns-native-macos-4.1.73.Final-osx-x86_64.jar:/usr/bin/../share/java/kafka/reactor-netty-http-brave-1.0.7.jar:/usr/bin/../share/java/kafka/asm-9.1.jar:/usr/bin/../share/java/kafka/zipkin-2.23.2.jar:/usr/bin/../share/java/kafka/google-cloud-storage-2.1.2.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/reactor-netty-1.0.7.jar:/usr/bin/../share/java/kafka/reactive-streams-1.0.3.jar:/usr/bin/../share/java/kafka/jna-5.6.0.jar:/usr/bin/../share/java/kafka/commons-compress-1.21.jar:/usr/bin/../share/java/kafka/guava-30.0-jre.jar:/usr/bin/../share/java/kafka/jackson-dataformat-properties-2.12.3.jar:/usr/bin/../share/java/kafka/auto-value-annotations-1.8.2.jar:/usr/bin/../share/java/kafka/bc-fips-1.0.2.jar:/usr/bin/../share/java/kafka/kotlin-stdlib-common-1.5.31.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/commons-lang3-3.11.jar:/usr/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/kafka/opencensus-proto-0.2.0.jar:/usr/bin/../share/java/kafka/azure-storage-internal-avro-12.0.5.jar:/usr/bin/../share/java/kafka/checker-qual-3.5.0.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.73.Final-linux-x86_64.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/jna-platform-5.6.0.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.
connect                            | 1.73.Final-linux-aarch_64.jar:/usr/bin/../share/java/confluent-metadata-service/snakeyaml-1.27.jar:/usr/bin/../share/java/confluent-metadata-service/rbac-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/jose4j-0.7.2.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-kafka-2.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-classes-epoll-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/common-utils-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/auto-value-annotations-1.8.1.jar:/usr/bin/../share/java/confluent-metadata-service/lz4-java-1.8.0.jar:/usr/bin/../share/java/confluent-metadata-service/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/confluent-metadata-service/confluent-audit-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-client-1.745.0.jar:/usr/bin/../share/java/confluent-metadata-service/commons-lang3-3.8.1.jar:/usr/bin/../share/java/confluent-metadata-service/security-extensions-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/jcommander-1.72.jar:/usr/bin/../share/java/confluent-metadata-service/rbac-common-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/netty-codec-http-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/confluent-metadata-service/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/metrics-core-2.2.0.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/netty-handler-proxy-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-common-2.34.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-core-2.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.el-api-4.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-events-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/rbac-api-server-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/javax.json-1.0.4.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/confluent-metadata-service/confluent-security-plugins-common-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-dataformat-yaml-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/antlr-runtime-3.5.2.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-api-2.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/jbcrypt-0.4.jar:/usr/bin/../share/java/confluent-metadata-service/jul-to-slf4j-1.7.30.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-metadata-service/authorizer-client-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/authorizer-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/bsh-2.0b6.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-json-jackson-2.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/auth-providers-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/concurrent-trees-2.6.1.jar:/usr/bin/../share/java/confluent-metadata-service/testng-6.14.3.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-proxy-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-native-unix-common-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-classes-kqueue-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/kafka-server-common-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/usr/bin/../share/java/confluent-metadata-service/icu4j-61.1.jar:/usr/bin/../share/java/confluent-metadata-service/javax.servlet-api-4.0.1.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-server-2.34.jar:/usr/bin/../share/java/confluent-metadata-service/rest-authorizer-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/netty-codec-socks-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/confluent-metadata-service/ce-kafka-http-server-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/bcpkix-fips-1.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-bean-validation-2.34.jar:/usr/bin/../share/java/confluent-metadata-service/confluent-resource-names-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-api-1.745.0.jar:/usr/bin/../share/java/confluent-metadata-service/protobuf-java-3.17.3.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-client-2.34.jar:/usr/bin/../share/java/confluent-metadata-service/antlr4-4.9.2.jar:/usr/bin/../share/java/confluent-metadata-service/gson-2.8.6.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.el-3.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/confluent-metadata-service/ST4-4.3.jar:/usr/bin/../share/java/confluent-metadata-service/org.abego.treelayout.core-1.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/bctls-fips-1.0.10.jar:/usr/bin/../share/java/confluent-metadata-service/antlr4-runtime-4.9.2.jar:/usr/bin/../share/java/confluent-metadata-service/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-native-kqueue-4.1.73.Final-osx-x86_64.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-core-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/kafka-client-plugins-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/annotations-3.0.1.jar:/usr/bin/../share/java/confluent-metadata-service/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-datatype-jsr310-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/bc-fips-1.0.2.1.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-dataformat-properties-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-metadata-service/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/confluent-metadata-service/opencensus-proto-0.2.0.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-native-epoll-4.1.73.Final-linux-x86_64.jar:/usr/bin/../share/java/rest-utils/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/rest-utils/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/lz4-java-1.8.0.jar:/usr/bin/../share/java/rest-utils/asm-tree-9.2.jar:/usr/bin/../share/java/rest-utils/asm-analysis-9.2.jar:/usr/bin/../share/java/rest-utils/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/rest-utils/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/rest-utils/checker-qual-3.8.0.jar:/usr/bin/../share/java/rest-utils/jetty-alpn-java-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-jndi-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-xml-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/jetty-webapp-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jersey-common-2.34.jar:/usr/bin/../share/java/rest-utils/jakarta.el-api-4.0.0.jar:/usr/bin/../share/java/rest-utils/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/rest-utils/javax-websocket-client-impl-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/rest-utils/hk2-locator-2.6.1.jar:/usr/bin/../shar
connect                            | e/java/rest-utils/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/rest-utils/j2objc-annotations-1.3.jar:/usr/bin/../share/java/rest-utils/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/rest-utils/websocket-client-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jackson-databind-2.12.3.jar:/usr/bin/../share/java/rest-utils/jetty-alpn-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/activation-1.1.1.jar:/usr/bin/../share/java/rest-utils/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/rest-utils/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/asm-9.2.jar:/usr/bin/../share/java/rest-utils/jboss-logging-3.3.2.Final.jar:/usr/bin/../share/java/rest-utils/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/hibernate-validator-6.1.7.Final.jar:/usr/bin/../share/java/rest-utils/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/hk2-utils-2.6.1.jar:/usr/bin/../share/java/rest-utils/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/rest-utils/websocket-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/failureaccess-1.0.1.jar:/usr/bin/../share/java/rest-utils/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/rest-utils/javassist-3.25.0-GA.jar:/usr/bin/../share/java/rest-utils/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/rest-utils/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/rest-utils-7.1.0.jar:/usr/bin/../share/java/rest-utils/javax-websocket-server-impl-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/kafka-clients-7.1.0-ccs.jar:/usr/bin/../share/java/rest-utils/websocket-api-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/rest-utils/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/rest-utils/websocket-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jersey-server-2.34.jar:/usr/bin/../share/java/rest-utils/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/websocket-common-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/hk2-api-2.6.1.jar:/usr/bin/../share/java/rest-utils/jersey-bean-validation-2.34.jar:/usr/bin/../share/java/rest-utils/jsr305-3.0.2.jar:/usr/bin/../share/java/rest-utils/jetty-jaas-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jersey-hk2-2.34.jar:/usr/bin/../share/java/rest-utils/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/rest-utils/asm-commons-9.2.jar:/usr/bin/../share/java/rest-utils/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/javax.websocket-client-api-1.0.jar:/usr/bin/../share/java/rest-utils/jersey-client-2.34.jar:/usr/bin/../share/java/rest-utils/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/rest-utils/classmate-1.3.4.jar:/usr/bin/../share/java/rest-utils/jakarta.el-3.0.3.jar:/usr/bin/../share/java/rest-utils/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/rest-utils/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/http2-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/http2-hpack-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/rest-utils/jetty-jmx-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/http2-common-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/guava-30.1.1-jre.jar:/usr/bin/../share/java/rest-utils/jackson-core-2.12.3.jar:/usr/bin/../share/java/rest-utils/jetty-annotations-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/rest-utils/javax.websocket-api-1.0.jar:/usr/bin/../share/java/rest-utils/jetty-plus-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jaxb-api-2.3.0.jar:/usr/bin/../share/java/rest-utils/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/rest-utils/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/confluent-common/common-utils-7.1.0.jar:/usr/bin/../share/java/confluent-common/common-config-7.1.0.jar:/usr/bin/../share/java/confluent-common/build-tools-7.1.0.jar:/usr/bin/../share/java/confluent-common/common-metrics-7.1.0.jar:/usr/bin/../share/java/confluent-common/slf4j-api-1.7.30.jar:/usr/bin/../share/java/ce-kafka-http-server/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/common-utils-7.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/lz4-java-1.8.0.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-tree-9.2.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-analysis-9.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/ce-kafka-http-server/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/ce-kafka-http-server/checker-qual-3.8.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-alpn-java-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-jndi-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-xml-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-webapp-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-common-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.el-api-4.0.0.jar:/usr/bin/../share/java/ce-kafka-http-server/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/ce-kafka-http-server/javax-websocket-client-impl-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/hk2-locator-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/ce-kafka-http-server/j2objc-annotations-1.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-client-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-databind-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-alpn-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/activation-1.1.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-9.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jboss-logging-3.3.2.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/hibernate-validator-6.1.7.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/hk2-utils-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/failureaccess-1.0.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/javassist-3.25.0-GA.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/rest-utils-7.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/javax-websocket-server-impl-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-api-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-resolver-4.1.73
connect                            | .Final.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-server-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-common-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/hk2-api-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/ce-kafka-http-server-7.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-bean-validation-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/jsr305-3.0.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-jaas-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-hk2-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-commons-9.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.websocket-client-api-1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-client-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/classmate-1.3.4.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.el-3.0.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/http2-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/slf4j-api-1.7.30.jar:/usr/bin/../share/java/ce-kafka-http-server/http2-hpack-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-jmx-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/http2-common-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/guava-30.1.1-jre.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-core-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-annotations-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.websocket-api-1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-plus-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jaxb-api-2.3.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/ce-kafka-rest-servlet/ce-kafka-rest-servlet-7.1.0.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/common-utils-7.1.0.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/lz4-java-1.8.0.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/ce-kafka-rest-extensions-7.1.0.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/slf4j-api-1.7.30.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/kafka-rest-lib/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-server-common-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-jdk8-1.5.31.jar:/usr/bin/../share/java/kafka-rest-lib/commons-logging-1.2.jar:/usr/bin/../share/java/kafka-rest-lib/scala-reflect-2.13.5.jar:/usr/bin/../share/java/kafka-rest-lib/confluent-log4j-1.2.17-cp10.jar:/usr/bin/../share/java/kafka-rest-lib/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/common-utils-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-compiler-embeddable-1.3.50.jar:/usr/bin/../share/java/kafka-rest-lib/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-jvm-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/common-config-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/kafka-rest-lib/checker-qual-3.8.0.jar:/usr/bin/../share/java/kafka-rest-lib/annotations-13.0.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-avro-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka-rest-lib/commons-collections-3.2.2.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-common-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/proto-google-common-protos-2.5.1.jar:/usr/bin/../share/java/kafka-rest-lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/kafka-rest-lib/okio-jvm-3.0.0.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-common-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-json-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-schema-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/scala-collection-compat_2.13-2.4.4.jar:/usr/bin/../share/java/kafka-rest-lib/scala-library-2.13.5.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-rest-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/j2objc-annotations-1.3.jar:/usr/bin/../share/java/kafka-rest-lib/json-20201115.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-storage-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-metadata-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-raft-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/org.everit.json.schema-1.12.2.jar:/usr/bin/../share/java/kafka-rest-lib/paranamer-2.8.jar:/usr/bin/../share/java/kafka-rest-lib/scala-java8-compat_2.13-1.0.0.jar:/usr/bin/../share/java/kafka-rest-lib/commons-validator-1.6.jar:/usr/bin/../share/java/kafka-rest-lib/jose4j-0.7.8.jar:/usr/bin/../share/java/kafka-rest-lib/joda-time-2.10.8.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-json-schema-provider-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-guava-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-storage-api-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/spotbugs-annotations-4.3.0.jar:/usr/bin/../share/java/kafka-rest-lib/wire-runtime-jvm-4.0.0.jar:/usr/bin/../share/java/kafka-rest-lib/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka-rest-lib/wire-schema-jvm-4.0.0.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-dataformat-csv-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/commons-digester-1.8.1.jar:/usr/bin/../share/java/kafka-rest-lib/commons-cli-1.4.jar:/usr/bin/../share/java/kafka-rest-lib/swagger-annotations-2.1.10.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-protobuf-types-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/failureaccess-1.0.1.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-protobuf-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/vavr-match-0.10.2.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-schema-registry-client-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-clients-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka-rest-lib/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-protobuf-provider-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka-rest-lib/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka-rest-lib/scala-logging_2.13-3.9.3.jar:/usr/bin/../share/java/kafka-rest-lib/logredactor-metrics-1.0.10.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-json-schema-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/kafka-re
connect                            | st-lib/jsr305-3.0.2.jar:/usr/bin/../share/java/kafka-rest-lib/validation-api-2.0.1.Final.jar:/usr/bin/../share/java/kafka-rest-lib/resilience4j-ratelimiter-1.7.1.jar:/usr/bin/../share/java/kafka-rest-lib/avro-1.11.0.jar:/usr/bin/../share/java/kafka-rest-lib/protobuf-java-3.17.3.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-jdk7-1.5.31.jar:/usr/bin/../share/java/kafka-rest-lib/auto-value-annotations-1.7.2.jar:/usr/bin/../share/java/kafka-rest-lib/gson-2.8.6.jar:/usr/bin/../share/java/kafka-rest-lib/kotlinx-coroutines-core-1.3.7.jar:/usr/bin/../share/java/kafka-rest-lib/resilience4j-core-1.7.1.jar:/usr/bin/../share/java/kafka-rest-lib/minimal-json-0.9.5.jar:/usr/bin/../share/java/kafka-rest-lib/re2j-1.6.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-module-parameter-names-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/kafka_2.13-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/kafka-rest-lib/handy-uri-templates-2.1.8.jar:/usr/bin/../share/java/kafka-rest-lib/guava-30.1.1-jre.jar:/usr/bin/../share/java/kafka-rest-lib/logredactor-1.0.10.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-core-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-script-runtime-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-module-scala_2.13-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-jsr310-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/usr/bin/../share/java/kafka-rest-lib/vavr-0.10.2.jar:/usr/bin/../share/java/kafka-rest-lib/classgraph-4.8.21.jar:/usr/bin/../share/java/kafka-rest-lib/commons-compress-1.21.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-joda-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/snakeyaml-1.27.jar:/usr/bin/../share/java/confluent-security/kafka-rest/cloudevents-kafka-2.0.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-runtime-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/lz4-java-1.8.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/telemetry-client-1.745.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/security-extensions-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-handler-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-codec-http-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/checker-qual-3.8.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/metrics-core-2.2.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/maven-artifact-3.8.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-common-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-transport-native-kqueue-4.1.65.Final-osx-x86_64.jar:/usr/bin/../share/java/confluent-security/kafka-rest/cloudevents-core-2.0.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/confluent-security/kafka-rest/telemetry-events-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-transport-native-epoll-4.1.65.Final-linux-x86_64.jar:/usr/bin/../share/java/confluent-security/kafka-rest/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/hk2-locator-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javax.json-1.0.4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-json-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/bcprov-jdk15on-1.68.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-security-plugins-common-7.1.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/reflections-0.9.12.jar:/usr/bin/../share/java/confluent-security/kafka-rest/j2objc-annotations-1.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-dataformat-yaml-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/antlr-runtime-3.5.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-databind-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/activation-1.1.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-api-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/confluent-security/kafka-rest/cloudevents-api-2.0.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jose4j-0.7.8.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/plexus-utils-3.2.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-tools-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jbcrypt-0.4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/hk2-utils-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/authorizer-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/cloudevents-json-jackson-2.0.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/bcpkix-jdk15on-1.68.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-tcnative-classes-2.0.46.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/swagger-annotations-2.1.10.jar:/usr/bin/../share/java/confluent-security/kafka-rest/failureaccess-1.0.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javassist-3.25.0-GA.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-serializers-new-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-schema-registry-client-7.1.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-transforms-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-licensing-new-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/argparse4j-0.7.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/icu4j-61.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-log4j-appender-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/commons-lang3-3.12.0.jar:/usr/bin/../share/java/confluent-security/kafk
connect                            | a-rest/javax.servlet-api-4.0.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-server-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/rest-authorizer-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/hk2-api-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jsr305-3.0.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-resource-names-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/telemetry-api-1.745.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/avro-1.11.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/protobuf-java-3.17.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-codec-socks-4.1.65.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-hk2-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-kafka-rest-security-plugin-7.1.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-client-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/antlr4-4.9.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/gson-2.8.6.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-transport-native-unix-common-4.1.65.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-ce-logs-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-handler-proxy-4.1.65.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/ST4-4.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/org.abego.treelayout.core-1.0.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/antlr4-runtime-4.9.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/guava-30.1.1-jre.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-core-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-client-plugins-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/annotations-3.0.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jaxb-api-2.3.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/commons-compress-1.21.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/opencensus-proto-0.2.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/broker-plugins-7.1.0-ce-test.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-jdk8-1.5.31.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-logging-1.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/common-utils-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-compiler-embeddable-1.3.50.jar:/usr/bin/../share/java/confluent-security/schema-validator/lz4-java-1.8.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-jvm-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/schema-validator/checker-qual-3.8.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/annotations-13.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-avro-serializer-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-collections-3.2.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-common-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/proto-google-common-protos-2.5.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/confluent-security/schema-validator/okio-jvm-3.0.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-common-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-schema-serializer-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/scala-library-2.13.5.jar:/usr/bin/../share/java/confluent-security/schema-validator/j2objc-annotations-1.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/json-20201115.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-databind-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/org.everit.json.schema-1.12.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-validator-1.6.jar:/usr/bin/../share/java/confluent-security/schema-validator/joda-time-2.10.8.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-json-schema-provider-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-guava-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/wire-runtime-jvm-4.0.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/wire-schema-jvm-4.0.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-digester-1.8.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/swagger-annotations-2.1.10.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-protobuf-types-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/failureaccess-1.0.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-schema-registry-client-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/caffeine-2.8.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-protobuf-provider-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/jsr305-3.0.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/validation-api-2.0.1.Final.jar:/usr/bin/../share/java/confluent-security/schema-validator/avro-1.11.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/protobuf-java-3.17.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-jdk7-1.5.31.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/gson-2.8.6.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlinx-coroutines-core-1.3.7.jar:/usr/bin/../share/java/confluent-security/schema-validator/confluent-schema-registry-validator-plugin-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/re2j-1.6.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-module-parameter-names-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/confluent-security/schema-validator/handy-uri-templates-2.1.8.jar:/usr/bin/../share/java/confluent-security/schema-validator/guav
connect                            | a-30.1.1-jre.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-core-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-script-runtime-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-jsr310-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/usr/bin/../share/java/confluent-security/schema-validator/classgraph-4.8.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-compress-1.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-joda-2.12.3.jar:/usr/bin/../support-metrics-client/build/dependant-libs-2.13.6/*:/usr/bin/../support-metrics-client/build/libs/*:/usr/bin/../share/java/confluent-telemetry/confluent-metrics-7.1.0-ce.jar:/usr/share/java/support-metrics-client/*
connect                            | 	os.spec = Linux, amd64, 5.19.0-50-generic
connect                            | 	os.vcpus = 4
connect                            |  (org.apache.kafka.connect.runtime.WorkerInfo)
connect                            | [2023-08-04 10:10:29,715] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectDistributed)
rest-proxy                         | [2023-08-04 10:10:30,033] INFO Adding listener with HTTP/2: NamedURI{uri=http://0.0.0.0:8082, name='null'} (io.confluent.rest.ApplicationServer)
connect                            | [2023-08-04 10:10:30,079] INFO Loading plugin from: /usr/share/java/cp-base-new (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
schema-registry                    | [2023-08-04 10:10:31,130] INFO Adding listener with HTTP/2: NamedURI{uri=http://0.0.0.0:8081, name='null'} (io.confluent.rest.ApplicationServer)
control-center                     | SLF4J: Class path contains multiple SLF4J bindings.
control-center                     | SLF4J: Found binding in [jar:file:/usr/share/java/acl/acl-7.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
control-center                     | SLF4J: Found binding in [jar:file:/usr/share/java/confluent-control-center/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
control-center                     | 
control-center                     | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
control-center                     | 
ksqldb-server                      | [2023-08-04 10:10:33,067] INFO KsqlConfig values: 
ksqldb-server                      | 	ksql.access.validator.enable = auto
ksqldb-server                      | 	ksql.assert.schema.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.assert.topic.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.authorization.cache.expiry.time.secs = 30
ksqldb-server                      | 	ksql.authorization.cache.max.entries = 10000
ksqldb-server                      | 	ksql.cast.strings.preserve.nulls = true
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.file = 
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.reload = false
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.source = NONE
ksqldb-server                      | 	ksql.connect.request.headers.plugin = null
ksqldb-server                      | 	ksql.connect.request.timeout.ms = 5000
ksqldb-server                      | 	ksql.connect.url = http://connect:8083
ksqldb-server                      | 	ksql.connect.worker.config = 
ksqldb-server                      | 	ksql.create.or.replace.enabled = true
ksqldb-server                      | 	ksql.endpoint.migrate.query = true
ksqldb-server                      | 	ksql.error.classifier.regex = 
ksqldb-server                      | 	ksql.extension.dir = ext
ksqldb-server                      | 	ksql.headers.columns.enabled = true
ksqldb-server                      | 	ksql.hidden.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.insert.into.values.enabled = true
ksqldb-server                      | 	ksql.internal.topic.min.insync.replicas = 1
ksqldb-server                      | 	ksql.internal.topic.replicas = 1
ksqldb-server                      | 	ksql.json_sr.converter.deserializer.enabled = true
ksqldb-server                      | 	ksql.lambdas.enabled = true
ksqldb-server                      | 	ksql.metastore.backup.location = 
ksqldb-server                      | 	ksql.metrics.extension = null
ksqldb-server                      | 	ksql.metrics.tags.custom = 
ksqldb-server                      | 	ksql.nested.error.set.null = true
ksqldb-server                      | 	ksql.new.query.planner.enabled = false
ksqldb-server                      | 	ksql.output.topic.name.prefix = 
ksqldb-server                      | 	ksql.persistence.default.format.key = KAFKA
ksqldb-server                      | 	ksql.persistence.default.format.value = null
ksqldb-server                      | 	ksql.persistence.wrap.single.values = null
ksqldb-server                      | 	ksql.persistent.prefix = query_
ksqldb-server                      | 	ksql.properties.overrides.denylist = []
ksqldb-server                      | 	ksql.pull.queries.enable = true
ksqldb-server                      | 	ksql.query.cleanup.shutdown.timeout.ms = 30000
ksqldb-server                      | 	ksql.query.error.max.queue.size = 10
ksqldb-server                      | 	ksql.query.persistent.active.limit = 2147483647
ksqldb-server                      | 	ksql.query.persistent.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.query.pull.enable.standby.reads = false
ksqldb-server                      | 	ksql.query.pull.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.pull.limit.clause.enabled = true
ksqldb-server                      | 	ksql.query.pull.max.allowed.offset.lag = 9223372036854775807
ksqldb-server                      | 	ksql.query.pull.max.concurrent.requests = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.qps = 2147483647
ksqldb-server                      | 	ksql.query.pull.metrics.enabled = true
ksqldb-server                      | 	ksql.query.pull.range.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.router.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.pull.stream.enabled = true
ksqldb-server                      | 	ksql.query.pull.table.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.push.v2.alos.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.catchup.consumer.msg.window = 50
ksqldb-server                      | 	ksql.query.push.v2.continuation.tokens.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.latest.reset.age.ms = 30000
ksqldb-server                      | 	ksql.query.push.v2.max.catchup.consumers = 5
ksqldb-server                      | 	ksql.query.push.v2.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.push.v2.metrics.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.new.latest.delay.ms = 5000
ksqldb-server                      | 	ksql.query.push.v2.registry.installed = false
ksqldb-server                      | 	ksql.query.retry.backoff.initial.ms = 15000
ksqldb-server                      | 	ksql.query.retry.backoff.max.ms = 900000
ksqldb-server                      | 	ksql.query.status.running.threshold.seconds = 300
ksqldb-server                      | 	ksql.query.transient.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.queryanonymizer.cluster_namespace = null
ksqldb-server                      | 	ksql.queryanonymizer.logs_enabled = true
ksqldb-server                      | 	ksql.readonly.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.runtime.feature.shared.enabled = false
ksqldb-server                      | 	ksql.schema.registry.url = http://schema-registry:8081
ksqldb-server                      | 	ksql.security.extension.class = null
ksqldb-server                      | 	ksql.service.id = default_
ksqldb-server                      | 	ksql.shared.runtimes.count = 2
ksqldb-server                      | 	ksql.sink.window.change.log.additional.retention = 1000000
ksqldb-server                      | 	ksql.source.table.materialization.enabled = true
ksqldb-server                      | 	ksql.streams.shutdown.timeout.ms = 300000
ksqldb-server                      | 	ksql.suppress.buffer.size.bytes = -1
ksqldb-server                      | 	ksql.suppress.enabled = true
ksqldb-server                      | 	ksql.timestamp.throw.on.invalid = false
ksqldb-server                      | 	ksql.transient.prefix = transient_
ksqldb-server                      | 	ksql.transient.query.cleanup.service.enable = true
ksqldb-server                      | 	ksql.transient.query.cleanup.service.initial.delay.seconds = 600
ksqldb-server                      | 	ksql.transient.query.cleanup.service.period.seconds = 600
ksqldb-server                      | 	ksql.udf.collect.metrics = false
ksqldb-server                      | 	ksql.udf.enable.security.manager = true
ksqldb-server                      | 	ksql.udfs.enabled = true
ksqldb-server                      | 	ksql.variable.substitution.enable = true
ksqldb-server                      | 	ksql.websocket.connection.max.timeout.ms = 3600000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.util.KsqlConfig)
control-center                     | SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]
control-center                     | [2023-08-04 10:10:34,237] WARN Invalid value 1 for configuration confluent.controlcenter.internal.topics.replication: Value must be at least 3 (io.confluent.controlcenter.ControlCenterConfig)
control-center                     | [2023-08-04 10:10:34,250] WARN Invalid value 1 for configuration confluent.controlcenter.internal.topics.replication: Value must be at least 3 (io.confluent.controlcenter.ControlCenterConfig)
control-center                     | [2023-08-04 10:10:34,255] INFO ControlCenterConfig values: 
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	confluent.controlcenter.alert.cluster.down.autocreate = false
control-center                     | 	confluent.controlcenter.alert.cluster.down.send.rate = 12
control-center                     | 	confluent.controlcenter.alert.cluster.down.to.email = 
control-center                     | 	confluent.controlcenter.alert.cluster.down.to.pagerduty.integrationkey = 
control-center                     | 	confluent.controlcenter.alert.cluster.down.to.webhookurl.slack = 
control-center                     | 	confluent.controlcenter.alert.max.trigger.events = 1000
control-center                     | 	confluent.controlcenter.armeria.healthcheck.force.http1 = false
control-center                     | 	confluent.controlcenter.auth.bearer.issuer = Confluent
control-center                     | 	confluent.controlcenter.auth.bearer.roles.claim = 
control-center                     | 	confluent.controlcenter.auth.restricted.roles = []
control-center                     | 	confluent.controlcenter.auth.session.expiration.ms = 0
control-center                     | 	confluent.controlcenter.broker.config.edit.enable = true
control-center                     | 	confluent.controlcenter.command.streams.start.timeout = 300000
control-center                     | 	confluent.controlcenter.command.topic = _confluent-command
control-center                     | 	confluent.controlcenter.command.topic.replication = 1
control-center                     | 	confluent.controlcenter.command.topic.retention.ms = 86400000
control-center                     | 	confluent.controlcenter.command.topic.segment.bytes = 1073741824
control-center                     | 	confluent.controlcenter.connect.connect.alias.name = 
control-center                     | 	confluent.controlcenter.connect.healthcheck.endpoint = /v1/metadata/id
control-center                     | 	confluent.controlcenter.consumer.metadata.timeout.ms = 15000
control-center                     | 	confluent.controlcenter.consumers.view.enable = true
control-center                     | 	confluent.controlcenter.data.dir = /var/lib/confluent-control-center
control-center                     | 	confluent.controlcenter.deprecated.views.enable = false
control-center                     | 	confluent.controlcenter.disk.skew.warning.min.bytes = 1073741824
control-center                     | 	confluent.controlcenter.embedded.kafkarest.enable = true
control-center                     | 	confluent.controlcenter.hostedmonitoring.enable = false
control-center                     | 	confluent.controlcenter.id = 1
control-center                     | 	confluent.controlcenter.internal.streams.start.timeout = 21600000
control-center                     | 	confluent.controlcenter.internal.topics.changelog.segment.bytes = 134217728
control-center                     | 	confluent.controlcenter.internal.topics.partitions = 1
control-center                     | 	confluent.controlcenter.internal.topics.replication = 1
control-center                     | 	confluent.controlcenter.internal.topics.retention.bytes = -1
control-center                     | 	confluent.controlcenter.internal.topics.retention.ms = 604800000
control-center                     | 	confluent.controlcenter.ksql.enable = true
control-center                     | 	confluent.controlcenter.ksql.ksql.alias.name = 
control-center                     | 	confluent.controlcenter.license.manager = _confluent-controlcenter-license-manager-7-4-1
control-center                     | 	confluent.controlcenter.license.manager.enable = true
control-center                     | 	confluent.controlcenter.mail.bounce.address = 
control-center                     | 	confluent.controlcenter.mail.enabled = false
control-center                     | 	confluent.controlcenter.mail.from = c3@confluent.io
control-center                     | 	confluent.controlcenter.mail.host.name = localhost
control-center                     | 	confluent.controlcenter.mail.password = [hidden]
control-center                     | 	confluent.controlcenter.mail.port = 587
control-center                     | 	confluent.controlcenter.mail.ssl.checkserveridentity = false
control-center                     | 	confluent.controlcenter.mail.ssl.port = 465
control-center                     | 	confluent.controlcenter.mail.starttls.required = false
control-center                     | 	confluent.controlcenter.mail.username = 
control-center                     | 	confluent.controlcenter.mds.client.idle.timeout = null
control-center                     | 	confluent.controlcenter.mds.client.max.requests.queued.per.destination = null
control-center                     | 	confluent.controlcenter.mode.enable = all
control-center                     | 	confluent.controlcenter.name = _confluent-controlcenter
control-center                     | 	confluent.controlcenter.private.installer.id = 
control-center                     | 	confluent.controlcenter.proactive.support.ui.cta.enable = true
control-center                     | 	confluent.controlcenter.purge.stale.cluster.enable = false
control-center                     | 	confluent.controlcenter.request.buffer.size.bytes = 10000
control-center                     | 	confluent.controlcenter.rest.advertised.url = 
control-center                     | 	confluent.controlcenter.rest.compression.enable = true
control-center                     | 	confluent.controlcenter.rest.csrf.prevention.enable = false
control-center                     | 	confluent.controlcenter.rest.csrf.prevention.token.endpoint = /csrf
control-center                     | 	confluent.controlcenter.rest.csrf.prevention.token.expiration.minutes = 30
control-center                     | 	confluent.controlcenter.rest.hsts.enable = true
control-center                     | 	confluent.controlcenter.rest.nosniff.prevention.enable = true
control-center                     | 	confluent.controlcenter.rest.port = 9021
control-center                     | 	confluent.controlcenter.rest.proxy.alias.name = 
control-center                     | 	confluent.controlcenter.sbk.ui.enable = true
control-center                     | 	confluent.controlcenter.schema.registry.enable = true
control-center                     | 	confluent.controlcenter.schema.registry.schema.registry.alias.name = 
control-center                     | 	confluent.controlcenter.schema.registry.url = [http://schema-registry:8081]
control-center                     | 	confluent.controlcenter.service.healthcheck.interval.sec = 20
control-center                     | 	confluent.controlcenter.streams.cache.max.bytes.buffering = 1073741824
control-center                     | 	confluent.controlcenter.streams.consumer.session.timeout.ms = 60000
control-center                     | 	confluent.controlcenter.streams.num.stream.threads = 12
control-center                     | 	confluent.controlcenter.streams.producer.compression.type = lz4
control-center                     | 	confluent.controlcenter.streams.producer.delivery.timeout.ms = 2147483647
control-center                     | 	confluent.controlcenter.streams.producer.linger.ms = 500
control-center                     | 	confluent.controlcenter.streams.producer.max.block.ms = 9223372036854775807
control-center                     | 	confluent.controlcenter.streams.producer.retries = 2147483647
control-center                     | 	confluent.controlcenter.streams.producer.retry.backoff.ms = 100
control-center                     | 	confluent.controlcenter.streams.task.timeout.ms = 0
control-center                     | 	confluent.controlcenter.streams.upgrade.from = 2.3
control-center                     | 	confluent.controlcenter.topic.inspection.enable = true
control-center                     | 	confluent.controlcenter.topic.inspection.message.max.bytes = 1048576
control-center                     | 	confluent.controlcenter.trigger.active-controller-count.enable = false
control-center                     | 	confluent.controlcenter.ui.acl.kafkarest.enable = false
control-center                     | 	confluent.controlcenter.ui.autoupdate.enable = true
control-center                     | 	confluent.controlcenter.ui.basepath = /
control-center                     | 	confluent.controlcenter.ui.broker.kafkarest.enable = false
control-center                     | 	confluent.controlcenter.ui.brokersettings.kafkarest.enable = true
control-center                     | 	confluent.controlcenter.ui.consumer.group.kafkarest.enable = false
control-center                     | 	confluent.controlcenter.ui.controller.chart.enable = false
control-center                     | 	confluent.controlcenter.ui.data.expired.threshold = 120
control-center                     | 	confluent.controlcenter.ui.external.css.files = null
control-center                     | 	confluent.controlcenter.ui.external.js.files = null
control-center                     | 	confluent.controlcenter.ui.replicator.monitoring.enable = true
control-center                     | 	confluent.controlcenter.ui.topic.kafkarest.enable = false
control-center                     | 	confluent.controlcenter.usage.data.collection.enable = true
control-center                     | 	confluent.controlcenter.use.default.jvm.truststore = false
control-center                     | 	confluent.controlcenter.use.default.os.truststore = false
control-center                     | 	confluent.controlcenter.webhook.enabled = true
control-center                     | 	confluent.license = 
control-center                     | 	confluent.metadata.basic.auth.user.info = [hidden]
control-center                     | 	confluent.metadata.bootstrap.server.urls = []
control-center                     | 	confluent.metadata.cluster.registry.enable = false
control-center                     | 	confluent.metadata.cluster.registry.merge.configuration.enable = true
control-center                     | 	confluent.metrics.broker.count.staleness.threshold.ms = 120000
control-center                     | 	confluent.metrics.topic = _confluent-metrics
control-center                     | 	confluent.metrics.topic.config.validate = false
control-center                     | 	confluent.metrics.topic.max.message.bytes = 10485760
control-center                     | 	confluent.metrics.topic.partitions = 12
control-center                     | 	confluent.metrics.topic.replication = 1
control-center                     | 	confluent.metrics.topic.retention.bytes = -1
control-center                     | 	confluent.metrics.topic.retention.ms = 259200000
control-center                     | 	confluent.metrics.topic.skip.backlog.minutes = 15
control-center                     | 	confluent.monitoring.interceptor.topic = _confluent-monitoring
control-center                     | 	confluent.monitoring.interceptor.topic.config.validate = false
control-center                     | 	confluent.monitoring.interceptor.topic.partitions = 1
control-center                     | 	confluent.monitoring.interceptor.topic.replication = 1
control-center                     | 	confluent.monitoring.interceptor.topic.retention.bytes = -1
control-center                     | 	confluent.monitoring.interceptor.topic.retention.ms = 259200000
control-center                     | 	confluent.monitoring.interceptor.topic.skip.backlog.minutes = 15
control-center                     | 	confluent.support.metrics.enable = true
control-center                     | 	confluent.support.metrics.segment.endpoint = https://analytics-api.confluent.io
control-center                     | 	confluent.support.metrics.segment.id = MORqDG61F2eE5mfxAXVqpEblmFG18nbv
control-center                     | 	public.key.path = 
control-center                     | 	zookeeper.connect = 
control-center                     |  (io.confluent.controlcenter.ControlCenterConfig)
ksqldb-server                      | [2023-08-04 10:10:34,259] INFO KsqlRestConfig values: 
ksqldb-server                      | 	access.control.allow.headers = []
ksqldb-server                      | 	access.control.allow.methods = []
ksqldb-server                      | 	access.control.allow.origin = 
ksqldb-server                      | 	authentication.method = NONE
ksqldb-server                      | 	authentication.realm = 
ksqldb-server                      | 	authentication.roles = [*]
ksqldb-server                      | 	authentication.skip.paths = []
ksqldb-server                      | 	ksql.advertised.listener = null
ksqldb-server                      | 	ksql.authentication.plugin.class = null
ksqldb-server                      | 	ksql.endpoint.logging.ignored.paths.regex = 
ksqldb-server                      | 	ksql.endpoint.logging.log.queries = false
ksqldb-server                      | 	ksql.healthcheck.interval.ms = 5000
ksqldb-server                      | 	ksql.heartbeat.check.interval.ms = 200
ksqldb-server                      | 	ksql.heartbeat.discover.interval.ms = 2000
ksqldb-server                      | 	ksql.heartbeat.enable = false
ksqldb-server                      | 	ksql.heartbeat.missed.threshold.ms = 3
ksqldb-server                      | 	ksql.heartbeat.send.interval.ms = 100
ksqldb-server                      | 	ksql.heartbeat.thread.pool.size = 3
ksqldb-server                      | 	ksql.heartbeat.window.ms = 2000
ksqldb-server                      | 	ksql.idle.connection.timeout.seconds = 86400
ksqldb-server                      | 	ksql.internal.http2.max.pool.size = 3000
ksqldb-server                      | 	ksql.internal.listener = null
ksqldb-server                      | 	ksql.internal.ssl.client.authentication = NONE
ksqldb-server                      | 	ksql.lag.reporting.enable = false
ksqldb-server                      | 	ksql.lag.reporting.send.interval.ms = 5000
ksqldb-server                      | 	ksql.local.commands.location = 
ksqldb-server                      | 	ksql.logging.server.rate.limited.request.paths = 
ksqldb-server                      | 	ksql.logging.server.rate.limited.response.codes = 
ksqldb-server                      | 	ksql.max.push.queries = 100
ksqldb-server                      | 	ksql.server.command.blocked.threshold.error.ms = 15000
ksqldb-server                      | 	ksql.server.command.response.timeout.ms = 5000
ksqldb-server                      | 	ksql.server.command.topic.migration.enabled = NONE
ksqldb-server                      | 	ksql.server.command.topic.rate.limit = 1.7976931348623157E308
ksqldb-server                      | 	ksql.server.error.messages = class io.confluent.ksql.rest.DefaultErrorMessages
ksqldb-server                      | 	ksql.server.exception.uncaught.handler.enable = false
ksqldb-server                      | 	ksql.server.install.dir = /usr
ksqldb-server                      | 	ksql.server.precondition.max.backoff.ms = 5000
ksqldb-server                      | 	ksql.server.preconditions = []
ksqldb-server                      | 	ksql.server.sni.check.enable = false
ksqldb-server                      | 	ksql.server.websockets.num.threads = 5
ksqldb-server                      | 	ksql.ssl.keystore.alias.external = 
ksqldb-server                      | 	ksql.ssl.keystore.alias.internal = 
ksqldb-server                      | 	ksql.verticle.instances = 8
ksqldb-server                      | 	ksql.worker.pool.size = 100
ksqldb-server                      | 	listeners = [http://0.0.0.0:8088]
ksqldb-server                      | 	query.stream.disconnect.check = 1000
ksqldb-server                      | 	ssl.cipher.suites = []
ksqldb-server                      | 	ssl.client.auth = false
ksqldb-server                      | 	ssl.client.authentication = NONE
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.key.password = [hidden]
ksqldb-server                      | 	ssl.keystore.location = 
ksqldb-server                      | 	ssl.keystore.password = [hidden]
ksqldb-server                      | 	ssl.keystore.reload = false
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.keystore.watch.location = 
ksqldb-server                      | 	ssl.truststore.location = 
ksqldb-server                      | 	ssl.truststore.password = [hidden]
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.rest.server.KsqlRestConfig)
ksqldb-server                      | [2023-08-04 10:10:34,302] INFO KsqlConfig values: 
ksqldb-server                      | 	ksql.access.validator.enable = auto
ksqldb-server                      | 	ksql.assert.schema.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.assert.topic.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.authorization.cache.expiry.time.secs = 30
ksqldb-server                      | 	ksql.authorization.cache.max.entries = 10000
ksqldb-server                      | 	ksql.cast.strings.preserve.nulls = true
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.file = 
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.reload = false
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.source = NONE
ksqldb-server                      | 	ksql.connect.request.headers.plugin = null
ksqldb-server                      | 	ksql.connect.request.timeout.ms = 5000
ksqldb-server                      | 	ksql.connect.url = http://connect:8083
ksqldb-server                      | 	ksql.connect.worker.config = 
ksqldb-server                      | 	ksql.create.or.replace.enabled = true
ksqldb-server                      | 	ksql.endpoint.migrate.query = true
ksqldb-server                      | 	ksql.error.classifier.regex = 
ksqldb-server                      | 	ksql.extension.dir = ext
ksqldb-server                      | 	ksql.headers.columns.enabled = true
ksqldb-server                      | 	ksql.hidden.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.insert.into.values.enabled = true
ksqldb-server                      | 	ksql.internal.topic.min.insync.replicas = 1
ksqldb-server                      | 	ksql.internal.topic.replicas = 1
ksqldb-server                      | 	ksql.json_sr.converter.deserializer.enabled = true
ksqldb-server                      | 	ksql.lambdas.enabled = true
ksqldb-server                      | 	ksql.metastore.backup.location = 
ksqldb-server                      | 	ksql.metrics.extension = null
ksqldb-server                      | 	ksql.metrics.tags.custom = 
ksqldb-server                      | 	ksql.nested.error.set.null = true
ksqldb-server                      | 	ksql.new.query.planner.enabled = false
ksqldb-server                      | 	ksql.output.topic.name.prefix = 
ksqldb-server                      | 	ksql.persistence.default.format.key = KAFKA
ksqldb-server                      | 	ksql.persistence.default.format.value = null
ksqldb-server                      | 	ksql.persistence.wrap.single.values = null
ksqldb-server                      | 	ksql.persistent.prefix = query_
ksqldb-server                      | 	ksql.properties.overrides.denylist = []
ksqldb-server                      | 	ksql.pull.queries.enable = true
ksqldb-server                      | 	ksql.query.cleanup.shutdown.timeout.ms = 30000
ksqldb-server                      | 	ksql.query.error.max.queue.size = 10
ksqldb-server                      | 	ksql.query.persistent.active.limit = 2147483647
ksqldb-server                      | 	ksql.query.persistent.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.query.pull.enable.standby.reads = false
ksqldb-server                      | 	ksql.query.pull.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.pull.limit.clause.enabled = true
ksqldb-server                      | 	ksql.query.pull.max.allowed.offset.lag = 9223372036854775807
ksqldb-server                      | 	ksql.query.pull.max.concurrent.requests = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.qps = 2147483647
ksqldb-server                      | 	ksql.query.pull.metrics.enabled = true
ksqldb-server                      | 	ksql.query.pull.range.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.router.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.pull.stream.enabled = true
ksqldb-server                      | 	ksql.query.pull.table.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.push.v2.alos.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.catchup.consumer.msg.window = 50
ksqldb-server                      | 	ksql.query.push.v2.continuation.tokens.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.latest.reset.age.ms = 30000
ksqldb-server                      | 	ksql.query.push.v2.max.catchup.consumers = 5
ksqldb-server                      | 	ksql.query.push.v2.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.push.v2.metrics.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.new.latest.delay.ms = 5000
ksqldb-server                      | 	ksql.query.push.v2.registry.installed = false
ksqldb-server                      | 	ksql.query.retry.backoff.initial.ms = 15000
ksqldb-server                      | 	ksql.query.retry.backoff.max.ms = 900000
ksqldb-server                      | 	ksql.query.status.running.threshold.seconds = 300
ksqldb-server                      | 	ksql.query.transient.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.queryanonymizer.cluster_namespace = null
ksqldb-server                      | 	ksql.queryanonymizer.logs_enabled = true
ksqldb-server                      | 	ksql.readonly.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.runtime.feature.shared.enabled = false
ksqldb-server                      | 	ksql.schema.registry.url = http://schema-registry:8081
ksqldb-server                      | 	ksql.security.extension.class = null
ksqldb-server                      | 	ksql.service.id = default_
ksqldb-server                      | 	ksql.shared.runtimes.count = 2
ksqldb-server                      | 	ksql.sink.window.change.log.additional.retention = 1000000
ksqldb-server                      | 	ksql.source.table.materialization.enabled = true
ksqldb-server                      | 	ksql.streams.shutdown.timeout.ms = 300000
ksqldb-server                      | 	ksql.suppress.buffer.size.bytes = -1
ksqldb-server                      | 	ksql.suppress.enabled = true
ksqldb-server                      | 	ksql.timestamp.throw.on.invalid = false
ksqldb-server                      | 	ksql.transient.prefix = transient_
ksqldb-server                      | 	ksql.transient.query.cleanup.service.enable = true
ksqldb-server                      | 	ksql.transient.query.cleanup.service.initial.delay.seconds = 600
ksqldb-server                      | 	ksql.transient.query.cleanup.service.period.seconds = 600
ksqldb-server                      | 	ksql.udf.collect.metrics = false
ksqldb-server                      | 	ksql.udf.enable.security.manager = true
ksqldb-server                      | 	ksql.udfs.enabled = true
ksqldb-server                      | 	ksql.variable.substitution.enable = true
ksqldb-server                      | 	ksql.websocket.connection.max.timeout.ms = 3600000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.util.KsqlConfig)
rest-proxy                         | [2023-08-04 10:10:35,781] INFO SchemaRegistryConfig values: 
rest-proxy                         | 	auto.register.schemas = false
rest-proxy                         | 	basic.auth.credentials.source = URL
rest-proxy                         | 	basic.auth.user.info = [hidden]
rest-proxy                         | 	bearer.auth.cache.expiry.buffer.seconds = 300
rest-proxy                         | 	bearer.auth.client.id = null
rest-proxy                         | 	bearer.auth.client.secret = null
rest-proxy                         | 	bearer.auth.credentials.source = STATIC_TOKEN
rest-proxy                         | 	bearer.auth.custom.provider.class = null
rest-proxy                         | 	bearer.auth.identity.pool.id = null
rest-proxy                         | 	bearer.auth.issuer.endpoint.url = null
rest-proxy                         | 	bearer.auth.logical.cluster = null
rest-proxy                         | 	bearer.auth.scope = null
rest-proxy                         | 	bearer.auth.scope.claim.name = scope
rest-proxy                         | 	bearer.auth.sub.claim.name = sub
rest-proxy                         | 	bearer.auth.token = [hidden]
rest-proxy                         | 	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
rest-proxy                         | 	http.connect.timeout.ms = 60000
rest-proxy                         | 	http.read.timeout.ms = 60000
rest-proxy                         | 	id.compatibility.strict = true
rest-proxy                         | 	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
rest-proxy                         | 	latest.cache.size = 1000
rest-proxy                         | 	latest.cache.ttl.sec = -1
rest-proxy                         | 	latest.compatibility.strict = true
rest-proxy                         | 	max.schemas.per.subject = 1000
rest-proxy                         | 	normalize.schemas = false
rest-proxy                         | 	proxy.host = 
rest-proxy                         | 	proxy.port = -1
rest-proxy                         | 	rule.actions = []
rest-proxy                         | 	rule.executors = []
rest-proxy                         | 	rule.service.loader.enable = true
rest-proxy                         | 	schema.format = null
rest-proxy                         | 	schema.reflection = false
rest-proxy                         | 	schema.registry.basic.auth.user.info = [hidden]
rest-proxy                         | 	schema.registry.ssl.cipher.suites = null
rest-proxy                         | 	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
rest-proxy                         | 	schema.registry.ssl.endpoint.identification.algorithm = https
rest-proxy                         | 	schema.registry.ssl.engine.factory.class = null
rest-proxy                         | 	schema.registry.ssl.key.password = null
rest-proxy                         | 	schema.registry.ssl.keymanager.algorithm = SunX509
rest-proxy                         | 	schema.registry.ssl.keystore.certificate.chain = null
rest-proxy                         | 	schema.registry.ssl.keystore.key = null
rest-proxy                         | 	schema.registry.ssl.keystore.location = null
rest-proxy                         | 	schema.registry.ssl.keystore.password = null
rest-proxy                         | 	schema.registry.ssl.keystore.type = JKS
rest-proxy                         | 	schema.registry.ssl.protocol = TLSv1.3
rest-proxy                         | 	schema.registry.ssl.provider = null
rest-proxy                         | 	schema.registry.ssl.secure.random.implementation = null
rest-proxy                         | 	schema.registry.ssl.trustmanager.algorithm = PKIX
rest-proxy                         | 	schema.registry.ssl.truststore.certificates = null
rest-proxy                         | 	schema.registry.ssl.truststore.location = null
rest-proxy                         | 	schema.registry.ssl.truststore.password = null
rest-proxy                         | 	schema.registry.ssl.truststore.type = JKS
rest-proxy                         | 	schema.registry.url = [http://schema-registry:8081]
rest-proxy                         | 	use.latest.version = false
rest-proxy                         | 	use.latest.with.metadata = null
rest-proxy                         | 	use.schema.id = -1
rest-proxy                         | 	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
rest-proxy                         |  (io.confluent.kafkarest.config.SchemaRegistryConfig)
ksqldb-server                      | [2023-08-04 10:10:36,013] INFO Blacklist file: /usr/ext/resource-blacklist.txt not found. No classes will be blacklisted (io.confluent.ksql.function.Blacklist)
rest-proxy                         | [2023-08-04 10:10:36,352] WARN Empty contextPath (org.eclipse.jetty.server.handler.ContextHandler)
rest-proxy                         | [2023-08-04 10:10:36,372] INFO Binding KafkaRestApplication to all listeners. (io.confluent.rest.Application)
schema-registry                    | [2023-08-04 10:10:37,066] INFO Found internal listener: NamedURI{uri=http://0.0.0.0:8081, name='null'} (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
schema-registry                    | [2023-08-04 10:10:37,080] INFO Setting my identity to version=1,host=schema-registry,port=8081,scheme=http,leaderEligibility=true (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
rest-proxy                         | [2023-08-04 10:10:38,299] INFO jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.18+10-LTS (org.eclipse.jetty.server.Server)
rest-proxy                         | [2023-08-04 10:10:39,686] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
rest-proxy                         | [2023-08-04 10:10:39,686] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
rest-proxy                         | [2023-08-04 10:10:39,736] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session)
schema-registry                    | [2023-08-04 10:10:40,532] INFO AdminClientConfig values: 
schema-registry                    | 	auto.include.jmx.reporter = true
schema-registry                    | 	bootstrap.servers = [PLAINTEXT://broker:29092]
schema-registry                    | 	client.dns.lookup = use_all_dns_ips
schema-registry                    | 	client.id = 
schema-registry                    | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
schema-registry                    | 	confluent.proxy.protocol.client.address = null
schema-registry                    | 	confluent.proxy.protocol.client.port = null
schema-registry                    | 	confluent.proxy.protocol.client.version = NONE
schema-registry                    | 	confluent.use.controller.listener = false
schema-registry                    | 	connections.max.idle.ms = 300000
schema-registry                    | 	default.api.timeout.ms = 60000
schema-registry                    | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
schema-registry                    | 	metadata.max.age.ms = 300000
schema-registry                    | 	metric.reporters = []
schema-registry                    | 	metrics.num.samples = 2
schema-registry                    | 	metrics.recording.level = INFO
schema-registry                    | 	metrics.sample.window.ms = 30000
schema-registry                    | 	receive.buffer.bytes = 65536
schema-registry                    | 	reconnect.backoff.max.ms = 1000
schema-registry                    | 	reconnect.backoff.ms = 50
schema-registry                    | 	request.timeout.ms = 30000
schema-registry                    | 	retries = 2147483647
schema-registry                    | 	retry.backoff.ms = 100
schema-registry                    | 	sasl.client.callback.handler.class = null
schema-registry                    | 	sasl.jaas.config = null
schema-registry                    | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
schema-registry                    | 	sasl.kerberos.min.time.before.relogin = 60000
schema-registry                    | 	sasl.kerberos.service.name = null
schema-registry                    | 	sasl.kerberos.ticket.renew.jitter = 0.05
schema-registry                    | 	sasl.kerberos.ticket.renew.window.factor = 0.8
schema-registry                    | 	sasl.login.callback.handler.class = null
schema-registry                    | 	sasl.login.class = null
schema-registry                    | 	sasl.login.connect.timeout.ms = null
schema-registry                    | 	sasl.login.read.timeout.ms = null
schema-registry                    | 	sasl.login.refresh.buffer.seconds = 300
schema-registry                    | 	sasl.login.refresh.min.period.seconds = 60
schema-registry                    | 	sasl.login.refresh.window.factor = 0.8
schema-registry                    | 	sasl.login.refresh.window.jitter = 0.05
schema-registry                    | 	sasl.login.retry.backoff.max.ms = 10000
schema-registry                    | 	sasl.login.retry.backoff.ms = 100
schema-registry                    | 	sasl.mechanism = GSSAPI
schema-registry                    | 	sasl.oauthbearer.clock.skew.seconds = 30
schema-registry                    | 	sasl.oauthbearer.expected.audience = null
schema-registry                    | 	sasl.oauthbearer.expected.issuer = null
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.url = null
schema-registry                    | 	sasl.oauthbearer.scope.claim.name = scope
schema-registry                    | 	sasl.oauthbearer.sub.claim.name = sub
schema-registry                    | 	sasl.oauthbearer.token.endpoint.url = null
schema-registry                    | 	security.protocol = PLAINTEXT
schema-registry                    | 	security.providers = null
schema-registry                    | 	send.buffer.bytes = 131072
schema-registry                    | 	socket.connection.setup.timeout.max.ms = 30000
schema-registry                    | 	socket.connection.setup.timeout.ms = 10000
schema-registry                    | 	ssl.cipher.suites = null
schema-registry                    | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
schema-registry                    | 	ssl.endpoint.identification.algorithm = https
schema-registry                    | 	ssl.engine.factory.class = null
schema-registry                    | 	ssl.key.password = null
schema-registry                    | 	ssl.keymanager.algorithm = SunX509
schema-registry                    | 	ssl.keystore.certificate.chain = null
schema-registry                    | 	ssl.keystore.key = null
schema-registry                    | 	ssl.keystore.location = null
schema-registry                    | 	ssl.keystore.password = null
schema-registry                    | 	ssl.keystore.type = JKS
schema-registry                    | 	ssl.protocol = TLSv1.3
schema-registry                    | 	ssl.provider = null
schema-registry                    | 	ssl.secure.random.implementation = null
schema-registry                    | 	ssl.trustmanager.algorithm = PKIX
schema-registry                    | 	ssl.truststore.certificates = null
schema-registry                    | 	ssl.truststore.location = null
schema-registry                    | 	ssl.truststore.password = null
schema-registry                    | 	ssl.truststore.type = JKS
schema-registry                    |  (org.apache.kafka.clients.admin.AdminClientConfig)
schema-registry                    | [2023-08-04 10:10:43,472] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:10:43,472] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:10:43,472] INFO Kafka startTimeMs: 1691143843352 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:10:53,509] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:53,857 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - The heartbeat of ResourceManager with id 89827682c0886bbe770fc245a2a729a7 timed out.
data-agrigator-taskmanager-1       | 2023-08-04 10:10:53,858 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close ResourceManager connection 89827682c0886bbe770fc245a2a729a7.
schema-registry                    | [2023-08-04 10:10:53,883] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
schema-registry                    | [2023-08-04 10:10:53,883] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
schema-registry                    | [2023-08-04 10:10:53,889] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
data-agrigator-taskmanager-1       | 2023-08-04 10:10:53,862 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000).
data-agrigator-taskmanager-1       | 2023-08-04 10:10:53,965 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:10:54,031 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
schema-registry                    | [2023-08-04 10:10:54,192] INFO Registering schema provider for AVRO: io.confluent.kafka.schemaregistry.avro.AvroSchemaProvider (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
schema-registry                    | [2023-08-04 10:10:54,193] INFO Registering schema provider for JSON: io.confluent.kafka.schemaregistry.json.JsonSchemaProvider (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
schema-registry                    | [2023-08-04 10:10:54,193] INFO Registering schema provider for PROTOBUF: io.confluent.kafka.schemaregistry.protobuf.ProtobufSchemaProvider (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
control-center                     | [2023-08-04 10:10:54,707] INFO Capturing metrics for topic names _confluent-monitoring _confluent-metrics (io.confluent.controlcenter.streams.WindowExtractor)
control-center                     | [2023-08-04 10:10:54,772] INFO transformerStore=MonitoringVerifierStore (io.confluent.controlcenter.streams.StreamsModule)
control-center                     | [2023-08-04 10:10:54,814] INFO transformerStore=MonitoringTriggerStore (io.confluent.controlcenter.streams.StreamsModule)
control-center                     | [2023-08-04 10:10:54,814] INFO transformerStore=TriggerActionsStore (io.confluent.controlcenter.streams.StreamsModule)
control-center                     | [2023-08-04 10:10:54,814] INFO transformerStore=TriggerEventsStore (io.confluent.controlcenter.streams.StreamsModule)
control-center                     | [2023-08-04 10:10:54,814] INFO transformerStore=AlertHistoryStore (io.confluent.controlcenter.streams.StreamsModule)
control-center                     | [2023-08-04 10:10:55,091] INFO StreamsConfig values: 
control-center                     | 	acceptable.recovery.lag = 10000
control-center                     | 	application.id = _confluent-controlcenter-7-4-1-1
control-center                     | 	application.server = 
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffered.records.per.partition = 100
control-center                     | 	built.in.metrics.version = latest
control-center                     | 	cache.max.bytes.buffering = 1073741824
control-center                     | 	client.id = 
control-center                     | 	commit.interval.ms = 30000
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndContinueExceptionHandler
control-center                     | 	default.dsl.store = rocksDB
control-center                     | 	default.key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
control-center                     | 	default.list.key.serde.inner = null
control-center                     | 	default.list.key.serde.type = null
control-center                     | 	default.list.value.serde.inner = null
control-center                     | 	default.list.value.serde.type = null
control-center                     | 	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
control-center                     | 	default.timestamp.extractor = class io.confluent.controlcenter.streams.WindowExtractor
control-center                     | 	default.value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
control-center                     | 	max.task.idle.ms = 0
control-center                     | 	max.warmup.replicas = 2
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	num.standby.replicas = 0
control-center                     | 	num.stream.threads = 12
control-center                     | 	poll.ms = 100
control-center                     | 	probing.rebalance.interval.ms = 600000
control-center                     | 	processing.guarantee = at_least_once
control-center                     | 	rack.aware.assignment.tags = []
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	repartition.purge.interval.ms = 30000
control-center                     | 	replication.factor = 1
control-center                     | 	request.timeout.ms = 40000
control-center                     | 	retries = 0
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	rocksdb.config.setter = class io.confluent.controlcenter.streams.RocksDBConfigurator
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	state.cleanup.delay.ms = 600000
control-center                     | 	state.dir = /var/lib/confluent-control-center/1/kafka-streams
control-center                     | 	statestore.cache.max.bytes = 10485760
control-center                     | 	task.timeout.ms = 0
control-center                     | 	topology.optimization = all
control-center                     | 	upgrade.from = 2.3
control-center                     | 	window.size.ms = null
control-center                     | 	windowed.inner.class.serde = null
control-center                     | 	windowstore.changelog.additional.retention.ms = 86400000
control-center                     |  (org.apache.kafka.streams.StreamsConfig)
schema-registry                    | [2023-08-04 10:10:55,616] INFO Initializing KafkaStore with broker endpoints: PLAINTEXT://broker:29092 (io.confluent.kafka.schemaregistry.storage.KafkaStore)
control-center                     | [2023-08-04 10:10:55,701] INFO ProducerConfig values: 
control-center                     | 	acks = -1
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	batch.size = 16384
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffer.memory = 33554432
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = confluent-control-center-heartbeat-sender-1
control-center                     | 	compression.type = lz4
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	delivery.timeout.ms = 2147483647
control-center                     | 	enable.idempotence = true
control-center                     | 	interceptor.classes = []
control-center                     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     | 	linger.ms = 500
control-center                     | 	max.block.ms = 9223372036854775807
control-center                     | 	max.in.flight.requests.per.connection = 1
control-center                     | 	max.request.size = 10485760
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metadata.max.idle.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partitioner.adaptive.partitioning.enable = true
control-center                     | 	partitioner.availability.timeout.ms = 0
control-center                     | 	partitioner.class = null
control-center                     | 	partitioner.ignore.keys = false
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 500
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	transaction.timeout.ms = 60000
control-center                     | 	transactional.id = null
control-center                     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                     | [2023-08-04 10:10:56,029] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:10:57,753] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:10:57,754] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:10:57,804] INFO Kafka startTimeMs: 1691143857744 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:10:59,392] INFO Fetching bootstrap cluster id (io.confluent.controlcenter.BootstrapClusterIdSupplier)
control-center                     | [2023-08-04 10:10:59,418] INFO AdminClientConfig values: 
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = 
control-center                     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	confluent.use.controller.listener = false
control-center                     | 	connections.max.idle.ms = 300000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                     | [2023-08-04 10:11:00,457] WARN These configurations '[consumer.session.timeout.ms, producer.max.block.ms, producer.retries, upgrade.from, producer.retry.backoff.ms, producer.linger.ms, producer.delivery.timeout.ms, task.timeout.ms, cache.max.bytes.buffering, producer.compression.type, num.stream.threads]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                     | [2023-08-04 10:11:00,464] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:00,467] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:00,472] INFO Kafka startTimeMs: 1691143860461 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:11:00,953] WARN No value specified for metadata.encoder.secret, sensitive metadata will not be encoded (io.confluent.kafka.schemaregistry.storage.encoder.MetadataEncoderService)
schema-registry                    | [2023-08-04 10:11:01,044] INFO AdminClientConfig values: 
schema-registry                    | 	auto.include.jmx.reporter = true
schema-registry                    | 	bootstrap.servers = [PLAINTEXT://broker:29092]
schema-registry                    | 	client.dns.lookup = use_all_dns_ips
schema-registry                    | 	client.id = 
schema-registry                    | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
schema-registry                    | 	confluent.proxy.protocol.client.address = null
schema-registry                    | 	confluent.proxy.protocol.client.port = null
schema-registry                    | 	confluent.proxy.protocol.client.version = NONE
schema-registry                    | 	confluent.use.controller.listener = false
schema-registry                    | 	connections.max.idle.ms = 300000
schema-registry                    | 	default.api.timeout.ms = 60000
schema-registry                    | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
schema-registry                    | 	metadata.max.age.ms = 300000
schema-registry                    | 	metric.reporters = []
schema-registry                    | 	metrics.num.samples = 2
schema-registry                    | 	metrics.recording.level = INFO
schema-registry                    | 	metrics.sample.window.ms = 30000
schema-registry                    | 	receive.buffer.bytes = 65536
schema-registry                    | 	reconnect.backoff.max.ms = 1000
schema-registry                    | 	reconnect.backoff.ms = 50
schema-registry                    | 	request.timeout.ms = 30000
schema-registry                    | 	retries = 2147483647
schema-registry                    | 	retry.backoff.ms = 100
schema-registry                    | 	sasl.client.callback.handler.class = null
schema-registry                    | 	sasl.jaas.config = null
schema-registry                    | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
schema-registry                    | 	sasl.kerberos.min.time.before.relogin = 60000
schema-registry                    | 	sasl.kerberos.service.name = null
schema-registry                    | 	sasl.kerberos.ticket.renew.jitter = 0.05
schema-registry                    | 	sasl.kerberos.ticket.renew.window.factor = 0.8
schema-registry                    | 	sasl.login.callback.handler.class = null
schema-registry                    | 	sasl.login.class = null
schema-registry                    | 	sasl.login.connect.timeout.ms = null
schema-registry                    | 	sasl.login.read.timeout.ms = null
schema-registry                    | 	sasl.login.refresh.buffer.seconds = 300
schema-registry                    | 	sasl.login.refresh.min.period.seconds = 60
schema-registry                    | 	sasl.login.refresh.window.factor = 0.8
schema-registry                    | 	sasl.login.refresh.window.jitter = 0.05
schema-registry                    | 	sasl.login.retry.backoff.max.ms = 10000
schema-registry                    | 	sasl.login.retry.backoff.ms = 100
schema-registry                    | 	sasl.mechanism = GSSAPI
schema-registry                    | 	sasl.oauthbearer.clock.skew.seconds = 30
schema-registry                    | 	sasl.oauthbearer.expected.audience = null
schema-registry                    | 	sasl.oauthbearer.expected.issuer = null
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.url = null
schema-registry                    | 	sasl.oauthbearer.scope.claim.name = scope
schema-registry                    | 	sasl.oauthbearer.sub.claim.name = sub
schema-registry                    | 	sasl.oauthbearer.token.endpoint.url = null
schema-registry                    | 	security.protocol = PLAINTEXT
schema-registry                    | 	security.providers = null
schema-registry                    | 	send.buffer.bytes = 131072
schema-registry                    | 	socket.connection.setup.timeout.max.ms = 30000
schema-registry                    | 	socket.connection.setup.timeout.ms = 10000
schema-registry                    | 	ssl.cipher.suites = null
schema-registry                    | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
schema-registry                    | 	ssl.endpoint.identification.algorithm = https
schema-registry                    | 	ssl.engine.factory.class = null
schema-registry                    | 	ssl.key.password = null
schema-registry                    | 	ssl.keymanager.algorithm = SunX509
schema-registry                    | 	ssl.keystore.certificate.chain = null
schema-registry                    | 	ssl.keystore.key = null
schema-registry                    | 	ssl.keystore.location = null
schema-registry                    | 	ssl.keystore.password = null
schema-registry                    | 	ssl.keystore.type = JKS
schema-registry                    | 	ssl.protocol = TLSv1.3
schema-registry                    | 	ssl.provider = null
schema-registry                    | 	ssl.secure.random.implementation = null
schema-registry                    | 	ssl.trustmanager.algorithm = PKIX
schema-registry                    | 	ssl.truststore.certificates = null
schema-registry                    | 	ssl.truststore.location = null
schema-registry                    | 	ssl.truststore.password = null
schema-registry                    | 	ssl.truststore.type = JKS
schema-registry                    |  (org.apache.kafka.clients.admin.AdminClientConfig)
schema-registry                    | [2023-08-04 10:11:01,294] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:11:01,318] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:11:01,329] INFO Kafka startTimeMs: 1691143861294 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:11:01,584] INFO Creating schemas topic _schemas (io.confluent.kafka.schemaregistry.storage.KafkaStore)
schema-registry                    | [2023-08-04 10:11:01,598] WARN Creating the schema topic _schemas using a replication factor of 1, which is less than the desired one of 3. If this is a production environment, it's crucial to add more brokers and increase the replication factor of the topic. (io.confluent.kafka.schemaregistry.storage.KafkaStore)
broker                             | [2023-08-04 10:11:01,830] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_schemas', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:01,845] INFO [Controller 1] Created topic _schemas with topic ID Tj_6_gizR86RoKJiuNP8Fw. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:01,851] INFO [Controller 1] ConfigResource(type=TOPIC, name='_schemas'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:01,853] INFO [Controller 1] Created partition _schemas-0 with topic ID Tj_6_gizR86RoKJiuNP8Fw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:01,919] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:01,930] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:01,941] INFO [Broker id=1] Creating new partition _schemas-0 with topic id Tj_6_gizR86RoKJiuNP8Fw. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:01,997] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:11:02,081] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
schema-registry                    | [2023-08-04 10:11:02,114] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
schema-registry                    | [2023-08-04 10:11:02,115] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
broker                             | [2023-08-04 10:11:02,229] INFO [LogLoader partition=_schemas-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
schema-registry                    | [2023-08-04 10:11:02,240] INFO ProducerConfig values: 
schema-registry                    | 	acks = -1
schema-registry                    | 	auto.include.jmx.reporter = true
schema-registry                    | 	batch.size = 16384
schema-registry                    | 	bootstrap.servers = [PLAINTEXT://broker:29092]
schema-registry                    | 	buffer.memory = 33554432
schema-registry                    | 	client.dns.lookup = use_all_dns_ips
schema-registry                    | 	client.id = producer-1
schema-registry                    | 	compression.type = none
schema-registry                    | 	confluent.proxy.protocol.client.address = null
schema-registry                    | 	confluent.proxy.protocol.client.port = null
schema-registry                    | 	confluent.proxy.protocol.client.version = NONE
schema-registry                    | 	connections.max.idle.ms = 540000
schema-registry                    | 	delivery.timeout.ms = 120000
schema-registry                    | 	enable.idempotence = false
schema-registry                    | 	interceptor.classes = []
schema-registry                    | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
schema-registry                    | 	linger.ms = 0
schema-registry                    | 	max.block.ms = 60000
schema-registry                    | 	max.in.flight.requests.per.connection = 5
schema-registry                    | 	max.request.size = 1048576
schema-registry                    | 	metadata.max.age.ms = 300000
schema-registry                    | 	metadata.max.idle.ms = 300000
schema-registry                    | 	metric.reporters = []
schema-registry                    | 	metrics.num.samples = 2
schema-registry                    | 	metrics.recording.level = INFO
schema-registry                    | 	metrics.sample.window.ms = 30000
schema-registry                    | 	partitioner.adaptive.partitioning.enable = true
schema-registry                    | 	partitioner.availability.timeout.ms = 0
schema-registry                    | 	partitioner.class = null
schema-registry                    | 	partitioner.ignore.keys = false
schema-registry                    | 	receive.buffer.bytes = 32768
schema-registry                    | 	reconnect.backoff.max.ms = 1000
schema-registry                    | 	reconnect.backoff.ms = 50
schema-registry                    | 	request.timeout.ms = 30000
schema-registry                    | 	retries = 0
schema-registry                    | 	retry.backoff.ms = 100
schema-registry                    | 	sasl.client.callback.handler.class = null
schema-registry                    | 	sasl.jaas.config = null
schema-registry                    | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
schema-registry                    | 	sasl.kerberos.min.time.before.relogin = 60000
schema-registry                    | 	sasl.kerberos.service.name = null
schema-registry                    | 	sasl.kerberos.ticket.renew.jitter = 0.05
schema-registry                    | 	sasl.kerberos.ticket.renew.window.factor = 0.8
schema-registry                    | 	sasl.login.callback.handler.class = null
schema-registry                    | 	sasl.login.class = null
schema-registry                    | 	sasl.login.connect.timeout.ms = null
schema-registry                    | 	sasl.login.read.timeout.ms = null
schema-registry                    | 	sasl.login.refresh.buffer.seconds = 300
schema-registry                    | 	sasl.login.refresh.min.period.seconds = 60
schema-registry                    | 	sasl.login.refresh.window.factor = 0.8
schema-registry                    | 	sasl.login.refresh.window.jitter = 0.05
schema-registry                    | 	sasl.login.retry.backoff.max.ms = 10000
schema-registry                    | 	sasl.login.retry.backoff.ms = 100
schema-registry                    | 	sasl.mechanism = GSSAPI
schema-registry                    | 	sasl.oauthbearer.clock.skew.seconds = 30
schema-registry                    | 	sasl.oauthbearer.expected.audience = null
schema-registry                    | 	sasl.oauthbearer.expected.issuer = null
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.url = null
schema-registry                    | 	sasl.oauthbearer.scope.claim.name = scope
schema-registry                    | 	sasl.oauthbearer.sub.claim.name = sub
schema-registry                    | 	sasl.oauthbearer.token.endpoint.url = null
schema-registry                    | 	security.protocol = PLAINTEXT
schema-registry                    | 	security.providers = null
schema-registry                    | 	send.buffer.bytes = 131072
schema-registry                    | 	socket.connection.setup.timeout.max.ms = 30000
schema-registry                    | 	socket.connection.setup.timeout.ms = 10000
schema-registry                    | 	ssl.cipher.suites = null
schema-registry                    | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
schema-registry                    | 	ssl.endpoint.identification.algorithm = https
schema-registry                    | 	ssl.engine.factory.class = null
schema-registry                    | 	ssl.key.password = null
schema-registry                    | 	ssl.keymanager.algorithm = SunX509
schema-registry                    | 	ssl.keystore.certificate.chain = null
schema-registry                    | 	ssl.keystore.key = null
schema-registry                    | 	ssl.keystore.location = null
schema-registry                    | 	ssl.keystore.password = null
schema-registry                    | 	ssl.keystore.type = JKS
schema-registry                    | 	ssl.protocol = TLSv1.3
schema-registry                    | 	ssl.provider = null
schema-registry                    | 	ssl.secure.random.implementation = null
schema-registry                    | 	ssl.trustmanager.algorithm = PKIX
schema-registry                    | 	ssl.truststore.certificates = null
schema-registry                    | 	ssl.truststore.location = null
schema-registry                    | 	ssl.truststore.password = null
schema-registry                    | 	ssl.truststore.type = JKS
schema-registry                    | 	transaction.timeout.ms = 60000
schema-registry                    | 	transactional.id = null
schema-registry                    | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
schema-registry                    |  (org.apache.kafka.clients.producer.ProducerConfig)
broker                             | [2023-08-04 10:11:02,282] INFO Created log for partition _schemas-0 in /tmp/kraft-combined-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:02,293] INFO [Partition _schemas-0 broker=1] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:02,301] INFO [Partition _schemas-0 broker=1] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:02,319] INFO [Broker id=1] Leader _schemas-0 with topic id Some(Tj_6_gizR86RoKJiuNP8Fw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:02,410] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
schema-registry                    | [2023-08-04 10:11:02,599] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:11:02,718] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:11:02,729] INFO Kafka startTimeMs: 1691143862599 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:11:02,930] INFO [Producer clientId=producer-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:03,213] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
schema-registry                    | [2023-08-04 10:11:03,226] INFO Kafka store reader thread starting consumer (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
schema-registry                    | [2023-08-04 10:11:03,352] INFO ConsumerConfig values: 
schema-registry                    | 	allow.auto.create.topics = true
schema-registry                    | 	auto.commit.interval.ms = 5000
schema-registry                    | 	auto.include.jmx.reporter = true
schema-registry                    | 	auto.offset.reset = earliest
schema-registry                    | 	bootstrap.servers = [PLAINTEXT://broker:29092]
schema-registry                    | 	check.crcs = true
schema-registry                    | 	client.dns.lookup = use_all_dns_ips
schema-registry                    | 	client.id = KafkaStore-reader-_schemas
schema-registry                    | 	client.rack = 
schema-registry                    | 	confluent.proxy.protocol.client.address = null
schema-registry                    | 	confluent.proxy.protocol.client.port = null
schema-registry                    | 	confluent.proxy.protocol.client.version = NONE
schema-registry                    | 	connections.max.idle.ms = 540000
schema-registry                    | 	default.api.timeout.ms = 60000
schema-registry                    | 	enable.auto.commit = false
schema-registry                    | 	exclude.internal.topics = true
schema-registry                    | 	fetch.max.bytes = 52428800
schema-registry                    | 	fetch.max.wait.ms = 500
schema-registry                    | 	fetch.min.bytes = 1
schema-registry                    | 	group.id = schema-registry-schema-registry-8081
schema-registry                    | 	group.instance.id = null
schema-registry                    | 	heartbeat.interval.ms = 3000
schema-registry                    | 	interceptor.classes = []
schema-registry                    | 	internal.leave.group.on.close = true
schema-registry                    | 	internal.throw.on.fetch.stable.offset.unsupported = false
schema-registry                    | 	isolation.level = read_uncommitted
schema-registry                    | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
schema-registry                    | 	max.partition.fetch.bytes = 1048576
schema-registry                    | 	max.poll.interval.ms = 300000
schema-registry                    | 	max.poll.records = 500
schema-registry                    | 	metadata.max.age.ms = 300000
schema-registry                    | 	metric.reporters = []
schema-registry                    | 	metrics.num.samples = 2
schema-registry                    | 	metrics.recording.level = INFO
schema-registry                    | 	metrics.sample.window.ms = 30000
schema-registry                    | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
schema-registry                    | 	receive.buffer.bytes = 65536
schema-registry                    | 	reconnect.backoff.max.ms = 1000
schema-registry                    | 	reconnect.backoff.ms = 50
schema-registry                    | 	request.timeout.ms = 30000
schema-registry                    | 	retry.backoff.ms = 100
schema-registry                    | 	sasl.client.callback.handler.class = null
schema-registry                    | 	sasl.jaas.config = null
schema-registry                    | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
schema-registry                    | 	sasl.kerberos.min.time.before.relogin = 60000
schema-registry                    | 	sasl.kerberos.service.name = null
schema-registry                    | 	sasl.kerberos.ticket.renew.jitter = 0.05
schema-registry                    | 	sasl.kerberos.ticket.renew.window.factor = 0.8
schema-registry                    | 	sasl.login.callback.handler.class = null
schema-registry                    | 	sasl.login.class = null
schema-registry                    | 	sasl.login.connect.timeout.ms = null
schema-registry                    | 	sasl.login.read.timeout.ms = null
schema-registry                    | 	sasl.login.refresh.buffer.seconds = 300
schema-registry                    | 	sasl.login.refresh.min.period.seconds = 60
schema-registry                    | 	sasl.login.refresh.window.factor = 0.8
schema-registry                    | 	sasl.login.refresh.window.jitter = 0.05
schema-registry                    | 	sasl.login.retry.backoff.max.ms = 10000
schema-registry                    | 	sasl.login.retry.backoff.ms = 100
schema-registry                    | 	sasl.mechanism = GSSAPI
schema-registry                    | 	sasl.oauthbearer.clock.skew.seconds = 30
schema-registry                    | 	sasl.oauthbearer.expected.audience = null
schema-registry                    | 	sasl.oauthbearer.expected.issuer = null
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
schema-registry                    | 	sasl.oauthbearer.jwks.endpoint.url = null
schema-registry                    | 	sasl.oauthbearer.scope.claim.name = scope
schema-registry                    | 	sasl.oauthbearer.sub.claim.name = sub
schema-registry                    | 	sasl.oauthbearer.token.endpoint.url = null
schema-registry                    | 	security.protocol = PLAINTEXT
schema-registry                    | 	security.providers = null
schema-registry                    | 	send.buffer.bytes = 131072
schema-registry                    | 	session.timeout.ms = 45000
schema-registry                    | 	socket.connection.setup.timeout.max.ms = 30000
schema-registry                    | 	socket.connection.setup.timeout.ms = 10000
schema-registry                    | 	ssl.cipher.suites = null
schema-registry                    | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
schema-registry                    | 	ssl.endpoint.identification.algorithm = https
schema-registry                    | 	ssl.engine.factory.class = null
schema-registry                    | 	ssl.key.password = null
schema-registry                    | 	ssl.keymanager.algorithm = SunX509
schema-registry                    | 	ssl.keystore.certificate.chain = null
schema-registry                    | 	ssl.keystore.key = null
schema-registry                    | 	ssl.keystore.location = null
schema-registry                    | 	ssl.keystore.password = null
schema-registry                    | 	ssl.keystore.type = JKS
schema-registry                    | 	ssl.protocol = TLSv1.3
schema-registry                    | 	ssl.provider = null
schema-registry                    | 	ssl.secure.random.implementation = null
schema-registry                    | 	ssl.trustmanager.algorithm = PKIX
schema-registry                    | 	ssl.truststore.certificates = null
schema-registry                    | 	ssl.truststore.location = null
schema-registry                    | 	ssl.truststore.password = null
schema-registry                    | 	ssl.truststore.type = JKS
schema-registry                    | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
schema-registry                    |  (org.apache.kafka.clients.consumer.ConsumerConfig)
rest-proxy                         | [2023-08-04 10:11:03,411] INFO HV000001: Hibernate Validator 6.1.7.Final (org.hibernate.validator.internal.util.Version)
schema-registry                    | [2023-08-04 10:11:03,985] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:11:03,986] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:11:04,002] INFO Kafka startTimeMs: 1691143863984 (org.apache.kafka.common.utils.AppInfoParser)
data-agrigator-taskmanager-1       | 2023-08-04 10:11:04,089 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:11:04,093 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
schema-registry                    | [2023-08-04 10:11:04,140] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:04,265] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Assigned to partition(s): _schemas-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
schema-registry                    | [2023-08-04 10:11:04,305] INFO Seeking to beginning for all partitions (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
schema-registry                    | [2023-08-04 10:11:04,313] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Seeking to earliest offset of partition _schemas-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
schema-registry                    | [2023-08-04 10:11:04,333] INFO Initialized last consumed offset to -1 (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
schema-registry                    | [2023-08-04 10:11:04,403] INFO [kafka-store-reader-thread-_schemas]: Starting (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
schema-registry                    | [2023-08-04 10:11:05,009] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Resetting the last seen epoch of partition _schemas-0 to 0 since the associated topicId changed from null to Tj_6_gizR86RoKJiuNP8Fw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:11:05,497] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/cp-base-new/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:11:05,498] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:11:05,498] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:11:05,499] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:11:05,499] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:11:05,499] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:11:05,541] INFO Loading plugin from: /usr/share/java/confluent-control-center (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
schema-registry                    | [2023-08-04 10:11:06,942] INFO [Producer clientId=producer-1] Resetting the last seen epoch of partition _schemas-0 to 0 since the associated topicId changed from null to Tj_6_gizR86RoKJiuNP8Fw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:11:07,230] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:11:07,367] INFO Bootstrap cluster id MkU3OEVBNTcwNTJENDM2Qg (io.confluent.controlcenter.BootstrapClusterIdSupplier)
control-center                     | [2023-08-04 10:11:07,432] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] ProducerId set to 0 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
schema-registry                    | [2023-08-04 10:11:08,044] INFO Wait to catch up until the offset at 0 (io.confluent.kafka.schemaregistry.storage.KafkaStore)
schema-registry                    | [2023-08-04 10:11:09,071] INFO Reached offset at 0 (io.confluent.kafka.schemaregistry.storage.KafkaStore)
schema-registry                    | [2023-08-04 10:11:09,073] INFO Joining schema registry with Kafka-based coordination (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
schema-registry                    | [2023-08-04 10:11:09,200] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:11:09,205] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:11:09,206] INFO Kafka startTimeMs: 1691143869200 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:11:09,327] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _schemas-0 to 0 since the associated topicId changed from null to Tj_6_gizR86RoKJiuNP8Fw (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,328] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
broker                             | [2023-08-04 10:11:09,364] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
broker                             | [2023-08-04 10:11:09,376] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='__consumer_offsets', numPartitions=50, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='compression.type', value='producer'), CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='segment.bytes', value='104857600')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,391] INFO [Controller 1] Created topic __consumer_offsets with topic ID En_RJ3y3THKwRrol6fr06Q. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,392] INFO [Controller 1] ConfigResource(type=TOPIC, name='__consumer_offsets'): set configuration compression.type to producer (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:09,392] INFO [Controller 1] ConfigResource(type=TOPIC, name='__consumer_offsets'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:09,393] INFO [Controller 1] ConfigResource(type=TOPIC, name='__consumer_offsets'): set configuration segment.bytes to 104857600 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:09,393] INFO [Controller 1] Created partition __consumer_offsets-0 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,411] INFO [Controller 1] Created partition __consumer_offsets-1 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,412] INFO [Controller 1] Created partition __consumer_offsets-2 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,413] INFO [Controller 1] Created partition __consumer_offsets-3 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,413] INFO [Controller 1] Created partition __consumer_offsets-4 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,419] INFO [Controller 1] Created partition __consumer_offsets-5 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,427] INFO [Controller 1] Created partition __consumer_offsets-6 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,431] INFO [Controller 1] Created partition __consumer_offsets-7 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,432] INFO [Controller 1] Created partition __consumer_offsets-8 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,433] INFO [Controller 1] Created partition __consumer_offsets-9 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,439] INFO [Controller 1] Created partition __consumer_offsets-10 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,445] INFO [Controller 1] Created partition __consumer_offsets-11 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,445] INFO [Controller 1] Created partition __consumer_offsets-12 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,446] INFO [Controller 1] Created partition __consumer_offsets-13 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,449] INFO [Controller 1] Created partition __consumer_offsets-14 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,458] INFO [Controller 1] Created partition __consumer_offsets-15 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,471] INFO [Controller 1] Created partition __consumer_offsets-16 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,472] INFO [Controller 1] Created partition __consumer_offsets-17 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,474] INFO [Controller 1] Created partition __consumer_offsets-18 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,474] INFO [Controller 1] Created partition __consumer_offsets-19 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,480] INFO [Controller 1] Created partition __consumer_offsets-20 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,483] INFO [Controller 1] Created partition __consumer_offsets-21 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,484] INFO [Controller 1] Created partition __consumer_offsets-22 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,489] INFO [Controller 1] Created partition __consumer_offsets-23 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,495] INFO [Controller 1] Created partition __consumer_offsets-24 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,498] INFO [Controller 1] Created partition __consumer_offsets-25 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,501] INFO [Controller 1] Created partition __consumer_offsets-26 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,503] INFO [Controller 1] Created partition __consumer_offsets-27 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,521] INFO [Controller 1] Created partition __consumer_offsets-28 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,522] INFO [Controller 1] Created partition __consumer_offsets-29 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,523] INFO [Controller 1] Created partition __consumer_offsets-30 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,528] INFO [Controller 1] Created partition __consumer_offsets-31 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,529] INFO [Controller 1] Created partition __consumer_offsets-32 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,529] INFO [Controller 1] Created partition __consumer_offsets-33 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,530] INFO [Controller 1] Created partition __consumer_offsets-34 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,530] INFO [Controller 1] Created partition __consumer_offsets-35 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,531] INFO [Controller 1] Created partition __consumer_offsets-36 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,531] INFO [Controller 1] Created partition __consumer_offsets-37 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,532] INFO [Controller 1] Created partition __consumer_offsets-38 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,535] INFO [Controller 1] Created partition __consumer_offsets-39 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,539] INFO [Controller 1] Created partition __consumer_offsets-40 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,540] INFO [Controller 1] Created partition __consumer_offsets-41 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,544] INFO [Controller 1] Created partition __consumer_offsets-42 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,545] INFO [Controller 1] Created partition __consumer_offsets-43 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,546] INFO [Controller 1] Created partition __consumer_offsets-44 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,546] INFO [Controller 1] Created partition __consumer_offsets-45 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,551] INFO [Controller 1] Created partition __consumer_offsets-46 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,552] INFO [Controller 1] Created partition __consumer_offsets-47 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,559] INFO [Controller 1] Created partition __consumer_offsets-48 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,560] INFO [Controller 1] Created partition __consumer_offsets-49 with topic ID En_RJ3y3THKwRrol6fr06Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:09,594] INFO [Broker id=1] Transitioning 50 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:09,608] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:09,616] INFO [Broker id=1] Creating new partition __consumer_offsets-13 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:09,657] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:09,676] INFO Created log for partition __consumer_offsets-13 in /tmp/kraft-combined-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:09,680] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:09,690] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:09,691] INFO [Broker id=1] Leader __consumer_offsets-13 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:09,706] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-17 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,707] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-11 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,707] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-23 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,708] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-40 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,715] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-5 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,716] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-0 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,717] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-29 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,723] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-46 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,727] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-30 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,728] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-4 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,729] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-39 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,729] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-42 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,730] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-36 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
broker                             | [2023-08-04 10:11:09,730] INFO [Broker id=1] Creating new partition __consumer_offsets-46 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:09,731] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-48 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,732] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-10 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,733] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-13 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,739] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-45 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,740] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-16 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,743] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-28 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,744] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-34 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,745] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-19 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,746] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-22 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,746] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-31 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,751] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-2 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,752] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-25 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,753] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-20 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,754] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-26 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,755] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-14 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,755] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-32 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,759] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-37 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,760] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-8 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,767] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-43 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,768] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-7 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,769] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-49 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,769] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-33 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,770] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-1 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,771] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-27 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,779] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-24 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,780] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-21 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,780] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-47 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,781] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-3 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,782] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-9 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,782] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-15 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,783] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-18 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,784] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-44 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,785] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-6 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,786] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-35 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,791] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-38 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,793] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-41 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:11:09,794] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-12 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
broker                             | [2023-08-04 10:11:09,797] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:09,843] INFO Created log for partition __consumer_offsets-46 in /tmp/kraft-combined-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:09,843] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:09,844] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:09,844] INFO [Broker id=1] Leader __consumer_offsets-46 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:09,855] INFO [Broker id=1] Creating new partition __consumer_offsets-9 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:09,880] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:09,899] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:09,917] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:09,944] INFO Created log for partition __consumer_offsets-9 in /tmp/kraft-combined-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:09,951] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:09,952] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:09,953] INFO [Broker id=1] Leader __consumer_offsets-9 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:09,961] INFO [Broker id=1] Creating new partition __consumer_offsets-42 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:10,007] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:10,029] INFO Created log for partition __consumer_offsets-42 in /tmp/kraft-combined-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:10,030] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,031] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,032] INFO [Broker id=1] Leader __consumer_offsets-42 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:10,048] INFO [Broker id=1] Creating new partition __consumer_offsets-21 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:10,105] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:10,122] INFO Created log for partition __consumer_offsets-21 in /tmp/kraft-combined-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:10,185] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,186] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,189] INFO [Broker id=1] Leader __consumer_offsets-21 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:10,218] INFO [Broker id=1] Creating new partition __consumer_offsets-17 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:10,272] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:10,273] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:10,279] INFO [Schema registry clientId=sr-1, groupId=schema-registry] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:10,283] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:10,285] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:10,302] INFO Created log for partition __consumer_offsets-17 in /tmp/kraft-combined-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:10,303] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,304] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,305] INFO [Broker id=1] Leader __consumer_offsets-17 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:10,314] INFO [Broker id=1] Creating new partition __consumer_offsets-30 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:10,339] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:10,346] INFO Created log for partition __consumer_offsets-30 in /tmp/kraft-combined-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:10,348] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,351] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,352] INFO [Broker id=1] Leader __consumer_offsets-30 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:10,359] INFO [Broker id=1] Creating new partition __consumer_offsets-26 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:10,374] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
schema-registry                    | [2023-08-04 10:11:10,385] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker                             | [2023-08-04 10:11:10,416] INFO Created log for partition __consumer_offsets-26 in /tmp/kraft-combined-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
schema-registry                    | [2023-08-04 10:11:10,415] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:10,417] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
schema-registry                    | [2023-08-04 10:11:10,418] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:10,418] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
schema-registry                    | [2023-08-04 10:11:10,418] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:10,418] INFO [Broker id=1] Leader __consumer_offsets-26 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:10,430] INFO [Broker id=1] Creating new partition __consumer_offsets-5 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:10,441] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:10,458] INFO Created log for partition __consumer_offsets-5 in /tmp/kraft-combined-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:10,462] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,463] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,464] INFO [Broker id=1] Leader __consumer_offsets-5 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:10,477] INFO [Broker id=1] Creating new partition __consumer_offsets-38 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:10,504] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:10,509] INFO Created log for partition __consumer_offsets-38 in /tmp/kraft-combined-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:10,510] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,510] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,516] INFO [Broker id=1] Leader __consumer_offsets-38 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:10,529] INFO [Broker id=1] Creating new partition __consumer_offsets-1 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:10,553] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:10,568] INFO Created log for partition __consumer_offsets-1 in /tmp/kraft-combined-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:10,569] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,571] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,572] INFO [Broker id=1] Leader __consumer_offsets-1 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:10,576] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:10,578] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:10,580] INFO [Broker id=1] Creating new partition __consumer_offsets-34 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:10,595] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:10,605] INFO Created log for partition __consumer_offsets-34 in /tmp/kraft-combined-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:10,606] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,606] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,615] INFO [Broker id=1] Leader __consumer_offsets-34 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:10,618] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:10,622] INFO [Broker id=1] Creating new partition __consumer_offsets-16 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:10,623] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:10,623] INFO [Schema registry clientId=sr-1, groupId=schema-registry] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:10,624] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:10,633] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:10,645] INFO Created log for partition __consumer_offsets-16 in /tmp/kraft-combined-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:10,646] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,648] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,648] INFO [Broker id=1] Leader __consumer_offsets-16 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:10,657] INFO [Broker id=1] Creating new partition __consumer_offsets-45 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:10,678] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:10,703] INFO Created log for partition __consumer_offsets-45 in /tmp/kraft-combined-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:10,703] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,704] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,704] INFO [Broker id=1] Leader __consumer_offsets-45 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:10,715] INFO [Broker id=1] Creating new partition __consumer_offsets-12 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:10,739] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker                             | [2023-08-04 10:11:10,741] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:10,746] INFO Created log for partition __consumer_offsets-12 in /tmp/kraft-combined-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:10,752] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,752] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,755] INFO [Broker id=1] Leader __consumer_offsets-12 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:10,756] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:10,757] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:10,757] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:10,771] INFO [Broker id=1] Creating new partition __consumer_offsets-41 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:10,784] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:10,806] INFO Created log for partition __consumer_offsets-41 in /tmp/kraft-combined-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:10,819] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,820] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,821] INFO [Broker id=1] Leader __consumer_offsets-41 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:10,829] INFO [Broker id=1] Creating new partition __consumer_offsets-24 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:10,850] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:10,862] INFO Created log for partition __consumer_offsets-24 in /tmp/kraft-combined-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:10,869] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,875] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,876] INFO [Broker id=1] Leader __consumer_offsets-24 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:10,876] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:10,878] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:10,887] INFO [Broker id=1] Creating new partition __consumer_offsets-20 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:10,906] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:10,920] INFO Created log for partition __consumer_offsets-20 in /tmp/kraft-combined-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:10,920] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,921] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,922] INFO [Broker id=1] Leader __consumer_offsets-20 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:10,922] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:10,922] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:10,923] INFO [Schema registry clientId=sr-1, groupId=schema-registry] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:10,923] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:10,939] INFO [Broker id=1] Creating new partition __consumer_offsets-49 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:10,956] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:10,965] INFO Created log for partition __consumer_offsets-49 in /tmp/kraft-combined-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:10,966] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,967] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:10,968] INFO [Broker id=1] Leader __consumer_offsets-49 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:10,979] INFO [Broker id=1] Creating new partition __consumer_offsets-0 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:10,990] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:10,999] INFO Created log for partition __consumer_offsets-0 in /tmp/kraft-combined-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:11,004] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,005] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,005] INFO [Broker id=1] Leader __consumer_offsets-0 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:11,014] INFO [Broker id=1] Creating new partition __consumer_offsets-29 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:11,024] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
schema-registry                    | [2023-08-04 10:11:11,035] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker                             | [2023-08-04 10:11:11,054] INFO Created log for partition __consumer_offsets-29 in /tmp/kraft-combined-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:11,054] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,055] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,055] INFO [Broker id=1] Leader __consumer_offsets-29 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:11,063] INFO [Broker id=1] Creating new partition __consumer_offsets-25 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:11,076] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
schema-registry                    | [2023-08-04 10:11:11,084] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,085] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,086] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:11,087] INFO Created log for partition __consumer_offsets-25 in /tmp/kraft-combined-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:11,094] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,103] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,104] INFO [Broker id=1] Leader __consumer_offsets-25 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:11,111] INFO [Broker id=1] Creating new partition __consumer_offsets-8 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:11,125] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:11,137] INFO Created log for partition __consumer_offsets-8 in /tmp/kraft-combined-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:11,152] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,152] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,153] INFO [Broker id=1] Leader __consumer_offsets-8 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:11,197] INFO [Broker id=1] Creating new partition __consumer_offsets-37 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:11,204] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,208] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,238] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,253] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,254] INFO [Schema registry clientId=sr-1, groupId=schema-registry] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,257] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:11,309] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:11,374] INFO Created log for partition __consumer_offsets-37 in /tmp/kraft-combined-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
schema-registry                    | [2023-08-04 10:11:11,379] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker                             | [2023-08-04 10:11:11,374] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,382] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,382] INFO [Broker id=1] Leader __consumer_offsets-37 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:11,409] INFO [Broker id=1] Creating new partition __consumer_offsets-4 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:11,420] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:11,424] INFO Created log for partition __consumer_offsets-4 in /tmp/kraft-combined-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:11,435] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
schema-registry                    | [2023-08-04 10:11:11,438] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,441] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:11,448] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,455] INFO [Broker id=1] Leader __consumer_offsets-4 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:11,475] INFO [Broker id=1] Creating new partition __consumer_offsets-33 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:11,514] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,517] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,518] INFO [Schema registry clientId=sr-1, groupId=schema-registry] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,519] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:11,524] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:11,529] INFO Created log for partition __consumer_offsets-33 in /tmp/kraft-combined-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:11,532] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,534] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,536] INFO [Broker id=1] Leader __consumer_offsets-33 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:11,569] INFO [Broker id=1] Creating new partition __consumer_offsets-15 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:11,621] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:11:11,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,632] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:11,697] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:11,714] INFO Created log for partition __consumer_offsets-15 in /tmp/kraft-combined-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:11,714] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,714] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,741] INFO [Broker id=1] Leader __consumer_offsets-15 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:11,755] INFO [Broker id=1] Creating new partition __consumer_offsets-48 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:11,755] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,757] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:11,786] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:11,790] INFO Created log for partition __consumer_offsets-48 in /tmp/kraft-combined-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:11,791] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,792] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,793] INFO [Broker id=1] Leader __consumer_offsets-48 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:11,818] INFO [Broker id=1] Creating new partition __consumer_offsets-11 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:11,825] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,825] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,826] INFO [Schema registry clientId=sr-1, groupId=schema-registry] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,826] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:11,853] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:11,865] INFO Created log for partition __consumer_offsets-11 in /tmp/kraft-combined-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:11,866] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,882] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,882] INFO [Broker id=1] Leader __consumer_offsets-11 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:11,919] INFO [Broker id=1] Creating new partition __consumer_offsets-44 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:11,927] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker                             | [2023-08-04 10:11:11,952] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:11,973] INFO Created log for partition __consumer_offsets-44 in /tmp/kraft-combined-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:11,974] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:11,975] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
schema-registry                    | [2023-08-04 10:11:11,977] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,978] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:11,978] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:11,981] INFO [Broker id=1] Leader __consumer_offsets-44 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:12,003] INFO [Broker id=1] Creating new partition __consumer_offsets-23 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:12,046] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:12,050] INFO Created log for partition __consumer_offsets-23 in /tmp/kraft-combined-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:12,050] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,073] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,083] INFO [Broker id=1] Leader __consumer_offsets-23 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:12,097] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,099] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:12,113] INFO [Broker id=1] Creating new partition __consumer_offsets-19 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:12,157] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:12,166] INFO Created log for partition __consumer_offsets-19 in /tmp/kraft-combined-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:12,173] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,174] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,175] INFO [Broker id=1] Leader __consumer_offsets-19 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:12,190] INFO [Broker id=1] Creating new partition __consumer_offsets-32 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:12,200] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,200] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,200] INFO [Schema registry clientId=sr-1, groupId=schema-registry] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,201] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:12,229] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:12,232] INFO Created log for partition __consumer_offsets-32 in /tmp/kraft-combined-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:12,236] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,239] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,240] INFO [Broker id=1] Leader __consumer_offsets-32 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:12,251] INFO [Broker id=1] Creating new partition __consumer_offsets-28 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:12,287] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:12,293] INFO Created log for partition __consumer_offsets-28 in /tmp/kraft-combined-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:12,297] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,298] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,300] INFO [Broker id=1] Leader __consumer_offsets-28 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:12,302] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker                             | [2023-08-04 10:11:12,310] INFO [Broker id=1] Creating new partition __consumer_offsets-7 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:12,320] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,320] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,320] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:12,332] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:12,345] INFO Created log for partition __consumer_offsets-7 in /tmp/kraft-combined-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:12,347] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,348] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,357] INFO [Broker id=1] Leader __consumer_offsets-7 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:12,367] INFO [Broker id=1] Creating new partition __consumer_offsets-40 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:12,391] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:12,430] INFO Created log for partition __consumer_offsets-40 in /tmp/kraft-combined-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:12,432] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,433] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
schema-registry                    | [2023-08-04 10:11:12,435] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,436] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:12,437] INFO [Broker id=1] Leader __consumer_offsets-40 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:12,446] INFO [Broker id=1] Creating new partition __consumer_offsets-3 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:12,460] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,462] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,462] INFO [Schema registry clientId=sr-1, groupId=schema-registry] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,463] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:12,465] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:12,500] INFO Created log for partition __consumer_offsets-3 in /tmp/kraft-combined-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:12,515] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,519] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,520] INFO [Broker id=1] Leader __consumer_offsets-3 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:12,531] INFO [Broker id=1] Creating new partition __consumer_offsets-36 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:12,562] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
schema-registry                    | [2023-08-04 10:11:12,566] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:11:12,587] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,588] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,588] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:12,605] INFO Created log for partition __consumer_offsets-36 in /tmp/kraft-combined-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:12,606] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,606] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,607] INFO [Broker id=1] Leader __consumer_offsets-36 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:12,621] INFO [Broker id=1] Creating new partition __consumer_offsets-47 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:12,638] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:12,642] INFO Created log for partition __consumer_offsets-47 in /tmp/kraft-combined-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:12,647] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,648] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,649] INFO [Broker id=1] Leader __consumer_offsets-47 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:12,658] INFO [Broker id=1] Creating new partition __consumer_offsets-14 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:12,873] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,874] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,914] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,914] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,914] INFO [Schema registry clientId=sr-1, groupId=schema-registry] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:12,914] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:12,919] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:12,922] INFO Created log for partition __consumer_offsets-14 in /tmp/kraft-combined-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:12,937] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,945] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:12,947] INFO [Broker id=1] Leader __consumer_offsets-14 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:12,967] INFO [Broker id=1] Creating new partition __consumer_offsets-43 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:13,015] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:11:13,053] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,053] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,053] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:13,062] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:13,075] INFO Created log for partition __consumer_offsets-43 in /tmp/kraft-combined-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:13,086] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:13,097] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:13,098] INFO [Broker id=1] Leader __consumer_offsets-43 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:13,118] INFO [Broker id=1] Creating new partition __consumer_offsets-10 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:13,172] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:13,177] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
schema-registry                    | [2023-08-04 10:11:13,180] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,237] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,241] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,242] INFO [Schema registry clientId=sr-1, groupId=schema-registry] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,243] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:13,261] INFO Created log for partition __consumer_offsets-10 in /tmp/kraft-combined-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:13,269] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:13,278] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:13,279] INFO [Broker id=1] Leader __consumer_offsets-10 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:13,308] INFO [Broker id=1] Creating new partition __consumer_offsets-22 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:13,345] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:11:13,380] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,381] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,382] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:13,438] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:13,466] INFO Created log for partition __consumer_offsets-22 in /tmp/kraft-combined-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:13,476] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:13,478] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:13,482] INFO [Broker id=1] Leader __consumer_offsets-22 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:13,495] INFO [Broker id=1] Creating new partition __consumer_offsets-18 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:13,512] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,513] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,550] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,551] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,552] INFO [Schema registry clientId=sr-1, groupId=schema-registry] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,553] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:13,563] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:13,574] INFO Created log for partition __consumer_offsets-18 in /tmp/kraft-combined-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:13,574] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:13,574] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:13,575] INFO [Broker id=1] Leader __consumer_offsets-18 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:13,585] INFO [Broker id=1] Creating new partition __consumer_offsets-31 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
broker                             | [2023-08-04 10:11:13,601] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:13,605] INFO Created log for partition __consumer_offsets-31 in /tmp/kraft-combined-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:13,606] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:13,607] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:13,608] INFO [Broker id=1] Leader __consumer_offsets-31 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:13,622] INFO [Broker id=1] Creating new partition __consumer_offsets-27 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:13,655] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:11:13,665] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,666] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,666] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:13,675] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:13,690] INFO Created log for partition __consumer_offsets-27 in /tmp/kraft-combined-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:13,690] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:13,691] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:13,691] INFO [Broker id=1] Leader __consumer_offsets-27 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:13,719] INFO [Broker id=1] Creating new partition __consumer_offsets-39 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:13,794] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,795] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:13,840] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:13,886] INFO Created log for partition __consumer_offsets-39 in /tmp/kraft-combined-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:13,892] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
schema-registry                    | [2023-08-04 10:11:13,901] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,902] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,903] INFO [Schema registry clientId=sr-1, groupId=schema-registry] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:13,904] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:13,912] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:13,913] INFO [Broker id=1] Leader __consumer_offsets-39 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:13,991] INFO [Broker id=1] Creating new partition __consumer_offsets-6 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:14,018] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:11:14,042] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:14,043] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:14,043] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:14,061] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:14,065] INFO Created log for partition __consumer_offsets-6 in /tmp/kraft-combined-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:14,082] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:14,082] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:14,087] INFO [Broker id=1] Leader __consumer_offsets-6 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:14,119] INFO [Broker id=1] Creating new partition __consumer_offsets-35 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:14,150] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:14,153] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
data-agrigator-taskmanager-1       | 2023-08-04 10:11:14,178 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1       | 2023-08-04 10:11:14,177 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
broker                             | [2023-08-04 10:11:14,199] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:14,210] INFO Created log for partition __consumer_offsets-35 in /tmp/kraft-combined-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
schema-registry                    | [2023-08-04 10:11:14,208] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:14,209] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:14,209] INFO [Schema registry clientId=sr-1, groupId=schema-registry] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:14,209] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:14,237] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:14,238] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:14,239] INFO [Broker id=1] Leader __consumer_offsets-35 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:14,271] INFO [Broker id=1] Creating new partition __consumer_offsets-2 with topic id En_RJ3y3THKwRrol6fr06Q. (state.change.logger)
schema-registry                    | [2023-08-04 10:11:14,310] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker                             | [2023-08-04 10:11:14,316] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
schema-registry                    | [2023-08-04 10:11:14,330] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:14,330] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:14,331] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:14,356] INFO Created log for partition __consumer_offsets-2 in /tmp/kraft-combined-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:14,363] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:14,364] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:14,366] INFO [Broker id=1] Leader __consumer_offsets-2 with topic id Some(En_RJ3y3THKwRrol6fr06Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:14,432] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
schema-registry                    | [2023-08-04 10:11:14,437] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:14,439] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:14,448] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,476] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
schema-registry                    | [2023-08-04 10:11:14,477] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:14,477] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:14,478] INFO [Schema registry clientId=sr-1, groupId=schema-registry] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:14,479] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:14,498] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,504] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,506] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,507] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,523] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,523] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,524] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,525] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,525] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,526] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,527] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,528] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,528] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,533] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,534] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,536] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,536] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,539] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,539] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,540] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,541] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,541] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,542] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,547] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,547] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,548] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,549] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,550] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,550] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,575] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,577] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
schema-registry                    | [2023-08-04 10:11:14,580] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker                             | [2023-08-04 10:11:14,584] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,588] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,588] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,589] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,596] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,596] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,598] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,599] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,603] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,604] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,610] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
schema-registry                    | [2023-08-04 10:11:14,615] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:14,615] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:14,615] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:14,621] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,632] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,669] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,669] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,669] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,669] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,669] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,669] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,669] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,669] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,669] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,670] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,670] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,670] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,670] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,670] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,670] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,670] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,670] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,670] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,670] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,671] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,671] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,683] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,683] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,683] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,683] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,684] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,684] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,684] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,684] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,684] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,684] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,684] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,684] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,697] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,697] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,697] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,697] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,697] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,698] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,698] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,698] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,698] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,698] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,698] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,698] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,698] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,698] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,698] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,699] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,709] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 196 milliseconds for epoch 0, of which 23 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,758] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 259 milliseconds for epoch 0, of which 258 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,761] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,761] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,761] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:14,762] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 255 milliseconds for epoch 0, of which 253 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,778] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 255 milliseconds for epoch 0, of which 254 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,779] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 255 milliseconds for epoch 0, of which 255 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
schema-registry                    | [2023-08-04 10:11:14,776] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:14,777] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:14,784] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 258 milliseconds for epoch 0, of which 258 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,787] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 260 milliseconds for epoch 0, of which 260 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,789] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 261 milliseconds for epoch 0, of which 260 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,776] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,795] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 260 milliseconds for epoch 0, of which 260 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,803] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 264 milliseconds for epoch 0, of which 259 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,807] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:14,812] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 272 milliseconds for epoch 0, of which 272 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,814] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 273 milliseconds for epoch 0, of which 273 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,819] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 277 milliseconds for epoch 0, of which 274 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,820] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 272 milliseconds for epoch 0, of which 272 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,874] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 325 milliseconds for epoch 0, of which 306 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
schema-registry                    | [2023-08-04 10:11:14,878] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:14,880] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 330 milliseconds for epoch 0, of which 329 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,886] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 303 milliseconds for epoch 0, of which 302 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,889] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 301 milliseconds for epoch 0, of which 299 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,897] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 302 milliseconds for epoch 0, of which 301 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,912] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 314 milliseconds for epoch 0, of which 313 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,926] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 326 milliseconds for epoch 0, of which 326 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,940] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 330 milliseconds for epoch 0, of which 330 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,947] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 315 milliseconds for epoch 0, of which 310 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,952] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 283 milliseconds for epoch 0, of which 282 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,960] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 290 milliseconds for epoch 0, of which 290 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,965] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 296 milliseconds for epoch 0, of which 295 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,973] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 304 milliseconds for epoch 0, of which 304 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
schema-registry                    | [2023-08-04 10:11:14,976] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:14,978] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 308 milliseconds for epoch 0, of which 307 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
schema-registry                    | [2023-08-04 10:11:14,980] INFO [Schema registry clientId=sr-1, groupId=schema-registry] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:14,981] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:14,988] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 318 milliseconds for epoch 0, of which 317 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,993] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 323 milliseconds for epoch 0, of which 322 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,995] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 325 milliseconds for epoch 0, of which 324 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:14,997] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 327 milliseconds for epoch 0, of which 326 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,004] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 334 milliseconds for epoch 0, of which 333 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,011] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 339 milliseconds for epoch 0, of which 339 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,015] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 332 milliseconds for epoch 0, of which 331 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,020] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 336 milliseconds for epoch 0, of which 335 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,026] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 342 milliseconds for epoch 0, of which 341 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,028] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 344 milliseconds for epoch 0, of which 344 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,046] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 362 milliseconds for epoch 0, of which 347 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,058] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 374 milliseconds for epoch 0, of which 363 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,058] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 361 milliseconds for epoch 0, of which 361 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,062] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 364 milliseconds for epoch 0, of which 364 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,062] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 364 milliseconds for epoch 0, of which 364 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
schema-registry                    | [2023-08-04 10:11:15,088] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:11:15,102] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:15,103] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:15,103] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:15,105] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 407 milliseconds for epoch 0, of which 407 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,114] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 416 milliseconds for epoch 0, of which 415 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,136] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 438 milliseconds for epoch 0, of which 438 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,144] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 446 milliseconds for epoch 0, of which 445 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,149] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 450 milliseconds for epoch 0, of which 449 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,161] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 400 milliseconds for epoch 0, of which 399 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:11:15,170] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 377 milliseconds for epoch 0, of which 376 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
schema-registry                    | [2023-08-04 10:11:15,211] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:15,213] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:15,439] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-b7ca4d5f-3414-46c5-9779-24e8259cf145 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
schema-registry                    | [2023-08-04 10:11:15,538] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: need to re-join with the given member-id: sr-1-b7ca4d5f-3414-46c5-9779-24e8259cf145 (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:15,544] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:11:15,545] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker                             | [2023-08-04 10:11:15,658] INFO [GroupCoordinator 1]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-b7ca4d5f-3414-46c5-9779-24e8259cf145 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:11:15,739] INFO [GroupCoordinator 1]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
schema-registry                    | [2023-08-04 10:11:15,752] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Successfully joined group with generation Generation{generationId=1, memberId='sr-1-b7ca4d5f-3414-46c5-9779-24e8259cf145', protocol='v0'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
rest-proxy                         | [2023-08-04 10:11:15,795] INFO Started o.e.j.s.ServletContextHandler@4aeaadc1{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
broker                             | [2023-08-04 10:11:15,941] INFO [GroupCoordinator 1]: Assignment received from leader sr-1-b7ca4d5f-3414-46c5-9779-24e8259cf145 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:11:16,132] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                     | 	metrics.jmx.prefix = confluent.controlcenter
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = true
control-center                     | 	port = 9021
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
schema-registry                    | [2023-08-04 10:11:16,140] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Successfully synced group in generation Generation{generationId=1, memberId='sr-1-b7ca4d5f-3414-46c5-9779-24e8259cf145', protocol='v0'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
rest-proxy                         | [2023-08-04 10:11:16,152] INFO Started o.e.j.s.ServletContextHandler@107e5441{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
control-center                     | [2023-08-04 10:11:16,154] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                     | 	metrics.jmx.prefix = confluent.controlcenter
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = true
control-center                     | 	port = 9021
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
control-center                     | [2023-08-04 10:11:16,176] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.jmx.prefix = rest-utils
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = false
control-center                     | 	port = 8080
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
schema-registry                    | [2023-08-04 10:11:16,184] INFO Finished rebalance with leader election result: Assignment{version=1, error=0, leader='sr-1-b7ca4d5f-3414-46c5-9779-24e8259cf145', leaderIdentity=version=1,host=schema-registry,port=8081,scheme=http,leaderEligibility=true} (io.confluent.kafka.schemaregistry.leaderelector.kafka.KafkaGroupLeaderElector)
control-center                     | [2023-08-04 10:11:16,213] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.jmx.prefix = rest-utils
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = false
control-center                     | 	port = 8080
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
control-center                     | [2023-08-04 10:11:16,237] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.jmx.prefix = rest-utils
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = false
control-center                     | 	port = 8080
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
control-center                     | [2023-08-04 10:11:16,254] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.jmx.prefix = rest-utils
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = false
control-center                     | 	port = 8080
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
control-center                     | [2023-08-04 10:11:16,268] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.jmx.prefix = rest-utils
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = false
control-center                     | 	port = 8080
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
schema-registry                    | [2023-08-04 10:11:16,544] INFO Wait to catch up until the offset at 1 (io.confluent.kafka.schemaregistry.storage.KafkaStore)
schema-registry                    | [2023-08-04 10:11:16,658] INFO Reached offset at 1 (io.confluent.kafka.schemaregistry.storage.KafkaStore)
rest-proxy                         | [2023-08-04 10:11:16,700] INFO Started NetworkTrafficServerConnector@2ef14fe{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8082} (org.eclipse.jetty.server.AbstractConnector)
rest-proxy                         | [2023-08-04 10:11:16,730] INFO Started @63471ms (org.eclipse.jetty.server.Server)
rest-proxy                         | [2023-08-04 10:11:16,730] INFO Server started, listening for requests... (io.confluent.kafkarest.KafkaRestMain)
control-center                     | [2023-08-04 10:11:16,746] INFO com.linecorp.armeria.verboseExceptions: rate-limit=10 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:16,814] INFO com.linecorp.armeria.preferredIpV4Addresses:  (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:17,168] INFO com.linecorp.armeria.verboseSocketExceptions: false (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:17,169] INFO com.linecorp.armeria.verboseResponses: false (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:17,169] INFO com.linecorp.armeria.warnNettyVersions: true (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:17,674] WARN Inconsistent Netty versions detected: {netty-buffer=netty-buffer-4.1.86.Final.cde0e2d050, netty-codec=netty-codec-4.1.86.Final.cde0e2d050, netty-codec-dns=netty-codec-dns-4.1.86.Final.cde0e2d050, netty-codec-http=netty-codec-http-4.1.86.Final.cde0e2d050, netty-codec-http2=netty-codec-http2-4.1.86.Final.cde0e2d (repository: dirty), netty-codec-socks=netty-codec-socks-4.1.86.Final.cde0e2d (repository: dirty), netty-common=netty-common-4.1.86.Final.cde0e2d050, netty-handler=netty-handler-4.1.86.Final.cde0e2d050, netty-handler-proxy=netty-handler-proxy-4.1.92.Final.acc3525 (repository: dirty), netty-resolver=netty-resolver-4.1.86.Final.cde0e2d050, netty-resolver-dns=netty-resolver-dns-4.1.86.Final.cde0e2d050, netty-resolver-dns-classes-macos=netty-resolver-dns-classes-macos-4.1.86.Final.cde0e2d050, netty-resolver-dns-native-macos=netty-resolver-dns-native-macos-4.1.86.Final.cde0e2d050, netty-transport=netty-transport-4.1.86.Final.cde0e2d050, netty-transport-classes-epoll=netty-transport-classes-epoll-4.1.86.Final.cde0e2d (repository: dirty), netty-transport-classes-kqueue=netty-transport-classes-kqueue-4.1.86.Final.cde0e2d050, netty-transport-native-epoll=netty-transport-native-epoll-4.1.86.Final.cde0e2d (repository: dirty), netty-transport-native-kqueue=netty-transport-native-kqueue-4.1.86.Final.cde0e2d050, netty-transport-native-unix-common=netty-transport-native-unix-common-4.1.86.Final.cde0e2d050} This means 1) you specified Netty versions inconsistently in your build or 2) the Netty JARs in the classpath were repackaged or shaded incorrectly. Specify the '-Dcom.linecorp.armeria.warnNettyVersions=false' JVM option to disable this warning at the risk of unexpected Netty behavior, if you think it is a false positive. (com.linecorp.armeria.internal.common.util.TransportTypeProvider)
schema-registry                    | [2023-08-04 10:11:17,994] INFO Binding SchemaRegistryRestApplication to all listeners. (io.confluent.rest.Application)
control-center                     | [2023-08-04 10:11:19,407] INFO com.linecorp.armeria.useEpoll: false (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,413] INFO com.linecorp.armeria.transportType: nio (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,558] INFO com.linecorp.armeria.maxNumConnections: 2147483647 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,570] INFO com.linecorp.armeria.numCommonWorkers: 8 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,571] INFO com.linecorp.armeria.numCommonBlockingTaskThreads: 200 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,587] INFO com.linecorp.armeria.defaultMaxRequestLength: 10485760 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,598] INFO com.linecorp.armeria.defaultMaxResponseLength: 10485760 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,599] INFO com.linecorp.armeria.defaultRequestTimeoutMillis: 10000 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,600] INFO com.linecorp.armeria.defaultResponseTimeoutMillis: 15000 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,605] INFO com.linecorp.armeria.defaultConnectTimeoutMillis: 3200 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,611] INFO com.linecorp.armeria.defaultWriteTimeoutMillis: 1000 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,612] INFO com.linecorp.armeria.defaultServerIdleTimeoutMillis: 15000 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,614] INFO com.linecorp.armeria.defaultClientIdleTimeoutMillis: 10000 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,615] INFO com.linecorp.armeria.defaultPingIntervalMillis: 0 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,635] INFO com.linecorp.armeria.defaultMaxServerNumRequestsPerConnection: 0 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,637] INFO com.linecorp.armeria.defaultMaxClientNumRequestsPerConnection: 0 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,644] INFO com.linecorp.armeria.defaultMaxServerConnectionAgeMillis: 0 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,658] INFO com.linecorp.armeria.defaultMaxClientConnectionAgeMillis: 0 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,669] INFO com.linecorp.armeria.defaultServerConnectionDrainDurationMicros: 1000000 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,670] INFO com.linecorp.armeria.defaultHttp2InitialConnectionWindowSize: 1048576 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,672] INFO com.linecorp.armeria.defaultHttp2InitialStreamWindowSize: 1048576 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,678] INFO com.linecorp.armeria.defaultHttp2MaxFrameSize: 16384 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,684] INFO com.linecorp.armeria.defaultHttp2MaxStreamsPerConnection: 2147483647 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,689] INFO com.linecorp.armeria.defaultHttp2MaxHeaderListSize: 8192 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,690] INFO com.linecorp.armeria.defaultHttp1MaxInitialLineLength: 4096 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,695] INFO com.linecorp.armeria.defaultHttp1MaxHeaderSize: 8192 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,699] INFO com.linecorp.armeria.defaultHttp1MaxChunkSize: 8192 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,700] INFO com.linecorp.armeria.defaultUseHttp2Preface: true (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,700] INFO com.linecorp.armeria.defaultUseHttp1Pipelining: false (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,701] INFO com.linecorp.armeria.defaultBackoffSpec: exponential=200:10000,jitter=0.2 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,702] INFO com.linecorp.armeria.defaultMaxTotalAttempts: 10 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,729] INFO com.linecorp.armeria.routeCache: maximumSize=4096 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,729] INFO com.linecorp.armeria.routeDecoratorCache: maximumSize=4096 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,731] INFO com.linecorp.armeria.parsedPathCache: maximumSize=4096 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,731] INFO com.linecorp.armeria.headerValueCache: maximumSize=4096 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,742] INFO com.linecorp.armeria.cachedHeaders: :authority,:scheme,:method,accept-encoding,content-type (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,751] INFO com.linecorp.armeria.fileServiceCache: maximumSize=1024 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,755] INFO com.linecorp.armeria.dnsCacheSpec: maximumSize=4096 (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,757] INFO com.linecorp.armeria.annotatedServiceExceptionVerbosity: unhandled (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,773] INFO com.linecorp.armeria.useJdkDnsResolver: false (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,776] INFO com.linecorp.armeria.reportBlockedEventLoop: true (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,783] INFO com.linecorp.armeria.validateHeaders: true (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,784] INFO com.linecorp.armeria.tlsAllowUnsafeCiphers: false (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,786] INFO com.linecorp.armeria.transientServiceOptions:  (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,821] INFO com.linecorp.armeria.useLegacyRouteDecoratorOrdering: false (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,825] INFO com.linecorp.armeria.useDefaultSocketOptions: true (default) (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:19,827] INFO Using nio (com.linecorp.armeria.common.Flags)
schema-registry                    | [2023-08-04 10:11:20,023] INFO jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.18+10-LTS (org.eclipse.jetty.server.Server)
schema-registry                    | [2023-08-04 10:11:21,408] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
schema-registry                    | [2023-08-04 10:11:21,408] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
schema-registry                    | [2023-08-04 10:11:21,443] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session)
control-center                     | [2023-08-04 10:11:21,467] INFO IPv6: disabled (from /proc/sys/net/ipv6/conf/all/disable_ipv6) (com.linecorp.armeria.common.util.SystemInfo)
control-center                     | [2023-08-04 10:11:22,622] INFO com.linecorp.armeria.useOpenSsl: true (default) (com.linecorp.armeria.common.Flags)
data-agrigator-taskmanager-1       | 2023-08-04 10:11:24,263 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:11:24,289 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                     | [2023-08-04 10:11:25,999] INFO Using OpenSSL: BoringSSL, 0x1010107f (com.linecorp.armeria.common.Flags)
control-center                     | [2023-08-04 10:11:26,000] INFO com.linecorp.armeria.dumpOpenSslInfo: false (default) (com.linecorp.armeria.common.Flags)
schema-registry                    | Aug 04, 2023 10:11:28 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
schema-registry                    | WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.ConfigResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.ConfigResource will be ignored. 
schema-registry                    | Aug 04, 2023 10:11:28 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
schema-registry                    | WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.ContextsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.ContextsResource will be ignored. 
schema-registry                    | Aug 04, 2023 10:11:28 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
schema-registry                    | WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.SubjectsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.SubjectsResource will be ignored. 
schema-registry                    | Aug 04, 2023 10:11:28 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
schema-registry                    | WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.SchemasResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.SchemasResource will be ignored. 
schema-registry                    | Aug 04, 2023 10:11:28 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
schema-registry                    | WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.SubjectVersionsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.SubjectVersionsResource will be ignored. 
schema-registry                    | Aug 04, 2023 10:11:28 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
schema-registry                    | WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.CompatibilityResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.CompatibilityResource will be ignored. 
schema-registry                    | Aug 04, 2023 10:11:28 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
schema-registry                    | WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.ModeResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.ModeResource will be ignored. 
schema-registry                    | Aug 04, 2023 10:11:28 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
schema-registry                    | WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.ServerMetadataResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.ServerMetadataResource will be ignored. 
control-center                     | [2023-08-04 10:11:28,875] INFO HV000001: Hibernate Validator null (org.hibernate.validator.internal.util.Version)
schema-registry                    | [2023-08-04 10:11:30,781] INFO HV000001: Hibernate Validator 6.1.7.Final (org.hibernate.validator.internal.util.Version)
control-center                     | [2023-08-04 10:11:31,152] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                     | 	metrics.jmx.prefix = confluent.controlcenter
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = true
control-center                     | 	port = 9021
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
control-center                     | [2023-08-04 10:11:32,146] INFO getPersistentStoreTopicNames=[_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center                     | [2023-08-04 10:11:32,196] INFO getLruStoreTopicNames=[_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center                     | [2023-08-04 10:11:32,223] INFO getWindowedStoreTopicNames=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center                     | [2023-08-04 10:11:32,243] INFO getLogAppendTimeIntermediateTopicNames=[_confluent-controlcenter-7-4-1-1-cluster-rekey, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store] (io.confluent.controlcenter.ControlCenterModule)
control-center                     | [2023-08-04 10:11:32,245] INFO intermediateTopics=[_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center                     | [2023-08-04 10:11:32,272] INFO StreamsConfig values: 
control-center                     | 	acceptable.recovery.lag = 10000
control-center                     | 	application.id = _confluent-controlcenter-7-4-1-1-command
control-center                     | 	application.server = 
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffered.records.per.partition = 1000
control-center                     | 	built.in.metrics.version = latest
control-center                     | 	cache.max.bytes.buffering = 0
control-center                     | 	client.id = 
control-center                     | 	commit.interval.ms = 30000
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
control-center                     | 	default.dsl.store = rocksDB
control-center                     | 	default.key.serde = null
control-center                     | 	default.list.key.serde.inner = null
control-center                     | 	default.list.key.serde.type = null
control-center                     | 	default.list.value.serde.inner = null
control-center                     | 	default.list.value.serde.type = null
control-center                     | 	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
control-center                     | 	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
control-center                     | 	default.value.serde = null
control-center                     | 	max.task.idle.ms = 0
control-center                     | 	max.warmup.replicas = 2
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	num.standby.replicas = 0
control-center                     | 	num.stream.threads = 1
control-center                     | 	poll.ms = 100
control-center                     | 	probing.rebalance.interval.ms = 600000
control-center                     | 	processing.guarantee = at_least_once
control-center                     | 	rack.aware.assignment.tags = []
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	repartition.purge.interval.ms = 30000
control-center                     | 	replication.factor = -1
control-center                     | 	request.timeout.ms = 40000
control-center                     | 	retries = 0
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	rocksdb.config.setter = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	state.cleanup.delay.ms = 600000
control-center                     | 	state.dir = /var/lib/confluent-control-center/1/cp-command
control-center                     | 	statestore.cache.max.bytes = 10485760
control-center                     | 	task.timeout.ms = 0
control-center                     | 	topology.optimization = all
control-center                     | 	upgrade.from = 2.3
control-center                     | 	window.size.ms = null
control-center                     | 	windowed.inner.class.serde = null
control-center                     | 	windowstore.changelog.additional.retention.ms = 86400000
control-center                     |  (org.apache.kafka.streams.StreamsConfig)
control-center                     | [2023-08-04 10:11:32,372] WARN Deprecated config cache.max.bytes.buffering is set, and will be used; we suggest setting the new config statestore.cache.max.bytes instead as deprecated cache.max.bytes.buffering would be removed in the future. (org.apache.kafka.streams.internals.StreamsConfigUtils)
control-center                     | [2023-08-04 10:11:32,515] INFO No process id found on disk, got fresh process id 1b232679-b755-411f-9a4d-31973ee6bb71 (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                     | [2023-08-04 10:11:32,815] INFO AdminClientConfig values: 
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-admin
control-center                     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	confluent.use.controller.listener = false
control-center                     | 	connections.max.idle.ms = 300000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                     | [2023-08-04 10:11:32,905] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:32,905] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:32,905] INFO Kafka startTimeMs: 1691143892905 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:32,925] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71] Kafka Streams version: 7.4.1-ce (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:11:32,925] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71] Kafka Streams commit ID: 96cc303d3f85bf31 (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:11:33,029] WARN Deprecated config cache.max.bytes.buffering is set, and will be used; we suggest setting the new config statestore.cache.max.bytes instead as deprecated cache.max.bytes.buffering would be removed in the future. (org.apache.kafka.streams.internals.StreamsConfigUtils)
control-center                     | [2023-08-04 10:11:33,066] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:11:33,089] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = true
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-restore-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = null
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 300000
control-center                     | 	max.poll.records = 1000
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:11:33,330] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:33,330] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:33,330] INFO Kafka startTimeMs: 1691143893330 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:33,397] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:11:33,409] INFO ProducerConfig values: 
control-center                     | 	acks = -1
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	batch.size = 16384
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffer.memory = 33554432
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-producer
control-center                     | 	compression.type = lz4
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	delivery.timeout.ms = 2147483647
control-center                     | 	enable.idempotence = true
control-center                     | 	interceptor.classes = []
control-center                     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     | 	linger.ms = 500
control-center                     | 	max.block.ms = 9223372036854775807
control-center                     | 	max.in.flight.requests.per.connection = 5
control-center                     | 	max.request.size = 10485760
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metadata.max.idle.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partitioner.adaptive.partitioning.enable = true
control-center                     | 	partitioner.availability.timeout.ms = 0
control-center                     | 	partitioner.class = null
control-center                     | 	partitioner.ignore.keys = false
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	transaction.timeout.ms = 60000
control-center                     | 	transactional.id = null
control-center                     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                     | [2023-08-04 10:11:33,425] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:11:33,510] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:33,511] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:33,511] INFO Kafka startTimeMs: 1691143893510 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:33,605] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:11:33,613] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-producer] ProducerId set to 1 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                     | [2023-08-04 10:11:33,632] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:11:33,657] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = false
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = _confluent-controlcenter-7-4-1-1-command
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 300000
control-center                     | 	max.poll.records = 1000
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:11:33,834] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:11:33,836] WARN stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:11:33,977] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:33,979] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:33,979] INFO Kafka startTimeMs: 1691143893977 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:34,065] INFO ProducerConfig values: 
control-center                     | 	acks = -1
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	batch.size = 16384
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffer.memory = 33554432
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = c3-command
control-center                     | 	compression.type = lz4
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	delivery.timeout.ms = 2147483647
control-center                     | 	enable.idempotence = true
control-center                     | 	interceptor.classes = []
control-center                     | 	key.serializer = class io.confluent.serializers.ProtoSerde
control-center                     | 	linger.ms = 500
control-center                     | 	max.block.ms = 9223372036854775807
control-center                     | 	max.in.flight.requests.per.connection = 5
control-center                     | 	max.request.size = 10485760
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metadata.max.idle.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partitioner.adaptive.partitioning.enable = true
control-center                     | 	partitioner.availability.timeout.ms = 0
control-center                     | 	partitioner.class = null
control-center                     | 	partitioner.ignore.keys = false
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	transaction.timeout.ms = 60000
control-center                     | 	transactional.id = null
control-center                     | 	value.serializer = class io.confluent.serializers.ProtoSerde
control-center                     |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                     | [2023-08-04 10:11:34,082] INFO [Producer clientId=c3-command] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:11:34,130] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:34,145] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:34,145] INFO Kafka startTimeMs: 1691143894129 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:34,214] INFO [Producer clientId=c3-command] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:11:34,233] INFO [Producer clientId=c3-command] ProducerId set to 2 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
data-agrigator-taskmanager-1       | 2023-08-04 10:11:34,343 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:11:34,354 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                     | [2023-08-04 10:11:34,411] INFO getPersistentStoreTopicNames=[_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center                     | [2023-08-04 10:11:34,425] INFO getLruStoreTopicNames=[_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center                     | [2023-08-04 10:11:34,437] INFO getWindowedStoreTopicNames=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center                     | [2023-08-04 10:11:34,469] INFO getLogAppendTimeIntermediateTopicNames=[_confluent-controlcenter-7-4-1-1-cluster-rekey, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store] (io.confluent.controlcenter.ControlCenterModule)
control-center                     | [2023-08-04 10:11:34,476] INFO intermediateTopics=[_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition] (io.confluent.controlcenter.ControlCenterModule)
schema-registry                    | [2023-08-04 10:11:34,615] INFO Started o.e.j.s.ServletContextHandler@7e744f43{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
schema-registry                    | [2023-08-04 10:11:34,808] INFO Started o.e.j.s.ServletContextHandler@716e431d{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
schema-registry                    | [2023-08-04 10:11:35,221] INFO Started NetworkTrafficServerConnector@115667d{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8081} (org.eclipse.jetty.server.AbstractConnector)
schema-registry                    | [2023-08-04 10:11:35,226] INFO Started @82035ms (org.eclipse.jetty.server.Server)
schema-registry                    | [2023-08-04 10:11:35,255] INFO Schema Registry version: 7.4.1 commitId: f4ff7e1dd48c4f284ceb318402bf5676127638f7 (io.confluent.kafka.schemaregistry.rest.SchemaRegistryMain)
schema-registry                    | [2023-08-04 10:11:35,255] INFO Server started, listening for requests... (io.confluent.kafka.schemaregistry.rest.SchemaRegistryMain)
control-center                     | WARNING: An illegal reflective access operation has occurred
control-center                     | WARNING: Illegal reflective access by retrofit2.Platform (file:/usr/share/java/acl/acl-7.4.1.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class,int)
control-center                     | WARNING: Please consider reporting this to the maintainers of retrofit2.Platform
control-center                     | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
control-center                     | WARNING: All illegal access operations will be denied in a future release
control-center                     | [2023-08-04 10:11:36,417] INFO StreamsConfig values: 
control-center                     | 	acceptable.recovery.lag = 10000
control-center                     | 	application.id = _confluent-controlcenter-7-4-1-1-command
control-center                     | 	application.server = 
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffered.records.per.partition = 1000
control-center                     | 	built.in.metrics.version = latest
control-center                     | 	cache.max.bytes.buffering = 0
control-center                     | 	client.id = 
control-center                     | 	commit.interval.ms = 30000
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
control-center                     | 	default.dsl.store = rocksDB
control-center                     | 	default.key.serde = null
control-center                     | 	default.list.key.serde.inner = null
control-center                     | 	default.list.key.serde.type = null
control-center                     | 	default.list.value.serde.inner = null
control-center                     | 	default.list.value.serde.type = null
control-center                     | 	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
control-center                     | 	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
control-center                     | 	default.value.serde = null
control-center                     | 	max.task.idle.ms = 0
control-center                     | 	max.warmup.replicas = 2
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	num.standby.replicas = 0
control-center                     | 	num.stream.threads = 1
control-center                     | 	poll.ms = 100
control-center                     | 	probing.rebalance.interval.ms = 600000
control-center                     | 	processing.guarantee = at_least_once
control-center                     | 	rack.aware.assignment.tags = []
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	repartition.purge.interval.ms = 30000
control-center                     | 	replication.factor = -1
control-center                     | 	request.timeout.ms = 40000
control-center                     | 	retries = 0
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	rocksdb.config.setter = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	state.cleanup.delay.ms = 600000
control-center                     | 	state.dir = /var/lib/confluent-control-center/1/cp-command
control-center                     | 	statestore.cache.max.bytes = 10485760
control-center                     | 	task.timeout.ms = 0
control-center                     | 	topology.optimization = all
control-center                     | 	upgrade.from = 2.3
control-center                     | 	window.size.ms = null
control-center                     | 	windowed.inner.class.serde = null
control-center                     | 	windowstore.changelog.additional.retention.ms = 86400000
control-center                     |  (org.apache.kafka.streams.StreamsConfig)
control-center                     | [2023-08-04 10:11:36,468] INFO AdminClientConfig values: 
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1
control-center                     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	confluent.use.controller.listener = false
control-center                     | 	connections.max.idle.ms = 300000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                     | [2023-08-04 10:11:36,506] WARN These configurations '[replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                     | [2023-08-04 10:11:36,515] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:36,516] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:36,517] INFO Kafka startTimeMs: 1691143896515 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:36,807] INFO App info kafka.admin.client for _confluent-controlcenter-license-manager-7-4-1-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:36,848] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:11:36,848] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:11:36,848] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:11:36,879] INFO Starting License Store (io.confluent.license.LicenseStore)
control-center                     | [2023-08-04 10:11:36,879] INFO Starting KafkaBasedLog with topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
control-center                     | [2023-08-04 10:11:36,884] INFO AdminClientConfig values: 
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1
control-center                     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	confluent.use.controller.listener = false
control-center                     | 	connections.max.idle.ms = 300000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                     | [2023-08-04 10:11:36,930] WARN These configurations '[replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                     | [2023-08-04 10:11:36,942] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:36,943] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:36,943] INFO Kafka startTimeMs: 1691143896942 (org.apache.kafka.common.utils.AppInfoParser)
broker                             | [2023-08-04 10:11:37,032] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:37,033] INFO [Controller 1] Created topic _confluent-command with topic ID 26mdvJTUTviIQl0i3yP-hQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:37,033] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-command'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:37,033] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-command'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:37,034] INFO [Controller 1] Created partition _confluent-command-0 with topic ID 26mdvJTUTviIQl0i3yP-hQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:37,062] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:37,062] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-command-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:37,062] INFO [Broker id=1] Creating new partition _confluent-command-0 with topic id 26mdvJTUTviIQl0i3yP-hQ. (state.change.logger)
broker                             | [2023-08-04 10:11:37,076] INFO [LogLoader partition=_confluent-command-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:37,097] INFO Created log for partition _confluent-command-0 in /tmp/kraft-combined-logs/_confluent-command-0 with properties {cleanup.policy=compact, min.insync.replicas=1} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:37,104] INFO [Partition _confluent-command-0 broker=1] No checkpointed highwatermark is found for partition _confluent-command-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:37,104] INFO [Partition _confluent-command-0 broker=1] Log loaded for partition _confluent-command-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:37,105] INFO [Broker id=1] Leader _confluent-command-0 with topic id Some(26mdvJTUTviIQl0i3yP-hQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:37,116] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-command with new configuration : cleanup.policy -> compact,min.insync.replicas -> 1 (kafka.server.metadata.DynamicConfigPublisher)
control-center                     | [2023-08-04 10:11:37,136] INFO App info kafka.admin.client for _confluent-controlcenter-license-manager-7-4-1-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:37,153] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:11:37,157] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:11:37,158] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:11:37,161] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = true
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1-global-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = null
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 300000
control-center                     | 	max.poll.records = 1000
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 120000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:11:37,202] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:37,203] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:37,204] INFO Kafka startTimeMs: 1691143897202 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:37,292] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:11:37,326] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:11:37,327] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:11:37,328] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:11:37,364] INFO App info kafka.consumer for _confluent-controlcenter-license-manager-7-4-1-1-global-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:37,367] INFO ProducerConfig values: 
control-center                     | 	acks = -1
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	batch.size = 16384
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffer.memory = 33554432
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1
control-center                     | 	compression.type = lz4
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	delivery.timeout.ms = 2147483647
control-center                     | 	enable.idempotence = false
control-center                     | 	interceptor.classes = []
control-center                     | 	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
control-center                     | 	linger.ms = 500
control-center                     | 	max.block.ms = 9223372036854775807
control-center                     | 	max.in.flight.requests.per.connection = 1
control-center                     | 	max.request.size = 10485760
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metadata.max.idle.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partitioner.adaptive.partitioning.enable = true
control-center                     | 	partitioner.availability.timeout.ms = 0
control-center                     | 	partitioner.class = null
control-center                     | 	partitioner.ignore.keys = false
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	transaction.timeout.ms = 60000
control-center                     | 	transactional.id = null
control-center                     | 	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
control-center                     |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                     | [2023-08-04 10:11:37,435] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:37,439] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:37,441] INFO Kafka startTimeMs: 1691143897435 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:37,451] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = true
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = earliest
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1-global-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = null
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 300000
control-center                     | 	max.poll.records = 1000
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 120000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:11:37,501] INFO [Producer clientId=_confluent-controlcenter-license-manager-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:11:37,519] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:37,536] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:37,537] INFO Kafka startTimeMs: 1691143897519 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:37,580] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:11:37,591] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Assigned to partition(s): _confluent-command-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:11:37,616] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Seeking to earliest offset of partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:11:37,676] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-command-0 to 0 since the associated topicId changed from null to 26mdvJTUTviIQl0i3yP-hQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:11:37,775] INFO Finished reading KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
control-center                     | [2023-08-04 10:11:37,776] INFO Started KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
control-center                     | [2023-08-04 10:11:37,777] INFO Started License Store (io.confluent.license.LicenseStore)
control-center                     | [2023-08-04 10:11:38,493] INFO AdminClientConfig values: 
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1
control-center                     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	confluent.use.controller.listener = false
control-center                     | 	connections.max.idle.ms = 300000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                     | [2023-08-04 10:11:38,556] WARN These configurations '[replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                     | [2023-08-04 10:11:38,572] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:38,579] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:38,580] INFO Kafka startTimeMs: 1691143898572 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:38,657] INFO App info kafka.admin.client for _confluent-controlcenter-license-manager-7-4-1-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:38,695] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:11:38,695] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:11:38,695] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:11:39,591] INFO AdminClientConfig values: 
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1
control-center                     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	confluent.use.controller.listener = false
control-center                     | 	connections.max.idle.ms = 300000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                     | [2023-08-04 10:11:39,620] WARN These configurations '[replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                     | [2023-08-04 10:11:39,627] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:39,628] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:39,629] INFO Kafka startTimeMs: 1691143899627 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:39,661] INFO App info kafka.admin.client for _confluent-controlcenter-license-manager-7-4-1-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:11:39,676] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:11:39,677] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:11:39,677] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:11:39,678] INFO License for single cluster, single node (io.confluent.license.LicenseManager)
control-center                     | [2023-08-04 10:11:39,682] INFO License: Free Tier license for Confluent Enterprise. (io.confluent.controlcenter.license.LicenseModule)
control-center                     | [2023-08-04 10:11:39,682] INFO License: Free Tier license for Confluent Enterprise. (io.confluent.controlcenter.license.LicenseModule)
control-center                     | [2023-08-04 10:11:39,706] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                     | 	metrics.jmx.prefix = confluent.controlcenter
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = true
control-center                     | 	port = 9021
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
ksqldb-server                      | [2023-08-04 10:11:43,052] INFO Adding function ComplexFunction for method public java.lang.Object io.confluent.ksql.function.UdfLoaderTest$ComplexUdf.foo(java.lang.String) (io.confluent.ksql.function.UdfLoader)
control-center                     | [2023-08-04 10:11:43,574] INFO Reflections took 3756 ms to scan 5 urls, producing 215 keys and 847 values  (org.reflections.Reflections)
control-center                     | [2023-08-04 10:11:43,713] INFO EventEmitterConfig values: 
control-center                     |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
control-center                     | [2023-08-04 10:11:43,718] INFO EventEmitterConfig values: 
control-center                     |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
ksqldb-server                      | [2023-08-04 10:11:43,808] INFO Adding function ConfigurableUdf for method public int io.confluent.ksql.function.UdfLoaderTest$ConfigurableUdf.foo(int) (io.confluent.ksql.function.UdfLoader)
control-center                     | [2023-08-04 10:11:43,820] INFO Linux CPU collector enabled: true (io.confluent.telemetry.ConfluentTelemetryConfig)
ksqldb-server                      | [2023-08-04 10:11:43,823] INFO Adding function DecimalStruct for method public org.apache.kafka.connect.data.Struct io.confluent.ksql.function.UdfLoaderTest$DecimalStructUdf.getDecimalStruct() (io.confluent.ksql.function.UdfLoader)
control-center                     | [2023-08-04 10:11:43,821] INFO Using cpu metric: io\.confluent\.kafka\.server/server/linux_system_cpu_utilization (io.confluent.telemetry.ConfluentTelemetryConfig)
ksqldb-server                      | [2023-08-04 10:11:43,854] INFO Adding function KsqlStructUdf for method public org.apache.kafka.connect.data.Struct io.confluent.ksql.function.UdfLoaderTest$KsqlStructUdf.getDecimalStruct() (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:43,869] INFO Adding function ReturnDecimal for method public java.math.BigDecimal io.confluent.ksql.function.UdfLoaderTest$ReturnDecimalUdf.foo(java.math.BigDecimal) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:43,882] INFO Adding function ReturnIncompatible for method public java.lang.String io.confluent.ksql.function.UdfLoaderTest$ReturnIncompatibleUdf.foo(java.math.BigDecimal) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:43,887] INFO Adding function SomeFunction for method public int io.confluent.ksql.function.UdfLoaderTest$SomeFunctionUdf.foo(java.lang.String,java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:43,897] INFO Adding function AS_VALUE for method public java.lang.Object io.confluent.ksql.function.udf.AsValue.asValue(java.lang.Object) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:43,904] INFO Adding function bad_test_udf for method public org.apache.kafka.connect.data.Struct io.confluent.ksql.function.udf.BadTestUdf.returnList(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:43,910] INFO Adding function bad_udf for method public java.lang.String io.confluent.ksql.function.udf.BadUdf.blowUp(int) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:43,916] INFO Adding function bad_udf for method public int io.confluent.ksql.function.udf.BadUdf.mightThrow(boolean) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:43,919] INFO Adding function bad_udf for method public java.lang.String io.confluent.ksql.function.udf.BadUdf.returnNull(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:43,924] INFO Adding function test_udf for method public java.lang.String io.confluent.ksql.function.udf.TestUdf.doStuffLongVarargs(long[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:43,986] INFO Adding function test_udf for method public java.lang.String io.confluent.ksql.function.udf.TestUdf.doStuffStruct(org.apache.kafka.connect.data.Struct) (io.confluent.ksql.function.UdfLoader)
control-center                     | [2023-08-04 10:11:44,011] INFO ConfluentTelemetryConfig values: 
control-center                     | 	confluent.telemetry.api.key = null
control-center                     | 	confluent.telemetry.api.secret = null
control-center                     | 	confluent.telemetry.debug.enabled = false
control-center                     | 	confluent.telemetry.enabled = false
control-center                     | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
control-center                     | 	confluent.telemetry.events.enable = true
control-center                     | 	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*
control-center                     | 	confluent.telemetry.metrics.collector.interval.ms = 60000
control-center                     | 	confluent.telemetry.metrics.collector.slo.enabled = false
control-center                     | 	confluent.telemetry.proxy.password = null
control-center                     | 	confluent.telemetry.proxy.url = null
control-center                     | 	confluent.telemetry.proxy.username = null
control-center                     |  (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                     | [2023-08-04 10:11:44,016] INFO VolumeMetricsCollectorConfig values: 
control-center                     | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
control-center                     |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
ksqldb-server                      | [2023-08-04 10:11:44,078] INFO Adding function test_udf for method public org.apache.kafka.connect.data.Struct io.confluent.ksql.function.udf.TestUdf.returnStructStuff() (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,086] INFO Adding function test_udf for method public java.lang.String io.confluent.ksql.function.udf.TestUdf.doStuffIntString(int,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,088] INFO Adding function test_udf for method public java.lang.String io.confluent.ksql.function.udf.TestUdf.doStuffLongString(long,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,090] INFO Adding function test_udf for method public java.lang.String io.confluent.ksql.function.udf.TestUdf.doStuffLongLongString(long,long,java.lang.String) (io.confluent.ksql.function.UdfLoader)
control-center                     | [2023-08-04 10:11:44,100] INFO HttpExporterConfig values: 
control-center                     | 	api.key = null
control-center                     | 	api.secret = null
control-center                     | 	buffer.batch.duration.max.ms = null
control-center                     | 	buffer.batch.items.max = null
control-center                     | 	buffer.inflight.submissions.max = null
control-center                     | 	buffer.pending.batches.max = null
control-center                     | 	client.attempts.max = null
control-center                     | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center                     | 	client.compression = null
control-center                     | 	client.connect.timeout.ms = null
control-center                     | 	client.contentType = null
control-center                     | 	client.request.timeout.ms = null
control-center                     | 	client.retry.delay.seconds = null
control-center                     | 	enabled = false
control-center                     | 	events.enabled = true
control-center                     | 	metrics.enabled = true
control-center                     | 	metrics.include = null
control-center                     | 	proxy.password = null
control-center                     | 	proxy.url = null
control-center                     | 	proxy.username = null
control-center                     | 	type = http
control-center                     |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
ksqldb-server                      | [2023-08-04 10:11:44,102] INFO Adding function WhenCondition for method public boolean io.confluent.ksql.function.udf.WhenCondition.evaluate(boolean,boolean) (io.confluent.ksql.function.UdfLoader)
control-center                     | [2023-08-04 10:11:44,110] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
ksqldb-server                      | [2023-08-04 10:11:44,111] INFO Adding function WhenResult for method public int io.confluent.ksql.function.udf.WhenResult.evaluate(int,boolean) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,116] INFO Adding function array_concat for method public java.util.List io.confluent.ksql.function.udf.array.ArrayConcat.concat(java.util.List,java.util.List) (io.confluent.ksql.function.UdfLoader)
control-center                     | [2023-08-04 10:11:44,120] INFO RemoteConfigConfiguration values: 
control-center                     | 	enabled = true
control-center                     | 	polling.interval.ms = 60000
control-center                     |  (io.confluent.shaded.io.confluent.telemetry.config.remote.RemoteConfigConfiguration)
ksqldb-server                      | [2023-08-04 10:11:44,131] INFO Adding function array_distinct for method public java.util.List io.confluent.ksql.function.udf.array.ArrayDistinct.distinct(java.util.List) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,139] INFO Adding function array_except for method public java.util.List io.confluent.ksql.function.udf.array.ArrayExcept.except(java.util.List,java.util.List) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,144] INFO Adding function array_intersect for method public java.util.List io.confluent.ksql.function.udf.array.ArrayIntersect.intersect(java.util.List,java.util.List) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,153] INFO Adding function ARRAY_JOIN for method public java.lang.String io.confluent.ksql.function.udf.array.ArrayJoin.join(java.util.List,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,158] INFO Adding function ARRAY_JOIN for method public java.lang.String io.confluent.ksql.function.udf.array.ArrayJoin.join(java.util.List) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,165] INFO Adding function ARRAY_LENGTH for method public java.lang.Integer io.confluent.ksql.function.udf.array.ArrayLength.calcArrayLength(java.util.List) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,175] INFO Adding function array_max for method public java.lang.Comparable io.confluent.ksql.function.udf.array.ArrayMax.arrayMax(java.util.List) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,193] INFO Adding function array_min for method public java.lang.Comparable io.confluent.ksql.function.udf.array.ArrayMin.arrayMin(java.util.List) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,211] INFO Adding function array_remove for method public java.util.List io.confluent.ksql.function.udf.array.ArrayRemove.remove(java.util.List,java.lang.Object) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,244] INFO Adding function array_sort for method public java.util.List io.confluent.ksql.function.udf.array.ArraySort.arraySortDefault(java.util.List) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,250] INFO Adding function array_sort for method public java.util.List io.confluent.ksql.function.udf.array.ArraySort.arraySortWithDirection(java.util.List,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,256] INFO Adding function array_union for method public java.util.List io.confluent.ksql.function.udf.array.ArrayUnion.union(java.util.List,java.util.List) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,280] INFO Adding function ENTRIES for method public java.util.List io.confluent.ksql.function.udf.array.Entries.entriesString(java.util.Map,boolean) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,336] INFO Adding function ENTRIES for method public java.util.List io.confluent.ksql.function.udf.array.Entries.entriesInt(java.util.Map,boolean) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,353] INFO Adding function ENTRIES for method public java.util.List io.confluent.ksql.function.udf.array.Entries.entriesBigInt(java.util.Map,boolean) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,373] INFO Adding function ENTRIES for method public java.util.List io.confluent.ksql.function.udf.array.Entries.entriesDouble(java.util.Map,boolean) (io.confluent.ksql.function.UdfLoader)
control-center                     | [2023-08-04 10:11:44,384] WARN Ignoring redefinition of existing telemetry label controlcenter.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
ksqldb-server                      | [2023-08-04 10:11:44,404] INFO Adding function ENTRIES for method public java.util.List io.confluent.ksql.function.udf.array.Entries.entriesBoolean(java.util.Map,boolean) (io.confluent.ksql.function.UdfLoader)
control-center                     | [2023-08-04 10:11:44,405] INFO ConfluentTelemetryConfig values: 
control-center                     | 	confluent.telemetry.api.key = null
control-center                     | 	confluent.telemetry.api.secret = null
control-center                     | 	confluent.telemetry.debug.enabled = false
control-center                     | 	confluent.telemetry.enabled = false
control-center                     | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
control-center                     | 	confluent.telemetry.events.enable = true
control-center                     | 	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.controlcenter/.*(metrics_input_topic_progress|monitoring_input_topic_progress|misconfigured_topics|missing_topic_configurations|broker_log_persistent_dir|cluster_offline|streams_status|total_lag|request_latency|response_size|response_rate).*
control-center                     | 	confluent.telemetry.metrics.collector.interval.ms = 60000
control-center                     | 	confluent.telemetry.metrics.collector.slo.enabled = false
control-center                     | 	confluent.telemetry.proxy.password = null
control-center                     | 	confluent.telemetry.proxy.url = null
control-center                     | 	confluent.telemetry.proxy.username = null
control-center                     |  (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                     | [2023-08-04 10:11:44,406] INFO VolumeMetricsCollectorConfig values: 
control-center                     | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
control-center                     |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
control-center                     | [2023-08-04 10:11:44,406] INFO HttpExporterConfig values: 
control-center                     | 	api.key = null
control-center                     | 	api.secret = null
control-center                     | 	buffer.batch.duration.max.ms = null
control-center                     | 	buffer.batch.items.max = null
control-center                     | 	buffer.inflight.submissions.max = null
control-center                     | 	buffer.pending.batches.max = null
control-center                     | 	client.attempts.max = null
control-center                     | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center                     | 	client.compression = null
control-center                     | 	client.connect.timeout.ms = null
control-center                     | 	client.contentType = null
control-center                     | 	client.request.timeout.ms = null
control-center                     | 	client.retry.delay.seconds = null
control-center                     | 	enabled = false
control-center                     | 	events.enabled = true
control-center                     | 	metrics.enabled = true
control-center                     | 	metrics.include = null
control-center                     | 	proxy.password = null
control-center                     | 	proxy.url = null
control-center                     | 	proxy.username = null
control-center                     | 	type = http
control-center                     |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
control-center                     | [2023-08-04 10:11:44,407] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                     | [2023-08-04 10:11:44,407] INFO RemoteConfigConfiguration values: 
control-center                     | 	enabled = true
control-center                     | 	polling.interval.ms = 60000
control-center                     |  (io.confluent.shaded.io.confluent.telemetry.config.remote.RemoteConfigConfiguration)
data-agrigator-taskmanager-1       | 2023-08-04 10:11:44,408 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                     | [2023-08-04 10:11:44,418] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
data-agrigator-taskmanager-1       | 2023-08-04 10:11:44,401 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
ksqldb-server                      | [2023-08-04 10:11:44,427] INFO Adding function GENERATE_SERIES for method public java.util.List io.confluent.ksql.function.udf.array.GenerateSeries.generateSeriesInt(int,int,int) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,432] INFO Adding function GENERATE_SERIES for method public java.util.List io.confluent.ksql.function.udf.array.GenerateSeries.generateSeriesInt(int,int) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,436] INFO Adding function GENERATE_SERIES for method public java.util.List io.confluent.ksql.function.udf.array.GenerateSeries.generateSeriesLong(long,long,int) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,440] INFO Adding function GENERATE_SERIES for method public java.util.List io.confluent.ksql.function.udf.array.GenerateSeries.generateSeriesLong(long,long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,445] INFO Adding function bigint_from_bytes for method public java.lang.Long io.confluent.ksql.function.udf.conversions.BigIntFromBytes.bigIntFromBytes(java.nio.ByteBuffer,java.lang.String) (io.confluent.ksql.function.UdfLoader)
control-center                     | [2023-08-04 10:11:44,453] INFO EventLoggerConfig values: 
control-center                     | 	event.logger.cloudevent.codec = structured
control-center                     | 	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter
control-center                     |  (io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig)
ksqldb-server                      | [2023-08-04 10:11:44,454] INFO Adding function bigint_from_bytes for method public java.lang.Long io.confluent.ksql.function.udf.conversions.BigIntFromBytes.bigIntFromBytes(java.nio.ByteBuffer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,460] INFO Adding function double_from_bytes for method public java.lang.Double io.confluent.ksql.function.udf.conversions.DoubleFromBytes.doubleFromBytes(java.nio.ByteBuffer,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,462] INFO Adding function double_from_bytes for method public java.lang.Double io.confluent.ksql.function.udf.conversions.DoubleFromBytes.doubleFromBytes(java.nio.ByteBuffer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,472] INFO Adding function int_from_bytes for method public java.lang.Integer io.confluent.ksql.function.udf.conversions.IntFromBytes.intFromBytes(java.nio.ByteBuffer,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,476] INFO Adding function int_from_bytes for method public java.lang.Integer io.confluent.ksql.function.udf.conversions.IntFromBytes.intFromBytes(java.nio.ByteBuffer) (io.confluent.ksql.function.UdfLoader)
control-center                     | [2023-08-04 10:11:44,486] INFO HttpExporterConfig values: 
control-center                     | 	api.key = null
control-center                     | 	api.secret = null
control-center                     | 	buffer.batch.duration.max.ms = null
control-center                     | 	buffer.batch.items.max = null
control-center                     | 	buffer.inflight.submissions.max = null
control-center                     | 	buffer.pending.batches.max = null
control-center                     | 	client.attempts.max = null
control-center                     | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center                     | 	client.compression = null
control-center                     | 	client.connect.timeout.ms = null
control-center                     | 	client.request.timeout.ms = null
control-center                     | 	client.retry.delay.seconds = null
control-center                     | 	enabled = false
control-center                     | 	events.enabled = true
control-center                     | 	metrics.enabled = true
control-center                     | 	proxy.password = null
control-center                     | 	proxy.url = null
control-center                     | 	proxy.username = null
control-center                     | 	type = http
control-center                     |  (io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig)
ksqldb-server                      | [2023-08-04 10:11:44,489] INFO Adding function convert_tz for method public java.sql.Timestamp io.confluent.ksql.function.udf.datetime.ConvertTz.convertTz(java.sql.Timestamp,java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,494] INFO Adding function dateadd for method public java.sql.Date io.confluent.ksql.function.udf.datetime.DateAdd.dateAdd(java.util.concurrent.TimeUnit,java.lang.Integer,java.sql.Date) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,515] INFO Adding function datesub for method public java.sql.Date io.confluent.ksql.function.udf.datetime.DateSub.dateSub(java.util.concurrent.TimeUnit,java.lang.Integer,java.sql.Date) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,809] INFO Adding function datetostring for method public java.lang.String io.confluent.ksql.function.udf.datetime.DateToString.dateToString(int,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,839] INFO Adding function format_date for method public java.lang.String io.confluent.ksql.function.udf.datetime.FormatDate.formatDate(java.sql.Date,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,845] INFO Adding function format_time for method public java.lang.String io.confluent.ksql.function.udf.datetime.FormatTime.formatTime(java.sql.Time,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,858] INFO Adding function format_timestamp for method public java.lang.String io.confluent.ksql.function.udf.datetime.FormatTimestamp.formatTimestamp(java.sql.Timestamp,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,870] INFO Adding function format_timestamp for method public java.lang.String io.confluent.ksql.function.udf.datetime.FormatTimestamp.formatTimestamp(java.sql.Timestamp,java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,874] INFO Adding function from_days for method public java.sql.Date io.confluent.ksql.function.udf.datetime.FromDays.fromDays(int) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,890] INFO Adding function from_unixtime for method public java.sql.Timestamp io.confluent.ksql.function.udf.datetime.FromUnixTime.fromUnixTime(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,913] INFO Adding function parse_date for method public java.sql.Date io.confluent.ksql.function.udf.datetime.ParseDate.parseDate(java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,932] INFO Adding function parse_time for method public java.sql.Time io.confluent.ksql.function.udf.datetime.ParseTime.parseTime(java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,983] INFO Adding function parse_timestamp for method public java.sql.Timestamp io.confluent.ksql.function.udf.datetime.ParseTimestamp.parseTimestamp(java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:44,987] INFO Adding function parse_timestamp for method public java.sql.Timestamp io.confluent.ksql.function.udf.datetime.ParseTimestamp.parseTimestamp(java.lang.String,java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:45,003] INFO Adding function stringtodate for method public int io.confluent.ksql.function.udf.datetime.StringToDate.stringToDate(java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:45,026] INFO Adding function stringtotimestamp for method public long io.confluent.ksql.function.udf.datetime.StringToTimestamp.stringToTimestamp(java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:45,032] INFO Adding function stringtotimestamp for method public long io.confluent.ksql.function.udf.datetime.StringToTimestamp.stringToTimestamp(java.lang.String,java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:45,055] INFO Adding function timeadd for method public java.sql.Time io.confluent.ksql.function.udf.datetime.TimeAdd.timeAdd(java.util.concurrent.TimeUnit,java.lang.Integer,java.sql.Time) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:45,059] INFO Adding function timesub for method public java.sql.Time io.confluent.ksql.function.udf.datetime.TimeSub.timeSub(java.util.concurrent.TimeUnit,java.lang.Integer,java.sql.Time) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:45,075] INFO Adding function timestampadd for method public java.sql.Timestamp io.confluent.ksql.function.udf.datetime.TimestampAdd.timestampAdd(java.util.concurrent.TimeUnit,java.lang.Integer,java.sql.Timestamp) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:45,080] INFO Adding function timestampsub for method public java.sql.Timestamp io.confluent.ksql.function.udf.datetime.TimestampSub.timestampSub(java.util.concurrent.TimeUnit,java.lang.Integer,java.sql.Timestamp) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:45,115] INFO Adding function timestamptostring for method public java.lang.String io.confluent.ksql.function.udf.datetime.TimestampToString.timestampToString(long,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:45,122] INFO Adding function timestamptostring for method public java.lang.String io.confluent.ksql.function.udf.datetime.TimestampToString.timestampToString(long,java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:45,143] INFO Adding function unix_date for method public int io.confluent.ksql.function.udf.datetime.UnixDate.unixDate() (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:45,147] INFO Adding function unix_date for method public java.lang.Integer io.confluent.ksql.function.udf.datetime.UnixDate.unixDate(java.sql.Date) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:45,152] INFO Adding function unix_timestamp for method public long io.confluent.ksql.function.udf.datetime.UnixTimestamp.unixTimestamp() (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:45,154] INFO Adding function unix_timestamp for method public java.lang.Long io.confluent.ksql.function.udf.datetime.UnixTimestamp.unixTimestamp(java.sql.Timestamp) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:45,213] INFO Adding function geo_distance for method public java.lang.Double io.confluent.ksql.function.udf.geo.GeoDistance.geoDistance(double,double,double,double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:45,220] INFO Adding function geo_distance for method public java.lang.Double io.confluent.ksql.function.udf.geo.GeoDistance.geoDistance(double,double,double,double,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:45,232] INFO Adding function IS_JSON_STRING for method public boolean io.confluent.ksql.function.udf.json.IsJsonString.check(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:46,966] INFO Adding function JSON_ARRAY_CONTAINS for method public java.lang.Boolean io.confluent.ksql.function.udf.json.JsonArrayContains.contains(java.lang.String,java.lang.Object) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:46,982] INFO Adding function JSON_ARRAY_LENGTH for method public java.lang.Integer io.confluent.ksql.function.udf.json.JsonArrayLength.length(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:46,989] INFO Adding function JSON_CONCAT for method public java.lang.String io.confluent.ksql.function.udf.json.JsonConcat.concat(java.lang.String[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:46,996] INFO Adding function extractjsonfield for method public java.lang.String io.confluent.ksql.function.udf.json.JsonExtractString.extract(java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,001] INFO Adding function JSON_ITEMS for method public java.util.List io.confluent.ksql.function.udf.json.JsonItems.items(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,005] INFO Adding function JSON_KEYS for method public java.util.List io.confluent.ksql.function.udf.json.JsonKeys.keys(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,010] INFO Adding function JSON_RECORDS for method public java.util.Map io.confluent.ksql.function.udf.json.JsonRecords.records(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,018] INFO Adding function TO_JSON_STRING for method public java.lang.String io.confluent.ksql.function.udf.json.ToJsonString.toJsonString(java.lang.Object) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,022] INFO Adding function filter for method public java.util.Map io.confluent.ksql.function.udf.lambda.Filter.filterMap(java.util.Map,java.util.function.BiFunction) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,026] INFO Adding function filter for method public java.util.List io.confluent.ksql.function.udf.lambda.Filter.filterArray(java.util.List,java.util.function.Function) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,043] INFO Adding function reduce for method public java.lang.Object io.confluent.ksql.function.udf.lambda.Reduce.reduceMap(java.util.Map,java.lang.Object,io.confluent.ksql.execution.codegen.helpers.TriFunction) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,046] INFO Adding function reduce for method public java.lang.Object io.confluent.ksql.function.udf.lambda.Reduce.reduceArray(java.util.List,java.lang.Object,java.util.function.BiFunction) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,051] INFO Adding function transform for method public java.util.Map io.confluent.ksql.function.udf.lambda.Transform.transformMap(java.util.Map,java.util.function.BiFunction,java.util.function.BiFunction) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,054] INFO Adding function transform for method public java.util.List io.confluent.ksql.function.udf.lambda.Transform.transformArray(java.util.List,java.util.function.Function) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,058] INFO Adding function ARRAY_CONTAINS for method public boolean io.confluent.ksql.function.udf.list.ArrayContains.contains(java.util.List,java.lang.Object) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,075] INFO Adding function slice for method public java.util.List io.confluent.ksql.function.udf.list.Slice.slice(java.util.List,java.lang.Integer,java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,080] INFO Adding function AS_MAP for method public final java.util.Map io.confluent.ksql.function.udf.map.AsMap.asMap(java.util.List,java.util.List) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,099] INFO Adding function map_keys for method public java.util.List io.confluent.ksql.function.udf.map.MapKeys.mapKeys(java.util.Map) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,102] INFO Adding function map_union for method public java.util.Map io.confluent.ksql.function.udf.map.MapUnion.union(java.util.Map,java.util.Map) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,115] INFO Adding function map_values for method public java.util.List io.confluent.ksql.function.udf.map.MapValues.mapValues(java.util.Map) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,119] INFO Adding function Abs for method public java.math.BigDecimal io.confluent.ksql.function.udf.math.Abs.abs(java.math.BigDecimal) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,121] INFO Adding function Abs for method public java.lang.Double io.confluent.ksql.function.udf.math.Abs.abs(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,123] INFO Adding function Abs for method public java.lang.Long io.confluent.ksql.function.udf.math.Abs.abs(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,124] INFO Adding function Abs for method public java.lang.Integer io.confluent.ksql.function.udf.math.Abs.abs(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,134] INFO Adding function acos for method public java.lang.Double io.confluent.ksql.function.udf.math.Acos.acos(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,136] INFO Adding function acos for method public java.lang.Double io.confluent.ksql.function.udf.math.Acos.acos(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,137] INFO Adding function acos for method public java.lang.Double io.confluent.ksql.function.udf.math.Acos.acos(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,142] INFO Adding function asin for method public java.lang.Double io.confluent.ksql.function.udf.math.Asin.asin(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,148] INFO Adding function asin for method public java.lang.Double io.confluent.ksql.function.udf.math.Asin.asin(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,155] INFO Adding function asin for method public java.lang.Double io.confluent.ksql.function.udf.math.Asin.asin(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,160] INFO Adding function atan for method public java.lang.Double io.confluent.ksql.function.udf.math.Atan.atan(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,176] INFO Adding function atan for method public java.lang.Double io.confluent.ksql.function.udf.math.Atan.atan(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,178] INFO Adding function atan for method public java.lang.Double io.confluent.ksql.function.udf.math.Atan.atan(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,182] INFO Adding function atan2 for method public java.lang.Double io.confluent.ksql.function.udf.math.Atan2.atan2(java.lang.Double,java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,184] INFO Adding function atan2 for method public java.lang.Double io.confluent.ksql.function.udf.math.Atan2.atan2(java.lang.Long,java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,187] INFO Adding function atan2 for method public java.lang.Double io.confluent.ksql.function.udf.math.Atan2.atan2(java.lang.Integer,java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,192] INFO Adding function cbrt for method public java.lang.Double io.confluent.ksql.function.udf.math.Cbrt.cbrt(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,206] INFO Adding function cbrt for method public java.lang.Double io.confluent.ksql.function.udf.math.Cbrt.cbrt(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,208] INFO Adding function cbrt for method public java.lang.Double io.confluent.ksql.function.udf.math.Cbrt.cbrt(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,223] INFO Adding function Ceil for method public java.math.BigDecimal io.confluent.ksql.function.udf.math.Ceil.ceil(java.math.BigDecimal) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,225] INFO Adding function Ceil for method public java.lang.Double io.confluent.ksql.function.udf.math.Ceil.ceil(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,227] INFO Adding function Ceil for method public java.lang.Long io.confluent.ksql.function.udf.math.Ceil.ceil(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,240] INFO Adding function Ceil for method public java.lang.Integer io.confluent.ksql.function.udf.math.Ceil.ceil(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,244] INFO Adding function cos for method public java.lang.Double io.confluent.ksql.function.udf.math.Cos.cos(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,246] INFO Adding function cos for method public java.lang.Double io.confluent.ksql.function.udf.math.Cos.cos(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,248] INFO Adding function cos for method public java.lang.Double io.confluent.ksql.function.udf.math.Cos.cos(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,268] INFO Adding function cosh for method public java.lang.Double io.confluent.ksql.function.udf.math.Cosh.cosh(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,270] INFO Adding function cosh for method public java.lang.Double io.confluent.ksql.function.udf.math.Cosh.cosh(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,271] INFO Adding function cosh for method public java.lang.Double io.confluent.ksql.function.udf.math.Cosh.cosh(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,276] INFO Adding function cot for method public java.lang.Double io.confluent.ksql.function.udf.math.Cot.cot(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,289] INFO Adding function cot for method public java.lang.Double io.confluent.ksql.function.udf.math.Cot.cot(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,291] INFO Adding function cot for method public java.lang.Double io.confluent.ksql.function.udf.math.Cot.cot(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,296] INFO Adding function degrees for method public java.lang.Double io.confluent.ksql.function.udf.math.Degrees.degrees(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,312] INFO Adding function degrees for method public java.lang.Double io.confluent.ksql.function.udf.math.Degrees.degrees(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,314] INFO Adding function degrees for method public java.lang.Double io.confluent.ksql.function.udf.math.Degrees.degrees(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,318] INFO Adding function exp for method public java.lang.Double io.confluent.ksql.function.udf.math.Exp.exp(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,324] INFO Adding function exp for method public java.lang.Double io.confluent.ksql.function.udf.math.Exp.exp(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,326] INFO Adding function exp for method public java.lang.Double io.confluent.ksql.function.udf.math.Exp.exp(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,332] INFO Adding function Floor for method public java.math.BigDecimal io.confluent.ksql.function.udf.math.Floor.floor(java.math.BigDecimal) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,339] INFO Adding function Floor for method public java.lang.Double io.confluent.ksql.function.udf.math.Floor.floor(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,341] INFO Adding function Floor for method public java.lang.Long io.confluent.ksql.function.udf.math.Floor.floor(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,346] INFO Adding function Floor for method public java.lang.Integer io.confluent.ksql.function.udf.math.Floor.floor(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,358] INFO Adding function greatest for method public java.sql.Time io.confluent.ksql.function.udf.math.Greatest.greatest(java.sql.Time,java.sql.Time[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,379] INFO Adding function greatest for method public java.sql.Date io.confluent.ksql.function.udf.math.Greatest.greatest(java.sql.Date,java.sql.Date[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,385] INFO Adding function greatest for method public java.nio.ByteBuffer io.confluent.ksql.function.udf.math.Greatest.greatest(java.nio.ByteBuffer,java.nio.ByteBuffer[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,390] INFO Adding function greatest for method public java.sql.Timestamp io.confluent.ksql.function.udf.math.Greatest.greatest(java.sql.Timestamp,java.sql.Timestamp[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,402] INFO Adding function greatest for method public java.math.BigDecimal io.confluent.ksql.function.udf.math.Greatest.greatest(java.math.BigDecimal,java.math.BigDecimal[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,412] INFO Adding function greatest for method public java.lang.Integer io.confluent.ksql.function.udf.math.Greatest.greatest(java.lang.Integer,java.lang.Integer[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,427] INFO Adding function greatest for method public java.lang.Long io.confluent.ksql.function.udf.math.Greatest.greatest(java.lang.Long,java.lang.Long[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,430] INFO Adding function greatest for method public java.lang.Double io.confluent.ksql.function.udf.math.Greatest.greatest(java.lang.Double,java.lang.Double[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,434] INFO Adding function greatest for method public java.lang.String io.confluent.ksql.function.udf.math.Greatest.greatest(java.lang.String,java.lang.String[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,455] INFO Adding function least for method public java.sql.Time io.confluent.ksql.function.udf.math.Least.least(java.sql.Time,java.sql.Time[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,463] INFO Adding function least for method public java.sql.Timestamp io.confluent.ksql.function.udf.math.Least.least(java.sql.Timestamp,java.sql.Timestamp[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,479] INFO Adding function least for method public java.math.BigDecimal io.confluent.ksql.function.udf.math.Least.least(java.math.BigDecimal,java.math.BigDecimal[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,491] INFO Adding function least for method public java.sql.Date io.confluent.ksql.function.udf.math.Least.least(java.sql.Date,java.sql.Date[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,493] INFO Adding function least for method public java.lang.Long io.confluent.ksql.function.udf.math.Least.least(java.lang.Long,java.lang.Long[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,516] INFO Adding function least for method public java.lang.Double io.confluent.ksql.function.udf.math.Least.least(java.lang.Double,java.lang.Double[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,521] INFO Adding function least for method public java.lang.String io.confluent.ksql.function.udf.math.Least.least(java.lang.String,java.lang.String[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,530] INFO Adding function least for method public java.nio.ByteBuffer io.confluent.ksql.function.udf.math.Least.least(java.nio.ByteBuffer,java.nio.ByteBuffer[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,567] INFO Adding function least for method public java.lang.Integer io.confluent.ksql.function.udf.math.Least.least(java.lang.Integer,java.lang.Integer[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,585] INFO Adding function ln for method public java.lang.Double io.confluent.ksql.function.udf.math.Ln.ln(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,615] INFO Adding function ln for method public java.lang.Double io.confluent.ksql.function.udf.math.Ln.ln(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,617] INFO Adding function ln for method public java.lang.Double io.confluent.ksql.function.udf.math.Ln.ln(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,641] INFO Adding function log for method public java.lang.Double io.confluent.ksql.function.udf.math.Log.log(java.lang.Integer,java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,645] INFO Adding function log for method public java.lang.Double io.confluent.ksql.function.udf.math.Log.log(java.lang.Long,java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,656] INFO Adding function log for method public java.lang.Double io.confluent.ksql.function.udf.math.Log.log(java.lang.Double,java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,658] INFO Adding function log for method public java.lang.Double io.confluent.ksql.function.udf.math.Log.log(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,660] INFO Adding function log for method public java.lang.Double io.confluent.ksql.function.udf.math.Log.log(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,661] INFO Adding function log for method public java.lang.Double io.confluent.ksql.function.udf.math.Log.log(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,667] INFO Adding function pi for method public java.lang.Double io.confluent.ksql.function.udf.math.Pi.pi() (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,673] INFO Adding function power for method public java.lang.Double io.confluent.ksql.function.udf.math.Power.power(java.lang.Double,java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,675] INFO Adding function power for method public java.lang.Double io.confluent.ksql.function.udf.math.Power.power(java.lang.Long,java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,684] INFO Adding function power for method public java.lang.Double io.confluent.ksql.function.udf.math.Power.power(java.lang.Integer,java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,689] INFO Adding function radians for method public java.lang.Double io.confluent.ksql.function.udf.math.Radians.radians(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,690] INFO Adding function radians for method public java.lang.Double io.confluent.ksql.function.udf.math.Radians.radians(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,694] INFO Adding function radians for method public java.lang.Double io.confluent.ksql.function.udf.math.Radians.radians(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,706] INFO Adding function random for method public java.lang.Double io.confluent.ksql.function.udf.math.Random.random() (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,711] INFO Adding function Round for method public java.lang.Double io.confluent.ksql.function.udf.math.Round.round(java.lang.Double,java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,713] INFO Adding function Round for method public java.math.BigDecimal io.confluent.ksql.function.udf.math.Round.round(java.math.BigDecimal) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,718] INFO Adding function Round for method public java.math.BigDecimal io.confluent.ksql.function.udf.math.Round.round(java.math.BigDecimal,java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,722] INFO Adding function Round for method public java.lang.Long io.confluent.ksql.function.udf.math.Round.round(long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,723] INFO Adding function Round for method public java.lang.Long io.confluent.ksql.function.udf.math.Round.round(int) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,724] INFO Adding function Round for method public java.lang.Long io.confluent.ksql.function.udf.math.Round.round(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,728] INFO Adding function sign for method public java.lang.Integer io.confluent.ksql.function.udf.math.Sign.sign(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,729] INFO Adding function sign for method public java.lang.Integer io.confluent.ksql.function.udf.math.Sign.sign(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,729] INFO Adding function sign for method public java.lang.Integer io.confluent.ksql.function.udf.math.Sign.sign(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,733] INFO Adding function sin for method public java.lang.Double io.confluent.ksql.function.udf.math.Sin.sin(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,734] INFO Adding function sin for method public java.lang.Double io.confluent.ksql.function.udf.math.Sin.sin(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,734] INFO Adding function sin for method public java.lang.Double io.confluent.ksql.function.udf.math.Sin.sin(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,738] INFO Adding function sinh for method public java.lang.Double io.confluent.ksql.function.udf.math.Sinh.sinh(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,739] INFO Adding function sinh for method public java.lang.Double io.confluent.ksql.function.udf.math.Sinh.sinh(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,740] INFO Adding function sinh for method public java.lang.Double io.confluent.ksql.function.udf.math.Sinh.sinh(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,746] INFO Adding function sqrt for method public java.lang.Double io.confluent.ksql.function.udf.math.Sqrt.sqrt(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,748] INFO Adding function sqrt for method public java.lang.Double io.confluent.ksql.function.udf.math.Sqrt.sqrt(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,749] INFO Adding function sqrt for method public java.lang.Double io.confluent.ksql.function.udf.math.Sqrt.sqrt(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,752] INFO Adding function tan for method public java.lang.Double io.confluent.ksql.function.udf.math.Tan.tan(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,753] INFO Adding function tan for method public java.lang.Double io.confluent.ksql.function.udf.math.Tan.tan(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,754] INFO Adding function tan for method public java.lang.Double io.confluent.ksql.function.udf.math.Tan.tan(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,763] INFO Adding function tanh for method public java.lang.Double io.confluent.ksql.function.udf.math.Tanh.tanh(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,764] INFO Adding function tanh for method public java.lang.Double io.confluent.ksql.function.udf.math.Tanh.tanh(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,765] INFO Adding function tanh for method public java.lang.Double io.confluent.ksql.function.udf.math.Tanh.tanh(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,774] INFO Adding function trunc for method public java.lang.Double io.confluent.ksql.function.udf.math.Trunc.trunc(java.lang.Double,java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,777] INFO Adding function trunc for method public java.math.BigDecimal io.confluent.ksql.function.udf.math.Trunc.trunc(java.math.BigDecimal) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,780] INFO Adding function trunc for method public java.math.BigDecimal io.confluent.ksql.function.udf.math.Trunc.trunc(java.math.BigDecimal,java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,788] INFO Adding function trunc for method public java.lang.Long io.confluent.ksql.function.udf.math.Trunc.trunc(java.lang.Long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,790] INFO Adding function trunc for method public java.lang.Long io.confluent.ksql.function.udf.math.Trunc.trunc(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,793] INFO Adding function trunc for method public java.lang.Long io.confluent.ksql.function.udf.math.Trunc.trunc(java.lang.Double) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,815] INFO Adding function COALESCE for method public final java.lang.Object io.confluent.ksql.function.udf.nulls.Coalesce.coalesce(java.lang.Object,java.lang.Object[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,824] INFO Adding function IFNULL for method public final java.lang.Object io.confluent.ksql.function.udf.nulls.IfNull.ifNull(java.lang.Object,java.lang.Object) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,837] INFO Adding function NULLIF for method public final java.lang.Object io.confluent.ksql.function.udf.nulls.NullIf.nullIf(java.lang.Object,java.lang.Object) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,857] INFO Adding function Chr for method public java.lang.String io.confluent.ksql.function.udf.string.Chr.chr(java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,859] INFO Adding function Chr for method public java.lang.String io.confluent.ksql.function.udf.string.Chr.chr(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,869] INFO Adding function concat for method public java.lang.String io.confluent.ksql.function.udf.string.Concat.concat(java.lang.String[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,874] INFO Adding function concat for method public java.nio.ByteBuffer io.confluent.ksql.function.udf.string.Concat.concat(java.nio.ByteBuffer[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,884] INFO Adding function concat_ws for method public java.lang.String io.confluent.ksql.function.udf.string.ConcatWS.concatWS(java.lang.String[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,897] INFO Adding function concat_ws for method public java.nio.ByteBuffer io.confluent.ksql.function.udf.string.ConcatWS.concatWS(java.nio.ByteBuffer[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,903] INFO Adding function elt for method public java.lang.String io.confluent.ksql.function.udf.string.Elt.elt(int,java.lang.String[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,969] INFO Adding function encode for method public java.lang.String io.confluent.ksql.function.udf.string.Encode.encode(java.lang.String,java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,974] INFO Adding function field for method public int io.confluent.ksql.function.udf.string.Field.field(java.lang.String,java.lang.String[]) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,990] INFO Adding function from_bytes for method public java.lang.String io.confluent.ksql.function.udf.string.FromBytes.fromBytes(java.nio.ByteBuffer,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:47,995] INFO Adding function initcap for method public java.lang.String io.confluent.ksql.function.udf.string.InitCap.initcap(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,011] INFO Adding function instr for method public int io.confluent.ksql.function.udf.string.Instr.instr(java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,013] INFO Adding function instr for method public int io.confluent.ksql.function.udf.string.Instr.instr(java.lang.String,java.lang.String,int,int) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,015] INFO Adding function instr for method public int io.confluent.ksql.function.udf.string.Instr.instr(java.lang.String,java.lang.String,int) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,026] INFO Adding function lcase for method public java.lang.String io.confluent.ksql.function.udf.string.LCase.lcase(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,030] INFO Adding function LPad for method public java.lang.String io.confluent.ksql.function.udf.string.LPad.lpad(java.lang.String,java.lang.Integer,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,034] INFO Adding function LPad for method public java.nio.ByteBuffer io.confluent.ksql.function.udf.string.LPad.lpad(java.nio.ByteBuffer,java.lang.Integer,java.nio.ByteBuffer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,040] INFO Adding function len for method public java.lang.Integer io.confluent.ksql.function.udf.string.Len.len(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,043] INFO Adding function len for method public java.lang.Integer io.confluent.ksql.function.udf.string.Len.len(java.nio.ByteBuffer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,051] INFO Adding function mask for method public java.lang.String io.confluent.ksql.function.udf.string.Mask.mask(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,062] INFO Adding function mask for method public java.lang.String io.confluent.ksql.function.udf.string.Mask.mask(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,074] INFO Adding function mask_keep_left for method public java.lang.String io.confluent.ksql.function.udf.string.MaskKeepLeft.mask(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,080] INFO Adding function mask_keep_left for method public java.lang.String io.confluent.ksql.function.udf.string.MaskKeepLeft.mask(java.lang.String,int) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,085] INFO Adding function mask_keep_right for method public java.lang.String io.confluent.ksql.function.udf.string.MaskKeepRight.mask(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,097] INFO Adding function mask_keep_right for method public java.lang.String io.confluent.ksql.function.udf.string.MaskKeepRight.mask(java.lang.String,int) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,107] INFO Adding function mask_left for method public java.lang.String io.confluent.ksql.function.udf.string.MaskLeft.mask(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,188] INFO Adding function mask_left for method public java.lang.String io.confluent.ksql.function.udf.string.MaskLeft.mask(java.lang.String,int) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,192] INFO Adding function mask_right for method public java.lang.String io.confluent.ksql.function.udf.string.MaskRight.mask(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,214] INFO Adding function mask_right for method public java.lang.String io.confluent.ksql.function.udf.string.MaskRight.mask(java.lang.String,int) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,236] INFO Adding function RPad for method public java.lang.String io.confluent.ksql.function.udf.string.RPad.rpad(java.lang.String,java.lang.Integer,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,238] INFO Adding function RPad for method public java.nio.ByteBuffer io.confluent.ksql.function.udf.string.RPad.rpad(java.nio.ByteBuffer,java.lang.Integer,java.nio.ByteBuffer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,265] INFO Adding function regexp_extract for method public java.lang.String io.confluent.ksql.function.udf.string.RegexpExtract.regexpExtract(java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,279] INFO Adding function regexp_extract for method public java.lang.String io.confluent.ksql.function.udf.string.RegexpExtract.regexpExtract(java.lang.String,java.lang.String,java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,299] INFO Adding function regexp_extract_all for method public java.util.List io.confluent.ksql.function.udf.string.RegexpExtractAll.regexpExtractAll(java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,303] INFO Adding function regexp_extract_all for method public java.util.List io.confluent.ksql.function.udf.string.RegexpExtractAll.regexpExtractAll(java.lang.String,java.lang.String,java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,316] INFO Adding function regexp_replace for method public java.lang.String io.confluent.ksql.function.udf.string.RegexpReplace.regexpReplace(java.lang.String,java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,352] INFO Adding function regexp_split_to_array for method public java.util.List io.confluent.ksql.function.udf.string.RegexpSplitToArray.regexpSplit(java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,370] INFO Adding function replace for method public java.lang.String io.confluent.ksql.function.udf.string.Replace.replace(java.lang.String,java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,377] INFO Adding function split for method public java.util.List io.confluent.ksql.function.udf.string.Split.split(java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,388] INFO Adding function split for method public java.util.List io.confluent.ksql.function.udf.string.Split.split(java.nio.ByteBuffer,java.nio.ByteBuffer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,420] INFO Adding function split_to_map for method public java.util.Map io.confluent.ksql.function.udf.string.SplitToMap.splitToMap(java.lang.String,java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,444] INFO Adding function substring for method public java.nio.ByteBuffer io.confluent.ksql.function.udf.string.Substring.substring(java.nio.ByteBuffer,java.lang.Integer,java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,452] INFO Adding function substring for method public java.nio.ByteBuffer io.confluent.ksql.function.udf.string.Substring.substring(java.nio.ByteBuffer,java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,453] INFO Adding function substring for method public java.lang.String io.confluent.ksql.function.udf.string.Substring.substring(java.lang.String,java.lang.Integer,java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,463] INFO Adding function substring for method public java.lang.String io.confluent.ksql.function.udf.string.Substring.substring(java.lang.String,java.lang.Integer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,479] INFO Adding function to_bytes for method public java.nio.ByteBuffer io.confluent.ksql.function.udf.string.ToBytes.toBytes(java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,488] INFO Adding function trim for method public java.lang.String io.confluent.ksql.function.udf.string.Trim.trim(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,496] INFO Adding function ucase for method public java.lang.String io.confluent.ksql.function.udf.string.UCase.ucase(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,500] INFO Adding function UUID for method public java.lang.String io.confluent.ksql.function.udf.string.Uuid.uuid() (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,505] INFO Adding function UUID for method public java.lang.String io.confluent.ksql.function.udf.string.Uuid.uuid(java.nio.ByteBuffer) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,522] INFO Adding function url_decode_param for method public java.lang.String io.confluent.ksql.function.udf.url.UrlDecodeParam.decodeParam(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,526] INFO Adding function url_encode_param for method public java.lang.String io.confluent.ksql.function.udf.url.UrlEncodeParam.encodeParam(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,559] INFO Adding function url_extract_fragment for method public java.lang.String io.confluent.ksql.function.udf.url.UrlExtractFragment.extractFragment(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,619] INFO Adding function url_extract_host for method public java.lang.String io.confluent.ksql.function.udf.url.UrlExtractHost.extractHost(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,925] INFO Adding function url_extract_parameter for method public java.lang.String io.confluent.ksql.function.udf.url.UrlExtractParameter.extractParam(java.lang.String,java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,930] INFO Adding function url_extract_path for method public java.lang.String io.confluent.ksql.function.udf.url.UrlExtractPath.extractPath(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,935] INFO Adding function url_extract_port for method public java.lang.Integer io.confluent.ksql.function.udf.url.UrlExtractPort.extractPort(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,955] INFO Adding function url_extract_protocol for method public java.lang.String io.confluent.ksql.function.udf.url.UrlExtractProtocol.extractProtocol(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,969] INFO Adding function url_extract_query for method public java.lang.String io.confluent.ksql.function.udf.url.UrlExtractQuery.extractQuery(java.lang.String) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:48,978] INFO Adding function E2EConfigurableUdf for method public long io.confluent.ksql.integration.EndToEndIntegrationTest$ConfigurableUdf.foo(long) (io.confluent.ksql.function.UdfLoader)
ksqldb-server                      | [2023-08-04 10:11:49,028] INFO Adding UDAF name=bad_test_udaf from path=internal class=class io.confluent.ksql.function.udaf.BadTestUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,131] INFO Adding UDAF name=bad_test_udaf from path=internal class=class io.confluent.ksql.function.udaf.BadTestUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,147] INFO Adding UDAF name=bad_test_udaf from path=internal class=class io.confluent.ksql.function.udaf.BadTestUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,149] INFO Adding UDAF name=bad_test_udaf from path=internal class=class io.confluent.ksql.function.udaf.BadTestUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,160] INFO Adding UDAF name=bad_test_udaf from path=internal class=class io.confluent.ksql.function.udaf.BadTestUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,186] INFO Adding UDAF name=bad_test_udaf from path=internal class=class io.confluent.ksql.function.udaf.BadTestUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,196] INFO Adding UDAF name=bad_test_udaf from path=internal class=class io.confluent.ksql.function.udaf.BadTestUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,214] INFO Adding UDAF name=bad_test_udaf from path=internal class=class io.confluent.ksql.function.udaf.BadTestUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,219] INFO Adding UDAF name=bad_test_udaf from path=internal class=class io.confluent.ksql.function.udaf.BadTestUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,247] INFO Adding UDAF name=FIVE_ARG from path=internal class=class io.confluent.ksql.function.udaf.FiveArgUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,262] INFO Adding UDAF name=FOUR_ARG from path=internal class=class io.confluent.ksql.function.udaf.FourArgUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,290] INFO Adding UDAF name=GENERIC_VAR_ARG from path=internal class=class io.confluent.ksql.function.udaf.GenericVarArgUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,300] INFO Adding UDAF name=MID_VAR_ARG from path=internal class=class io.confluent.ksql.function.udaf.MiddleVarArgUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,319] INFO Adding UDAF name=MULTI_ARG from path=internal class=class io.confluent.ksql.function.udaf.MultiArgUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,325] INFO Adding UDAF name=OBJ_COL_ARG from path=internal class=class io.confluent.ksql.function.udaf.ObjVarColArgUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,342] INFO Adding UDAF name=test_udaf from path=internal class=class io.confluent.ksql.function.udaf.TestUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,345] INFO Adding UDAF name=test_udaf from path=internal class=class io.confluent.ksql.function.udaf.TestUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,351] INFO Adding UDAF name=test_udaf from path=internal class=class io.confluent.ksql.function.udaf.TestUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,353] INFO Adding UDAF name=test_udaf from path=internal class=class io.confluent.ksql.function.udaf.TestUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,378] INFO Adding UDAF name=test_udaf from path=internal class=class io.confluent.ksql.function.udaf.TestUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,380] INFO Adding UDAF name=test_udaf from path=internal class=class io.confluent.ksql.function.udaf.TestUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,393] INFO Adding UDAF name=VAR_ARG from path=internal class=class io.confluent.ksql.function.udaf.VarArgUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,417] INFO Adding UDAF name=collect_list from path=internal class=class io.confluent.ksql.function.udaf.array.CollectListUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,424] INFO Adding UDAF name=collect_set from path=internal class=class io.confluent.ksql.function.udaf.array.CollectSetUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,441] INFO Adding UDAF name=ATTR from path=internal class=class io.confluent.ksql.function.udaf.attr.Attr (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,461] INFO Adding UDAF name=avg from path=internal class=class io.confluent.ksql.function.udaf.average.AverageUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,475] INFO Adding UDAF name=avg from path=internal class=class io.confluent.ksql.function.udaf.average.AverageUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,478] INFO Adding UDAF name=avg from path=internal class=class io.confluent.ksql.function.udaf.average.AverageUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,484] INFO Adding UDAF name=correlation from path=internal class=class io.confluent.ksql.function.udaf.correlation.CorrelationUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,496] INFO Adding UDAF name=correlation from path=internal class=class io.confluent.ksql.function.udaf.correlation.CorrelationUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,499] INFO Adding UDAF name=correlation from path=internal class=class io.confluent.ksql.function.udaf.correlation.CorrelationUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,504] INFO Adding UDAF name=COUNT_DISTINCT from path=internal class=class io.confluent.ksql.function.udaf.count.CountDistinct (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,512] INFO Adding UDAF name=COUNT from path=internal class=class io.confluent.ksql.function.udaf.count.CountKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,522] INFO Adding UDAF name=histogram from path=internal class=class io.confluent.ksql.function.udaf.map.HistogramUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,540] INFO Adding UDAF name=MAX from path=internal class=class io.confluent.ksql.function.udaf.max.MaxKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,542] INFO Adding UDAF name=MAX from path=internal class=class io.confluent.ksql.function.udaf.max.MaxKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,545] INFO Adding UDAF name=MAX from path=internal class=class io.confluent.ksql.function.udaf.max.MaxKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,548] INFO Adding UDAF name=MAX from path=internal class=class io.confluent.ksql.function.udaf.max.MaxKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,571] INFO Adding UDAF name=MAX from path=internal class=class io.confluent.ksql.function.udaf.max.MaxKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,573] INFO Adding UDAF name=MAX from path=internal class=class io.confluent.ksql.function.udaf.max.MaxKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,575] INFO Adding UDAF name=MAX from path=internal class=class io.confluent.ksql.function.udaf.max.MaxKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,577] INFO Adding UDAF name=MAX from path=internal class=class io.confluent.ksql.function.udaf.max.MaxKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,579] INFO Adding UDAF name=MAX from path=internal class=class io.confluent.ksql.function.udaf.max.MaxKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,587] INFO Adding UDAF name=MIN from path=internal class=class io.confluent.ksql.function.udaf.min.MinKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,589] INFO Adding UDAF name=MIN from path=internal class=class io.confluent.ksql.function.udaf.min.MinKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,591] INFO Adding UDAF name=MIN from path=internal class=class io.confluent.ksql.function.udaf.min.MinKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,592] INFO Adding UDAF name=MIN from path=internal class=class io.confluent.ksql.function.udaf.min.MinKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,594] INFO Adding UDAF name=MIN from path=internal class=class io.confluent.ksql.function.udaf.min.MinKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,595] INFO Adding UDAF name=MIN from path=internal class=class io.confluent.ksql.function.udaf.min.MinKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,596] INFO Adding UDAF name=MIN from path=internal class=class io.confluent.ksql.function.udaf.min.MinKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,597] INFO Adding UDAF name=MIN from path=internal class=class io.confluent.ksql.function.udaf.min.MinKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,598] INFO Adding UDAF name=MIN from path=internal class=class io.confluent.ksql.function.udaf.min.MinKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,603] INFO Adding UDAF name=EARLIEST_BY_OFFSET from path=internal class=class io.confluent.ksql.function.udaf.offset.EarliestByOffset (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,605] INFO Adding UDAF name=EARLIEST_BY_OFFSET from path=internal class=class io.confluent.ksql.function.udaf.offset.EarliestByOffset (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,606] INFO Adding UDAF name=EARLIEST_BY_OFFSET from path=internal class=class io.confluent.ksql.function.udaf.offset.EarliestByOffset (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,608] INFO Adding UDAF name=EARLIEST_BY_OFFSET from path=internal class=class io.confluent.ksql.function.udaf.offset.EarliestByOffset (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,613] INFO Adding UDAF name=LATEST_BY_OFFSET from path=internal class=class io.confluent.ksql.function.udaf.offset.LatestByOffset (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,614] INFO Adding UDAF name=LATEST_BY_OFFSET from path=internal class=class io.confluent.ksql.function.udaf.offset.LatestByOffset (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,615] INFO Adding UDAF name=LATEST_BY_OFFSET from path=internal class=class io.confluent.ksql.function.udaf.offset.LatestByOffset (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,616] INFO Adding UDAF name=LATEST_BY_OFFSET from path=internal class=class io.confluent.ksql.function.udaf.offset.LatestByOffset (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,621] INFO Adding UDAF name=STDDEV_SAMP from path=internal class=class io.confluent.ksql.function.udaf.stddev.StandardDeviationSampUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,622] INFO Adding UDAF name=STDDEV_SAMP from path=internal class=class io.confluent.ksql.function.udaf.stddev.StandardDeviationSampUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,624] INFO Adding UDAF name=STDDEV_SAMP from path=internal class=class io.confluent.ksql.function.udaf.stddev.StandardDeviationSampUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,628] INFO Adding UDAF name=STDDEV_SAMPLE from path=internal class=class io.confluent.ksql.function.udaf.stddev.StandardDeviationSampleUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,630] INFO Adding UDAF name=STDDEV_SAMPLE from path=internal class=class io.confluent.ksql.function.udaf.stddev.StandardDeviationSampleUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,631] INFO Adding UDAF name=STDDEV_SAMPLE from path=internal class=class io.confluent.ksql.function.udaf.stddev.StandardDeviationSampleUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,650] INFO Adding UDAF name=sum_list from path=internal class=class io.confluent.ksql.function.udaf.sum.ListSumUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,652] INFO Adding UDAF name=sum_list from path=internal class=class io.confluent.ksql.function.udaf.sum.ListSumUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,653] INFO Adding UDAF name=sum_list from path=internal class=class io.confluent.ksql.function.udaf.sum.ListSumUdaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,661] INFO Adding UDAF name=SUM from path=internal class=class io.confluent.ksql.function.udaf.sum.SumKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,664] INFO Adding UDAF name=SUM from path=internal class=class io.confluent.ksql.function.udaf.sum.SumKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,667] INFO Adding UDAF name=SUM from path=internal class=class io.confluent.ksql.function.udaf.sum.SumKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,669] INFO Adding UDAF name=SUM from path=internal class=class io.confluent.ksql.function.udaf.sum.SumKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,679] INFO Adding UDAF name=TOPK from path=internal class=class io.confluent.ksql.function.udaf.topk.TopkKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,684] INFO Adding UDAF name=TOPK from path=internal class=class io.confluent.ksql.function.udaf.topk.TopkKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,688] INFO Adding UDAF name=TOPK from path=internal class=class io.confluent.ksql.function.udaf.topk.TopkKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,690] INFO Adding UDAF name=TOPK from path=internal class=class io.confluent.ksql.function.udaf.topk.TopkKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,700] INFO Adding UDAF name=TOPKDISTINCT from path=internal class=class io.confluent.ksql.function.udaf.topkdistinct.TopkDistinctKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,704] INFO Adding UDAF name=TOPKDISTINCT from path=internal class=class io.confluent.ksql.function.udaf.topkdistinct.TopkDistinctKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,707] INFO Adding UDAF name=TOPKDISTINCT from path=internal class=class io.confluent.ksql.function.udaf.topkdistinct.TopkDistinctKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,709] INFO Adding UDAF name=TOPKDISTINCT from path=internal class=class io.confluent.ksql.function.udaf.topkdistinct.TopkDistinctKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,713] INFO Adding UDAF name=TOPKDISTINCT from path=internal class=class io.confluent.ksql.function.udaf.topkdistinct.TopkDistinctKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,720] INFO Adding UDAF name=TOPKDISTINCT from path=internal class=class io.confluent.ksql.function.udaf.topkdistinct.TopkDistinctKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,726] INFO Adding UDAF name=TOPKDISTINCT from path=internal class=class io.confluent.ksql.function.udaf.topkdistinct.TopkDistinctKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,729] INFO Adding UDAF name=TOPKDISTINCT from path=internal class=class io.confluent.ksql.function.udaf.topkdistinct.TopkDistinctKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:49,750] INFO Adding UDAF name=TOPKDISTINCT from path=internal class=class io.confluent.ksql.function.udaf.topkdistinct.TopkDistinctKudaf (io.confluent.ksql.function.UdafLoader)
ksqldb-server                      | [2023-08-04 10:11:50,176] INFO UDFs can't be loaded as as dir /usr/ext doesn't exist or is not a directory (io.confluent.ksql.function.UserFunctionLoader)
control-center                     | [2023-08-04 10:11:50,221] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter)
ksqldb-server                      | [2023-08-04 10:11:50,238] INFO KsqlRestConfig values: 
ksqldb-server                      | 	access.control.allow.headers = []
ksqldb-server                      | 	access.control.allow.methods = []
ksqldb-server                      | 	access.control.allow.origin = 
ksqldb-server                      | 	authentication.method = NONE
ksqldb-server                      | 	authentication.realm = 
ksqldb-server                      | 	authentication.roles = [*]
ksqldb-server                      | 	authentication.skip.paths = []
ksqldb-server                      | 	ksql.advertised.listener = null
ksqldb-server                      | 	ksql.authentication.plugin.class = null
ksqldb-server                      | 	ksql.endpoint.logging.ignored.paths.regex = 
ksqldb-server                      | 	ksql.endpoint.logging.log.queries = false
ksqldb-server                      | 	ksql.healthcheck.interval.ms = 5000
ksqldb-server                      | 	ksql.heartbeat.check.interval.ms = 200
ksqldb-server                      | 	ksql.heartbeat.discover.interval.ms = 2000
ksqldb-server                      | 	ksql.heartbeat.enable = false
ksqldb-server                      | 	ksql.heartbeat.missed.threshold.ms = 3
ksqldb-server                      | 	ksql.heartbeat.send.interval.ms = 100
ksqldb-server                      | 	ksql.heartbeat.thread.pool.size = 3
ksqldb-server                      | 	ksql.heartbeat.window.ms = 2000
ksqldb-server                      | 	ksql.idle.connection.timeout.seconds = 86400
ksqldb-server                      | 	ksql.internal.http2.max.pool.size = 3000
ksqldb-server                      | 	ksql.internal.listener = null
ksqldb-server                      | 	ksql.internal.ssl.client.authentication = NONE
ksqldb-server                      | 	ksql.lag.reporting.enable = false
ksqldb-server                      | 	ksql.lag.reporting.send.interval.ms = 5000
ksqldb-server                      | 	ksql.local.commands.location = 
ksqldb-server                      | 	ksql.logging.server.rate.limited.request.paths = 
ksqldb-server                      | 	ksql.logging.server.rate.limited.response.codes = 
ksqldb-server                      | 	ksql.max.push.queries = 100
ksqldb-server                      | 	ksql.server.command.blocked.threshold.error.ms = 15000
ksqldb-server                      | 	ksql.server.command.response.timeout.ms = 5000
ksqldb-server                      | 	ksql.server.command.topic.migration.enabled = NONE
ksqldb-server                      | 	ksql.server.command.topic.rate.limit = 1.7976931348623157E308
ksqldb-server                      | 	ksql.server.error.messages = class io.confluent.ksql.rest.DefaultErrorMessages
ksqldb-server                      | 	ksql.server.exception.uncaught.handler.enable = false
ksqldb-server                      | 	ksql.server.install.dir = /usr
ksqldb-server                      | 	ksql.server.precondition.max.backoff.ms = 5000
ksqldb-server                      | 	ksql.server.preconditions = []
ksqldb-server                      | 	ksql.server.sni.check.enable = false
ksqldb-server                      | 	ksql.server.websockets.num.threads = 5
ksqldb-server                      | 	ksql.ssl.keystore.alias.external = 
ksqldb-server                      | 	ksql.ssl.keystore.alias.internal = 
ksqldb-server                      | 	ksql.verticle.instances = 8
ksqldb-server                      | 	ksql.worker.pool.size = 100
ksqldb-server                      | 	listeners = [http://0.0.0.0:8088]
ksqldb-server                      | 	query.stream.disconnect.check = 1000
ksqldb-server                      | 	ssl.cipher.suites = []
ksqldb-server                      | 	ssl.client.auth = false
ksqldb-server                      | 	ssl.client.authentication = NONE
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.key.password = [hidden]
ksqldb-server                      | 	ssl.keystore.location = 
ksqldb-server                      | 	ssl.keystore.password = [hidden]
ksqldb-server                      | 	ssl.keystore.reload = false
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.keystore.watch.location = 
ksqldb-server                      | 	ssl.truststore.location = 
ksqldb-server                      | 	ssl.truststore.password = [hidden]
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.rest.server.KsqlRestConfig)
control-center                     | [2023-08-04 10:11:50,680] INFO Logging initialized @88648ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
control-center                     | [2023-08-04 10:11:51,210] INFO CONTROL CENTER UI
control-center                     | 
control-center                     | By using Control Center, subject to any license you may have with Confluent, you agree to the Confluent Data Protection Agreement.  In particular, please note that the version check feature of Control Center is enabled.
control-center                     | 
control-center                     | With this enabled, this instance is configured to collect and report certain data (version information, time stamped session IDs, instance ID, instance uptime, license key for subscription customers, IP address, and other product data)  to Confluent, Inc. ("Confluent") or its parent, subsidiaries, affiliates or service providers every hour.  By proceeding with `confluent.support.metrics.enable=true`, you agree to all such collection, transfer and use of Version information by Confluent. You can turn the version check feature off by setting `confluent.support.metrics.enable=false` in the Control Center configuration and restarting Control Center.  See the Confluent Enterprise documentation for further information.
control-center                     |  (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | 
control-center                     | [2023-08-04 10:11:51,375] INFO Starting Control Center version=7.4.1 (io.confluent.controlcenter.application.AllControlCenter)
control-center                     | [2023-08-04 10:11:51,420] INFO topicListings=[(name=_confluent-command, topicId=26mdvJTUTviIQl0i3yP-hQ, internal=false)] (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:11:51,457] INFO missingTopics=[_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, _confluent-metrics, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-cluster-rekey, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, _confluent-monitoring, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition] (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:11:51,476] INFO extantTopics=[_confluent-command] (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:11:51,513] INFO checking topicDescription=(name=_confluent-command, internal=false, partitions=(partition=0, leader=broker:29092 (id: 1 rack: null), replicas=broker:29092 (id: 1 rack: null), isr=broker:29092 (id: 1 rack: null)), authorizedOperations=null) (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:11:51,522] INFO found topic=_confluent-command with partitions=1 (io.confluent.controlcenter.KafkaHelper)
broker                             | [2023-08-04 10:11:51,565] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:51,566] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition with topic ID STq7PyopRJWACFWKlYOr2A. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:51,566] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,566] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,566] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,567] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,600] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,601] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,601] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,601] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,602] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with topic ID STq7PyopRJWACFWKlYOr2A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:51,632] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:51,640] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:51,647] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with topic id STq7PyopRJWACFWKlYOr2A. (state.change.logger)
broker                             | [2023-08-04 10:11:51,730] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:51,731] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog with topic ID ZjoO_4HfTKWmowjlTjE6sw. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:51,731] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,732] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,737] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,738] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,740] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,740] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,741] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,741] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,745] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with topic ID ZjoO_4HfTKWmowjlTjE6sw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:51,747] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:51,794] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:51,802] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:51,870] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:51,871] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with topic id Some(STq7PyopRJWACFWKlYOr2A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:51,880] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:51,895] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:51,897] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog with topic ID Qlt_SM49Sl6zBrVxalhSRw. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:51,898] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,899] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,899] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,900] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,903] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,904] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,905] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,905] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,906] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with topic ID Qlt_SM49Sl6zBrVxalhSRw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:51,908] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:51,914] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:51,915] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with topic id ZjoO_4HfTKWmowjlTjE6sw. (state.change.logger)
broker                             | [2023-08-04 10:11:51,926] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:51,931] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:51,958] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:51,959] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:51,973] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with topic id Some(ZjoO_4HfTKWmowjlTjE6sw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:51,978] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:51,987] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog with topic ID UbhlmVOjQAusw3dsEKYqeQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:51,989] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,990] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,991] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,991] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,991] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,992] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,992] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,992] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:51,992] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with topic ID UbhlmVOjQAusw3dsEKYqeQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,025] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:52,076] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,084] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition with topic ID qrRJJzmBRu6JLAarYC2FXQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,087] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,087] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,088] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,088] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,088] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,089] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,089] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,090] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,091] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with topic ID qrRJJzmBRu6JLAarYC2FXQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,107] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:52,111] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:52,112] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with topic id Qlt_SM49Sl6zBrVxalhSRw. (state.change.logger)
broker                             | [2023-08-04 10:11:52,155] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:52,156] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,168] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog with topic ID dxHSpn7BThGwIYeZu2BHmA. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,168] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,169] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,169] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,170] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,170] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,171] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,172] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,172] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,173] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with topic ID dxHSpn7BThGwIYeZu2BHmA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,180] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:52,212] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:52,230] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:52,244] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with topic id Some(Qlt_SM49Sl6zBrVxalhSRw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:52,245] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,246] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog with topic ID tnG6SGoITzy4Bdh8lieE1g. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,248] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,256] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,257] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,258] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,259] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,260] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,261] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,269] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,270] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with topic ID tnG6SGoITzy4Bdh8lieE1g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,261] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:52,282] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:52,289] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:52,289] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with topic id UbhlmVOjQAusw3dsEKYqeQ. (state.change.logger)
broker                             | [2023-08-04 10:11:52,333] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:52,337] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,345] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition with topic ID _vYRjW7IRXK7apsabDMiKQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,356] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,357] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,358] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,359] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,360] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,367] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,375] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,369] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:52,399] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:52,411] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:52,391] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,413] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with topic ID _vYRjW7IRXK7apsabDMiKQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,415] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with topic id Some(UbhlmVOjQAusw3dsEKYqeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:52,440] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:52,451] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:52,453] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:52,455] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with topic id qrRJJzmBRu6JLAarYC2FXQ. (state.change.logger)
broker                             | [2023-08-04 10:11:52,472] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:52,480] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,512] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store with topic ID RTYUY2zsS26iu53cLq8UPg. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,514] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,515] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,516] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,519] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,520] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,521] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with topic ID RTYUY2zsS26iu53cLq8UPg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,535] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:52,550] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:52,552] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:52,553] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with topic id Some(qrRJJzmBRu6JLAarYC2FXQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:52,590] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:52,611] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,626] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog with topic ID 27wkC6UWTRWT8JkErrPTLw. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,626] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,627] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,628] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,625] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:52,632] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:52,633] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with topic id dxHSpn7BThGwIYeZu2BHmA. (state.change.logger)
broker                             | [2023-08-04 10:11:52,651] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,651] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,652] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,652] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,652] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,653] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with topic ID 27wkC6UWTRWT8JkErrPTLw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,658] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:52,666] INFO Created log for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:52,680] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:52,681] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:52,682] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with topic id Some(dxHSpn7BThGwIYeZu2BHmA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:52,702] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:52,707] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:52,714] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,724] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog with topic ID Hsu6BGHpQ3KehegKKf6yyg. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,725] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,726] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,745] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,750] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,751] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,752] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,752] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,754] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,763] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with topic ID Hsu6BGHpQ3KehegKKf6yyg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,751] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:52,783] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with topic id tnG6SGoITzy4Bdh8lieE1g. (state.change.logger)
broker                             | [2023-08-04 10:11:52,845] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,851] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition with topic ID zgm-W2fpQnOGMZXkBmgLQw. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,852] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,853] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,856] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,856] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,857] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,858] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,859] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,860] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,860] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with topic ID zgm-W2fpQnOGMZXkBmgLQw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,871] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:52,882] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:52,905] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:52,919] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:52,919] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with topic id Some(tnG6SGoITzy4Bdh8lieE1g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:52,937] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:52,958] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,968] INFO [Controller 1] Created topic _confluent-metrics with topic ID WXyI51KeTHCt18UI0bi43A. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,969] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,970] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration max.message.bytes to 10485760 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,971] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,972] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,973] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration retention.ms to 259200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,979] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,980] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:52,981] INFO [Controller 1] Created partition _confluent-metrics-0 with topic ID WXyI51KeTHCt18UI0bi43A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,982] INFO [Controller 1] Created partition _confluent-metrics-1 with topic ID WXyI51KeTHCt18UI0bi43A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,984] INFO [Controller 1] Created partition _confluent-metrics-2 with topic ID WXyI51KeTHCt18UI0bi43A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,995] INFO [Controller 1] Created partition _confluent-metrics-3 with topic ID WXyI51KeTHCt18UI0bi43A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,998] INFO [Controller 1] Created partition _confluent-metrics-4 with topic ID WXyI51KeTHCt18UI0bi43A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,003] INFO [Controller 1] Created partition _confluent-metrics-5 with topic ID WXyI51KeTHCt18UI0bi43A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,006] INFO [Controller 1] Created partition _confluent-metrics-6 with topic ID WXyI51KeTHCt18UI0bi43A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:52,962] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:53,092] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:53,098] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with topic id _vYRjW7IRXK7apsabDMiKQ. (state.change.logger)
broker                             | [2023-08-04 10:11:53,085] INFO [Controller 1] Created partition _confluent-metrics-7 with topic ID WXyI51KeTHCt18UI0bi43A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,126] INFO [Controller 1] Created partition _confluent-metrics-8 with topic ID WXyI51KeTHCt18UI0bi43A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,126] INFO [Controller 1] Created partition _confluent-metrics-9 with topic ID WXyI51KeTHCt18UI0bi43A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,127] INFO [Controller 1] Created partition _confluent-metrics-10 with topic ID WXyI51KeTHCt18UI0bi43A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,127] INFO [Controller 1] Created partition _confluent-metrics-11 with topic ID WXyI51KeTHCt18UI0bi43A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,140] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:53,147] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:53,175] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:53,189] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:53,192] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with topic id Some(_vYRjW7IRXK7apsabDMiKQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:11:53,204] INFO Starting server (io.confluent.ksql.rest.server.KsqlServerMain)
broker                             | [2023-08-04 10:11:53,221] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,224] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog with topic ID P4lKo9gcStGZyM8e3fWDyw. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,225] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,225] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,225] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,225] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,225] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,225] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,251] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,251] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,251] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with topic ID P4lKo9gcStGZyM8e3fWDyw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,236] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:53,261] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:11:53,268] INFO KsqlConfig values: 
ksqldb-server                      | 	ksql.access.validator.enable = auto
ksqldb-server                      | 	ksql.assert.schema.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.assert.topic.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.authorization.cache.expiry.time.secs = 30
ksqldb-server                      | 	ksql.authorization.cache.max.entries = 10000
ksqldb-server                      | 	ksql.cast.strings.preserve.nulls = true
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.file = 
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.reload = false
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.source = NONE
ksqldb-server                      | 	ksql.connect.request.headers.plugin = null
ksqldb-server                      | 	ksql.connect.request.timeout.ms = 5000
ksqldb-server                      | 	ksql.connect.url = http://connect:8083
ksqldb-server                      | 	ksql.connect.worker.config = 
ksqldb-server                      | 	ksql.create.or.replace.enabled = true
ksqldb-server                      | 	ksql.endpoint.migrate.query = true
ksqldb-server                      | 	ksql.error.classifier.regex = 
ksqldb-server                      | 	ksql.extension.dir = ext
ksqldb-server                      | 	ksql.headers.columns.enabled = true
ksqldb-server                      | 	ksql.hidden.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.insert.into.values.enabled = true
ksqldb-server                      | 	ksql.internal.topic.min.insync.replicas = 1
ksqldb-server                      | 	ksql.internal.topic.replicas = 1
ksqldb-server                      | 	ksql.json_sr.converter.deserializer.enabled = true
ksqldb-server                      | 	ksql.lambdas.enabled = true
ksqldb-server                      | 	ksql.metastore.backup.location = 
ksqldb-server                      | 	ksql.metrics.extension = null
ksqldb-server                      | 	ksql.metrics.tags.custom = 
ksqldb-server                      | 	ksql.nested.error.set.null = true
ksqldb-server                      | 	ksql.new.query.planner.enabled = false
ksqldb-server                      | 	ksql.output.topic.name.prefix = 
ksqldb-server                      | 	ksql.persistence.default.format.key = KAFKA
ksqldb-server                      | 	ksql.persistence.default.format.value = null
ksqldb-server                      | 	ksql.persistence.wrap.single.values = null
ksqldb-server                      | 	ksql.persistent.prefix = query_
ksqldb-server                      | 	ksql.properties.overrides.denylist = []
ksqldb-server                      | 	ksql.pull.queries.enable = true
ksqldb-server                      | 	ksql.query.cleanup.shutdown.timeout.ms = 30000
ksqldb-server                      | 	ksql.query.error.max.queue.size = 10
ksqldb-server                      | 	ksql.query.persistent.active.limit = 2147483647
ksqldb-server                      | 	ksql.query.persistent.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.query.pull.enable.standby.reads = false
ksqldb-server                      | 	ksql.query.pull.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.pull.limit.clause.enabled = true
ksqldb-server                      | 	ksql.query.pull.max.allowed.offset.lag = 9223372036854775807
ksqldb-server                      | 	ksql.query.pull.max.concurrent.requests = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.qps = 2147483647
ksqldb-server                      | 	ksql.query.pull.metrics.enabled = true
ksqldb-server                      | 	ksql.query.pull.range.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.router.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.pull.stream.enabled = true
ksqldb-server                      | 	ksql.query.pull.table.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.push.v2.alos.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.catchup.consumer.msg.window = 50
ksqldb-server                      | 	ksql.query.push.v2.continuation.tokens.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.latest.reset.age.ms = 30000
ksqldb-server                      | 	ksql.query.push.v2.max.catchup.consumers = 5
ksqldb-server                      | 	ksql.query.push.v2.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.push.v2.metrics.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.new.latest.delay.ms = 5000
ksqldb-server                      | 	ksql.query.push.v2.registry.installed = false
ksqldb-server                      | 	ksql.query.retry.backoff.initial.ms = 15000
ksqldb-server                      | 	ksql.query.retry.backoff.max.ms = 900000
ksqldb-server                      | 	ksql.query.status.running.threshold.seconds = 300
ksqldb-server                      | 	ksql.query.transient.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.queryanonymizer.cluster_namespace = null
ksqldb-server                      | 	ksql.queryanonymizer.logs_enabled = true
ksqldb-server                      | 	ksql.readonly.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.runtime.feature.shared.enabled = false
ksqldb-server                      | 	ksql.schema.registry.url = http://schema-registry:8081
ksqldb-server                      | 	ksql.security.extension.class = null
ksqldb-server                      | 	ksql.service.id = default_
ksqldb-server                      | 	ksql.shared.runtimes.count = 2
ksqldb-server                      | 	ksql.sink.window.change.log.additional.retention = 1000000
ksqldb-server                      | 	ksql.source.table.materialization.enabled = true
ksqldb-server                      | 	ksql.streams.shutdown.timeout.ms = 300000
ksqldb-server                      | 	ksql.suppress.buffer.size.bytes = -1
ksqldb-server                      | 	ksql.suppress.enabled = true
ksqldb-server                      | 	ksql.timestamp.throw.on.invalid = false
ksqldb-server                      | 	ksql.transient.prefix = transient_
ksqldb-server                      | 	ksql.transient.query.cleanup.service.enable = true
ksqldb-server                      | 	ksql.transient.query.cleanup.service.initial.delay.seconds = 600
ksqldb-server                      | 	ksql.transient.query.cleanup.service.period.seconds = 600
ksqldb-server                      | 	ksql.udf.collect.metrics = false
ksqldb-server                      | 	ksql.udf.enable.security.manager = true
ksqldb-server                      | 	ksql.udfs.enabled = true
ksqldb-server                      | 	ksql.variable.substitution.enable = true
ksqldb-server                      | 	ksql.websocket.connection.max.timeout.ms = 3600000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.util.KsqlConfig)
broker                             | [2023-08-04 10:11:53,293] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:53,315] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with topic id RTYUY2zsS26iu53cLq8UPg. (state.change.logger)
broker                             | [2023-08-04 10:11:53,325] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-cluster-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,332] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-cluster-rekey with topic ID w4L152RES8qYs0ThphAjlg. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,334] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-cluster-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,335] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-cluster-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,338] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-cluster-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,339] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-cluster-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,341] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-cluster-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,345] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 with topic ID w4L152RES8qYs0ThphAjlg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,389] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:53,420] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,423] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition with topic ID RPkOqg4kSBaqVbVHl5cH9w. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,424] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,425] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,421] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:53,459] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,477] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,477] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,477] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:53,477] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,477] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:53,477] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,478] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with topic id Some(RTYUY2zsS26iu53cLq8UPg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:53,478] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,478] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with topic ID RPkOqg4kSBaqVbVHl5cH9w and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,532] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,533] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog with topic ID 3CHyfz78QgSOj8D2PYTkuA. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,533] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,533] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,534] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,534] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,534] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,535] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,536] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,537] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,538] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with topic ID 3CHyfz78QgSOj8D2PYTkuA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,543] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:53,549] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:53,553] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:53,559] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with topic id 27wkC6UWTRWT8JkErrPTLw. (state.change.logger)
broker                             | [2023-08-04 10:11:53,639] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,640] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition with topic ID wbsqiFi3RFiBKXg6x7JVQg. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,645] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,645] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,646] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,647] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,648] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,648] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,649] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with topic ID wbsqiFi3RFiBKXg6x7JVQg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,700] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:53,718] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:53,726] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:53,731] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:53,732] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with topic id Some(27wkC6UWTRWT8JkErrPTLw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:53,747] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
ksqldb-server                      | [2023-08-04 10:11:53,750] INFO JsonConverterConfig values: 
ksqldb-server                      | 	converter.type = value
ksqldb-server                      | 	decimal.format = NUMERIC
ksqldb-server                      | 	schemas.cache.size = 1000
ksqldb-server                      | 	schemas.enable = false
ksqldb-server                      |  (org.apache.kafka.connect.json.JsonConverterConfig)
broker                             | [2023-08-04 10:11:53,760] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog with topic ID CyEPox4GQNy8kLG7WEQWrQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,769] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,775] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,776] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,777] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,778] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,779] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,780] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,768] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:53,784] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,784] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with topic ID CyEPox4GQNy8kLG7WEQWrQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,800] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:53,812] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:53,812] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with topic id Hsu6BGHpQ3KehegKKf6yyg. (state.change.logger)
broker                             | [2023-08-04 10:11:53,881] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,892] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog with topic ID 8kdvQ-ncQWWnL9X7ZnXiBA. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,892] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,896] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,896] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,896] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,896] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,896] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,897] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,897] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,897] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with topic ID 8kdvQ-ncQWWnL9X7ZnXiBA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,945] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:53,969] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,970] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition with topic ID c1c8GxRtS4u85J4-7gRYng. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:53,970] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,973] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,974] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:53,987] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,020] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,021] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,021] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,021] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,001] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:54,053] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:54,056] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with topic ID c1c8GxRtS4u85J4-7gRYng and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,056] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:54,060] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with topic id Some(Hsu6BGHpQ3KehegKKf6yyg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:54,076] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:54,081] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:54,086] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:54,086] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with topic id zgm-W2fpQnOGMZXkBmgLQw. (state.change.logger)
broker                             | [2023-08-04 10:11:54,142] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:54,146] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,147] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition with topic ID mNO1aiiEQ1aUPUFDBwdR-w. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,148] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,149] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,149] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,150] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,151] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,152] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,153] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,154] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,155] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with topic ID mNO1aiiEQ1aUPUFDBwdR-w and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,156] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:54,169] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:54,170] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:54,171] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with topic id Some(zgm-W2fpQnOGMZXkBmgLQw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:54,197] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:54,201] INFO [Broker id=1] Transitioning 12 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:54,202] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-metrics-11, _confluent-metrics-9, _confluent-metrics-10, _confluent-metrics-7, _confluent-metrics-8, _confluent-metrics-5, _confluent-metrics-6, _confluent-metrics-3, _confluent-metrics-4, _confluent-metrics-1, _confluent-metrics-2, _confluent-metrics-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:54,203] INFO [Broker id=1] Creating new partition _confluent-metrics-11 with topic id WXyI51KeTHCt18UI0bi43A. (state.change.logger)
broker                             | [2023-08-04 10:11:54,248] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,294] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey with topic ID jGx_3gCrTDmXXO7DlfFZkw. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,296] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,297] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,298] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,299] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,300] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,300] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with topic ID jGx_3gCrTDmXXO7DlfFZkw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,312] INFO [LogLoader partition=_confluent-metrics-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:54,390] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,403] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey with topic ID 5zxMVZraQkS0mMTEISnuCA. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,406] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,410] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,417] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,419] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,420] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,422] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with topic ID 5zxMVZraQkS0mMTEISnuCA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,405] INFO Created log for partition _confluent-metrics-11 in /tmp/kraft-combined-logs/_confluent-metrics-11 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:54,439] INFO [Partition _confluent-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-11 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:54,440] INFO [Partition _confluent-metrics-11 broker=1] Log loaded for partition _confluent-metrics-11 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:54,440] INFO [Broker id=1] Leader _confluent-metrics-11 with topic id Some(WXyI51KeTHCt18UI0bi43A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:54,475] INFO [Broker id=1] Creating new partition _confluent-metrics-9 with topic id WXyI51KeTHCt18UI0bi43A. (state.change.logger)
broker                             | [2023-08-04 10:11:54,509] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,522] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey with topic ID I5wV8QWRRJuTL-yY1xI20g. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,529] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,530] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,532] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,545] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,546] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,549] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with topic ID I5wV8QWRRJuTL-yY1xI20g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
data-agrigator-taskmanager-1       | 2023-08-04 10:11:54,557 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
broker                             | [2023-08-04 10:11:54,522] INFO [LogLoader partition=_confluent-metrics-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
data-agrigator-taskmanager-1       | 2023-08-04 10:11:54,568 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
broker                             | [2023-08-04 10:11:54,571] INFO Created log for partition _confluent-metrics-9 in /tmp/kraft-combined-logs/_confluent-metrics-9 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:54,575] INFO [Partition _confluent-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-9 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:54,612] INFO [Partition _confluent-metrics-9 broker=1] Log loaded for partition _confluent-metrics-9 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:54,621] INFO [Broker id=1] Leader _confluent-metrics-9 with topic id Some(WXyI51KeTHCt18UI0bi43A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:54,634] INFO [Broker id=1] Creating new partition _confluent-metrics-10 with topic id WXyI51KeTHCt18UI0bi43A. (state.change.logger)
broker                             | [2023-08-04 10:11:54,643] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,644] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog with topic ID gpn2CyU5S3CepZalgg_6ug. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,645] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,645] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,648] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,648] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,648] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,648] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,649] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,651] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,652] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with topic ID gpn2CyU5S3CepZalgg_6ug and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,657] INFO [LogLoader partition=_confluent-metrics-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:54,687] INFO Created log for partition _confluent-metrics-10 in /tmp/kraft-combined-logs/_confluent-metrics-10 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:54,692] INFO [Partition _confluent-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-10 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:54,694] INFO [Partition _confluent-metrics-10 broker=1] Log loaded for partition _confluent-metrics-10 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:54,696] INFO [Broker id=1] Leader _confluent-metrics-10 with topic id Some(WXyI51KeTHCt18UI0bi43A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:54,735] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,737] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition with topic ID 7CG1vSjFRcuvphmKW10WEA. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,740] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,742] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,744] INFO [Broker id=1] Creating new partition _confluent-metrics-7 with topic id WXyI51KeTHCt18UI0bi43A. (state.change.logger)
broker                             | [2023-08-04 10:11:54,754] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,754] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,764] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,765] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,766] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,768] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,770] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with topic ID 7CG1vSjFRcuvphmKW10WEA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,792] INFO [LogLoader partition=_confluent-metrics-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:54,833] INFO Created log for partition _confluent-metrics-7 in /tmp/kraft-combined-logs/_confluent-metrics-7 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:54,841] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,844] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition with topic ID g31qr5BRRCeyT-Ctg5GhYw. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,845] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,846] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,847] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,847] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,848] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,849] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,852] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,852] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,853] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with topic ID g31qr5BRRCeyT-Ctg5GhYw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,854] INFO [Partition _confluent-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-7 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:54,923] INFO [Partition _confluent-metrics-7 broker=1] Log loaded for partition _confluent-metrics-7 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:54,923] INFO [Broker id=1] Leader _confluent-metrics-7 with topic id Some(WXyI51KeTHCt18UI0bi43A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:54,926] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,936] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog with topic ID 1LLF15hWQe-xqLOpsOrbpA. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,938] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,940] INFO [Broker id=1] Creating new partition _confluent-metrics-8 with topic id WXyI51KeTHCt18UI0bi43A. (state.change.logger)
broker                             | [2023-08-04 10:11:54,940] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,941] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,942] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,942] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,943] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,944] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,955] INFO [LogLoader partition=_confluent-metrics-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:54,963] INFO Created log for partition _confluent-metrics-8 in /tmp/kraft-combined-logs/_confluent-metrics-8 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:54,963] INFO [Partition _confluent-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-8 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:54,967] INFO [Partition _confluent-metrics-8 broker=1] Log loaded for partition _confluent-metrics-8 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:54,968] INFO [Broker id=1] Leader _confluent-metrics-8 with topic id Some(WXyI51KeTHCt18UI0bi43A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:54,975] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:54,993] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with topic ID 1LLF15hWQe-xqLOpsOrbpA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:54,987] INFO [Broker id=1] Creating new partition _confluent-metrics-5 with topic id WXyI51KeTHCt18UI0bi43A. (state.change.logger)
broker                             | [2023-08-04 10:11:55,063] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,078] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition with topic ID igf_bdToRC6wz8tLXU8ZWA. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,079] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,079] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,080] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,080] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,081] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,081] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,082] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,083] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,083] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with topic ID igf_bdToRC6wz8tLXU8ZWA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,108] INFO [LogLoader partition=_confluent-metrics-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:55,164] INFO Created log for partition _confluent-metrics-5 in /tmp/kraft-combined-logs/_confluent-metrics-5 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:55,171] INFO [Partition _confluent-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-5 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,166] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,172] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition with topic ID _zkEEKNOSh-BwqO66EWICw. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,174] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,175] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,179] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,180] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,174] INFO [Partition _confluent-metrics-5 broker=1] Log loaded for partition _confluent-metrics-5 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,181] INFO [Broker id=1] Leader _confluent-metrics-5 with topic id Some(WXyI51KeTHCt18UI0bi43A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:55,189] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,191] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,191] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,192] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,193] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with topic ID _zkEEKNOSh-BwqO66EWICw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,200] INFO [Broker id=1] Creating new partition _confluent-metrics-6 with topic id WXyI51KeTHCt18UI0bi43A. (state.change.logger)
broker                             | [2023-08-04 10:11:55,277] INFO [LogLoader partition=_confluent-metrics-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:55,289] INFO Created log for partition _confluent-metrics-6 in /tmp/kraft-combined-logs/_confluent-metrics-6 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:55,291] INFO [Partition _confluent-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-6 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,292] INFO [Partition _confluent-metrics-6 broker=1] Log loaded for partition _confluent-metrics-6 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,293] INFO [Broker id=1] Leader _confluent-metrics-6 with topic id Some(WXyI51KeTHCt18UI0bi43A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:55,301] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,301] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog with topic ID tkBvdtlJRWily0KDogf0lg. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,315] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,302] INFO [Broker id=1] Creating new partition _confluent-metrics-3 with topic id WXyI51KeTHCt18UI0bi43A. (state.change.logger)
broker                             | [2023-08-04 10:11:55,330] INFO [LogLoader partition=_confluent-metrics-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:55,318] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,333] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,334] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,334] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,336] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,338] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,339] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,342] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with topic ID tkBvdtlJRWily0KDogf0lg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,348] INFO Created log for partition _confluent-metrics-3 in /tmp/kraft-combined-logs/_confluent-metrics-3 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:55,349] INFO [Partition _confluent-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-3 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,349] INFO [Partition _confluent-metrics-3 broker=1] Log loaded for partition _confluent-metrics-3 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,350] INFO [Broker id=1] Leader _confluent-metrics-3 with topic id Some(WXyI51KeTHCt18UI0bi43A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:55,361] INFO [Broker id=1] Creating new partition _confluent-metrics-4 with topic id WXyI51KeTHCt18UI0bi43A. (state.change.logger)
broker                             | [2023-08-04 10:11:55,398] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,403] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog with topic ID xgvCZkJ3RMurzOd2tG9eqg. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,405] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,407] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,408] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,410] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,411] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,412] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,412] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,413] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,414] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with topic ID xgvCZkJ3RMurzOd2tG9eqg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,427] INFO [LogLoader partition=_confluent-metrics-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:55,430] INFO Created log for partition _confluent-metrics-4 in /tmp/kraft-combined-logs/_confluent-metrics-4 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:55,431] INFO [Partition _confluent-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-4 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,432] INFO [Partition _confluent-metrics-4 broker=1] Log loaded for partition _confluent-metrics-4 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,433] INFO [Broker id=1] Leader _confluent-metrics-4 with topic id Some(WXyI51KeTHCt18UI0bi43A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:55,442] INFO [Broker id=1] Creating new partition _confluent-metrics-1 with topic id WXyI51KeTHCt18UI0bi43A. (state.change.logger)
broker                             | [2023-08-04 10:11:55,470] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,472] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog with topic ID Gcm9HudQQIePkLpcnSZatA. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,472] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,473] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,474] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,475] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,476] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,477] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,478] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,478] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,479] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with topic ID Gcm9HudQQIePkLpcnSZatA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,487] INFO [LogLoader partition=_confluent-metrics-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:55,491] INFO Created log for partition _confluent-metrics-1 in /tmp/kraft-combined-logs/_confluent-metrics-1 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:55,492] INFO [Partition _confluent-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-1 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,494] INFO [Partition _confluent-metrics-1 broker=1] Log loaded for partition _confluent-metrics-1 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,503] INFO [Broker id=1] Leader _confluent-metrics-1 with topic id Some(WXyI51KeTHCt18UI0bi43A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:55,538] INFO [Broker id=1] Creating new partition _confluent-metrics-2 with topic id WXyI51KeTHCt18UI0bi43A. (state.change.logger)
broker                             | [2023-08-04 10:11:55,580] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,582] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey with topic ID TL8DPoV0R9OJhSSwRHqD6A. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,583] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,585] INFO [LogLoader partition=_confluent-metrics-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:55,594] INFO Created log for partition _confluent-metrics-2 in /tmp/kraft-combined-logs/_confluent-metrics-2 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:55,595] INFO [Partition _confluent-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-2 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,595] INFO [Partition _confluent-metrics-2 broker=1] Log loaded for partition _confluent-metrics-2 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,596] INFO [Broker id=1] Leader _confluent-metrics-2 with topic id Some(WXyI51KeTHCt18UI0bi43A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:55,598] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,605] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,606] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,607] INFO [Broker id=1] Creating new partition _confluent-metrics-0 with topic id WXyI51KeTHCt18UI0bi43A. (state.change.logger)
broker                             | [2023-08-04 10:11:55,627] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,638] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with topic ID TL8DPoV0R9OJhSSwRHqD6A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,663] INFO [LogLoader partition=_confluent-metrics-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:55,702] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,714] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition with topic ID 5MSfzhgUTOCsJsKEWsUfUQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,718] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,726] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,726] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,727] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,727] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,728] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,729] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,730] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,730] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with topic ID 5MSfzhgUTOCsJsKEWsUfUQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,732] INFO Created log for partition _confluent-metrics-0 in /tmp/kraft-combined-logs/_confluent-metrics-0 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:55,736] INFO [Partition _confluent-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,740] INFO [Partition _confluent-metrics-0 broker=1] Log loaded for partition _confluent-metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,743] INFO [Broker id=1] Leader _confluent-metrics-0 with topic id Some(WXyI51KeTHCt18UI0bi43A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:55,755] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-metrics with new configuration : cleanup.policy -> delete,max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:55,761] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:55,761] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:55,761] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with topic id P4lKo9gcStGZyM8e3fWDyw. (state.change.logger)
broker                             | [2023-08-04 10:11:55,821] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:55,847] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:55,852] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,856] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,859] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with topic id Some(P4lKo9gcStGZyM8e3fWDyw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:55,879] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:55,882] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,884] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey with topic ID z2ogKsUbRf6X0CRLuVUTWw. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,885] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,885] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,886] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,889] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:55,892] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-cluster-rekey-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:55,892] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 with topic id w4L152RES8qYs0ThphAjlg. (state.change.logger)
broker                             | [2023-08-04 10:11:55,893] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,893] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,893] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with topic ID z2ogKsUbRf6X0CRLuVUTWw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,929] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-cluster-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:55,952] INFO Created log for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-cluster-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:55,964] INFO [Partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,969] INFO [Partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:55,971] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-cluster-rekey-0 with topic id Some(w4L152RES8qYs0ThphAjlg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:55,969] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,977] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition with topic ID qbl7Kw1mTrKwZUmrs21Ogw. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,977] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,978] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,979] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,980] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,980] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,980] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,981] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,981] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:55,982] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with topic ID qbl7Kw1mTrKwZUmrs21Ogw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:55,985] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-cluster-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:55,993] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:55,994] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:55,994] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with topic id RPkOqg4kSBaqVbVHl5cH9w. (state.change.logger)
broker                             | [2023-08-04 10:11:56,013] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:56,031] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:56,040] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:56,036] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,047] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition with topic ID 6A1t9PL6S9ibcqZ61sSDAQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,048] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,048] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,049] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,049] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,050] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,051] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,051] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,055] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,056] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with topic ID 6A1t9PL6S9ibcqZ61sSDAQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,058] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:56,059] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with topic id Some(RPkOqg4kSBaqVbVHl5cH9w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:56,073] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:56,076] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:56,076] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:56,076] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with topic id 3CHyfz78QgSOj8D2PYTkuA. (state.change.logger)
broker                             | [2023-08-04 10:11:56,096] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:56,099] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:56,102] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:56,102] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:56,103] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with topic id Some(3CHyfz78QgSOj8D2PYTkuA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:56,124] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,130] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog with topic ID xYBgTiSgQeeJ02q7jNXaDg. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,141] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:56,143] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,144] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,144] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,144] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,144] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,144] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,145] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,145] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,145] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with topic ID xYBgTiSgQeeJ02q7jNXaDg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,146] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:56,146] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:56,146] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with topic id wbsqiFi3RFiBKXg6x7JVQg. (state.change.logger)
broker                             | [2023-08-04 10:11:56,167] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:56,175] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:56,180] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:56,180] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:56,180] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with topic id Some(wbsqiFi3RFiBKXg6x7JVQg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:56,216] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition with new configuration : cleanup.policy -> delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 604800000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:56,224] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:56,232] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:56,241] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with topic id CyEPox4GQNy8kLG7WEQWrQ. (state.change.logger)
broker                             | [2023-08-04 10:11:56,240] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,255] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition with topic ID bKHlhox8TzeDMS5qRnHymg. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,255] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,255] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,255] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,255] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,255] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,255] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,256] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,256] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,256] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with topic ID bKHlhox8TzeDMS5qRnHymg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,395] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:56,399] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,401] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog with topic ID Ly7r4MYYR8iZXnKz11AfxQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,401] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,401] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,402] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,402] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,403] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,404] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,405] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,408] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,409] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with topic ID Ly7r4MYYR8iZXnKz11AfxQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,407] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:56,413] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:56,415] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:56,416] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with topic id Some(CyEPox4GQNy8kLG7WEQWrQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:56,454] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:56,460] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:56,464] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:56,465] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with topic id 8kdvQ-ncQWWnL9X7ZnXiBA. (state.change.logger)
broker                             | [2023-08-04 10:11:56,488] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,491] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog with topic ID HFZ8vq7-RZyhNR6Sk4e1ng. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,496] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,497] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,503] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:56,504] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,523] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,523] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,524] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,524] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,530] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,530] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with topic ID HFZ8vq7-RZyhNR6Sk4e1ng and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,532] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:56,543] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:56,547] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:56,548] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with topic id Some(8kdvQ-ncQWWnL9X7ZnXiBA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:56,569] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:56,575] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:56,576] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:56,577] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with topic id c1c8GxRtS4u85J4-7gRYng. (state.change.logger)
broker                             | [2023-08-04 10:11:56,601] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,616] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog with topic ID adG-RfPuSaSjxyV6nKsuDw. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,626] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,627] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,628] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,628] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,629] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,630] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,630] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,631] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,632] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with topic ID adG-RfPuSaSjxyV6nKsuDw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,729] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:56,763] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:56,808] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:56,810] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:56,819] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with topic id Some(c1c8GxRtS4u85J4-7gRYng) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:56,942] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:56,953] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:56,961] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:56,964] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with topic id mNO1aiiEQ1aUPUFDBwdR-w. (state.change.logger)
broker                             | [2023-08-04 10:11:56,984] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-monitoring', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,986] INFO [Controller 1] Created topic _confluent-monitoring with topic ID RjGiTb4XQTq3XkcLgjDFew. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:56,991] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,995] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,996] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,997] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration retention.ms to 259200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,998] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,998] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:56,999] INFO [Controller 1] Created partition _confluent-monitoring-0 with topic ID RjGiTb4XQTq3XkcLgjDFew and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:57,021] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:57,030] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:57,033] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:57,034] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:57,035] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with topic id Some(mNO1aiiEQ1aUPUFDBwdR-w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:57,040] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:57,041] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition with topic ID LV4LlNFGSOuelBMsyB9XFw. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:57,042] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,043] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,047] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,048] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,049] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,049] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,050] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,051] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,053] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with topic ID LV4LlNFGSOuelBMsyB9XFw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:57,066] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:57,069] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:57,070] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:57,071] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with topic id jGx_3gCrTDmXXO7DlfFZkw. (state.change.logger)
broker                             | [2023-08-04 10:11:57,099] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:57,102] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:57,107] INFO [Partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:57,108] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:57,111] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition with topic ID NGyXVef5Tbicz4WJpOfTdg. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:57,112] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,113] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,114] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,114] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,115] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,116] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,116] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,117] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,118] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with topic ID NGyXVef5Tbicz4WJpOfTdg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:57,119] INFO [Partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:57,120] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with topic id Some(jGx_3gCrTDmXXO7DlfFZkw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:57,136] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:57,139] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:57,140] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:57,141] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with topic id 5zxMVZraQkS0mMTEISnuCA. (state.change.logger)
broker                             | [2023-08-04 10:11:57,161] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:57,174] INFO Created log for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:57,178] INFO [Partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:57,180] INFO [Partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:57,197] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with topic id Some(5zxMVZraQkS0mMTEISnuCA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:57,227] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:57,252] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:57,256] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:57,258] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with topic id I5wV8QWRRJuTL-yY1xI20g. (state.change.logger)
broker                             | [2023-08-04 10:11:57,277] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:57,279] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition with topic ID EnSw6NjFQNmCXav_Zub-bw. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:57,284] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,290] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,291] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,292] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,294] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,297] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,298] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,300] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,301] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with topic ID EnSw6NjFQNmCXav_Zub-bw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:57,392] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:57,429] INFO Created log for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:57,457] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:57,463] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition with topic ID UORTMA9aQXuH9QFrp1lC6Q. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:57,464] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,464] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,465] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,466] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,466] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,467] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,467] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,468] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:11:57,469] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with topic ID UORTMA9aQXuH9QFrp1lC6Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:11:57,480] INFO [Partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:57,498] INFO [Partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:57,499] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with topic id Some(I5wV8QWRRJuTL-yY1xI20g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:57,532] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
control-center                     | [2023-08-04 10:11:57,549] INFO describing topics=[_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, _confluent-metrics, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-cluster-rekey, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, _confluent-monitoring, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition] (io.confluent.controlcenter.KafkaHelper)
broker                             | [2023-08-04 10:11:57,546] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:57,572] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:57,632] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with topic id gpn2CyU5S3CepZalgg_6ug. (state.change.logger)
broker                             | [2023-08-04 10:11:57,709] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:57,724] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:57,744] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:57,749] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:57,751] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with topic id Some(gpn2CyU5S3CepZalgg_6ug) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:57,784] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:57,802] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:57,807] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:57,809] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with topic id 7CG1vSjFRcuvphmKW10WEA. (state.change.logger)
broker                             | [2023-08-04 10:11:57,930] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:57,979] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:58,010] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,018] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,019] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with topic id Some(7CG1vSjFRcuvphmKW10WEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:58,040] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:58,043] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:58,044] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:58,044] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with topic id g31qr5BRRCeyT-Ctg5GhYw. (state.change.logger)
broker                             | [2023-08-04 10:11:58,061] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:58,064] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:58,068] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,068] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,069] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with topic id Some(g31qr5BRRCeyT-Ctg5GhYw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:58,087] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:58,090] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:58,090] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:58,090] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with topic id 1LLF15hWQe-xqLOpsOrbpA. (state.change.logger)
broker                             | [2023-08-04 10:11:58,115] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:58,118] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:58,126] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,130] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,131] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with topic id Some(1LLF15hWQe-xqLOpsOrbpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:58,141] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:58,147] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:58,154] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:58,160] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with topic id igf_bdToRC6wz8tLXU8ZWA. (state.change.logger)
broker                             | [2023-08-04 10:11:58,243] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:58,259] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:58,269] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,270] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,278] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with topic id Some(igf_bdToRC6wz8tLXU8ZWA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:58,287] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:58,290] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:58,290] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:58,290] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with topic id _zkEEKNOSh-BwqO66EWICw. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:11:58,306] INFO KsqlRestConfig values: 
ksqldb-server                      | 	access.control.allow.headers = []
ksqldb-server                      | 	access.control.allow.methods = []
ksqldb-server                      | 	access.control.allow.origin = 
ksqldb-server                      | 	authentication.method = NONE
ksqldb-server                      | 	authentication.realm = 
ksqldb-server                      | 	authentication.roles = [*]
ksqldb-server                      | 	authentication.skip.paths = []
ksqldb-server                      | 	ksql.advertised.listener = null
ksqldb-server                      | 	ksql.authentication.plugin.class = null
ksqldb-server                      | 	ksql.endpoint.logging.ignored.paths.regex = 
ksqldb-server                      | 	ksql.endpoint.logging.log.queries = false
ksqldb-server                      | 	ksql.healthcheck.interval.ms = 5000
ksqldb-server                      | 	ksql.heartbeat.check.interval.ms = 200
ksqldb-server                      | 	ksql.heartbeat.discover.interval.ms = 2000
ksqldb-server                      | 	ksql.heartbeat.enable = false
ksqldb-server                      | 	ksql.heartbeat.missed.threshold.ms = 3
ksqldb-server                      | 	ksql.heartbeat.send.interval.ms = 100
ksqldb-server                      | 	ksql.heartbeat.thread.pool.size = 3
ksqldb-server                      | 	ksql.heartbeat.window.ms = 2000
ksqldb-server                      | 	ksql.idle.connection.timeout.seconds = 86400
ksqldb-server                      | 	ksql.internal.http2.max.pool.size = 3000
ksqldb-server                      | 	ksql.internal.listener = null
ksqldb-server                      | 	ksql.internal.ssl.client.authentication = NONE
ksqldb-server                      | 	ksql.lag.reporting.enable = false
ksqldb-server                      | 	ksql.lag.reporting.send.interval.ms = 5000
ksqldb-server                      | 	ksql.local.commands.location = 
ksqldb-server                      | 	ksql.logging.server.rate.limited.request.paths = 
ksqldb-server                      | 	ksql.logging.server.rate.limited.response.codes = 
ksqldb-server                      | 	ksql.max.push.queries = 100
ksqldb-server                      | 	ksql.server.command.blocked.threshold.error.ms = 15000
ksqldb-server                      | 	ksql.server.command.response.timeout.ms = 5000
ksqldb-server                      | 	ksql.server.command.topic.migration.enabled = NONE
ksqldb-server                      | 	ksql.server.command.topic.rate.limit = 1.7976931348623157E308
ksqldb-server                      | 	ksql.server.error.messages = class io.confluent.ksql.rest.DefaultErrorMessages
ksqldb-server                      | 	ksql.server.exception.uncaught.handler.enable = false
ksqldb-server                      | 	ksql.server.install.dir = /usr
ksqldb-server                      | 	ksql.server.precondition.max.backoff.ms = 5000
ksqldb-server                      | 	ksql.server.preconditions = []
ksqldb-server                      | 	ksql.server.sni.check.enable = false
ksqldb-server                      | 	ksql.server.websockets.num.threads = 5
ksqldb-server                      | 	ksql.ssl.keystore.alias.external = 
ksqldb-server                      | 	ksql.ssl.keystore.alias.internal = 
ksqldb-server                      | 	ksql.verticle.instances = 8
ksqldb-server                      | 	ksql.worker.pool.size = 100
ksqldb-server                      | 	listeners = [http://0.0.0.0:8088]
ksqldb-server                      | 	query.stream.disconnect.check = 1000
ksqldb-server                      | 	ssl.cipher.suites = []
ksqldb-server                      | 	ssl.client.auth = false
ksqldb-server                      | 	ssl.client.authentication = NONE
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.key.password = [hidden]
ksqldb-server                      | 	ssl.keystore.location = 
ksqldb-server                      | 	ssl.keystore.password = [hidden]
ksqldb-server                      | 	ssl.keystore.reload = false
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.keystore.watch.location = 
ksqldb-server                      | 	ssl.truststore.location = 
ksqldb-server                      | 	ssl.truststore.password = [hidden]
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.rest.server.KsqlRestConfig)
ksqldb-server                      | [2023-08-04 10:11:58,318] INFO KsqlConfig values: 
ksqldb-server                      | 	ksql.access.validator.enable = auto
ksqldb-server                      | 	ksql.assert.schema.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.assert.topic.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.authorization.cache.expiry.time.secs = 30
ksqldb-server                      | 	ksql.authorization.cache.max.entries = 10000
ksqldb-server                      | 	ksql.cast.strings.preserve.nulls = true
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.file = 
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.reload = false
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.source = NONE
ksqldb-server                      | 	ksql.connect.request.headers.plugin = null
ksqldb-server                      | 	ksql.connect.request.timeout.ms = 5000
ksqldb-server                      | 	ksql.connect.url = http://connect:8083
ksqldb-server                      | 	ksql.connect.worker.config = 
ksqldb-server                      | 	ksql.create.or.replace.enabled = true
ksqldb-server                      | 	ksql.endpoint.migrate.query = true
ksqldb-server                      | 	ksql.error.classifier.regex = 
ksqldb-server                      | 	ksql.extension.dir = ext
ksqldb-server                      | 	ksql.headers.columns.enabled = true
ksqldb-server                      | 	ksql.hidden.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.insert.into.values.enabled = true
ksqldb-server                      | 	ksql.internal.topic.min.insync.replicas = 1
ksqldb-server                      | 	ksql.internal.topic.replicas = 1
ksqldb-server                      | 	ksql.json_sr.converter.deserializer.enabled = true
ksqldb-server                      | 	ksql.lambdas.enabled = true
ksqldb-server                      | 	ksql.metastore.backup.location = 
ksqldb-server                      | 	ksql.metrics.extension = null
ksqldb-server                      | 	ksql.metrics.tags.custom = 
ksqldb-server                      | 	ksql.nested.error.set.null = true
ksqldb-server                      | 	ksql.new.query.planner.enabled = false
ksqldb-server                      | 	ksql.output.topic.name.prefix = 
ksqldb-server                      | 	ksql.persistence.default.format.key = KAFKA
ksqldb-server                      | 	ksql.persistence.default.format.value = null
ksqldb-server                      | 	ksql.persistence.wrap.single.values = null
ksqldb-server                      | 	ksql.persistent.prefix = query_
ksqldb-server                      | 	ksql.properties.overrides.denylist = []
ksqldb-server                      | 	ksql.pull.queries.enable = true
ksqldb-server                      | 	ksql.query.cleanup.shutdown.timeout.ms = 30000
ksqldb-server                      | 	ksql.query.error.max.queue.size = 10
ksqldb-server                      | 	ksql.query.persistent.active.limit = 2147483647
ksqldb-server                      | 	ksql.query.persistent.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.query.pull.enable.standby.reads = false
ksqldb-server                      | 	ksql.query.pull.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.pull.limit.clause.enabled = true
ksqldb-server                      | 	ksql.query.pull.max.allowed.offset.lag = 9223372036854775807
ksqldb-server                      | 	ksql.query.pull.max.concurrent.requests = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.qps = 2147483647
ksqldb-server                      | 	ksql.query.pull.metrics.enabled = true
ksqldb-server                      | 	ksql.query.pull.range.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.router.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.pull.stream.enabled = true
ksqldb-server                      | 	ksql.query.pull.table.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.push.v2.alos.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.catchup.consumer.msg.window = 50
ksqldb-server                      | 	ksql.query.push.v2.continuation.tokens.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.latest.reset.age.ms = 30000
ksqldb-server                      | 	ksql.query.push.v2.max.catchup.consumers = 5
ksqldb-server                      | 	ksql.query.push.v2.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.push.v2.metrics.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.new.latest.delay.ms = 5000
ksqldb-server                      | 	ksql.query.push.v2.registry.installed = false
ksqldb-server                      | 	ksql.query.retry.backoff.initial.ms = 15000
ksqldb-server                      | 	ksql.query.retry.backoff.max.ms = 900000
ksqldb-server                      | 	ksql.query.status.running.threshold.seconds = 300
ksqldb-server                      | 	ksql.query.transient.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.queryanonymizer.cluster_namespace = null
ksqldb-server                      | 	ksql.queryanonymizer.logs_enabled = true
ksqldb-server                      | 	ksql.readonly.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.runtime.feature.shared.enabled = false
ksqldb-server                      | 	ksql.schema.registry.url = http://schema-registry:8081
ksqldb-server                      | 	ksql.security.extension.class = null
ksqldb-server                      | 	ksql.service.id = default_
ksqldb-server                      | 	ksql.shared.runtimes.count = 2
ksqldb-server                      | 	ksql.sink.window.change.log.additional.retention = 1000000
ksqldb-server                      | 	ksql.source.table.materialization.enabled = true
ksqldb-server                      | 	ksql.streams.shutdown.timeout.ms = 300000
ksqldb-server                      | 	ksql.suppress.buffer.size.bytes = -1
ksqldb-server                      | 	ksql.suppress.enabled = true
ksqldb-server                      | 	ksql.timestamp.throw.on.invalid = false
ksqldb-server                      | 	ksql.transient.prefix = transient_
ksqldb-server                      | 	ksql.transient.query.cleanup.service.enable = true
ksqldb-server                      | 	ksql.transient.query.cleanup.service.initial.delay.seconds = 600
ksqldb-server                      | 	ksql.transient.query.cleanup.service.period.seconds = 600
ksqldb-server                      | 	ksql.udf.collect.metrics = false
ksqldb-server                      | 	ksql.udf.enable.security.manager = true
ksqldb-server                      | 	ksql.udfs.enabled = true
ksqldb-server                      | 	ksql.variable.substitution.enable = true
ksqldb-server                      | 	ksql.websocket.connection.max.timeout.ms = 3600000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.util.KsqlConfig)
broker                             | [2023-08-04 10:11:58,358] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:58,361] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:58,374] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,374] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,374] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with topic id Some(_zkEEKNOSh-BwqO66EWICw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:58,390] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:58,393] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:58,395] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:58,395] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with topic id tkBvdtlJRWily0KDogf0lg. (state.change.logger)
broker                             | [2023-08-04 10:11:58,413] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
ksqldb-server                      | [2023-08-04 10:11:58,415] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:11:58,415] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:11:58,419] INFO Kafka startTimeMs: 1691143918390 (org.apache.kafka.common.utils.AppInfoParser)
broker                             | [2023-08-04 10:11:58,428] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:58,431] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,437] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,438] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with topic id Some(tkBvdtlJRWily0KDogf0lg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:58,454] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:58,456] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:58,456] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:58,456] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with topic id xgvCZkJ3RMurzOd2tG9eqg. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:11:58,464] INFO AdminClientConfig values: 
ksqldb-server                      | 	auto.include.jmx.reporter = true
ksqldb-server                      | 	bootstrap.servers = [broker:29092]
ksqldb-server                      | 	client.dns.lookup = use_all_dns_ips
ksqldb-server                      | 	client.id = 
ksqldb-server                      | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
ksqldb-server                      | 	confluent.proxy.protocol.client.address = null
ksqldb-server                      | 	confluent.proxy.protocol.client.port = null
ksqldb-server                      | 	confluent.proxy.protocol.client.version = NONE
ksqldb-server                      | 	confluent.use.controller.listener = false
ksqldb-server                      | 	connections.max.idle.ms = 300000
ksqldb-server                      | 	default.api.timeout.ms = 60000
ksqldb-server                      | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
ksqldb-server                      | 	metadata.max.age.ms = 300000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	metrics.num.samples = 2
ksqldb-server                      | 	metrics.recording.level = INFO
ksqldb-server                      | 	metrics.sample.window.ms = 30000
ksqldb-server                      | 	receive.buffer.bytes = 65536
ksqldb-server                      | 	reconnect.backoff.max.ms = 1000
ksqldb-server                      | 	reconnect.backoff.ms = 50
ksqldb-server                      | 	request.timeout.ms = 30000
ksqldb-server                      | 	retries = 2147483647
ksqldb-server                      | 	retry.backoff.ms = 100
ksqldb-server                      | 	sasl.client.callback.handler.class = null
ksqldb-server                      | 	sasl.jaas.config = null
ksqldb-server                      | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
ksqldb-server                      | 	sasl.kerberos.min.time.before.relogin = 60000
ksqldb-server                      | 	sasl.kerberos.service.name = null
ksqldb-server                      | 	sasl.kerberos.ticket.renew.jitter = 0.05
ksqldb-server                      | 	sasl.kerberos.ticket.renew.window.factor = 0.8
ksqldb-server                      | 	sasl.login.callback.handler.class = null
ksqldb-server                      | 	sasl.login.class = null
ksqldb-server                      | 	sasl.login.connect.timeout.ms = null
ksqldb-server                      | 	sasl.login.read.timeout.ms = null
ksqldb-server                      | 	sasl.login.refresh.buffer.seconds = 300
ksqldb-server                      | 	sasl.login.refresh.min.period.seconds = 60
ksqldb-server                      | 	sasl.login.refresh.window.factor = 0.8
ksqldb-server                      | 	sasl.login.refresh.window.jitter = 0.05
ksqldb-server                      | 	sasl.login.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.login.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.mechanism = GSSAPI
ksqldb-server                      | 	sasl.oauthbearer.clock.skew.seconds = 30
ksqldb-server                      | 	sasl.oauthbearer.expected.audience = null
ksqldb-server                      | 	sasl.oauthbearer.expected.issuer = null
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.url = null
ksqldb-server                      | 	sasl.oauthbearer.scope.claim.name = scope
ksqldb-server                      | 	sasl.oauthbearer.sub.claim.name = sub
ksqldb-server                      | 	sasl.oauthbearer.token.endpoint.url = null
ksqldb-server                      | 	security.protocol = PLAINTEXT
ksqldb-server                      | 	security.providers = null
ksqldb-server                      | 	send.buffer.bytes = 131072
ksqldb-server                      | 	socket.connection.setup.timeout.max.ms = 30000
ksqldb-server                      | 	socket.connection.setup.timeout.ms = 10000
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (org.apache.kafka.clients.admin.AdminClientConfig)
broker                             | [2023-08-04 10:11:58,521] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:58,524] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:58,543] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,559] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,560] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with topic id Some(xgvCZkJ3RMurzOd2tG9eqg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:58,581] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:58,586] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:58,592] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:58,592] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with topic id Gcm9HudQQIePkLpcnSZatA. (state.change.logger)
broker                             | [2023-08-04 10:11:58,621] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:58,633] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:58,664] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,665] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,665] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with topic id Some(Gcm9HudQQIePkLpcnSZatA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:58,730] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:58,734] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:58,739] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:58,740] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with topic id TL8DPoV0R9OJhSSwRHqD6A. (state.change.logger)
broker                             | [2023-08-04 10:11:58,784] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:58,830] INFO Created log for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:58,882] INFO [Partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,885] INFO [Partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,886] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with topic id Some(TL8DPoV0R9OJhSSwRHqD6A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:58,907] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:58,910] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:58,915] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:58,918] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with topic id 5MSfzhgUTOCsJsKEWsUfUQ. (state.change.logger)
broker                             | [2023-08-04 10:11:58,945] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:58,948] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:58,955] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,956] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:58,957] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with topic id Some(5MSfzhgUTOCsJsKEWsUfUQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:58,964] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:58,967] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:58,968] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:58,969] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with topic id z2ogKsUbRf6X0CRLuVUTWw. (state.change.logger)
broker                             | [2023-08-04 10:11:58,983] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:58,999] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:59,002] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,003] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,004] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with topic id Some(z2ogKsUbRf6X0CRLuVUTWw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:59,021] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:59,024] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:59,025] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:59,031] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with topic id qbl7Kw1mTrKwZUmrs21Ogw. (state.change.logger)
broker                             | [2023-08-04 10:11:59,100] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:59,119] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:59,124] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,124] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,124] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with topic id Some(qbl7Kw1mTrKwZUmrs21Ogw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:59,143] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:59,145] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:59,145] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:59,147] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with topic id 6A1t9PL6S9ibcqZ61sSDAQ. (state.change.logger)
broker                             | [2023-08-04 10:11:59,183] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:59,185] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:59,190] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,192] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,197] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with topic id Some(6A1t9PL6S9ibcqZ61sSDAQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:59,207] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:59,211] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:59,217] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:59,218] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with topic id xYBgTiSgQeeJ02q7jNXaDg. (state.change.logger)
broker                             | [2023-08-04 10:11:59,235] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:59,238] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:59,252] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,253] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,254] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with topic id Some(xYBgTiSgQeeJ02q7jNXaDg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:59,267] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:59,270] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:59,270] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:59,270] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with topic id bKHlhox8TzeDMS5qRnHymg. (state.change.logger)
broker                             | [2023-08-04 10:11:59,284] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:59,287] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:59,289] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,290] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,291] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with topic id Some(bKHlhox8TzeDMS5qRnHymg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:59,305] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:59,308] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:59,309] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:59,309] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with topic id Ly7r4MYYR8iZXnKz11AfxQ. (state.change.logger)
broker                             | [2023-08-04 10:11:59,320] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:59,329] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:59,507] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,508] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,509] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with topic id Some(Ly7r4MYYR8iZXnKz11AfxQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:59,526] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:59,545] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:59,551] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:59,551] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with topic id HFZ8vq7-RZyhNR6Sk4e1ng. (state.change.logger)
broker                             | [2023-08-04 10:11:59,589] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:59,597] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:59,620] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,620] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,620] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with topic id Some(HFZ8vq7-RZyhNR6Sk4e1ng) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:59,631] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:59,634] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:59,639] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:59,639] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with topic id adG-RfPuSaSjxyV6nKsuDw. (state.change.logger)
broker                             | [2023-08-04 10:11:59,677] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:59,681] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:59,683] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,683] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,683] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with topic id Some(adG-RfPuSaSjxyV6nKsuDw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:59,708] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:59,710] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:59,710] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-monitoring-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:59,711] INFO [Broker id=1] Creating new partition _confluent-monitoring-0 with topic id RjGiTb4XQTq3XkcLgjDFew. (state.change.logger)
broker                             | [2023-08-04 10:11:59,718] INFO [LogLoader partition=_confluent-monitoring-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:59,726] INFO Created log for partition _confluent-monitoring-0 in /tmp/kraft-combined-logs/_confluent-monitoring-0 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:59,733] INFO [Partition _confluent-monitoring-0 broker=1] No checkpointed highwatermark is found for partition _confluent-monitoring-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,733] INFO [Partition _confluent-monitoring-0 broker=1] Log loaded for partition _confluent-monitoring-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,734] INFO [Broker id=1] Leader _confluent-monitoring-0 with topic id Some(RjGiTb4XQTq3XkcLgjDFew) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:59,746] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-monitoring with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 259200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:59,749] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:59,750] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:59,750] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with topic id LV4LlNFGSOuelBMsyB9XFw. (state.change.logger)
broker                             | [2023-08-04 10:11:59,769] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:59,771] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:59,792] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,793] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,793] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with topic id Some(LV4LlNFGSOuelBMsyB9XFw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:59,805] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:59,808] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:59,808] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:59,809] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with topic id NGyXVef5Tbicz4WJpOfTdg. (state.change.logger)
broker                             | [2023-08-04 10:11:59,836] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:59,838] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:59,840] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,846] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,847] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with topic id Some(NGyXVef5Tbicz4WJpOfTdg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:59,859] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:59,863] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:59,864] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:59,867] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with topic id EnSw6NjFQNmCXav_Zub-bw. (state.change.logger)
broker                             | [2023-08-04 10:11:59,878] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:59,890] INFO Created log for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:59,900] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,901] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,901] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with topic id Some(EnSw6NjFQNmCXav_Zub-bw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:59,911] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:11:59,913] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:11:59,913] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:11:59,913] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with topic id UORTMA9aQXuH9QFrp1lC6Q. (state.change.logger)
broker                             | [2023-08-04 10:11:59,932] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:11:59,938] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864} (kafka.log.LogManager)
broker                             | [2023-08-04 10:11:59,940] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,943] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:11:59,943] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with topic id Some(UORTMA9aQXuH9QFrp1lC6Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:11:59,955] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
ksqldb-server                      | [2023-08-04 10:12:00,387] WARN These configurations '[awt.toolkit, log4j.configuration, producer.interceptor.classes, metrics.context.resource.version, metrics.context.resource.commit.id, ksql.server.install.dir, com.sun.management.jmxremote.authenticate, ksql.schema.registry.url, ksql.connect.url, file.encoding, cache.max.bytes.buffering, host.name, config.dir, com.sun.management.jmxremote.ssl, ksql.logging.processing.stream.auto.create, com.sun.management.jmxremote, jdk.debug, listeners, classpath, ksql.logging.processing.topic.auto.create, ksql.log.dir, jdk.vendor.version, ksql.logging.processing.topic.replication.factor, consumer.interceptor.classes]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
ksqldb-server                      | [2023-08-04 10:12:00,388] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:00,388] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:00,389] INFO Kafka startTimeMs: 1691143920388 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:03,061] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:03,081] INFO [AdminClient clientId=adminclient-1] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
ksqldb-server                      | org.apache.kafka.common.errors.TimeoutException: Timed out waiting to send the call. Call: fetchMetadata
ksqldb-server                      | [2023-08-04 10:12:03,096] INFO [AdminClient clientId=adminclient-1] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
ksqldb-server                      | [2023-08-04 10:12:03,180] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
ksqldb-server                      | [2023-08-04 10:12:03,180] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
ksqldb-server                      | [2023-08-04 10:12:03,180] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
ksqldb-server                      | [2023-08-04 10:12:03,181] INFO All preconditions passed, skipping precondition server start (io.confluent.ksql.rest.server.PreconditionChecker)
ksqldb-server                      | [2023-08-04 10:12:03,181] INFO Server up and running (io.confluent.ksql.rest.server.KsqlServerMain)
ksqldb-server                      | [2023-08-04 10:12:03,190] INFO Checking preconditions... (io.confluent.ksql.rest.server.PreconditionChecker)
ksqldb-server                      | [2023-08-04 10:12:03,190] INFO Server shutting down (io.confluent.ksql.rest.server.KsqlServerMain)
ksqldb-server                      | [2023-08-04 10:12:03,206] INFO KsqlConfig values: 
ksqldb-server                      | 	ksql.access.validator.enable = auto
ksqldb-server                      | 	ksql.assert.schema.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.assert.topic.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.authorization.cache.expiry.time.secs = 30
ksqldb-server                      | 	ksql.authorization.cache.max.entries = 10000
ksqldb-server                      | 	ksql.cast.strings.preserve.nulls = true
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.file = 
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.reload = false
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.source = NONE
ksqldb-server                      | 	ksql.connect.request.headers.plugin = null
ksqldb-server                      | 	ksql.connect.request.timeout.ms = 5000
ksqldb-server                      | 	ksql.connect.url = http://connect:8083
ksqldb-server                      | 	ksql.connect.worker.config = 
ksqldb-server                      | 	ksql.create.or.replace.enabled = true
ksqldb-server                      | 	ksql.endpoint.migrate.query = true
ksqldb-server                      | 	ksql.error.classifier.regex = 
ksqldb-server                      | 	ksql.extension.dir = ext
ksqldb-server                      | 	ksql.headers.columns.enabled = true
ksqldb-server                      | 	ksql.hidden.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.insert.into.values.enabled = true
ksqldb-server                      | 	ksql.internal.topic.min.insync.replicas = 1
ksqldb-server                      | 	ksql.internal.topic.replicas = 1
ksqldb-server                      | 	ksql.json_sr.converter.deserializer.enabled = true
ksqldb-server                      | 	ksql.lambdas.enabled = true
ksqldb-server                      | 	ksql.metastore.backup.location = 
ksqldb-server                      | 	ksql.metrics.extension = null
ksqldb-server                      | 	ksql.metrics.tags.custom = 
ksqldb-server                      | 	ksql.nested.error.set.null = true
ksqldb-server                      | 	ksql.new.query.planner.enabled = false
ksqldb-server                      | 	ksql.output.topic.name.prefix = 
ksqldb-server                      | 	ksql.persistence.default.format.key = KAFKA
ksqldb-server                      | 	ksql.persistence.default.format.value = null
ksqldb-server                      | 	ksql.persistence.wrap.single.values = null
ksqldb-server                      | 	ksql.persistent.prefix = query_
ksqldb-server                      | 	ksql.properties.overrides.denylist = []
ksqldb-server                      | 	ksql.pull.queries.enable = true
ksqldb-server                      | 	ksql.query.cleanup.shutdown.timeout.ms = 30000
ksqldb-server                      | 	ksql.query.error.max.queue.size = 10
ksqldb-server                      | 	ksql.query.persistent.active.limit = 2147483647
ksqldb-server                      | 	ksql.query.persistent.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.query.pull.enable.standby.reads = false
ksqldb-server                      | 	ksql.query.pull.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.pull.limit.clause.enabled = true
ksqldb-server                      | 	ksql.query.pull.max.allowed.offset.lag = 9223372036854775807
ksqldb-server                      | 	ksql.query.pull.max.concurrent.requests = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.qps = 2147483647
ksqldb-server                      | 	ksql.query.pull.metrics.enabled = true
ksqldb-server                      | 	ksql.query.pull.range.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.router.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.pull.stream.enabled = true
ksqldb-server                      | 	ksql.query.pull.table.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.push.v2.alos.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.catchup.consumer.msg.window = 50
ksqldb-server                      | 	ksql.query.push.v2.continuation.tokens.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.latest.reset.age.ms = 30000
ksqldb-server                      | 	ksql.query.push.v2.max.catchup.consumers = 5
ksqldb-server                      | 	ksql.query.push.v2.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.push.v2.metrics.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.new.latest.delay.ms = 5000
ksqldb-server                      | 	ksql.query.push.v2.registry.installed = false
ksqldb-server                      | 	ksql.query.retry.backoff.initial.ms = 15000
ksqldb-server                      | 	ksql.query.retry.backoff.max.ms = 900000
ksqldb-server                      | 	ksql.query.status.running.threshold.seconds = 300
ksqldb-server                      | 	ksql.query.transient.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.queryanonymizer.cluster_namespace = null
ksqldb-server                      | 	ksql.queryanonymizer.logs_enabled = true
ksqldb-server                      | 	ksql.readonly.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.runtime.feature.shared.enabled = false
ksqldb-server                      | 	ksql.schema.registry.url = http://schema-registry:8081
ksqldb-server                      | 	ksql.security.extension.class = null
ksqldb-server                      | 	ksql.service.id = default_
ksqldb-server                      | 	ksql.shared.runtimes.count = 2
ksqldb-server                      | 	ksql.sink.window.change.log.additional.retention = 1000000
ksqldb-server                      | 	ksql.source.table.materialization.enabled = true
ksqldb-server                      | 	ksql.streams.shutdown.timeout.ms = 300000
ksqldb-server                      | 	ksql.suppress.buffer.size.bytes = -1
ksqldb-server                      | 	ksql.suppress.enabled = true
ksqldb-server                      | 	ksql.timestamp.throw.on.invalid = false
ksqldb-server                      | 	ksql.transient.prefix = transient_
ksqldb-server                      | 	ksql.transient.query.cleanup.service.enable = true
ksqldb-server                      | 	ksql.transient.query.cleanup.service.initial.delay.seconds = 600
ksqldb-server                      | 	ksql.transient.query.cleanup.service.period.seconds = 600
ksqldb-server                      | 	ksql.udf.collect.metrics = false
ksqldb-server                      | 	ksql.udf.enable.security.manager = true
ksqldb-server                      | 	ksql.udfs.enabled = true
ksqldb-server                      | 	ksql.variable.substitution.enable = true
ksqldb-server                      | 	ksql.websocket.connection.max.timeout.ms = 3600000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.util.KsqlConfig)
ksqldb-server                      | [2023-08-04 10:12:03,220] INFO KsqlRestConfig values: 
ksqldb-server                      | 	access.control.allow.headers = []
ksqldb-server                      | 	access.control.allow.methods = []
ksqldb-server                      | 	access.control.allow.origin = 
ksqldb-server                      | 	authentication.method = NONE
ksqldb-server                      | 	authentication.realm = 
ksqldb-server                      | 	authentication.roles = [*]
ksqldb-server                      | 	authentication.skip.paths = []
ksqldb-server                      | 	ksql.advertised.listener = null
ksqldb-server                      | 	ksql.authentication.plugin.class = null
ksqldb-server                      | 	ksql.endpoint.logging.ignored.paths.regex = 
ksqldb-server                      | 	ksql.endpoint.logging.log.queries = false
ksqldb-server                      | 	ksql.healthcheck.interval.ms = 5000
ksqldb-server                      | 	ksql.heartbeat.check.interval.ms = 200
ksqldb-server                      | 	ksql.heartbeat.discover.interval.ms = 2000
ksqldb-server                      | 	ksql.heartbeat.enable = false
ksqldb-server                      | 	ksql.heartbeat.missed.threshold.ms = 3
ksqldb-server                      | 	ksql.heartbeat.send.interval.ms = 100
ksqldb-server                      | 	ksql.heartbeat.thread.pool.size = 3
ksqldb-server                      | 	ksql.heartbeat.window.ms = 2000
ksqldb-server                      | 	ksql.idle.connection.timeout.seconds = 86400
ksqldb-server                      | 	ksql.internal.http2.max.pool.size = 3000
ksqldb-server                      | 	ksql.internal.listener = null
ksqldb-server                      | 	ksql.internal.ssl.client.authentication = NONE
ksqldb-server                      | 	ksql.lag.reporting.enable = false
ksqldb-server                      | 	ksql.lag.reporting.send.interval.ms = 5000
ksqldb-server                      | 	ksql.local.commands.location = 
ksqldb-server                      | 	ksql.logging.server.rate.limited.request.paths = 
ksqldb-server                      | 	ksql.logging.server.rate.limited.response.codes = 
ksqldb-server                      | 	ksql.max.push.queries = 100
ksqldb-server                      | 	ksql.server.command.blocked.threshold.error.ms = 15000
ksqldb-server                      | 	ksql.server.command.response.timeout.ms = 5000
ksqldb-server                      | 	ksql.server.command.topic.migration.enabled = NONE
ksqldb-server                      | 	ksql.server.command.topic.rate.limit = 1.7976931348623157E308
ksqldb-server                      | 	ksql.server.error.messages = class io.confluent.ksql.rest.DefaultErrorMessages
ksqldb-server                      | 	ksql.server.exception.uncaught.handler.enable = false
ksqldb-server                      | 	ksql.server.install.dir = /usr
ksqldb-server                      | 	ksql.server.precondition.max.backoff.ms = 5000
ksqldb-server                      | 	ksql.server.preconditions = []
ksqldb-server                      | 	ksql.server.sni.check.enable = false
ksqldb-server                      | 	ksql.server.websockets.num.threads = 5
ksqldb-server                      | 	ksql.ssl.keystore.alias.external = 
ksqldb-server                      | 	ksql.ssl.keystore.alias.internal = 
ksqldb-server                      | 	ksql.verticle.instances = 8
ksqldb-server                      | 	ksql.worker.pool.size = 100
ksqldb-server                      | 	listeners = [http://0.0.0.0:8088]
ksqldb-server                      | 	query.stream.disconnect.check = 1000
ksqldb-server                      | 	ssl.cipher.suites = []
ksqldb-server                      | 	ssl.client.auth = false
ksqldb-server                      | 	ssl.client.authentication = NONE
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.key.password = [hidden]
ksqldb-server                      | 	ssl.keystore.location = 
ksqldb-server                      | 	ssl.keystore.password = [hidden]
ksqldb-server                      | 	ssl.keystore.reload = false
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.keystore.watch.location = 
ksqldb-server                      | 	ssl.truststore.location = 
ksqldb-server                      | 	ssl.truststore.password = [hidden]
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.rest.server.KsqlRestConfig)
ksqldb-server                      | [2023-08-04 10:12:03,272] INFO KsqlConfig values: 
ksqldb-server                      | 	ksql.access.validator.enable = auto
ksqldb-server                      | 	ksql.assert.schema.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.assert.topic.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.authorization.cache.expiry.time.secs = 30
ksqldb-server                      | 	ksql.authorization.cache.max.entries = 10000
ksqldb-server                      | 	ksql.cast.strings.preserve.nulls = true
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.file = 
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.reload = false
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.source = NONE
ksqldb-server                      | 	ksql.connect.request.headers.plugin = null
ksqldb-server                      | 	ksql.connect.request.timeout.ms = 5000
ksqldb-server                      | 	ksql.connect.url = http://connect:8083
ksqldb-server                      | 	ksql.connect.worker.config = 
ksqldb-server                      | 	ksql.create.or.replace.enabled = true
ksqldb-server                      | 	ksql.endpoint.migrate.query = true
ksqldb-server                      | 	ksql.error.classifier.regex = 
ksqldb-server                      | 	ksql.extension.dir = ext
ksqldb-server                      | 	ksql.headers.columns.enabled = true
ksqldb-server                      | 	ksql.hidden.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.insert.into.values.enabled = true
ksqldb-server                      | 	ksql.internal.topic.min.insync.replicas = 1
ksqldb-server                      | 	ksql.internal.topic.replicas = 1
ksqldb-server                      | 	ksql.json_sr.converter.deserializer.enabled = true
ksqldb-server                      | 	ksql.lambdas.enabled = true
ksqldb-server                      | 	ksql.metastore.backup.location = 
ksqldb-server                      | 	ksql.metrics.extension = null
ksqldb-server                      | 	ksql.metrics.tags.custom = 
ksqldb-server                      | 	ksql.nested.error.set.null = true
ksqldb-server                      | 	ksql.new.query.planner.enabled = false
ksqldb-server                      | 	ksql.output.topic.name.prefix = 
ksqldb-server                      | 	ksql.persistence.default.format.key = KAFKA
ksqldb-server                      | 	ksql.persistence.default.format.value = null
ksqldb-server                      | 	ksql.persistence.wrap.single.values = null
ksqldb-server                      | 	ksql.persistent.prefix = query_
ksqldb-server                      | 	ksql.properties.overrides.denylist = []
ksqldb-server                      | 	ksql.pull.queries.enable = true
ksqldb-server                      | 	ksql.query.cleanup.shutdown.timeout.ms = 30000
ksqldb-server                      | 	ksql.query.error.max.queue.size = 10
ksqldb-server                      | 	ksql.query.persistent.active.limit = 2147483647
ksqldb-server                      | 	ksql.query.persistent.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.query.pull.enable.standby.reads = false
ksqldb-server                      | 	ksql.query.pull.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.pull.limit.clause.enabled = true
ksqldb-server                      | 	ksql.query.pull.max.allowed.offset.lag = 9223372036854775807
ksqldb-server                      | 	ksql.query.pull.max.concurrent.requests = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.qps = 2147483647
ksqldb-server                      | 	ksql.query.pull.metrics.enabled = true
ksqldb-server                      | 	ksql.query.pull.range.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.router.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.pull.stream.enabled = true
ksqldb-server                      | 	ksql.query.pull.table.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.push.v2.alos.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.catchup.consumer.msg.window = 50
ksqldb-server                      | 	ksql.query.push.v2.continuation.tokens.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.latest.reset.age.ms = 30000
ksqldb-server                      | 	ksql.query.push.v2.max.catchup.consumers = 5
ksqldb-server                      | 	ksql.query.push.v2.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.push.v2.metrics.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.new.latest.delay.ms = 5000
ksqldb-server                      | 	ksql.query.push.v2.registry.installed = false
ksqldb-server                      | 	ksql.query.retry.backoff.initial.ms = 15000
ksqldb-server                      | 	ksql.query.retry.backoff.max.ms = 900000
ksqldb-server                      | 	ksql.query.status.running.threshold.seconds = 300
ksqldb-server                      | 	ksql.query.transient.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.queryanonymizer.cluster_namespace = null
ksqldb-server                      | 	ksql.queryanonymizer.logs_enabled = true
ksqldb-server                      | 	ksql.readonly.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.runtime.feature.shared.enabled = false
ksqldb-server                      | 	ksql.schema.registry.url = http://schema-registry:8081
ksqldb-server                      | 	ksql.security.extension.class = null
ksqldb-server                      | 	ksql.service.id = default_
ksqldb-server                      | 	ksql.shared.runtimes.count = 2
ksqldb-server                      | 	ksql.sink.window.change.log.additional.retention = 1000000
ksqldb-server                      | 	ksql.source.table.materialization.enabled = true
ksqldb-server                      | 	ksql.streams.shutdown.timeout.ms = 300000
ksqldb-server                      | 	ksql.suppress.buffer.size.bytes = -1
ksqldb-server                      | 	ksql.suppress.enabled = true
ksqldb-server                      | 	ksql.timestamp.throw.on.invalid = false
ksqldb-server                      | 	ksql.transient.prefix = transient_
ksqldb-server                      | 	ksql.transient.query.cleanup.service.enable = true
ksqldb-server                      | 	ksql.transient.query.cleanup.service.initial.delay.seconds = 600
ksqldb-server                      | 	ksql.transient.query.cleanup.service.period.seconds = 600
ksqldb-server                      | 	ksql.udf.collect.metrics = false
ksqldb-server                      | 	ksql.udf.enable.security.manager = true
ksqldb-server                      | 	ksql.udfs.enabled = true
ksqldb-server                      | 	ksql.variable.substitution.enable = true
ksqldb-server                      | 	ksql.websocket.connection.max.timeout.ms = 3600000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.util.KsqlConfig)
ksqldb-server                      | [2023-08-04 10:12:03,581] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:03,582] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:03,586] INFO Kafka startTimeMs: 1691143923578 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:03,591] INFO AdminClientConfig values: 
ksqldb-server                      | 	auto.include.jmx.reporter = true
ksqldb-server                      | 	bootstrap.servers = [broker:29092]
ksqldb-server                      | 	client.dns.lookup = use_all_dns_ips
ksqldb-server                      | 	client.id = 
ksqldb-server                      | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
ksqldb-server                      | 	confluent.proxy.protocol.client.address = null
ksqldb-server                      | 	confluent.proxy.protocol.client.port = null
ksqldb-server                      | 	confluent.proxy.protocol.client.version = NONE
ksqldb-server                      | 	confluent.use.controller.listener = false
ksqldb-server                      | 	connections.max.idle.ms = 300000
ksqldb-server                      | 	default.api.timeout.ms = 60000
ksqldb-server                      | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
ksqldb-server                      | 	metadata.max.age.ms = 300000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	metrics.num.samples = 2
ksqldb-server                      | 	metrics.recording.level = INFO
ksqldb-server                      | 	metrics.sample.window.ms = 30000
ksqldb-server                      | 	receive.buffer.bytes = 65536
ksqldb-server                      | 	reconnect.backoff.max.ms = 1000
ksqldb-server                      | 	reconnect.backoff.ms = 50
ksqldb-server                      | 	request.timeout.ms = 30000
ksqldb-server                      | 	retries = 2147483647
ksqldb-server                      | 	retry.backoff.ms = 100
ksqldb-server                      | 	sasl.client.callback.handler.class = null
ksqldb-server                      | 	sasl.jaas.config = null
ksqldb-server                      | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
ksqldb-server                      | 	sasl.kerberos.min.time.before.relogin = 60000
ksqldb-server                      | 	sasl.kerberos.service.name = null
ksqldb-server                      | 	sasl.kerberos.ticket.renew.jitter = 0.05
ksqldb-server                      | 	sasl.kerberos.ticket.renew.window.factor = 0.8
ksqldb-server                      | 	sasl.login.callback.handler.class = null
ksqldb-server                      | 	sasl.login.class = null
ksqldb-server                      | 	sasl.login.connect.timeout.ms = null
ksqldb-server                      | 	sasl.login.read.timeout.ms = null
ksqldb-server                      | 	sasl.login.refresh.buffer.seconds = 300
ksqldb-server                      | 	sasl.login.refresh.min.period.seconds = 60
ksqldb-server                      | 	sasl.login.refresh.window.factor = 0.8
ksqldb-server                      | 	sasl.login.refresh.window.jitter = 0.05
ksqldb-server                      | 	sasl.login.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.login.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.mechanism = GSSAPI
ksqldb-server                      | 	sasl.oauthbearer.clock.skew.seconds = 30
ksqldb-server                      | 	sasl.oauthbearer.expected.audience = null
ksqldb-server                      | 	sasl.oauthbearer.expected.issuer = null
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.url = null
ksqldb-server                      | 	sasl.oauthbearer.scope.claim.name = scope
ksqldb-server                      | 	sasl.oauthbearer.sub.claim.name = sub
ksqldb-server                      | 	sasl.oauthbearer.token.endpoint.url = null
ksqldb-server                      | 	security.protocol = PLAINTEXT
ksqldb-server                      | 	security.providers = null
ksqldb-server                      | 	send.buffer.bytes = 131072
ksqldb-server                      | 	socket.connection.setup.timeout.max.ms = 30000
ksqldb-server                      | 	socket.connection.setup.timeout.ms = 10000
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (org.apache.kafka.clients.admin.AdminClientConfig)
ksqldb-server                      | [2023-08-04 10:12:03,653] WARN These configurations '[metrics.context.resource.version, metrics.context.resource.commit.id]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
ksqldb-server                      | [2023-08-04 10:12:03,653] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:03,653] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:03,653] INFO Kafka startTimeMs: 1691143923653 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:04,289] INFO KsqlRestConfig values: 
ksqldb-server                      | 	access.control.allow.headers = []
ksqldb-server                      | 	access.control.allow.methods = []
ksqldb-server                      | 	access.control.allow.origin = 
ksqldb-server                      | 	authentication.method = NONE
ksqldb-server                      | 	authentication.realm = 
ksqldb-server                      | 	authentication.roles = [*]
ksqldb-server                      | 	authentication.skip.paths = []
ksqldb-server                      | 	ksql.advertised.listener = null
ksqldb-server                      | 	ksql.authentication.plugin.class = null
ksqldb-server                      | 	ksql.endpoint.logging.ignored.paths.regex = 
ksqldb-server                      | 	ksql.endpoint.logging.log.queries = false
ksqldb-server                      | 	ksql.healthcheck.interval.ms = 5000
ksqldb-server                      | 	ksql.heartbeat.check.interval.ms = 200
ksqldb-server                      | 	ksql.heartbeat.discover.interval.ms = 2000
ksqldb-server                      | 	ksql.heartbeat.enable = false
ksqldb-server                      | 	ksql.heartbeat.missed.threshold.ms = 3
ksqldb-server                      | 	ksql.heartbeat.send.interval.ms = 100
ksqldb-server                      | 	ksql.heartbeat.thread.pool.size = 3
ksqldb-server                      | 	ksql.heartbeat.window.ms = 2000
ksqldb-server                      | 	ksql.idle.connection.timeout.seconds = 86400
ksqldb-server                      | 	ksql.internal.http2.max.pool.size = 3000
ksqldb-server                      | 	ksql.internal.listener = null
ksqldb-server                      | 	ksql.internal.ssl.client.authentication = NONE
ksqldb-server                      | 	ksql.lag.reporting.enable = false
ksqldb-server                      | 	ksql.lag.reporting.send.interval.ms = 5000
ksqldb-server                      | 	ksql.local.commands.location = 
ksqldb-server                      | 	ksql.logging.server.rate.limited.request.paths = 
ksqldb-server                      | 	ksql.logging.server.rate.limited.response.codes = 
ksqldb-server                      | 	ksql.max.push.queries = 100
ksqldb-server                      | 	ksql.server.command.blocked.threshold.error.ms = 15000
ksqldb-server                      | 	ksql.server.command.response.timeout.ms = 5000
ksqldb-server                      | 	ksql.server.command.topic.migration.enabled = NONE
ksqldb-server                      | 	ksql.server.command.topic.rate.limit = 1.7976931348623157E308
ksqldb-server                      | 	ksql.server.error.messages = class io.confluent.ksql.rest.DefaultErrorMessages
ksqldb-server                      | 	ksql.server.exception.uncaught.handler.enable = false
ksqldb-server                      | 	ksql.server.install.dir = /usr
ksqldb-server                      | 	ksql.server.precondition.max.backoff.ms = 5000
ksqldb-server                      | 	ksql.server.preconditions = []
ksqldb-server                      | 	ksql.server.sni.check.enable = false
ksqldb-server                      | 	ksql.server.websockets.num.threads = 5
ksqldb-server                      | 	ksql.ssl.keystore.alias.external = 
ksqldb-server                      | 	ksql.ssl.keystore.alias.internal = 
ksqldb-server                      | 	ksql.verticle.instances = 8
ksqldb-server                      | 	ksql.worker.pool.size = 100
ksqldb-server                      | 	listeners = [http://0.0.0.0:8088]
ksqldb-server                      | 	query.stream.disconnect.check = 1000
ksqldb-server                      | 	ssl.cipher.suites = []
ksqldb-server                      | 	ssl.client.auth = false
ksqldb-server                      | 	ssl.client.authentication = NONE
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.key.password = [hidden]
ksqldb-server                      | 	ssl.keystore.location = 
ksqldb-server                      | 	ssl.keystore.password = [hidden]
ksqldb-server                      | 	ssl.keystore.reload = false
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.keystore.watch.location = 
ksqldb-server                      | 	ssl.truststore.location = 
ksqldb-server                      | 	ssl.truststore.password = [hidden]
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.rest.server.KsqlRestConfig)
ksqldb-server                      | [2023-08-04 10:12:04,311] INFO KsqlConfig values: 
ksqldb-server                      | 	ksql.access.validator.enable = auto
ksqldb-server                      | 	ksql.assert.schema.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.assert.topic.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.authorization.cache.expiry.time.secs = 30
ksqldb-server                      | 	ksql.authorization.cache.max.entries = 10000
ksqldb-server                      | 	ksql.cast.strings.preserve.nulls = true
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.file = 
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.reload = false
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.source = NONE
ksqldb-server                      | 	ksql.connect.request.headers.plugin = null
ksqldb-server                      | 	ksql.connect.request.timeout.ms = 5000
ksqldb-server                      | 	ksql.connect.url = http://connect:8083
ksqldb-server                      | 	ksql.connect.worker.config = 
ksqldb-server                      | 	ksql.create.or.replace.enabled = true
ksqldb-server                      | 	ksql.endpoint.migrate.query = true
ksqldb-server                      | 	ksql.error.classifier.regex = 
ksqldb-server                      | 	ksql.extension.dir = ext
ksqldb-server                      | 	ksql.headers.columns.enabled = true
ksqldb-server                      | 	ksql.hidden.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.insert.into.values.enabled = true
ksqldb-server                      | 	ksql.internal.topic.min.insync.replicas = 1
ksqldb-server                      | 	ksql.internal.topic.replicas = 1
ksqldb-server                      | 	ksql.json_sr.converter.deserializer.enabled = true
ksqldb-server                      | 	ksql.lambdas.enabled = true
ksqldb-server                      | 	ksql.metastore.backup.location = 
ksqldb-server                      | 	ksql.metrics.extension = null
ksqldb-server                      | 	ksql.metrics.tags.custom = 
ksqldb-server                      | 	ksql.nested.error.set.null = true
ksqldb-server                      | 	ksql.new.query.planner.enabled = false
ksqldb-server                      | 	ksql.output.topic.name.prefix = 
ksqldb-server                      | 	ksql.persistence.default.format.key = KAFKA
ksqldb-server                      | 	ksql.persistence.default.format.value = null
ksqldb-server                      | 	ksql.persistence.wrap.single.values = null
ksqldb-server                      | 	ksql.persistent.prefix = query_
ksqldb-server                      | 	ksql.properties.overrides.denylist = []
ksqldb-server                      | 	ksql.pull.queries.enable = true
ksqldb-server                      | 	ksql.query.cleanup.shutdown.timeout.ms = 30000
ksqldb-server                      | 	ksql.query.error.max.queue.size = 10
ksqldb-server                      | 	ksql.query.persistent.active.limit = 2147483647
ksqldb-server                      | 	ksql.query.persistent.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.query.pull.enable.standby.reads = false
ksqldb-server                      | 	ksql.query.pull.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.pull.limit.clause.enabled = true
ksqldb-server                      | 	ksql.query.pull.max.allowed.offset.lag = 9223372036854775807
ksqldb-server                      | 	ksql.query.pull.max.concurrent.requests = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.qps = 2147483647
ksqldb-server                      | 	ksql.query.pull.metrics.enabled = true
ksqldb-server                      | 	ksql.query.pull.range.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.router.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.pull.stream.enabled = true
ksqldb-server                      | 	ksql.query.pull.table.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.push.v2.alos.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.catchup.consumer.msg.window = 50
ksqldb-server                      | 	ksql.query.push.v2.continuation.tokens.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.latest.reset.age.ms = 30000
ksqldb-server                      | 	ksql.query.push.v2.max.catchup.consumers = 5
ksqldb-server                      | 	ksql.query.push.v2.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.push.v2.metrics.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.new.latest.delay.ms = 5000
ksqldb-server                      | 	ksql.query.push.v2.registry.installed = false
ksqldb-server                      | 	ksql.query.retry.backoff.initial.ms = 15000
ksqldb-server                      | 	ksql.query.retry.backoff.max.ms = 900000
ksqldb-server                      | 	ksql.query.status.running.threshold.seconds = 300
ksqldb-server                      | 	ksql.query.transient.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.queryanonymizer.cluster_namespace = null
ksqldb-server                      | 	ksql.queryanonymizer.logs_enabled = true
ksqldb-server                      | 	ksql.readonly.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.runtime.feature.shared.enabled = false
ksqldb-server                      | 	ksql.schema.registry.url = http://schema-registry:8081
ksqldb-server                      | 	ksql.security.extension.class = null
ksqldb-server                      | 	ksql.service.id = default_
ksqldb-server                      | 	ksql.shared.runtimes.count = 2
ksqldb-server                      | 	ksql.sink.window.change.log.additional.retention = 1000000
ksqldb-server                      | 	ksql.source.table.materialization.enabled = true
ksqldb-server                      | 	ksql.streams.shutdown.timeout.ms = 300000
ksqldb-server                      | 	ksql.suppress.buffer.size.bytes = -1
ksqldb-server                      | 	ksql.suppress.enabled = true
ksqldb-server                      | 	ksql.timestamp.throw.on.invalid = false
ksqldb-server                      | 	ksql.transient.prefix = transient_
ksqldb-server                      | 	ksql.transient.query.cleanup.service.enable = true
ksqldb-server                      | 	ksql.transient.query.cleanup.service.initial.delay.seconds = 600
ksqldb-server                      | 	ksql.transient.query.cleanup.service.period.seconds = 600
ksqldb-server                      | 	ksql.udf.collect.metrics = false
ksqldb-server                      | 	ksql.udf.enable.security.manager = true
ksqldb-server                      | 	ksql.udfs.enabled = true
ksqldb-server                      | 	ksql.variable.substitution.enable = true
ksqldb-server                      | 	ksql.websocket.connection.max.timeout.ms = 3600000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.util.KsqlConfig)
ksqldb-server                      | [2023-08-04 10:12:04,321] INFO ProcessingLogConfig values: 
ksqldb-server                      | 	ksql.logging.processing.rows.include = false
ksqldb-server                      | 	ksql.logging.processing.stream.auto.create = true
ksqldb-server                      | 	ksql.logging.processing.stream.name = KSQL_PROCESSING_LOG
ksqldb-server                      | 	ksql.logging.processing.topic.auto.create = true
ksqldb-server                      | 	ksql.logging.processing.topic.name = 
ksqldb-server                      | 	ksql.logging.processing.topic.partitions = 1
ksqldb-server                      | 	ksql.logging.processing.topic.replication.factor = 1
ksqldb-server                      |  (io.confluent.ksql.logging.processing.ProcessingLogConfig)
ksqldb-server                      | [2023-08-04 10:12:04,355] INFO Adding node level storage usage gauges (io.confluent.ksql.internal.StorageUtilizationMetricsReporter)
ksqldb-server                      | [2023-08-04 10:12:04,386] INFO KsqlConfig values: 
ksqldb-server                      | 	ksql.access.validator.enable = auto
ksqldb-server                      | 	ksql.assert.schema.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.assert.topic.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.authorization.cache.expiry.time.secs = 30
ksqldb-server                      | 	ksql.authorization.cache.max.entries = 10000
ksqldb-server                      | 	ksql.cast.strings.preserve.nulls = true
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.file = 
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.reload = false
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.source = NONE
ksqldb-server                      | 	ksql.connect.request.headers.plugin = null
ksqldb-server                      | 	ksql.connect.request.timeout.ms = 5000
ksqldb-server                      | 	ksql.connect.url = http://connect:8083
ksqldb-server                      | 	ksql.connect.worker.config = 
ksqldb-server                      | 	ksql.create.or.replace.enabled = true
ksqldb-server                      | 	ksql.endpoint.migrate.query = true
ksqldb-server                      | 	ksql.error.classifier.regex = 
ksqldb-server                      | 	ksql.extension.dir = ext
ksqldb-server                      | 	ksql.headers.columns.enabled = true
ksqldb-server                      | 	ksql.hidden.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.insert.into.values.enabled = true
ksqldb-server                      | 	ksql.internal.topic.min.insync.replicas = 1
ksqldb-server                      | 	ksql.internal.topic.replicas = 1
ksqldb-server                      | 	ksql.json_sr.converter.deserializer.enabled = true
ksqldb-server                      | 	ksql.lambdas.enabled = true
ksqldb-server                      | 	ksql.metastore.backup.location = 
ksqldb-server                      | 	ksql.metrics.extension = null
ksqldb-server                      | 	ksql.metrics.tags.custom = 
ksqldb-server                      | 	ksql.nested.error.set.null = true
ksqldb-server                      | 	ksql.new.query.planner.enabled = false
ksqldb-server                      | 	ksql.output.topic.name.prefix = 
ksqldb-server                      | 	ksql.persistence.default.format.key = KAFKA
ksqldb-server                      | 	ksql.persistence.default.format.value = null
ksqldb-server                      | 	ksql.persistence.wrap.single.values = null
ksqldb-server                      | 	ksql.persistent.prefix = query_
ksqldb-server                      | 	ksql.properties.overrides.denylist = []
ksqldb-server                      | 	ksql.pull.queries.enable = true
ksqldb-server                      | 	ksql.query.cleanup.shutdown.timeout.ms = 30000
ksqldb-server                      | 	ksql.query.error.max.queue.size = 10
ksqldb-server                      | 	ksql.query.persistent.active.limit = 2147483647
ksqldb-server                      | 	ksql.query.persistent.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.query.pull.enable.standby.reads = false
ksqldb-server                      | 	ksql.query.pull.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.pull.limit.clause.enabled = true
ksqldb-server                      | 	ksql.query.pull.max.allowed.offset.lag = 9223372036854775807
ksqldb-server                      | 	ksql.query.pull.max.concurrent.requests = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.qps = 2147483647
ksqldb-server                      | 	ksql.query.pull.metrics.enabled = true
ksqldb-server                      | 	ksql.query.pull.range.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.router.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.pull.stream.enabled = true
ksqldb-server                      | 	ksql.query.pull.table.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.push.v2.alos.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.catchup.consumer.msg.window = 50
ksqldb-server                      | 	ksql.query.push.v2.continuation.tokens.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.latest.reset.age.ms = 30000
ksqldb-server                      | 	ksql.query.push.v2.max.catchup.consumers = 5
ksqldb-server                      | 	ksql.query.push.v2.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.push.v2.metrics.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.new.latest.delay.ms = 5000
ksqldb-server                      | 	ksql.query.push.v2.registry.installed = false
ksqldb-server                      | 	ksql.query.retry.backoff.initial.ms = 15000
ksqldb-server                      | 	ksql.query.retry.backoff.max.ms = 900000
ksqldb-server                      | 	ksql.query.status.running.threshold.seconds = 300
ksqldb-server                      | 	ksql.query.transient.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.queryanonymizer.cluster_namespace = null
ksqldb-server                      | 	ksql.queryanonymizer.logs_enabled = true
ksqldb-server                      | 	ksql.readonly.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.runtime.feature.shared.enabled = false
ksqldb-server                      | 	ksql.schema.registry.url = http://schema-registry:8081
ksqldb-server                      | 	ksql.security.extension.class = null
ksqldb-server                      | 	ksql.service.id = default_
ksqldb-server                      | 	ksql.shared.runtimes.count = 2
ksqldb-server                      | 	ksql.sink.window.change.log.additional.retention = 1000000
ksqldb-server                      | 	ksql.source.table.materialization.enabled = true
ksqldb-server                      | 	ksql.streams.shutdown.timeout.ms = 300000
ksqldb-server                      | 	ksql.suppress.buffer.size.bytes = -1
ksqldb-server                      | 	ksql.suppress.enabled = true
ksqldb-server                      | 	ksql.timestamp.throw.on.invalid = false
ksqldb-server                      | 	ksql.transient.prefix = transient_
ksqldb-server                      | 	ksql.transient.query.cleanup.service.enable = true
ksqldb-server                      | 	ksql.transient.query.cleanup.service.initial.delay.seconds = 600
ksqldb-server                      | 	ksql.transient.query.cleanup.service.period.seconds = 600
ksqldb-server                      | 	ksql.udf.collect.metrics = false
ksqldb-server                      | 	ksql.udf.enable.security.manager = true
ksqldb-server                      | 	ksql.udfs.enabled = true
ksqldb-server                      | 	ksql.variable.substitution.enable = true
ksqldb-server                      | 	ksql.websocket.connection.max.timeout.ms = 3600000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.util.KsqlConfig)
ksqldb-server                      | [2023-08-04 10:12:04,572] INFO KsqlConfig values: 
ksqldb-server                      | 	ksql.access.validator.enable = auto
ksqldb-server                      | 	ksql.assert.schema.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.assert.topic.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.authorization.cache.expiry.time.secs = 30
ksqldb-server                      | 	ksql.authorization.cache.max.entries = 10000
ksqldb-server                      | 	ksql.cast.strings.preserve.nulls = true
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.file = 
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.reload = false
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.source = NONE
ksqldb-server                      | 	ksql.connect.request.headers.plugin = null
ksqldb-server                      | 	ksql.connect.request.timeout.ms = 5000
ksqldb-server                      | 	ksql.connect.url = http://connect:8083
ksqldb-server                      | 	ksql.connect.worker.config = 
ksqldb-server                      | 	ksql.create.or.replace.enabled = true
ksqldb-server                      | 	ksql.endpoint.migrate.query = true
ksqldb-server                      | 	ksql.error.classifier.regex = 
ksqldb-server                      | 	ksql.extension.dir = ext
ksqldb-server                      | 	ksql.headers.columns.enabled = true
ksqldb-server                      | 	ksql.hidden.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.insert.into.values.enabled = true
ksqldb-server                      | 	ksql.internal.topic.min.insync.replicas = 1
ksqldb-server                      | 	ksql.internal.topic.replicas = 1
ksqldb-server                      | 	ksql.json_sr.converter.deserializer.enabled = true
ksqldb-server                      | 	ksql.lambdas.enabled = true
ksqldb-server                      | 	ksql.metastore.backup.location = 
ksqldb-server                      | 	ksql.metrics.extension = null
ksqldb-server                      | 	ksql.metrics.tags.custom = 
ksqldb-server                      | 	ksql.nested.error.set.null = true
ksqldb-server                      | 	ksql.new.query.planner.enabled = false
ksqldb-server                      | 	ksql.output.topic.name.prefix = 
ksqldb-server                      | 	ksql.persistence.default.format.key = KAFKA
ksqldb-server                      | 	ksql.persistence.default.format.value = null
ksqldb-server                      | 	ksql.persistence.wrap.single.values = null
ksqldb-server                      | 	ksql.persistent.prefix = query_
ksqldb-server                      | 	ksql.properties.overrides.denylist = []
ksqldb-server                      | 	ksql.pull.queries.enable = true
ksqldb-server                      | 	ksql.query.cleanup.shutdown.timeout.ms = 30000
ksqldb-server                      | 	ksql.query.error.max.queue.size = 10
ksqldb-server                      | 	ksql.query.persistent.active.limit = 2147483647
ksqldb-server                      | 	ksql.query.persistent.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.query.pull.enable.standby.reads = false
ksqldb-server                      | 	ksql.query.pull.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.pull.limit.clause.enabled = true
ksqldb-server                      | 	ksql.query.pull.max.allowed.offset.lag = 9223372036854775807
ksqldb-server                      | 	ksql.query.pull.max.concurrent.requests = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.qps = 2147483647
ksqldb-server                      | 	ksql.query.pull.metrics.enabled = true
ksqldb-server                      | 	ksql.query.pull.range.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.router.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.pull.stream.enabled = true
ksqldb-server                      | 	ksql.query.pull.table.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.push.v2.alos.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.catchup.consumer.msg.window = 50
ksqldb-server                      | 	ksql.query.push.v2.continuation.tokens.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.latest.reset.age.ms = 30000
ksqldb-server                      | 	ksql.query.push.v2.max.catchup.consumers = 5
ksqldb-server                      | 	ksql.query.push.v2.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.push.v2.metrics.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.new.latest.delay.ms = 5000
ksqldb-server                      | 	ksql.query.push.v2.registry.installed = false
ksqldb-server                      | 	ksql.query.retry.backoff.initial.ms = 15000
ksqldb-server                      | 	ksql.query.retry.backoff.max.ms = 900000
ksqldb-server                      | 	ksql.query.status.running.threshold.seconds = 300
ksqldb-server                      | 	ksql.query.transient.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.queryanonymizer.cluster_namespace = null
ksqldb-server                      | 	ksql.queryanonymizer.logs_enabled = true
ksqldb-server                      | 	ksql.readonly.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.runtime.feature.shared.enabled = false
ksqldb-server                      | 	ksql.schema.registry.url = http://schema-registry:8081
ksqldb-server                      | 	ksql.security.extension.class = null
ksqldb-server                      | 	ksql.service.id = default_
ksqldb-server                      | 	ksql.shared.runtimes.count = 2
ksqldb-server                      | 	ksql.sink.window.change.log.additional.retention = 1000000
ksqldb-server                      | 	ksql.source.table.materialization.enabled = true
ksqldb-server                      | 	ksql.streams.shutdown.timeout.ms = 300000
ksqldb-server                      | 	ksql.suppress.buffer.size.bytes = -1
ksqldb-server                      | 	ksql.suppress.enabled = true
ksqldb-server                      | 	ksql.timestamp.throw.on.invalid = false
ksqldb-server                      | 	ksql.transient.prefix = transient_
ksqldb-server                      | 	ksql.transient.query.cleanup.service.enable = true
ksqldb-server                      | 	ksql.transient.query.cleanup.service.initial.delay.seconds = 600
ksqldb-server                      | 	ksql.transient.query.cleanup.service.period.seconds = 600
ksqldb-server                      | 	ksql.udf.collect.metrics = false
ksqldb-server                      | 	ksql.udf.enable.security.manager = true
ksqldb-server                      | 	ksql.udfs.enabled = true
ksqldb-server                      | 	ksql.variable.substitution.enable = true
ksqldb-server                      | 	ksql.websocket.connection.max.timeout.ms = 3600000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.util.KsqlConfig)
data-agrigator-taskmanager-1       | 2023-08-04 10:12:04,643 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:12:04,655 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
ksqldb-server                      | [2023-08-04 10:12:04,893] INFO Reporting number of leaked topics: 0 (io.confluent.ksql.internal.LeakedResourcesMetrics)
ksqldb-server                      | [2023-08-04 10:12:04,899] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
ksqldb-server                      | [2023-08-04 10:12:04,901] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:04,901] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:04,901] INFO Kafka startTimeMs: 1691143924901 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:04,907] INFO Reporting number of leaked state files: 0 (io.confluent.ksql.internal.LeakedResourcesMetrics)
ksqldb-server                      | [2023-08-04 10:12:04,907] INFO AdminClientConfig values: 
ksqldb-server                      | 	auto.include.jmx.reporter = true
ksqldb-server                      | 	bootstrap.servers = [broker:29092]
ksqldb-server                      | 	client.dns.lookup = use_all_dns_ips
ksqldb-server                      | 	client.id = 
ksqldb-server                      | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
ksqldb-server                      | 	confluent.proxy.protocol.client.address = null
ksqldb-server                      | 	confluent.proxy.protocol.client.port = null
ksqldb-server                      | 	confluent.proxy.protocol.client.version = NONE
ksqldb-server                      | 	confluent.use.controller.listener = false
ksqldb-server                      | 	connections.max.idle.ms = 300000
ksqldb-server                      | 	default.api.timeout.ms = 60000
ksqldb-server                      | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
ksqldb-server                      | 	metadata.max.age.ms = 300000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	metrics.num.samples = 2
ksqldb-server                      | 	metrics.recording.level = INFO
ksqldb-server                      | 	metrics.sample.window.ms = 30000
ksqldb-server                      | 	receive.buffer.bytes = 65536
ksqldb-server                      | 	reconnect.backoff.max.ms = 1000
ksqldb-server                      | 	reconnect.backoff.ms = 50
ksqldb-server                      | 	request.timeout.ms = 30000
ksqldb-server                      | 	retries = 2147483647
ksqldb-server                      | 	retry.backoff.ms = 100
ksqldb-server                      | 	sasl.client.callback.handler.class = null
ksqldb-server                      | 	sasl.jaas.config = null
ksqldb-server                      | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
ksqldb-server                      | 	sasl.kerberos.min.time.before.relogin = 60000
ksqldb-server                      | 	sasl.kerberos.service.name = null
ksqldb-server                      | 	sasl.kerberos.ticket.renew.jitter = 0.05
ksqldb-server                      | 	sasl.kerberos.ticket.renew.window.factor = 0.8
ksqldb-server                      | 	sasl.login.callback.handler.class = null
ksqldb-server                      | 	sasl.login.class = null
ksqldb-server                      | 	sasl.login.connect.timeout.ms = null
ksqldb-server                      | 	sasl.login.read.timeout.ms = null
ksqldb-server                      | 	sasl.login.refresh.buffer.seconds = 300
ksqldb-server                      | 	sasl.login.refresh.min.period.seconds = 60
ksqldb-server                      | 	sasl.login.refresh.window.factor = 0.8
ksqldb-server                      | 	sasl.login.refresh.window.jitter = 0.05
ksqldb-server                      | 	sasl.login.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.login.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.mechanism = GSSAPI
ksqldb-server                      | 	sasl.oauthbearer.clock.skew.seconds = 30
ksqldb-server                      | 	sasl.oauthbearer.expected.audience = null
ksqldb-server                      | 	sasl.oauthbearer.expected.issuer = null
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.url = null
ksqldb-server                      | 	sasl.oauthbearer.scope.claim.name = scope
ksqldb-server                      | 	sasl.oauthbearer.sub.claim.name = sub
ksqldb-server                      | 	sasl.oauthbearer.token.endpoint.url = null
ksqldb-server                      | 	security.protocol = PLAINTEXT
ksqldb-server                      | 	security.providers = null
ksqldb-server                      | 	send.buffer.bytes = 131072
ksqldb-server                      | 	socket.connection.setup.timeout.max.ms = 30000
ksqldb-server                      | 	socket.connection.setup.timeout.ms = 10000
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (org.apache.kafka.clients.admin.AdminClientConfig)
ksqldb-server                      | [2023-08-04 10:12:04,909] INFO Reporting number of leaked topics after cleanup: 0 (io.confluent.ksql.internal.LeakedResourcesMetrics)
ksqldb-server                      | [2023-08-04 10:12:04,913] INFO Reporting number of leaked state directories after cleanup: 0 (io.confluent.ksql.internal.LeakedResourcesMetrics)
ksqldb-server                      | [2023-08-04 10:12:04,982] WARN These configurations '[awt.toolkit, log4j.configuration, producer.interceptor.classes, metrics.context.resource.version, metrics.context.resource.kafka.cluster.id, metrics.context.resource.type, metrics.context.resource.commit.id, ksql.server.install.dir, com.sun.management.jmxremote.authenticate, ksql.schema.registry.url, ksql.connect.url, file.encoding, cache.max.bytes.buffering, host.name, config.dir, com.sun.management.jmxremote.ssl, metrics.context.resource.cluster.id, ksql.logging.processing.stream.auto.create, com.sun.management.jmxremote, jdk.debug, listeners, classpath, ksql.logging.processing.topic.auto.create, ksql.log.dir, jdk.vendor.version, ksql.logging.processing.topic.replication.factor, consumer.interceptor.classes]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
ksqldb-server                      | [2023-08-04 10:12:04,985] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:04,985] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:04,985] INFO Kafka startTimeMs: 1691143924985 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:04,999] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:05,000] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:05,000] INFO Kafka startTimeMs: 1691143924999 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:05,001] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:05,001] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:05,001] INFO Kafka startTimeMs: 1691143925001 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:05,081] INFO ConsumerConfig values: 
ksqldb-server                      | 	allow.auto.create.topics = true
ksqldb-server                      | 	auto.commit.interval.ms = 5000
ksqldb-server                      | 	auto.include.jmx.reporter = true
ksqldb-server                      | 	auto.offset.reset = none
ksqldb-server                      | 	bootstrap.servers = [broker:29092]
ksqldb-server                      | 	check.crcs = true
ksqldb-server                      | 	client.dns.lookup = use_all_dns_ips
ksqldb-server                      | 	client.id = consumer-null-1
ksqldb-server                      | 	client.rack = 
ksqldb-server                      | 	confluent.proxy.protocol.client.address = null
ksqldb-server                      | 	confluent.proxy.protocol.client.port = null
ksqldb-server                      | 	confluent.proxy.protocol.client.version = NONE
ksqldb-server                      | 	connections.max.idle.ms = 540000
ksqldb-server                      | 	default.api.timeout.ms = 60000
ksqldb-server                      | 	enable.auto.commit = true
ksqldb-server                      | 	exclude.internal.topics = true
ksqldb-server                      | 	fetch.max.bytes = 52428800
ksqldb-server                      | 	fetch.max.wait.ms = 500
ksqldb-server                      | 	fetch.min.bytes = 1
ksqldb-server                      | 	group.id = null
ksqldb-server                      | 	group.instance.id = null
ksqldb-server                      | 	heartbeat.interval.ms = 3000
ksqldb-server                      | 	interceptor.classes = []
ksqldb-server                      | 	internal.leave.group.on.close = true
ksqldb-server                      | 	internal.throw.on.fetch.stable.offset.unsupported = false
ksqldb-server                      | 	isolation.level = read_committed
ksqldb-server                      | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
ksqldb-server                      | 	max.partition.fetch.bytes = 1048576
ksqldb-server                      | 	max.poll.interval.ms = 300000
ksqldb-server                      | 	max.poll.records = 500
ksqldb-server                      | 	metadata.max.age.ms = 300000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	metrics.num.samples = 2
ksqldb-server                      | 	metrics.recording.level = INFO
ksqldb-server                      | 	metrics.sample.window.ms = 30000
ksqldb-server                      | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
ksqldb-server                      | 	receive.buffer.bytes = 65536
ksqldb-server                      | 	reconnect.backoff.max.ms = 1000
ksqldb-server                      | 	reconnect.backoff.ms = 50
ksqldb-server                      | 	request.timeout.ms = 30000
ksqldb-server                      | 	retry.backoff.ms = 100
ksqldb-server                      | 	sasl.client.callback.handler.class = null
ksqldb-server                      | 	sasl.jaas.config = null
ksqldb-server                      | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
ksqldb-server                      | 	sasl.kerberos.min.time.before.relogin = 60000
ksqldb-server                      | 	sasl.kerberos.service.name = null
ksqldb-server                      | 	sasl.kerberos.ticket.renew.jitter = 0.05
ksqldb-server                      | 	sasl.kerberos.ticket.renew.window.factor = 0.8
ksqldb-server                      | 	sasl.login.callback.handler.class = null
ksqldb-server                      | 	sasl.login.class = null
ksqldb-server                      | 	sasl.login.connect.timeout.ms = null
ksqldb-server                      | 	sasl.login.read.timeout.ms = null
ksqldb-server                      | 	sasl.login.refresh.buffer.seconds = 300
ksqldb-server                      | 	sasl.login.refresh.min.period.seconds = 60
ksqldb-server                      | 	sasl.login.refresh.window.factor = 0.8
ksqldb-server                      | 	sasl.login.refresh.window.jitter = 0.05
ksqldb-server                      | 	sasl.login.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.login.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.mechanism = GSSAPI
ksqldb-server                      | 	sasl.oauthbearer.clock.skew.seconds = 30
ksqldb-server                      | 	sasl.oauthbearer.expected.audience = null
ksqldb-server                      | 	sasl.oauthbearer.expected.issuer = null
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.url = null
ksqldb-server                      | 	sasl.oauthbearer.scope.claim.name = scope
ksqldb-server                      | 	sasl.oauthbearer.sub.claim.name = sub
ksqldb-server                      | 	sasl.oauthbearer.token.endpoint.url = null
ksqldb-server                      | 	security.protocol = PLAINTEXT
ksqldb-server                      | 	security.providers = null
ksqldb-server                      | 	send.buffer.bytes = 131072
ksqldb-server                      | 	session.timeout.ms = 45000
ksqldb-server                      | 	socket.connection.setup.timeout.max.ms = 30000
ksqldb-server                      | 	socket.connection.setup.timeout.ms = 10000
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
ksqldb-server                      |  (org.apache.kafka.clients.consumer.ConsumerConfig)
ksqldb-server                      | [2023-08-04 10:12:05,262] WARN These configurations '[awt.toolkit, log4j.configuration, producer.interceptor.classes, metrics.context.resource.version, metrics.context.resource.kafka.cluster.id, metrics.context.resource.type, metrics.context.resource.commit.id, ksql.server.install.dir, com.sun.management.jmxremote.authenticate, ksql.schema.registry.url, ksql.connect.url, file.encoding, cache.max.bytes.buffering, host.name, config.dir, com.sun.management.jmxremote.ssl, metrics.context.resource.cluster.id, ksql.logging.processing.stream.auto.create, com.sun.management.jmxremote, jdk.debug, listeners, classpath, ksql.logging.processing.topic.auto.create, ksql.log.dir, jdk.vendor.version, ksql.logging.processing.topic.replication.factor, consumer.interceptor.classes]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig)
ksqldb-server                      | [2023-08-04 10:12:05,262] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:05,262] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:05,262] INFO Kafka startTimeMs: 1691143925262 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:05,578] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:05,579] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:05,579] INFO Kafka startTimeMs: 1691143925577 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:05,582] INFO AdminClientConfig values: 
ksqldb-server                      | 	auto.include.jmx.reporter = true
ksqldb-server                      | 	bootstrap.servers = [broker:29092]
ksqldb-server                      | 	client.dns.lookup = use_all_dns_ips
ksqldb-server                      | 	client.id = 
ksqldb-server                      | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
ksqldb-server                      | 	confluent.proxy.protocol.client.address = null
ksqldb-server                      | 	confluent.proxy.protocol.client.port = null
ksqldb-server                      | 	confluent.proxy.protocol.client.version = NONE
ksqldb-server                      | 	confluent.use.controller.listener = false
ksqldb-server                      | 	connections.max.idle.ms = 300000
ksqldb-server                      | 	default.api.timeout.ms = 60000
ksqldb-server                      | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
ksqldb-server                      | 	metadata.max.age.ms = 300000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	metrics.num.samples = 2
ksqldb-server                      | 	metrics.recording.level = INFO
ksqldb-server                      | 	metrics.sample.window.ms = 30000
ksqldb-server                      | 	receive.buffer.bytes = 65536
ksqldb-server                      | 	reconnect.backoff.max.ms = 1000
ksqldb-server                      | 	reconnect.backoff.ms = 50
ksqldb-server                      | 	request.timeout.ms = 30000
ksqldb-server                      | 	retries = 2147483647
ksqldb-server                      | 	retry.backoff.ms = 100
ksqldb-server                      | 	sasl.client.callback.handler.class = null
ksqldb-server                      | 	sasl.jaas.config = null
ksqldb-server                      | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
ksqldb-server                      | 	sasl.kerberos.min.time.before.relogin = 60000
ksqldb-server                      | 	sasl.kerberos.service.name = null
ksqldb-server                      | 	sasl.kerberos.ticket.renew.jitter = 0.05
ksqldb-server                      | 	sasl.kerberos.ticket.renew.window.factor = 0.8
ksqldb-server                      | 	sasl.login.callback.handler.class = null
ksqldb-server                      | 	sasl.login.class = null
ksqldb-server                      | 	sasl.login.connect.timeout.ms = null
ksqldb-server                      | 	sasl.login.read.timeout.ms = null
ksqldb-server                      | 	sasl.login.refresh.buffer.seconds = 300
ksqldb-server                      | 	sasl.login.refresh.min.period.seconds = 60
ksqldb-server                      | 	sasl.login.refresh.window.factor = 0.8
ksqldb-server                      | 	sasl.login.refresh.window.jitter = 0.05
ksqldb-server                      | 	sasl.login.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.login.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.mechanism = GSSAPI
ksqldb-server                      | 	sasl.oauthbearer.clock.skew.seconds = 30
ksqldb-server                      | 	sasl.oauthbearer.expected.audience = null
ksqldb-server                      | 	sasl.oauthbearer.expected.issuer = null
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.url = null
ksqldb-server                      | 	sasl.oauthbearer.scope.claim.name = scope
ksqldb-server                      | 	sasl.oauthbearer.sub.claim.name = sub
ksqldb-server                      | 	sasl.oauthbearer.token.endpoint.url = null
ksqldb-server                      | 	security.protocol = PLAINTEXT
ksqldb-server                      | 	security.providers = null
ksqldb-server                      | 	send.buffer.bytes = 131072
ksqldb-server                      | 	socket.connection.setup.timeout.max.ms = 30000
ksqldb-server                      | 	socket.connection.setup.timeout.ms = 10000
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (org.apache.kafka.clients.admin.AdminClientConfig)
ksqldb-server                      | [2023-08-04 10:12:05,636] WARN These configurations '[metrics.context.resource.cluster.id, metrics.context.resource.version, metrics.context.resource.kafka.cluster.id, metrics.context.resource.type, metrics.context.resource.commit.id]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
ksqldb-server                      | [2023-08-04 10:12:05,638] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:05,639] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:05,640] INFO Kafka startTimeMs: 1691143925637 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:06,508] INFO Starting server (io.confluent.ksql.rest.server.KsqlServerMain)
ksqldb-server                      | [2023-08-04 10:12:06,803] INFO KsqlRestConfig values: 
ksqldb-server                      | 	access.control.allow.headers = []
ksqldb-server                      | 	access.control.allow.methods = []
ksqldb-server                      | 	access.control.allow.origin = 
ksqldb-server                      | 	authentication.method = NONE
ksqldb-server                      | 	authentication.realm = 
ksqldb-server                      | 	authentication.roles = [*]
ksqldb-server                      | 	authentication.skip.paths = []
ksqldb-server                      | 	ksql.advertised.listener = null
ksqldb-server                      | 	ksql.authentication.plugin.class = null
ksqldb-server                      | 	ksql.endpoint.logging.ignored.paths.regex = 
ksqldb-server                      | 	ksql.endpoint.logging.log.queries = false
ksqldb-server                      | 	ksql.healthcheck.interval.ms = 5000
ksqldb-server                      | 	ksql.heartbeat.check.interval.ms = 200
ksqldb-server                      | 	ksql.heartbeat.discover.interval.ms = 2000
ksqldb-server                      | 	ksql.heartbeat.enable = false
ksqldb-server                      | 	ksql.heartbeat.missed.threshold.ms = 3
ksqldb-server                      | 	ksql.heartbeat.send.interval.ms = 100
ksqldb-server                      | 	ksql.heartbeat.thread.pool.size = 3
ksqldb-server                      | 	ksql.heartbeat.window.ms = 2000
ksqldb-server                      | 	ksql.idle.connection.timeout.seconds = 86400
ksqldb-server                      | 	ksql.internal.http2.max.pool.size = 3000
ksqldb-server                      | 	ksql.internal.listener = null
ksqldb-server                      | 	ksql.internal.ssl.client.authentication = NONE
ksqldb-server                      | 	ksql.lag.reporting.enable = false
ksqldb-server                      | 	ksql.lag.reporting.send.interval.ms = 5000
ksqldb-server                      | 	ksql.local.commands.location = 
ksqldb-server                      | 	ksql.logging.server.rate.limited.request.paths = 
ksqldb-server                      | 	ksql.logging.server.rate.limited.response.codes = 
ksqldb-server                      | 	ksql.max.push.queries = 100
ksqldb-server                      | 	ksql.server.command.blocked.threshold.error.ms = 15000
ksqldb-server                      | 	ksql.server.command.response.timeout.ms = 5000
ksqldb-server                      | 	ksql.server.command.topic.migration.enabled = NONE
ksqldb-server                      | 	ksql.server.command.topic.rate.limit = 1.7976931348623157E308
ksqldb-server                      | 	ksql.server.error.messages = class io.confluent.ksql.rest.DefaultErrorMessages
ksqldb-server                      | 	ksql.server.exception.uncaught.handler.enable = false
ksqldb-server                      | 	ksql.server.install.dir = /usr
ksqldb-server                      | 	ksql.server.precondition.max.backoff.ms = 5000
ksqldb-server                      | 	ksql.server.preconditions = []
ksqldb-server                      | 	ksql.server.sni.check.enable = false
ksqldb-server                      | 	ksql.server.websockets.num.threads = 5
ksqldb-server                      | 	ksql.ssl.keystore.alias.external = 
ksqldb-server                      | 	ksql.ssl.keystore.alias.internal = 
ksqldb-server                      | 	ksql.verticle.instances = 8
ksqldb-server                      | 	ksql.worker.pool.size = 100
ksqldb-server                      | 	listeners = [http://0.0.0.0:8088]
ksqldb-server                      | 	query.stream.disconnect.check = 1000
ksqldb-server                      | 	ssl.cipher.suites = []
ksqldb-server                      | 	ssl.client.auth = false
ksqldb-server                      | 	ssl.client.authentication = NONE
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.key.password = [hidden]
ksqldb-server                      | 	ssl.keystore.location = 
ksqldb-server                      | 	ssl.keystore.password = [hidden]
ksqldb-server                      | 	ssl.keystore.reload = false
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.keystore.watch.location = 
ksqldb-server                      | 	ssl.truststore.location = 
ksqldb-server                      | 	ssl.truststore.password = [hidden]
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.rest.server.KsqlRestConfig)
ksqldb-server                      | [2023-08-04 10:12:06,869] INFO ProcessingLogConfig values: 
ksqldb-server                      | 	ksql.logging.processing.rows.include = false
ksqldb-server                      | 	ksql.logging.processing.stream.auto.create = false
ksqldb-server                      | 	ksql.logging.processing.stream.name = KSQL_PROCESSING_LOG
ksqldb-server                      | 	ksql.logging.processing.topic.auto.create = false
ksqldb-server                      | 	ksql.logging.processing.topic.name = 
ksqldb-server                      | 	ksql.logging.processing.topic.partitions = 1
ksqldb-server                      | 	ksql.logging.processing.topic.replication.factor = 1
ksqldb-server                      |  (io.confluent.ksql.logging.processing.ProcessingLogConfig)
connect                            | [2023-08-04 10:12:08,147] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-control-center/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,148] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,149] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,150] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,151] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,152] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,153] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,154] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,155] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,155] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,156] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,157] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,158] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,159] INFO Added plugin 'io.confluent.connect.json.JsonSchemaConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,160] INFO Added plugin 'io.confluent.connect.protobuf.ProtobufConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,161] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,169] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,171] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,172] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,173] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,175] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,176] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,177] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,178] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,186] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,186] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,186] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,186] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,187] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,187] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,187] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,187] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,188] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,188] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,188] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,188] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,189] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,189] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,189] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,189] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,190] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,193] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,194] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,194] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,194] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,194] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,195] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,195] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,200] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,200] INFO Added plugin 'io.confluent.kafka.secretregistry.client.config.provider.SecretConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,201] INFO Added plugin 'io.confluent.connect.security.ConnectSecurityExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:12:08,265] INFO Loading plugin from: /usr/share/java/schema-registry (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
ksqldb-server                      | [2023-08-04 10:12:08,358] INFO API server started (io.confluent.ksql.api.server.Server)
ksqldb-server                      | [2023-08-04 10:12:08,370] WARN first 'listeners' config uses wildcard address: http://ksqldb-server:8088. Intra-node communication will only work between nodes running on the same machine. (io.confluent.ksql.rest.server.KsqlRestConfig)
ksqldb-server                      | [2023-08-04 10:12:08,372] INFO Using first 'listeners' config for intra-node communication: http://ksqldb-server:8088 (io.confluent.ksql.rest.server.KsqlRestConfig)
ksqldb-server                      | [2023-08-04 10:12:08,378] INFO KsqlConfig values: 
ksqldb-server                      | 	ksql.access.validator.enable = auto
ksqldb-server                      | 	ksql.assert.schema.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.assert.topic.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.authorization.cache.expiry.time.secs = 30
ksqldb-server                      | 	ksql.authorization.cache.max.entries = 10000
ksqldb-server                      | 	ksql.cast.strings.preserve.nulls = true
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.file = 
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.reload = false
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.source = NONE
ksqldb-server                      | 	ksql.connect.request.headers.plugin = null
ksqldb-server                      | 	ksql.connect.request.timeout.ms = 5000
ksqldb-server                      | 	ksql.connect.url = http://connect:8083
ksqldb-server                      | 	ksql.connect.worker.config = 
ksqldb-server                      | 	ksql.create.or.replace.enabled = true
ksqldb-server                      | 	ksql.endpoint.migrate.query = true
ksqldb-server                      | 	ksql.error.classifier.regex = 
ksqldb-server                      | 	ksql.extension.dir = ext
ksqldb-server                      | 	ksql.headers.columns.enabled = true
ksqldb-server                      | 	ksql.hidden.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.insert.into.values.enabled = true
ksqldb-server                      | 	ksql.internal.topic.min.insync.replicas = 1
ksqldb-server                      | 	ksql.internal.topic.replicas = 1
ksqldb-server                      | 	ksql.json_sr.converter.deserializer.enabled = true
ksqldb-server                      | 	ksql.lambdas.enabled = true
ksqldb-server                      | 	ksql.metastore.backup.location = 
ksqldb-server                      | 	ksql.metrics.extension = null
ksqldb-server                      | 	ksql.metrics.tags.custom = 
ksqldb-server                      | 	ksql.nested.error.set.null = true
ksqldb-server                      | 	ksql.new.query.planner.enabled = false
ksqldb-server                      | 	ksql.output.topic.name.prefix = 
ksqldb-server                      | 	ksql.persistence.default.format.key = KAFKA
ksqldb-server                      | 	ksql.persistence.default.format.value = null
ksqldb-server                      | 	ksql.persistence.wrap.single.values = null
ksqldb-server                      | 	ksql.persistent.prefix = query_
ksqldb-server                      | 	ksql.properties.overrides.denylist = []
ksqldb-server                      | 	ksql.pull.queries.enable = true
ksqldb-server                      | 	ksql.query.cleanup.shutdown.timeout.ms = 30000
ksqldb-server                      | 	ksql.query.error.max.queue.size = 10
ksqldb-server                      | 	ksql.query.persistent.active.limit = 2147483647
ksqldb-server                      | 	ksql.query.persistent.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.query.pull.enable.standby.reads = false
ksqldb-server                      | 	ksql.query.pull.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.pull.limit.clause.enabled = true
ksqldb-server                      | 	ksql.query.pull.max.allowed.offset.lag = 9223372036854775807
ksqldb-server                      | 	ksql.query.pull.max.concurrent.requests = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.qps = 2147483647
ksqldb-server                      | 	ksql.query.pull.metrics.enabled = true
ksqldb-server                      | 	ksql.query.pull.range.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.router.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.pull.stream.enabled = true
ksqldb-server                      | 	ksql.query.pull.table.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.push.v2.alos.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.catchup.consumer.msg.window = 50
ksqldb-server                      | 	ksql.query.push.v2.continuation.tokens.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.latest.reset.age.ms = 30000
ksqldb-server                      | 	ksql.query.push.v2.max.catchup.consumers = 5
ksqldb-server                      | 	ksql.query.push.v2.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.push.v2.metrics.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.new.latest.delay.ms = 5000
ksqldb-server                      | 	ksql.query.push.v2.registry.installed = false
ksqldb-server                      | 	ksql.query.retry.backoff.initial.ms = 15000
ksqldb-server                      | 	ksql.query.retry.backoff.max.ms = 900000
ksqldb-server                      | 	ksql.query.status.running.threshold.seconds = 300
ksqldb-server                      | 	ksql.query.transient.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.queryanonymizer.cluster_namespace = null
ksqldb-server                      | 	ksql.queryanonymizer.logs_enabled = true
ksqldb-server                      | 	ksql.readonly.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.runtime.feature.shared.enabled = false
ksqldb-server                      | 	ksql.schema.registry.url = http://schema-registry:8081
ksqldb-server                      | 	ksql.security.extension.class = null
ksqldb-server                      | 	ksql.service.id = default_
ksqldb-server                      | 	ksql.shared.runtimes.count = 2
ksqldb-server                      | 	ksql.sink.window.change.log.additional.retention = 1000000
ksqldb-server                      | 	ksql.source.table.materialization.enabled = true
ksqldb-server                      | 	ksql.streams.shutdown.timeout.ms = 300000
ksqldb-server                      | 	ksql.suppress.buffer.size.bytes = -1
ksqldb-server                      | 	ksql.suppress.enabled = true
ksqldb-server                      | 	ksql.timestamp.throw.on.invalid = false
ksqldb-server                      | 	ksql.transient.prefix = transient_
ksqldb-server                      | 	ksql.transient.query.cleanup.service.enable = true
ksqldb-server                      | 	ksql.transient.query.cleanup.service.initial.delay.seconds = 600
ksqldb-server                      | 	ksql.transient.query.cleanup.service.period.seconds = 600
ksqldb-server                      | 	ksql.udf.collect.metrics = false
ksqldb-server                      | 	ksql.udf.enable.security.manager = true
ksqldb-server                      | 	ksql.udfs.enabled = true
ksqldb-server                      | 	ksql.variable.substitution.enable = true
ksqldb-server                      | 	ksql.websocket.connection.max.timeout.ms = 3600000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.util.KsqlConfig)
ksqldb-server                      | [2023-08-04 10:12:08,732] INFO ProcessingLogConfig values: 
ksqldb-server                      | 	ksql.logging.processing.rows.include = false
ksqldb-server                      | 	ksql.logging.processing.stream.auto.create = false
ksqldb-server                      | 	ksql.logging.processing.stream.name = KSQL_PROCESSING_LOG
ksqldb-server                      | 	ksql.logging.processing.topic.auto.create = false
ksqldb-server                      | 	ksql.logging.processing.topic.name = 
ksqldb-server                      | 	ksql.logging.processing.topic.partitions = 1
ksqldb-server                      | 	ksql.logging.processing.topic.replication.factor = 1
ksqldb-server                      |  (io.confluent.ksql.logging.processing.ProcessingLogConfig)
ksqldb-server                      | [2023-08-04 10:12:08,736] INFO KsqlRestConfig values: 
ksqldb-server                      | 	access.control.allow.headers = []
ksqldb-server                      | 	access.control.allow.methods = []
ksqldb-server                      | 	access.control.allow.origin = 
ksqldb-server                      | 	authentication.method = NONE
ksqldb-server                      | 	authentication.realm = 
ksqldb-server                      | 	authentication.roles = [*]
ksqldb-server                      | 	authentication.skip.paths = []
ksqldb-server                      | 	ksql.advertised.listener = null
ksqldb-server                      | 	ksql.authentication.plugin.class = null
ksqldb-server                      | 	ksql.endpoint.logging.ignored.paths.regex = 
ksqldb-server                      | 	ksql.endpoint.logging.log.queries = false
ksqldb-server                      | 	ksql.healthcheck.interval.ms = 5000
ksqldb-server                      | 	ksql.heartbeat.check.interval.ms = 200
ksqldb-server                      | 	ksql.heartbeat.discover.interval.ms = 2000
ksqldb-server                      | 	ksql.heartbeat.enable = false
ksqldb-server                      | 	ksql.heartbeat.missed.threshold.ms = 3
ksqldb-server                      | 	ksql.heartbeat.send.interval.ms = 100
ksqldb-server                      | 	ksql.heartbeat.thread.pool.size = 3
ksqldb-server                      | 	ksql.heartbeat.window.ms = 2000
ksqldb-server                      | 	ksql.idle.connection.timeout.seconds = 86400
ksqldb-server                      | 	ksql.internal.http2.max.pool.size = 3000
ksqldb-server                      | 	ksql.internal.listener = null
ksqldb-server                      | 	ksql.internal.ssl.client.authentication = NONE
ksqldb-server                      | 	ksql.lag.reporting.enable = false
ksqldb-server                      | 	ksql.lag.reporting.send.interval.ms = 5000
ksqldb-server                      | 	ksql.local.commands.location = 
ksqldb-server                      | 	ksql.logging.server.rate.limited.request.paths = 
ksqldb-server                      | 	ksql.logging.server.rate.limited.response.codes = 
ksqldb-server                      | 	ksql.max.push.queries = 100
ksqldb-server                      | 	ksql.server.command.blocked.threshold.error.ms = 15000
ksqldb-server                      | 	ksql.server.command.response.timeout.ms = 5000
ksqldb-server                      | 	ksql.server.command.topic.migration.enabled = NONE
ksqldb-server                      | 	ksql.server.command.topic.rate.limit = 1.7976931348623157E308
ksqldb-server                      | 	ksql.server.error.messages = class io.confluent.ksql.rest.DefaultErrorMessages
ksqldb-server                      | 	ksql.server.exception.uncaught.handler.enable = false
ksqldb-server                      | 	ksql.server.install.dir = /usr
ksqldb-server                      | 	ksql.server.precondition.max.backoff.ms = 5000
ksqldb-server                      | 	ksql.server.preconditions = []
ksqldb-server                      | 	ksql.server.sni.check.enable = false
ksqldb-server                      | 	ksql.server.websockets.num.threads = 5
ksqldb-server                      | 	ksql.ssl.keystore.alias.external = 
ksqldb-server                      | 	ksql.ssl.keystore.alias.internal = 
ksqldb-server                      | 	ksql.verticle.instances = 8
ksqldb-server                      | 	ksql.worker.pool.size = 100
ksqldb-server                      | 	listeners = [http://0.0.0.0:8088]
ksqldb-server                      | 	query.stream.disconnect.check = 1000
ksqldb-server                      | 	ssl.cipher.suites = []
ksqldb-server                      | 	ssl.client.auth = false
ksqldb-server                      | 	ssl.client.authentication = NONE
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.key.password = [hidden]
ksqldb-server                      | 	ssl.keystore.location = 
ksqldb-server                      | 	ssl.keystore.password = [hidden]
ksqldb-server                      | 	ssl.keystore.reload = false
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.keystore.watch.location = 
ksqldb-server                      | 	ssl.truststore.location = 
ksqldb-server                      | 	ssl.truststore.password = [hidden]
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.rest.server.KsqlRestConfig)
ksqldb-server                      | [2023-08-04 10:12:08,889] WARN Creating topic _confluent-ksql-default__command_topic with replication factor of 1 which is less than 2. This is not advisable in a production environment.  (io.confluent.ksql.rest.util.KsqlInternalTopicUtils)
ksqldb-server                      | [2023-08-04 10:12:08,902] INFO Creating topic '_confluent-ksql-default__command_topic'  (io.confluent.ksql.services.KafkaTopicClient)
broker                             | [2023-08-04 10:12:08,929] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-ksql-default__command_topic', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='-1'), CreateableTopicConfig(name='unclean.leader.election.enable', value='false')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:08,930] INFO [Controller 1] Created topic _confluent-ksql-default__command_topic with topic ID qszbKrAmRm-xGzAdJbM2PQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:08,930] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-ksql-default__command_topic'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:12:08,930] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-ksql-default__command_topic'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:12:08,931] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-ksql-default__command_topic'): set configuration retention.ms to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:12:08,931] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-ksql-default__command_topic'): set configuration unclean.leader.election.enable to false (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:12:08,937] INFO [Controller 1] Created partition _confluent-ksql-default__command_topic-0 with topic ID qszbKrAmRm-xGzAdJbM2PQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:08,975] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:12:08,976] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-ksql-default__command_topic-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:12:08,976] INFO [Broker id=1] Creating new partition _confluent-ksql-default__command_topic-0 with topic id qszbKrAmRm-xGzAdJbM2PQ. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:12:08,992] INFO [Consumer clientId=consumer-null-1, groupId=null] Assigned to partition(s): _confluent-ksql-default__command_topic-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
broker                             | [2023-08-04 10:12:08,997] INFO [LogLoader partition=_confluent-ksql-default__command_topic-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:09,002] INFO Created log for partition _confluent-ksql-default__command_topic-0 in /tmp/kraft-combined-logs/_confluent-ksql-default__command_topic-0 with properties {cleanup.policy=delete, min.insync.replicas=1, retention.ms=-1, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:09,017] INFO [Partition _confluent-ksql-default__command_topic-0 broker=1] No checkpointed highwatermark is found for partition _confluent-ksql-default__command_topic-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:09,018] INFO [Partition _confluent-ksql-default__command_topic-0 broker=1] Log loaded for partition _confluent-ksql-default__command_topic-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:09,021] INFO [Broker id=1] Leader _confluent-ksql-default__command_topic-0 with topic id Some(qszbKrAmRm-xGzAdJbM2PQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:12:09,023] INFO Creating topic 'default_ksql_processing_log'  (io.confluent.ksql.services.KafkaTopicClient)
broker                             | [2023-08-04 10:12:09,031] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='default_ksql_processing_log', numPartitions=1, replicationFactor=1, assignments=[], configs=[]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:09,032] INFO [Controller 1] Created topic default_ksql_processing_log with topic ID NaFdOZr7T36qpM3L26ausQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:09,032] INFO [Controller 1] Created partition default_ksql_processing_log-0 with topic ID NaFdOZr7T36qpM3L26ausQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:09,036] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-ksql-default__command_topic with new configuration : cleanup.policy -> delete,min.insync.replicas -> 1,retention.ms -> -1,unclean.leader.election.enable -> false (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:12:09,053] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:12:09,055] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(default_ksql_processing_log-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:12:09,057] INFO [Broker id=1] Creating new partition default_ksql_processing_log-0 with topic id NaFdOZr7T36qpM3L26ausQ. (state.change.logger)
broker                             | [2023-08-04 10:12:09,083] INFO [LogLoader partition=default_ksql_processing_log-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:09,089] INFO Created log for partition default_ksql_processing_log-0 in /tmp/kraft-combined-logs/default_ksql_processing_log-0 with properties {} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:09,093] INFO [Partition default_ksql_processing_log-0 broker=1] No checkpointed highwatermark is found for partition default_ksql_processing_log-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:09,097] INFO [Partition default_ksql_processing_log-0 broker=1] Log loaded for partition default_ksql_processing_log-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:09,098] INFO [Broker id=1] Leader default_ksql_processing_log-0 with topic id Some(NaFdOZr7T36qpM3L26ausQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:12:09,154] INFO [Consumer clientId=consumer-null-1, groupId=null] Resetting the last seen epoch of partition _confluent-ksql-default__command_topic-0 to 0 since the associated topicId changed from null to qszbKrAmRm-xGzAdJbM2PQ (org.apache.kafka.clients.Metadata)
ksqldb-server                      | [2023-08-04 10:12:09,164] INFO [Consumer clientId=consumer-null-1, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
ksqldb-server                      | [2023-08-04 10:12:09,256] INFO [Consumer clientId=consumer-null-1, groupId=null] Seeking to earliest offset of partition _confluent-ksql-default__command_topic-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
ksqldb-server                      | [2023-08-04 10:12:09,272] INFO Reading prior command records up to offset 0 (io.confluent.ksql.rest.server.CommandTopic)
ksqldb-server                      | [2023-08-04 10:12:09,336] INFO Restoring previous state from 0 commands. (io.confluent.ksql.rest.server.computation.CommandRunner)
ksqldb-server                      | [2023-08-04 10:12:09,348] INFO Restarting 0 queries. (io.confluent.ksql.rest.server.computation.CommandRunner)
ksqldb-server                      | [2023-08-04 10:12:09,357] INFO Restore complete (io.confluent.ksql.rest.server.computation.CommandRunner)
ksqldb-server                      | [2023-08-04 10:12:09,476] INFO Received: KsqlRequest{configOverrides={}, requestProperties={}, commandSequenceNumber=Optional.empty} (7e9e9830-df93-3f03-9c27-8fdb9ccea366): list STREAMS; (io.confluent.ksql.logging.query.QueryLogger)
ksqldb-server                      | [2023-08-04 10:12:09,907] INFO There are no queries assigned to Gen 2 runtimes yet. (io.confluent.ksql.engine.RuntimeAssignor)
ksqldb-server                      | [2023-08-04 10:12:09,908] INFO ProcessingLogConfig values: 
ksqldb-server                      | 	ksql.logging.processing.rows.include = false
ksqldb-server                      | 	ksql.logging.processing.stream.auto.create = false
ksqldb-server                      | 	ksql.logging.processing.stream.name = KSQL_PROCESSING_LOG
ksqldb-server                      | 	ksql.logging.processing.topic.auto.create = false
ksqldb-server                      | 	ksql.logging.processing.topic.name = 
ksqldb-server                      | 	ksql.logging.processing.topic.partitions = 1
ksqldb-server                      | 	ksql.logging.processing.topic.replication.factor = 1
ksqldb-server                      |  (io.confluent.ksql.logging.processing.ProcessingLogConfig)
ksqldb-server                      | [2023-08-04 10:12:10,464] INFO Query created (7e9e9830-df93-3f03-9c27-8fdb9ccea366): list STREAMS; (io.confluent.ksql.logging.query.QueryLogger)
ksqldb-server                      | [2023-08-04 10:12:10,515] INFO Processed successfully: KsqlRequest{configOverrides={}, requestProperties={}, commandSequenceNumber=Optional.empty} (7e9e9830-df93-3f03-9c27-8fdb9ccea366): list STREAMS; (io.confluent.ksql.logging.query.QueryLogger)
ksqldb-server                      | [2023-08-04 10:12:10,637] INFO Received: KsqlRequest{configOverrides={}, requestProperties={}, commandSequenceNumber=Optional.empty} (fb4c327f-85bb-3fe8-b8e2-45b9d778598c): CREATE STREAM stream1 (column1 VARCHAR, column2 VARCHAR, column3 BIGINT, column4 STRUCT<INT, STRUCT<VARCHAR, VARCHAR, VARCHAR, ARRAY<VARCHAR>, VARCHAR>, STRUCT<VARCHAR, VARCHAR, ARRAY<VARCHAR>>, STRUCT<VARCHAR>, STRUCT<VARCHAR, VARCHAR, VARCHAR, ARRAY<VARCHAR>, VARCHAR>, STRUCT<VARCHAR, VARCHAR, ARRAY<VARCHAR>>>) WITH (KAFKA_TOPIC='[string]', VALUE_FORMAT='[string]', KEY_FORMAT='[string]'); (io.confluent.ksql.logging.query.QueryLogger)
ksqldb-server                      | [2023-08-04 10:12:10,664] INFO There are no queries assigned to Gen 2 runtimes yet. (io.confluent.ksql.engine.RuntimeAssignor)
ksqldb-server                      | [2023-08-04 10:12:10,664] INFO ProcessingLogConfig values: 
ksqldb-server                      | 	ksql.logging.processing.rows.include = false
ksqldb-server                      | 	ksql.logging.processing.stream.auto.create = false
ksqldb-server                      | 	ksql.logging.processing.stream.name = KSQL_PROCESSING_LOG
ksqldb-server                      | 	ksql.logging.processing.topic.auto.create = false
ksqldb-server                      | 	ksql.logging.processing.topic.name = 
ksqldb-server                      | 	ksql.logging.processing.topic.partitions = 1
ksqldb-server                      | 	ksql.logging.processing.topic.replication.factor = 1
ksqldb-server                      |  (io.confluent.ksql.logging.processing.ProcessingLogConfig)
ksqldb-server                      | [2023-08-04 10:12:11,199] INFO KsqlConfig values: 
ksqldb-server                      | 	ksql.access.validator.enable = auto
ksqldb-server                      | 	ksql.assert.schema.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.assert.topic.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.authorization.cache.expiry.time.secs = 30
ksqldb-server                      | 	ksql.authorization.cache.max.entries = 10000
ksqldb-server                      | 	ksql.cast.strings.preserve.nulls = true
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.file = 
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.reload = false
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.source = NONE
ksqldb-server                      | 	ksql.connect.request.headers.plugin = null
ksqldb-server                      | 	ksql.connect.request.timeout.ms = 5000
ksqldb-server                      | 	ksql.connect.url = http://connect:8083
ksqldb-server                      | 	ksql.connect.worker.config = 
ksqldb-server                      | 	ksql.create.or.replace.enabled = true
ksqldb-server                      | 	ksql.endpoint.migrate.query = true
ksqldb-server                      | 	ksql.error.classifier.regex = 
ksqldb-server                      | 	ksql.extension.dir = ext
ksqldb-server                      | 	ksql.headers.columns.enabled = true
ksqldb-server                      | 	ksql.hidden.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.insert.into.values.enabled = true
ksqldb-server                      | 	ksql.internal.topic.min.insync.replicas = 1
ksqldb-server                      | 	ksql.internal.topic.replicas = 1
ksqldb-server                      | 	ksql.json_sr.converter.deserializer.enabled = true
ksqldb-server                      | 	ksql.lambdas.enabled = true
ksqldb-server                      | 	ksql.metastore.backup.location = 
ksqldb-server                      | 	ksql.metrics.extension = null
ksqldb-server                      | 	ksql.metrics.tags.custom = 
ksqldb-server                      | 	ksql.nested.error.set.null = true
ksqldb-server                      | 	ksql.new.query.planner.enabled = false
ksqldb-server                      | 	ksql.output.topic.name.prefix = 
ksqldb-server                      | 	ksql.persistence.default.format.key = KAFKA
ksqldb-server                      | 	ksql.persistence.default.format.value = null
ksqldb-server                      | 	ksql.persistence.wrap.single.values = null
ksqldb-server                      | 	ksql.persistent.prefix = query_
ksqldb-server                      | 	ksql.properties.overrides.denylist = []
ksqldb-server                      | 	ksql.pull.queries.enable = true
ksqldb-server                      | 	ksql.query.cleanup.shutdown.timeout.ms = 30000
ksqldb-server                      | 	ksql.query.error.max.queue.size = 10
ksqldb-server                      | 	ksql.query.persistent.active.limit = 2147483647
ksqldb-server                      | 	ksql.query.persistent.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.query.pull.enable.standby.reads = false
ksqldb-server                      | 	ksql.query.pull.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.pull.limit.clause.enabled = true
ksqldb-server                      | 	ksql.query.pull.max.allowed.offset.lag = 9223372036854775807
ksqldb-server                      | 	ksql.query.pull.max.concurrent.requests = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.qps = 2147483647
ksqldb-server                      | 	ksql.query.pull.metrics.enabled = true
ksqldb-server                      | 	ksql.query.pull.range.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.router.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.pull.stream.enabled = true
ksqldb-server                      | 	ksql.query.pull.table.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.push.v2.alos.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.catchup.consumer.msg.window = 50
ksqldb-server                      | 	ksql.query.push.v2.continuation.tokens.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.latest.reset.age.ms = 30000
ksqldb-server                      | 	ksql.query.push.v2.max.catchup.consumers = 5
ksqldb-server                      | 	ksql.query.push.v2.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.push.v2.metrics.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.new.latest.delay.ms = 5000
ksqldb-server                      | 	ksql.query.push.v2.registry.installed = false
ksqldb-server                      | 	ksql.query.retry.backoff.initial.ms = 15000
ksqldb-server                      | 	ksql.query.retry.backoff.max.ms = 900000
ksqldb-server                      | 	ksql.query.status.running.threshold.seconds = 300
ksqldb-server                      | 	ksql.query.transient.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.queryanonymizer.cluster_namespace = null
ksqldb-server                      | 	ksql.queryanonymizer.logs_enabled = true
ksqldb-server                      | 	ksql.readonly.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.runtime.feature.shared.enabled = false
ksqldb-server                      | 	ksql.schema.registry.url = http://schema-registry:8081
ksqldb-server                      | 	ksql.security.extension.class = null
ksqldb-server                      | 	ksql.service.id = default_
ksqldb-server                      | 	ksql.shared.runtimes.count = 2
ksqldb-server                      | 	ksql.sink.window.change.log.additional.retention = 1000000
ksqldb-server                      | 	ksql.source.table.materialization.enabled = true
ksqldb-server                      | 	ksql.streams.shutdown.timeout.ms = 300000
ksqldb-server                      | 	ksql.suppress.buffer.size.bytes = -1
ksqldb-server                      | 	ksql.suppress.enabled = true
ksqldb-server                      | 	ksql.timestamp.throw.on.invalid = false
ksqldb-server                      | 	ksql.transient.prefix = transient_
ksqldb-server                      | 	ksql.transient.query.cleanup.service.enable = true
ksqldb-server                      | 	ksql.transient.query.cleanup.service.initial.delay.seconds = 600
ksqldb-server                      | 	ksql.transient.query.cleanup.service.period.seconds = 600
ksqldb-server                      | 	ksql.udf.collect.metrics = false
ksqldb-server                      | 	ksql.udf.enable.security.manager = true
ksqldb-server                      | 	ksql.udfs.enabled = true
ksqldb-server                      | 	ksql.variable.substitution.enable = true
ksqldb-server                      | 	ksql.websocket.connection.max.timeout.ms = 3600000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.util.KsqlConfig)
ksqldb-server                      | [2023-08-04 10:12:11,267] INFO ProcessingLogConfig values: 
ksqldb-server                      | 	ksql.logging.processing.rows.include = false
ksqldb-server                      | 	ksql.logging.processing.stream.auto.create = false
ksqldb-server                      | 	ksql.logging.processing.stream.name = KSQL_PROCESSING_LOG
ksqldb-server                      | 	ksql.logging.processing.topic.auto.create = false
ksqldb-server                      | 	ksql.logging.processing.topic.name = 
ksqldb-server                      | 	ksql.logging.processing.topic.partitions = 1
ksqldb-server                      | 	ksql.logging.processing.topic.replication.factor = 1
ksqldb-server                      |  (io.confluent.ksql.logging.processing.ProcessingLogConfig)
ksqldb-server                      | [2023-08-04 10:12:11,425] INFO JsonConverterConfig values: 
ksqldb-server                      | 	converter.type = value
ksqldb-server                      | 	decimal.format = NUMERIC
ksqldb-server                      | 	schemas.cache.size = 1000
ksqldb-server                      | 	schemas.enable = false
ksqldb-server                      |  (org.apache.kafka.connect.json.JsonConverterConfig)
ksqldb-server                      | [2023-08-04 10:12:11,526] INFO Source KSQL_PROCESSING_LOG created on the metastore (io.confluent.ksql.metastore.MetaStoreImpl)
control-center                     | [2023-08-04 10:12:13,076] INFO describing topics=[_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, _confluent-metrics, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-cluster-rekey, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, _confluent-monitoring, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition] (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,155] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,157] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,158] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,159] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,160] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,163] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,179] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,183] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,184] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,184] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,184] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,185] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,185] INFO create=success topic=TopicInfo{name=_confluent-metrics, partitions=12, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,185] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,186] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-cluster-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,186] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,186] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,186] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,186] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,186] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,187] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,198] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,199] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,204] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,205] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,206] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,207] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,211] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,212] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,213] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,215] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,216] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,217] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,218] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,221] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,222] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,225] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,226] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,227] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,228] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,236] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,237] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,238] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,239] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,240] INFO create=success topic=TopicInfo{name=_confluent-monitoring, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,241] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,242] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,243] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,244] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,254] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = true
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = latest
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = will-delete-this
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:13,351] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:13,365] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:13,368] INFO Kafka startTimeMs: 1691143933351 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:13,369] INFO Setting offsets for topic=_confluent-monitoring (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,408] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
ksqldb-server                      | [2023-08-04 10:12:13,468] INFO Query created (fb4c327f-85bb-3fe8-b8e2-45b9d778598c): CREATE STREAM stream1 (column1 VARCHAR, column2 VARCHAR, column3 BIGINT, column4 STRUCT<INT, STRUCT<VARCHAR, VARCHAR, VARCHAR, ARRAY<VARCHAR>, VARCHAR>, STRUCT<VARCHAR, VARCHAR, ARRAY<VARCHAR>>, STRUCT<VARCHAR>, STRUCT<VARCHAR, VARCHAR, VARCHAR, ARRAY<VARCHAR>, VARCHAR>, STRUCT<VARCHAR, VARCHAR, ARRAY<VARCHAR>>>) WITH (KAFKA_TOPIC='[string]', VALUE_FORMAT='[string]', KEY_FORMAT='[string]'); (io.confluent.ksql.logging.query.QueryLogger)
control-center                     | [2023-08-04 10:12:13,461] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Assigned to partition(s): _confluent-monitoring-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:13,461] INFO found 1 topicPartitions for topic=_confluent-monitoring (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:13,488] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:13,511] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-monitoring-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:13,588] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
ksqldb-server                      | [2023-08-04 10:12:13,794] INFO ProducerConfig values: 
ksqldb-server                      | 	acks = -1
ksqldb-server                      | 	auto.include.jmx.reporter = true
ksqldb-server                      | 	batch.size = 16384
ksqldb-server                      | 	bootstrap.servers = [broker:29092]
ksqldb-server                      | 	buffer.memory = 33554432
ksqldb-server                      | 	client.dns.lookup = use_all_dns_ips
ksqldb-server                      | 	client.id = producer-default_
ksqldb-server                      | 	compression.type = none
ksqldb-server                      | 	confluent.proxy.protocol.client.address = null
ksqldb-server                      | 	confluent.proxy.protocol.client.port = null
ksqldb-server                      | 	confluent.proxy.protocol.client.version = NONE
ksqldb-server                      | 	connections.max.idle.ms = 540000
ksqldb-server                      | 	delivery.timeout.ms = 120000
ksqldb-server                      | 	enable.idempotence = true
ksqldb-server                      | 	interceptor.classes = []
ksqldb-server                      | 	key.serializer = class io.confluent.ksql.rest.server.computation.InternalTopicSerdes$InternalTopicSerializer
ksqldb-server                      | 	linger.ms = 0
ksqldb-server                      | 	max.block.ms = 60000
ksqldb-server                      | 	max.in.flight.requests.per.connection = 5
ksqldb-server                      | 	max.request.size = 1048576
ksqldb-server                      | 	metadata.max.age.ms = 300000
ksqldb-server                      | 	metadata.max.idle.ms = 300000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	metrics.num.samples = 2
ksqldb-server                      | 	metrics.recording.level = INFO
ksqldb-server                      | 	metrics.sample.window.ms = 30000
ksqldb-server                      | 	partitioner.adaptive.partitioning.enable = true
ksqldb-server                      | 	partitioner.availability.timeout.ms = 0
ksqldb-server                      | 	partitioner.class = null
ksqldb-server                      | 	partitioner.ignore.keys = false
ksqldb-server                      | 	receive.buffer.bytes = 32768
ksqldb-server                      | 	reconnect.backoff.max.ms = 1000
ksqldb-server                      | 	reconnect.backoff.ms = 50
ksqldb-server                      | 	request.timeout.ms = 30000
ksqldb-server                      | 	retries = 2147483647
ksqldb-server                      | 	retry.backoff.ms = 100
ksqldb-server                      | 	sasl.client.callback.handler.class = null
ksqldb-server                      | 	sasl.jaas.config = null
ksqldb-server                      | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
ksqldb-server                      | 	sasl.kerberos.min.time.before.relogin = 60000
ksqldb-server                      | 	sasl.kerberos.service.name = null
ksqldb-server                      | 	sasl.kerberos.ticket.renew.jitter = 0.05
ksqldb-server                      | 	sasl.kerberos.ticket.renew.window.factor = 0.8
ksqldb-server                      | 	sasl.login.callback.handler.class = null
ksqldb-server                      | 	sasl.login.class = null
ksqldb-server                      | 	sasl.login.connect.timeout.ms = null
ksqldb-server                      | 	sasl.login.read.timeout.ms = null
ksqldb-server                      | 	sasl.login.refresh.buffer.seconds = 300
ksqldb-server                      | 	sasl.login.refresh.min.period.seconds = 60
ksqldb-server                      | 	sasl.login.refresh.window.factor = 0.8
ksqldb-server                      | 	sasl.login.refresh.window.jitter = 0.05
ksqldb-server                      | 	sasl.login.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.login.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.mechanism = GSSAPI
ksqldb-server                      | 	sasl.oauthbearer.clock.skew.seconds = 30
ksqldb-server                      | 	sasl.oauthbearer.expected.audience = null
ksqldb-server                      | 	sasl.oauthbearer.expected.issuer = null
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.url = null
ksqldb-server                      | 	sasl.oauthbearer.scope.claim.name = scope
ksqldb-server                      | 	sasl.oauthbearer.sub.claim.name = sub
ksqldb-server                      | 	sasl.oauthbearer.token.endpoint.url = null
ksqldb-server                      | 	security.protocol = PLAINTEXT
ksqldb-server                      | 	security.providers = null
ksqldb-server                      | 	send.buffer.bytes = 131072
ksqldb-server                      | 	socket.connection.setup.timeout.max.ms = 30000
ksqldb-server                      | 	socket.connection.setup.timeout.ms = 10000
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      | 	transaction.timeout.ms = 60000
ksqldb-server                      | 	transactional.id = default_
ksqldb-server                      | 	value.serializer = class io.confluent.ksql.rest.server.computation.InternalTopicSerdes$InternalTopicSerializer
ksqldb-server                      |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                     | [2023-08-04 10:12:13,954] INFO Setting offsets for topic=_confluent-metrics (io.confluent.controlcenter.KafkaHelper)
ksqldb-server                      | [2023-08-04 10:12:14,008] INFO [Producer clientId=producer-default_, transactionalId=default_] Instantiated a transactional producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:12:14,028] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Assigned to partition(s): _confluent-metrics-11, _confluent-metrics-9, _confluent-metrics-10, _confluent-metrics-7, _confluent-metrics-8, _confluent-metrics-5, _confluent-metrics-6, _confluent-metrics-3, _confluent-metrics-4, _confluent-metrics-1, _confluent-metrics-2, _confluent-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:14,029] INFO found 12 topicPartitions for topic=_confluent-metrics (io.confluent.controlcenter.KafkaHelper)
control-center                     | [2023-08-04 10:12:14,048] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:14,048] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:14,050] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:14,050] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:14,050] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:14,050] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:14,050] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:14,051] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:14,051] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:14,051] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:14,051] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:14,051] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:14,104] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:14,104] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:14,105] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:14,105] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:14,105] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:14,105] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:14,105] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:14,105] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:14,106] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:14,106] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:14,106] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:14,106] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:14,190] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:12:14,191] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:12:14,192] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:12:14,240] INFO App info kafka.consumer for will-delete-this unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:14,248] INFO action=starting topology=command (io.confluent.controlcenter.application.AllControlCenter)
control-center                     | [2023-08-04 10:12:14,274] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71] State transition from CREATED to REBALANCING (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:12:14,297] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71] Started 1 stream threads (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:12:14,305] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:14,357] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:14,352] INFO waiting for streams to be in running state. Current state is REBALANCING (io.confluent.command.kafka.CommandStore)
control-center                     | [2023-08-04 10:12:14,410] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Subscribed to topic(s): _confluent-command (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:14,512] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Resetting the last seen epoch of partition _confluent-command-0 to 0 since the associated topicId changed from null to 26mdvJTUTviIQl0i3yP-hQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:14,517] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:14,519] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:14,551] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
ksqldb-server                      | [2023-08-04 10:12:14,615] WARN These configurations '[awt.toolkit, log4j.configuration, producer.interceptor.classes, metrics.context.resource.version, metrics.context.resource.kafka.cluster.id, metrics.context.resource.type, metrics.context.resource.commit.id, ksql.server.install.dir, com.sun.management.jmxremote.authenticate, ksql.schema.registry.url, ksql.connect.url, file.encoding, cache.max.bytes.buffering, host.name, config.dir, com.sun.management.jmxremote.ssl, metrics.context.resource.cluster.id, ksql.logging.processing.stream.auto.create, com.sun.management.jmxremote, jdk.debug, listeners, classpath, ksql.logging.processing.topic.auto.create, ksql.log.dir, jdk.vendor.version, ksql.logging.processing.topic.replication.factor, consumer.interceptor.classes]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig)
ksqldb-server                      | [2023-08-04 10:12:14,617] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:14,617] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:14,618] INFO Kafka startTimeMs: 1691143934616 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:14,669] INFO [Producer clientId=producer-default_, transactionalId=default_] Invoking InitProducerId for the first time in order to acquire a producer ID (org.apache.kafka.clients.producer.internals.TransactionManager)
data-agrigator-taskmanager-1       | 2023-08-04 10:12:14,704 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:12:14,712 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
broker                             | [2023-08-04 10:12:14,729] INFO Sent auto-creation request for Set(__transaction_state) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
ksqldb-server                      | [2023-08-04 10:12:14,730] INFO [Producer clientId=producer-default_, transactionalId=default_] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
broker                             | [2023-08-04 10:12:14,741] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='__transaction_state', numPartitions=50, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='compression.type', value='uncompressed'), CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='segment.bytes', value='104857600'), CreateableTopicConfig(name='unclean.leader.election.enable', value='false')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,746] INFO [Controller 1] Created topic __transaction_state with topic ID Z9JbXZcBRjCWlzBoXwvf4g. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,748] INFO [Controller 1] ConfigResource(type=TOPIC, name='__transaction_state'): set configuration compression.type to uncompressed (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:12:14,749] INFO [Controller 1] ConfigResource(type=TOPIC, name='__transaction_state'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:12:14,750] INFO [Controller 1] ConfigResource(type=TOPIC, name='__transaction_state'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:12:14,750] INFO [Controller 1] ConfigResource(type=TOPIC, name='__transaction_state'): set configuration segment.bytes to 104857600 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:12:14,759] INFO [Controller 1] ConfigResource(type=TOPIC, name='__transaction_state'): set configuration unclean.leader.election.enable to false (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:12:14,760] INFO [Controller 1] Created partition __transaction_state-0 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,761] INFO [Controller 1] Created partition __transaction_state-1 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,762] INFO [Controller 1] Created partition __transaction_state-2 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,763] INFO [Controller 1] Created partition __transaction_state-3 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,763] INFO [Controller 1] Created partition __transaction_state-4 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,765] INFO [Controller 1] Created partition __transaction_state-5 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,768] INFO [Controller 1] Created partition __transaction_state-6 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,768] INFO [Controller 1] Created partition __transaction_state-7 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,771] INFO [Controller 1] Created partition __transaction_state-8 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,772] INFO [Controller 1] Created partition __transaction_state-9 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,776] INFO [Controller 1] Created partition __transaction_state-10 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,777] INFO [Controller 1] Created partition __transaction_state-11 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,778] INFO [Controller 1] Created partition __transaction_state-12 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,790] INFO [Controller 1] Created partition __transaction_state-13 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,790] INFO [Controller 1] Created partition __transaction_state-14 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,794] INFO [Controller 1] Created partition __transaction_state-15 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,794] INFO [Controller 1] Created partition __transaction_state-16 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,794] INFO [Controller 1] Created partition __transaction_state-17 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,796] INFO [Controller 1] Created partition __transaction_state-18 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,807] INFO [Controller 1] Created partition __transaction_state-19 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,808] INFO [Controller 1] Created partition __transaction_state-20 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,808] INFO [Controller 1] Created partition __transaction_state-21 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,814] INFO [Controller 1] Created partition __transaction_state-22 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,815] INFO [Controller 1] Created partition __transaction_state-23 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,816] INFO [Controller 1] Created partition __transaction_state-24 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,817] INFO [Controller 1] Created partition __transaction_state-25 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,818] INFO [Controller 1] Created partition __transaction_state-26 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,827] INFO [Controller 1] Created partition __transaction_state-27 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,828] INFO [Controller 1] Created partition __transaction_state-28 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,829] INFO [Controller 1] Created partition __transaction_state-29 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,829] INFO [Controller 1] Created partition __transaction_state-30 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,830] INFO [Controller 1] Created partition __transaction_state-31 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,833] INFO [Controller 1] Created partition __transaction_state-32 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,838] INFO [Controller 1] Created partition __transaction_state-33 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,838] INFO [Controller 1] Created partition __transaction_state-34 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,839] INFO [Controller 1] Created partition __transaction_state-35 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,839] INFO [Controller 1] Created partition __transaction_state-36 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,840] INFO [Controller 1] Created partition __transaction_state-37 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,841] INFO [Controller 1] Created partition __transaction_state-38 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,846] INFO [Controller 1] Created partition __transaction_state-39 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,847] INFO [Controller 1] Created partition __transaction_state-40 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,848] INFO [Controller 1] Created partition __transaction_state-41 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,849] INFO [Controller 1] Created partition __transaction_state-42 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,850] INFO [Controller 1] Created partition __transaction_state-43 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,857] INFO [Controller 1] Created partition __transaction_state-44 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,858] INFO [Controller 1] Created partition __transaction_state-45 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,859] INFO [Controller 1] Created partition __transaction_state-46 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,860] INFO [Controller 1] Created partition __transaction_state-47 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,860] INFO [Controller 1] Created partition __transaction_state-48 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:12:14,861] INFO [Controller 1] Created partition __transaction_state-49 with topic ID Z9JbXZcBRjCWlzBoXwvf4g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
control-center                     | [2023-08-04 10:12:14,893] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
broker                             | [2023-08-04 10:12:14,901] INFO [Broker id=1] Transitioning 50 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:12:14,916] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__transaction_state-42, __transaction_state-13, __transaction_state-46, __transaction_state-17, __transaction_state-34, __transaction_state-5, __transaction_state-38, __transaction_state-9, __transaction_state-26, __transaction_state-30, __transaction_state-1, __transaction_state-18, __transaction_state-22, __transaction_state-12, __transaction_state-45, __transaction_state-16, __transaction_state-49, __transaction_state-4, __transaction_state-37, __transaction_state-8, __transaction_state-41, __transaction_state-29, __transaction_state-0, __transaction_state-33, __transaction_state-21, __transaction_state-25, __transaction_state-11, __transaction_state-44, __transaction_state-15, __transaction_state-48, __transaction_state-3, __transaction_state-36, __transaction_state-7, __transaction_state-40, __transaction_state-28, __transaction_state-32, __transaction_state-20, __transaction_state-24, __transaction_state-10, __transaction_state-43, __transaction_state-14, __transaction_state-47, __transaction_state-2, __transaction_state-35, __transaction_state-6, __transaction_state-39, __transaction_state-27, __transaction_state-31, __transaction_state-19, __transaction_state-23) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:12:14,916] INFO [Broker id=1] Creating new partition __transaction_state-42 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:14,917] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1-command in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer-638f9ec9-d7fa-469f-ab36-9a4198ba450a and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:14,923] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer-638f9ec9-d7fa-469f-ab36-9a4198ba450a (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:14,924] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:14,924] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                             | [2023-08-04 10:12:14,955] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1-command in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member _confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer-638f9ec9-d7fa-469f-ab36-9a4198ba450a with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:12:14,990] INFO [LogLoader partition=__transaction_state-42, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:14,993] INFO Created log for partition __transaction_state-42 in /tmp/kraft-combined-logs/__transaction_state-42 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
ksqldb-server                      | [2023-08-04 10:12:14,996] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:15,008] INFO [Partition __transaction_state-42 broker=1] No checkpointed highwatermark is found for partition __transaction_state-42 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,009] INFO [Partition __transaction_state-42 broker=1] Log loaded for partition __transaction_state-42 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,010] INFO [Broker id=1] Leader __transaction_state-42 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:15,019] INFO [Broker id=1] Creating new partition __transaction_state-13 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:15,030] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1-command generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:12:15,033] INFO [LogLoader partition=__transaction_state-13, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
control-center                     | [2023-08-04 10:12:15,038] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Successfully joined group with generation Generation{generationId=1, memberId='_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer-638f9ec9-d7fa-469f-ab36-9a4198ba450a', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                             | [2023-08-04 10:12:15,074] INFO Created log for partition __transaction_state-13 in /tmp/kraft-combined-logs/__transaction_state-13 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:15,075] INFO [Partition __transaction_state-13 broker=1] No checkpointed highwatermark is found for partition __transaction_state-13 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,076] INFO [Partition __transaction_state-13 broker=1] Log loaded for partition __transaction_state-13 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,077] INFO [Broker id=1] Leader __transaction_state-13 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:15,092] INFO [Broker id=1] Creating new partition __transaction_state-46 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:15,112] INFO [LogLoader partition=__transaction_state-46, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
control-center                     | [2023-08-04 10:12:15,113] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer] Skipping the repartition topic validation since there are no repartition topics. (org.apache.kafka.streams.processor.internals.RepartitionTopics)
broker                             | [2023-08-04 10:12:15,116] INFO Created log for partition __transaction_state-46 in /tmp/kraft-combined-logs/__transaction_state-46 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:15,118] INFO [Partition __transaction_state-46 broker=1] No checkpointed highwatermark is found for partition __transaction_state-46 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,119] INFO [Partition __transaction_state-46 broker=1] Log loaded for partition __transaction_state-46 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,119] INFO [Broker id=1] Leader __transaction_state-46 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:15,133] INFO [Broker id=1] Creating new partition __transaction_state-17 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:12:15,152] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:15,156] INFO [LogLoader partition=__transaction_state-17, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:15,163] INFO Created log for partition __transaction_state-17 in /tmp/kraft-combined-logs/__transaction_state-17 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:15,165] INFO [Partition __transaction_state-17 broker=1] No checkpointed highwatermark is found for partition __transaction_state-17 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,165] INFO [Partition __transaction_state-17 broker=1] Log loaded for partition __transaction_state-17 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,166] INFO [Broker id=1] Leader __transaction_state-17 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:15,198] INFO [Broker id=1] Creating new partition __transaction_state-34 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:15,242] INFO [LogLoader partition=__transaction_state-34, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:15,260] INFO Created log for partition __transaction_state-34 in /tmp/kraft-combined-logs/__transaction_state-34 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:15,268] INFO [Partition __transaction_state-34 broker=1] No checkpointed highwatermark is found for partition __transaction_state-34 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,269] INFO [Partition __transaction_state-34 broker=1] Log loaded for partition __transaction_state-34 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,270] INFO [Broker id=1] Leader __transaction_state-34 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:12:15,274] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:15,281] INFO [Broker id=1] Creating new partition __transaction_state-5 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
control-center                     | [2023-08-04 10:12:15,310] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Found no committed offset for partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                             | [2023-08-04 10:12:15,312] INFO [LogLoader partition=__transaction_state-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:15,316] INFO Created log for partition __transaction_state-5 in /tmp/kraft-combined-logs/__transaction_state-5 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
control-center                     | [2023-08-04 10:12:15,315] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer] All members participating in this rebalance: 
control-center                     | 1b232679-b755-411f-9a4d-31973ee6bb71: [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer-638f9ec9-d7fa-469f-ab36-9a4198ba450a]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
broker                             | [2023-08-04 10:12:15,323] INFO [Partition __transaction_state-5 broker=1] No checkpointed highwatermark is found for partition __transaction_state-5 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,324] INFO [Partition __transaction_state-5 broker=1] Log loaded for partition __transaction_state-5 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,325] INFO [Broker id=1] Leader __transaction_state-5 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:15,338] INFO [Broker id=1] Creating new partition __transaction_state-38 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
control-center                     | [2023-08-04 10:12:15,351] INFO Decided on assignment: {1b232679-b755-411f-9a4d-31973ee6bb71=[activeTasks: ([0_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([0_0=0]) clientTags: ([]) capacity: 1 assigned: 1]} with no followup probing rebalance. (org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor)
control-center                     | [2023-08-04 10:12:15,352] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer] Assigned tasks [0_0] including stateful [0_0] to clients as: 
control-center                     | 1b232679-b755-411f-9a4d-31973ee6bb71=[activeTasks: ([0_0]) standbyTasks: ([])]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:15,366] INFO waiting for streams to be in running state. Current state is REBALANCING (io.confluent.command.kafka.CommandStore)
broker                             | [2023-08-04 10:12:15,371] INFO [LogLoader partition=__transaction_state-38, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
ksqldb-server                      | [2023-08-04 10:12:15,385] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:15,392] INFO Created log for partition __transaction_state-38 in /tmp/kraft-combined-logs/__transaction_state-38 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
control-center                     | [2023-08-04 10:12:15,401] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer] Client 1b232679-b755-411f-9a4d-31973ee6bb71 per-consumer assignment:
control-center                     | 	prev owned active {}
control-center                     | 	prev owned standby {_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer-638f9ec9-d7fa-469f-ab36-9a4198ba450a=[]}
control-center                     | 	assigned active {_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer-638f9ec9-d7fa-469f-ab36-9a4198ba450a=[0_0]}
control-center                     | 	revoking active {}
control-center                     | 	assigned standby {}
control-center                     |  (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:15,402] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:15,403] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Finished assignment for group at generation 1: {_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer-638f9ec9-d7fa-469f-ab36-9a4198ba450a=Assignment(partitions=[_confluent-command-0], userDataSize=52)} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                             | [2023-08-04 10:12:15,410] INFO [Partition __transaction_state-38 broker=1] No checkpointed highwatermark is found for partition __transaction_state-38 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,411] INFO [Partition __transaction_state-38 broker=1] Log loaded for partition __transaction_state-38 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,412] INFO [Broker id=1] Leader __transaction_state-38 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:15,430] INFO [Broker id=1] Creating new partition __transaction_state-9 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:15,448] INFO [GroupCoordinator 1]: Assignment received from leader _confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer-638f9ec9-d7fa-469f-ab36-9a4198ba450a for group _confluent-controlcenter-7-4-1-1-command for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:15,478] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Successfully synced group in generation Generation{generationId=1, memberId='_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer-638f9ec9-d7fa-469f-ab36-9a4198ba450a', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:15,480] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Notifying assignor about the new Assignment(partitions=[_confluent-command-0], userDataSize=52) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:15,483] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:15,487] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Handle new assignment with:
control-center                     | 	New active tasks: [0_0]
control-center                     | 	New standby tasks: []
control-center                     | 	Existing active tasks: []
control-center                     | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
broker                             | [2023-08-04 10:12:15,490] INFO [LogLoader partition=__transaction_state-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
ksqldb-server                      | [2023-08-04 10:12:15,506] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:15,526] INFO Created log for partition __transaction_state-9 in /tmp/kraft-combined-logs/__transaction_state-9 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:15,534] INFO [Partition __transaction_state-9 broker=1] No checkpointed highwatermark is found for partition __transaction_state-9 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,535] INFO [Partition __transaction_state-9 broker=1] Log loaded for partition __transaction_state-9 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,536] INFO [Broker id=1] Leader __transaction_state-9 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:15,547] INFO [Broker id=1] Creating new partition __transaction_state-26 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:15,617] INFO [LogLoader partition=__transaction_state-26, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
ksqldb-server                      | [2023-08-04 10:12:15,617] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:15,630] INFO Created log for partition __transaction_state-26 in /tmp/kraft-combined-logs/__transaction_state-26 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:15,633] INFO [Partition __transaction_state-26 broker=1] No checkpointed highwatermark is found for partition __transaction_state-26 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,634] INFO [Partition __transaction_state-26 broker=1] Log loaded for partition __transaction_state-26 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,635] INFO [Broker id=1] Leader __transaction_state-26 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:15,655] INFO [Broker id=1] Creating new partition __transaction_state-30 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:15,673] INFO [LogLoader partition=__transaction_state-30, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:15,680] INFO Created log for partition __transaction_state-30 in /tmp/kraft-combined-logs/__transaction_state-30 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:15,680] INFO [Partition __transaction_state-30 broker=1] No checkpointed highwatermark is found for partition __transaction_state-30 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,681] INFO [Partition __transaction_state-30 broker=1] Log loaded for partition __transaction_state-30 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,687] INFO [Broker id=1] Leader __transaction_state-30 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:15,699] INFO [Broker id=1] Creating new partition __transaction_state-1 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:15,722] INFO [LogLoader partition=__transaction_state-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:15,725] INFO Created log for partition __transaction_state-1 in /tmp/kraft-combined-logs/__transaction_state-1 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
ksqldb-server                      | [2023-08-04 10:12:15,726] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:15,729] INFO [Partition __transaction_state-1 broker=1] No checkpointed highwatermark is found for partition __transaction_state-1 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,730] INFO [Partition __transaction_state-1 broker=1] Log loaded for partition __transaction_state-1 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,732] INFO [Broker id=1] Leader __transaction_state-1 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:15,746] INFO [Broker id=1] Creating new partition __transaction_state-18 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
control-center                     | [2023-08-04 10:12:15,751] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Adding newly assigned partitions: _confluent-command-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:15,752] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
broker                             | [2023-08-04 10:12:15,777] INFO [LogLoader partition=__transaction_state-18, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:15,780] INFO Created log for partition __transaction_state-18 in /tmp/kraft-combined-logs/__transaction_state-18 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:15,780] INFO [Partition __transaction_state-18 broker=1] No checkpointed highwatermark is found for partition __transaction_state-18 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,780] INFO [Partition __transaction_state-18 broker=1] Log loaded for partition __transaction_state-18 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,780] INFO [Broker id=1] Leader __transaction_state-18 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:15,788] INFO [Broker id=1] Creating new partition __transaction_state-22 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:15,800] INFO [LogLoader partition=__transaction_state-22, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:15,809] INFO Created log for partition __transaction_state-22 in /tmp/kraft-combined-logs/__transaction_state-22 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:15,809] INFO [Partition __transaction_state-22 broker=1] No checkpointed highwatermark is found for partition __transaction_state-22 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,817] INFO [Partition __transaction_state-22 broker=1] Log loaded for partition __transaction_state-22 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,818] INFO [Broker id=1] Leader __transaction_state-22 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:15,837] INFO [Broker id=1] Creating new partition __transaction_state-12 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
control-center                     | [2023-08-04 10:12:15,841] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Found no committed offset for partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
ksqldb-server                      | [2023-08-04 10:12:15,850] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:15,867] INFO [LogLoader partition=__transaction_state-12, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:15,879] INFO Created log for partition __transaction_state-12 in /tmp/kraft-combined-logs/__transaction_state-12 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:15,896] INFO [Partition __transaction_state-12 broker=1] No checkpointed highwatermark is found for partition __transaction_state-12 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,897] INFO [Partition __transaction_state-12 broker=1] Log loaded for partition __transaction_state-12 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,900] INFO [Broker id=1] Leader __transaction_state-12 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:15,911] INFO [Broker id=1] Creating new partition __transaction_state-45 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:15,938] INFO [LogLoader partition=__transaction_state-45, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:15,946] INFO Created log for partition __transaction_state-45 in /tmp/kraft-combined-logs/__transaction_state-45 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:15,947] INFO [Partition __transaction_state-45 broker=1] No checkpointed highwatermark is found for partition __transaction_state-45 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,948] INFO [Partition __transaction_state-45 broker=1] Log loaded for partition __transaction_state-45 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,950] INFO [Broker id=1] Leader __transaction_state-45 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:15,958] INFO [Broker id=1] Creating new partition __transaction_state-16 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:15,982] INFO [LogLoader partition=__transaction_state-16, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
ksqldb-server                      | [2023-08-04 10:12:15,984] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:15,987] INFO Created log for partition __transaction_state-16 in /tmp/kraft-combined-logs/__transaction_state-16 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:15,988] INFO [Partition __transaction_state-16 broker=1] No checkpointed highwatermark is found for partition __transaction_state-16 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,988] INFO [Partition __transaction_state-16 broker=1] Log loaded for partition __transaction_state-16 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:15,989] INFO [Broker id=1] Leader __transaction_state-16 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,001] INFO [Broker id=1] Creating new partition __transaction_state-49 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,010] INFO [LogLoader partition=__transaction_state-49, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,020] INFO Created log for partition __transaction_state-49 in /tmp/kraft-combined-logs/__transaction_state-49 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,020] INFO [Partition __transaction_state-49 broker=1] No checkpointed highwatermark is found for partition __transaction_state-49 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,021] INFO [Partition __transaction_state-49 broker=1] Log loaded for partition __transaction_state-49 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,021] INFO [Broker id=1] Leader __transaction_state-49 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,033] INFO [Broker id=1] Creating new partition __transaction_state-4 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,042] INFO [LogLoader partition=__transaction_state-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,048] INFO Created log for partition __transaction_state-4 in /tmp/kraft-combined-logs/__transaction_state-4 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,048] INFO [Partition __transaction_state-4 broker=1] No checkpointed highwatermark is found for partition __transaction_state-4 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,048] INFO [Partition __transaction_state-4 broker=1] Log loaded for partition __transaction_state-4 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,048] INFO [Broker id=1] Leader __transaction_state-4 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,060] INFO [Broker id=1] Creating new partition __transaction_state-37 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,068] INFO [LogLoader partition=__transaction_state-37, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,072] INFO Created log for partition __transaction_state-37 in /tmp/kraft-combined-logs/__transaction_state-37 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,073] INFO [Partition __transaction_state-37 broker=1] No checkpointed highwatermark is found for partition __transaction_state-37 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,073] INFO [Partition __transaction_state-37 broker=1] Log loaded for partition __transaction_state-37 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,074] INFO [Broker id=1] Leader __transaction_state-37 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,089] INFO [Broker id=1] Creating new partition __transaction_state-8 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:12:16,104] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:16,122] INFO [LogLoader partition=__transaction_state-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,132] INFO Created log for partition __transaction_state-8 in /tmp/kraft-combined-logs/__transaction_state-8 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,139] INFO [Partition __transaction_state-8 broker=1] No checkpointed highwatermark is found for partition __transaction_state-8 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,140] INFO [Partition __transaction_state-8 broker=1] Log loaded for partition __transaction_state-8 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,141] INFO [Broker id=1] Leader __transaction_state-8 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,157] INFO [Broker id=1] Creating new partition __transaction_state-41 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,202] INFO [LogLoader partition=__transaction_state-41, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
ksqldb-server                      | [2023-08-04 10:12:16,219] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:16,224] INFO Created log for partition __transaction_state-41 in /tmp/kraft-combined-logs/__transaction_state-41 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,225] INFO [Partition __transaction_state-41 broker=1] No checkpointed highwatermark is found for partition __transaction_state-41 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,229] INFO [Partition __transaction_state-41 broker=1] Log loaded for partition __transaction_state-41 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,230] INFO [Broker id=1] Leader __transaction_state-41 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,242] INFO [Broker id=1] Creating new partition __transaction_state-29 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,266] INFO [LogLoader partition=__transaction_state-29, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,284] INFO Created log for partition __transaction_state-29 in /tmp/kraft-combined-logs/__transaction_state-29 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,285] INFO [Partition __transaction_state-29 broker=1] No checkpointed highwatermark is found for partition __transaction_state-29 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,285] INFO [Partition __transaction_state-29 broker=1] Log loaded for partition __transaction_state-29 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,286] INFO [Broker id=1] Leader __transaction_state-29 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,297] INFO [Broker id=1] Creating new partition __transaction_state-0 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,305] INFO [LogLoader partition=__transaction_state-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,313] INFO Created log for partition __transaction_state-0 in /tmp/kraft-combined-logs/__transaction_state-0 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,315] INFO [Partition __transaction_state-0 broker=1] No checkpointed highwatermark is found for partition __transaction_state-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,315] INFO [Partition __transaction_state-0 broker=1] Log loaded for partition __transaction_state-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,316] INFO [Broker id=1] Leader __transaction_state-0 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:12:16,330] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:16,347] INFO [Broker id=1] Creating new partition __transaction_state-33 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
control-center                     | [2023-08-04 10:12:16,366] INFO waiting for streams to be in running state. Current state is REBALANCING (io.confluent.command.kafka.CommandStore)
broker                             | [2023-08-04 10:12:16,371] INFO [LogLoader partition=__transaction_state-33, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,377] INFO Created log for partition __transaction_state-33 in /tmp/kraft-combined-logs/__transaction_state-33 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,378] INFO [Partition __transaction_state-33 broker=1] No checkpointed highwatermark is found for partition __transaction_state-33 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,378] INFO [Partition __transaction_state-33 broker=1] Log loaded for partition __transaction_state-33 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,378] INFO [Broker id=1] Leader __transaction_state-33 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,406] INFO [Broker id=1] Creating new partition __transaction_state-21 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,444] INFO [LogLoader partition=__transaction_state-21, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
ksqldb-server                      | [2023-08-04 10:12:16,453] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:16,460] INFO Created log for partition __transaction_state-21 in /tmp/kraft-combined-logs/__transaction_state-21 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,461] INFO [Partition __transaction_state-21 broker=1] No checkpointed highwatermark is found for partition __transaction_state-21 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,461] INFO [Partition __transaction_state-21 broker=1] Log loaded for partition __transaction_state-21 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,461] INFO [Broker id=1] Leader __transaction_state-21 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,474] INFO [Broker id=1] Creating new partition __transaction_state-25 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,490] INFO [LogLoader partition=__transaction_state-25, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,493] INFO Created log for partition __transaction_state-25 in /tmp/kraft-combined-logs/__transaction_state-25 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,495] INFO [Partition __transaction_state-25 broker=1] No checkpointed highwatermark is found for partition __transaction_state-25 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,496] INFO [Partition __transaction_state-25 broker=1] Log loaded for partition __transaction_state-25 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,497] INFO [Broker id=1] Leader __transaction_state-25 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,520] INFO [Broker id=1] Creating new partition __transaction_state-11 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,537] INFO [LogLoader partition=__transaction_state-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,540] INFO Created log for partition __transaction_state-11 in /tmp/kraft-combined-logs/__transaction_state-11 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,547] INFO [Partition __transaction_state-11 broker=1] No checkpointed highwatermark is found for partition __transaction_state-11 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,547] INFO [Partition __transaction_state-11 broker=1] Log loaded for partition __transaction_state-11 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,548] INFO [Broker id=1] Leader __transaction_state-11 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:12:16,562] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:16,578] INFO [Broker id=1] Creating new partition __transaction_state-44 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,616] INFO [LogLoader partition=__transaction_state-44, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,628] INFO Created log for partition __transaction_state-44 in /tmp/kraft-combined-logs/__transaction_state-44 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,629] INFO [Partition __transaction_state-44 broker=1] No checkpointed highwatermark is found for partition __transaction_state-44 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,629] INFO [Partition __transaction_state-44 broker=1] Log loaded for partition __transaction_state-44 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,631] INFO [Broker id=1] Leader __transaction_state-44 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,653] INFO [Broker id=1] Creating new partition __transaction_state-15 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:12:16,673] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:16,682] INFO [LogLoader partition=__transaction_state-15, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,685] INFO Created log for partition __transaction_state-15 in /tmp/kraft-combined-logs/__transaction_state-15 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,686] INFO [Partition __transaction_state-15 broker=1] No checkpointed highwatermark is found for partition __transaction_state-15 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,686] INFO [Partition __transaction_state-15 broker=1] Log loaded for partition __transaction_state-15 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,687] INFO [Broker id=1] Leader __transaction_state-15 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,695] INFO [Broker id=1] Creating new partition __transaction_state-48 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,703] INFO [LogLoader partition=__transaction_state-48, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,715] INFO Created log for partition __transaction_state-48 in /tmp/kraft-combined-logs/__transaction_state-48 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,716] INFO [Partition __transaction_state-48 broker=1] No checkpointed highwatermark is found for partition __transaction_state-48 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,717] INFO [Partition __transaction_state-48 broker=1] Log loaded for partition __transaction_state-48 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,717] INFO [Broker id=1] Leader __transaction_state-48 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,730] INFO [Broker id=1] Creating new partition __transaction_state-3 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,741] INFO [LogLoader partition=__transaction_state-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,745] INFO Created log for partition __transaction_state-3 in /tmp/kraft-combined-logs/__transaction_state-3 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,749] INFO [Partition __transaction_state-3 broker=1] No checkpointed highwatermark is found for partition __transaction_state-3 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,749] INFO [Partition __transaction_state-3 broker=1] Log loaded for partition __transaction_state-3 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,749] INFO [Broker id=1] Leader __transaction_state-3 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,762] INFO [Broker id=1] Creating new partition __transaction_state-36 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,787] INFO [LogLoader partition=__transaction_state-36, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
ksqldb-server                      | [2023-08-04 10:12:16,789] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:16,789] INFO Created log for partition __transaction_state-36 in /tmp/kraft-combined-logs/__transaction_state-36 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,789] INFO [Partition __transaction_state-36 broker=1] No checkpointed highwatermark is found for partition __transaction_state-36 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,789] INFO [Partition __transaction_state-36 broker=1] Log loaded for partition __transaction_state-36 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,789] INFO [Broker id=1] Leader __transaction_state-36 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,802] INFO [Broker id=1] Creating new partition __transaction_state-7 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,820] INFO [LogLoader partition=__transaction_state-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,828] INFO Created log for partition __transaction_state-7 in /tmp/kraft-combined-logs/__transaction_state-7 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,829] INFO [Partition __transaction_state-7 broker=1] No checkpointed highwatermark is found for partition __transaction_state-7 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,829] INFO [Partition __transaction_state-7 broker=1] Log loaded for partition __transaction_state-7 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,830] INFO [Broker id=1] Leader __transaction_state-7 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,840] INFO [Broker id=1] Creating new partition __transaction_state-40 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,864] INFO [LogLoader partition=__transaction_state-40, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,875] INFO Created log for partition __transaction_state-40 in /tmp/kraft-combined-logs/__transaction_state-40 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,876] INFO [Partition __transaction_state-40 broker=1] No checkpointed highwatermark is found for partition __transaction_state-40 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,880] INFO [Partition __transaction_state-40 broker=1] Log loaded for partition __transaction_state-40 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,880] INFO [Broker id=1] Leader __transaction_state-40 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,888] INFO [Broker id=1] Creating new partition __transaction_state-28 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:12:16,911] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:16,922] INFO [LogLoader partition=__transaction_state-28, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,933] INFO Created log for partition __transaction_state-28 in /tmp/kraft-combined-logs/__transaction_state-28 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,934] INFO [Partition __transaction_state-28 broker=1] No checkpointed highwatermark is found for partition __transaction_state-28 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,935] INFO [Partition __transaction_state-28 broker=1] Log loaded for partition __transaction_state-28 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,936] INFO [Broker id=1] Leader __transaction_state-28 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,949] INFO [Broker id=1] Creating new partition __transaction_state-32 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,957] INFO [LogLoader partition=__transaction_state-32, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:16,977] INFO Created log for partition __transaction_state-32 in /tmp/kraft-combined-logs/__transaction_state-32 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:16,977] INFO [Partition __transaction_state-32 broker=1] No checkpointed highwatermark is found for partition __transaction_state-32 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,977] INFO [Partition __transaction_state-32 broker=1] Log loaded for partition __transaction_state-32 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:16,977] INFO [Broker id=1] Leader __transaction_state-32 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:16,986] INFO [Broker id=1] Creating new partition __transaction_state-20 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:16,993] INFO [LogLoader partition=__transaction_state-20, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:17,007] INFO Created log for partition __transaction_state-20 in /tmp/kraft-combined-logs/__transaction_state-20 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:17,008] INFO [Partition __transaction_state-20 broker=1] No checkpointed highwatermark is found for partition __transaction_state-20 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,008] INFO [Partition __transaction_state-20 broker=1] Log loaded for partition __transaction_state-20 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,008] INFO [Broker id=1] Leader __transaction_state-20 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:12:17,019] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:17,021] INFO [Broker id=1] Creating new partition __transaction_state-24 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:17,039] INFO [LogLoader partition=__transaction_state-24, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:17,041] INFO Created log for partition __transaction_state-24 in /tmp/kraft-combined-logs/__transaction_state-24 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:17,041] INFO [Partition __transaction_state-24 broker=1] No checkpointed highwatermark is found for partition __transaction_state-24 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,041] INFO [Partition __transaction_state-24 broker=1] Log loaded for partition __transaction_state-24 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,042] INFO [Broker id=1] Leader __transaction_state-24 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:17,058] INFO [Broker id=1] Creating new partition __transaction_state-10 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:17,070] INFO [LogLoader partition=__transaction_state-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:17,072] INFO Created log for partition __transaction_state-10 in /tmp/kraft-combined-logs/__transaction_state-10 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:17,072] INFO [Partition __transaction_state-10 broker=1] No checkpointed highwatermark is found for partition __transaction_state-10 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,072] INFO [Partition __transaction_state-10 broker=1] Log loaded for partition __transaction_state-10 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,073] INFO [Broker id=1] Leader __transaction_state-10 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:17,091] INFO [Broker id=1] Creating new partition __transaction_state-43 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:17,099] INFO [LogLoader partition=__transaction_state-43, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:17,103] INFO Created log for partition __transaction_state-43 in /tmp/kraft-combined-logs/__transaction_state-43 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:17,104] INFO [Partition __transaction_state-43 broker=1] No checkpointed highwatermark is found for partition __transaction_state-43 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,104] INFO [Partition __transaction_state-43 broker=1] Log loaded for partition __transaction_state-43 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,105] INFO [Broker id=1] Leader __transaction_state-43 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:17,120] INFO [Broker id=1] Creating new partition __transaction_state-14 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:12:17,128] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:17,134] INFO [LogLoader partition=__transaction_state-14, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:17,136] INFO Created log for partition __transaction_state-14 in /tmp/kraft-combined-logs/__transaction_state-14 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:17,137] INFO [Partition __transaction_state-14 broker=1] No checkpointed highwatermark is found for partition __transaction_state-14 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,138] INFO [Partition __transaction_state-14 broker=1] Log loaded for partition __transaction_state-14 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,139] INFO [Broker id=1] Leader __transaction_state-14 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:17,158] INFO [Broker id=1] Creating new partition __transaction_state-47 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:17,179] INFO [LogLoader partition=__transaction_state-47, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:17,192] INFO Created log for partition __transaction_state-47 in /tmp/kraft-combined-logs/__transaction_state-47 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:17,193] INFO [Partition __transaction_state-47 broker=1] No checkpointed highwatermark is found for partition __transaction_state-47 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,194] INFO [Partition __transaction_state-47 broker=1] Log loaded for partition __transaction_state-47 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,194] INFO [Broker id=1] Leader __transaction_state-47 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:17,208] INFO [Broker id=1] Creating new partition __transaction_state-2 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:17,221] INFO [LogLoader partition=__transaction_state-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
ksqldb-server                      | [2023-08-04 10:12:17,237] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:17,230] INFO Created log for partition __transaction_state-2 in /tmp/kraft-combined-logs/__transaction_state-2 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:17,251] INFO [Partition __transaction_state-2 broker=1] No checkpointed highwatermark is found for partition __transaction_state-2 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,252] INFO [Partition __transaction_state-2 broker=1] Log loaded for partition __transaction_state-2 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,252] INFO [Broker id=1] Leader __transaction_state-2 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:17,265] INFO [Broker id=1] Creating new partition __transaction_state-35 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:17,274] INFO [LogLoader partition=__transaction_state-35, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:17,277] INFO Created log for partition __transaction_state-35 in /tmp/kraft-combined-logs/__transaction_state-35 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:17,277] INFO [Partition __transaction_state-35 broker=1] No checkpointed highwatermark is found for partition __transaction_state-35 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,278] INFO [Partition __transaction_state-35 broker=1] Log loaded for partition __transaction_state-35 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,278] INFO [Broker id=1] Leader __transaction_state-35 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:17,295] INFO [Broker id=1] Creating new partition __transaction_state-6 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:17,332] INFO [LogLoader partition=__transaction_state-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:17,347] INFO Created log for partition __transaction_state-6 in /tmp/kraft-combined-logs/__transaction_state-6 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:17,360] INFO [Partition __transaction_state-6 broker=1] No checkpointed highwatermark is found for partition __transaction_state-6 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,361] INFO [Partition __transaction_state-6 broker=1] Log loaded for partition __transaction_state-6 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,368] INFO [Broker id=1] Leader __transaction_state-6 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center                     | [2023-08-04 10:12:17,373] INFO waiting for streams to be in running state. Current state is REBALANCING (io.confluent.command.kafka.CommandStore)
ksqldb-server                      | [2023-08-04 10:12:17,386] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:17,392] INFO [Broker id=1] Creating new partition __transaction_state-39 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:17,431] INFO [LogLoader partition=__transaction_state-39, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:17,438] INFO Created log for partition __transaction_state-39 in /tmp/kraft-combined-logs/__transaction_state-39 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:17,440] INFO [Partition __transaction_state-39 broker=1] No checkpointed highwatermark is found for partition __transaction_state-39 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,448] INFO [Partition __transaction_state-39 broker=1] Log loaded for partition __transaction_state-39 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,448] INFO [Broker id=1] Leader __transaction_state-39 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:17,461] INFO [Broker id=1] Creating new partition __transaction_state-27 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:17,473] INFO [LogLoader partition=__transaction_state-27, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:17,479] INFO Created log for partition __transaction_state-27 in /tmp/kraft-combined-logs/__transaction_state-27 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:17,485] INFO [Partition __transaction_state-27 broker=1] No checkpointed highwatermark is found for partition __transaction_state-27 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,485] INFO [Partition __transaction_state-27 broker=1] Log loaded for partition __transaction_state-27 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,485] INFO [Broker id=1] Leader __transaction_state-27 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:17,494] INFO [Broker id=1] Creating new partition __transaction_state-31 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
ksqldb-server                      | [2023-08-04 10:12:17,512] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:17,515] INFO [LogLoader partition=__transaction_state-31, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:17,518] INFO Created log for partition __transaction_state-31 in /tmp/kraft-combined-logs/__transaction_state-31 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:17,519] INFO [Partition __transaction_state-31 broker=1] No checkpointed highwatermark is found for partition __transaction_state-31 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,519] INFO [Partition __transaction_state-31 broker=1] Log loaded for partition __transaction_state-31 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,520] INFO [Broker id=1] Leader __transaction_state-31 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:17,527] INFO [Broker id=1] Creating new partition __transaction_state-19 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:17,548] INFO [LogLoader partition=__transaction_state-19, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:17,551] INFO Created log for partition __transaction_state-19 in /tmp/kraft-combined-logs/__transaction_state-19 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:17,551] INFO [Partition __transaction_state-19 broker=1] No checkpointed highwatermark is found for partition __transaction_state-19 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,552] INFO [Partition __transaction_state-19 broker=1] Log loaded for partition __transaction_state-19 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,552] INFO [Broker id=1] Leader __transaction_state-19 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:17,562] INFO [Broker id=1] Creating new partition __transaction_state-23 with topic id Z9JbXZcBRjCWlzBoXwvf4g. (state.change.logger)
broker                             | [2023-08-04 10:12:17,576] INFO [LogLoader partition=__transaction_state-23, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:12:17,580] INFO Created log for partition __transaction_state-23 in /tmp/kraft-combined-logs/__transaction_state-23 with properties {cleanup.policy=compact, compression.type="uncompressed", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:12:17,582] INFO [Partition __transaction_state-23 broker=1] No checkpointed highwatermark is found for partition __transaction_state-23 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,583] INFO [Partition __transaction_state-23 broker=1] Log loaded for partition __transaction_state-23 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:12:17,584] INFO [Broker id=1] Leader __transaction_state-23 with topic id Some(Z9JbXZcBRjCWlzBoXwvf4g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:12:17,602] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 42 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
ksqldb-server                      | [2023-08-04 10:12:17,628] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:17,677] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-42 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,690] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-42 in 24 milliseconds, of which 21 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,691] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 13 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
control-center                     | [2023-08-04 10:12:17,713] INFO Opening store commander in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore)
broker                             | [2023-08-04 10:12:17,722] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-42 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,723] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 46 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 17 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,725] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 34 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,726] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 5 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,727] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 38 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,728] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 9 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,729] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 26 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,730] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 30 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,731] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 1 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,732] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 18 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,733] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 22 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,734] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 12 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,735] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 45 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,736] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 16 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,737] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 49 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,738] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 4 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,739] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 37 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,740] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 8 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,741] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 41 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,742] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 29 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,745] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 0 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,746] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 33 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,748] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 21 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,749] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 25 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,750] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 11 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
ksqldb-server                      | [2023-08-04 10:12:17,750] INFO [Producer clientId=producer-default_, transactionalId=default_] Discovered transaction coordinator broker:29092 (id: 1 rack: null) (org.apache.kafka.clients.producer.internals.TransactionManager)
broker                             | [2023-08-04 10:12:17,755] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 44 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,723] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-13 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,756] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-13 in 33 milliseconds, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,757] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 15 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,758] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-13 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,758] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-46 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,761] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 48 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,762] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 3 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,768] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 36 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,769] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 7 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,770] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 40 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,770] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-46 in 37 milliseconds, of which 34 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,771] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 28 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,772] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-46 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,772] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-17 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,773] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-17 in 48 milliseconds, of which 47 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,774] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-17 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,774] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-34 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,775] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-34 in 49 milliseconds, of which 48 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,776] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-34 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,777] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-5 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,778] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-5 in 51 milliseconds, of which 50 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,773] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 32 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,781] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-5 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,784] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-38 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,785] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-38 in 57 milliseconds, of which 56 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,786] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-38 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
control-center                     | [2023-08-04 10:12:17,787] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] stream-task [0_0] State store commander did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-command-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:17,787] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] task [0_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
broker                             | [2023-08-04 10:12:17,788] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-9 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,789] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-9 in 60 milliseconds, of which 59 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,782] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 20 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,795] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-9 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,796] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-26 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,798] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-26 in 68 milliseconds, of which 66 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,796] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 24 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,800] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-26 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,801] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-30 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,802] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-30 in 71 milliseconds, of which 70 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,808] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-30 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,815] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-1 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,801] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 10 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,818] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 43 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,819] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 14 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,823] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 47 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,824] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 2 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,825] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 35 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,826] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 6 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,827] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 39 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,828] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 27 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 31 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,831] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 19 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:12:17,832] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 23 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
connect                            | [2023-08-04 10:12:17,833] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/schema-registry/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
broker                             | [2023-08-04 10:12:17,830] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-1 in 84 milliseconds, of which 83 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,835] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-1 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,836] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-18 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,837] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-18 in 103 milliseconds, of which 103 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,837] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-18 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
connect                            | [2023-08-04 10:12:17,838] INFO Loading plugin from: /usr/share/java/acl (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
broker                             | [2023-08-04 10:12:17,839] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-22 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,840] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-22 in 106 milliseconds, of which 105 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,840] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic __transaction_state with new configuration : compression.type -> uncompressed,cleanup.policy -> compact,min.insync.replicas -> 1,segment.bytes -> 104857600,unclean.leader.election.enable -> false (kafka.server.metadata.DynamicConfigPublisher)
broker                             | [2023-08-04 10:12:17,844] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-22 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,848] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-12 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,849] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-12 in 114 milliseconds, of which 113 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,857] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-12 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,858] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-45 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,867] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-45 in 131 milliseconds, of which 122 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,868] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-45 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,869] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-16 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,870] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-16 in 133 milliseconds, of which 132 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,870] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-16 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,871] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-49 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,872] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-49 in 134 milliseconds, of which 133 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,873] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-49 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,874] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-4 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,875] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-4 in 136 milliseconds, of which 135 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,877] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-4 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,878] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-37 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,879] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-37 in 139 milliseconds, of which 138 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,880] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-37 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,881] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-8 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,887] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-8 in 146 milliseconds, of which 140 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,891] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-8 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,892] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-41 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,893] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-41 in 151 milliseconds, of which 150 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,894] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-41 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,895] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-29 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,896] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-29 in 151 milliseconds, of which 150 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,897] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-29 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,898] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-0 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,899] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-0 in 153 milliseconds, of which 152 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,899] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-0 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,899] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-33 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,899] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-33 in 152 milliseconds, of which 152 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,900] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-33 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,900] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-21 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,900] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-21 in 151 milliseconds, of which 151 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,900] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-21 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,900] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-25 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,900] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-25 in 150 milliseconds, of which 150 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,900] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-25 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,901] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-11 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,901] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-11 in 146 milliseconds, of which 146 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,901] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-11 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,901] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-44 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,901] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-44 in 146 milliseconds, of which 146 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,901] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-44 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,901] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-15 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,901] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-15 in 144 milliseconds, of which 144 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,902] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-15 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,902] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-48 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,902] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-48 in 141 milliseconds, of which 141 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,902] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-48 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,902] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-3 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,902] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-3 in 135 milliseconds, of which 135 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,902] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-3 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,902] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-36 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,903] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-36 in 133 milliseconds, of which 133 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,903] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-36 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,903] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-7 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,903] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-7 in 133 milliseconds, of which 133 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,903] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-7 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,903] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-40 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,903] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-40 in 133 milliseconds, of which 133 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,903] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-40 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,903] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-28 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,904] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-28 in 132 milliseconds, of which 131 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,904] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-28 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,904] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-32 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,904] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-32 in 122 milliseconds, of which 122 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,904] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-32 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,904] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-20 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,904] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-20 in 108 milliseconds, of which 108 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,904] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-20 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,905] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-24 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,905] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-24 in 104 milliseconds, of which 104 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,905] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-24 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,905] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-10 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,905] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-10 in 88 milliseconds, of which 88 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,905] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-10 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,905] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-43 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,905] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-43 in 86 milliseconds, of which 86 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,906] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-43 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,906] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-14 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,906] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-14 in 83 milliseconds, of which 83 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,906] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-14 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,906] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-47 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,906] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-47 in 82 milliseconds, of which 82 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,906] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-47 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,906] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-2 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,906] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-2 in 81 milliseconds, of which 81 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,907] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-2 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,907] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-35 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,907] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-35 in 81 milliseconds, of which 81 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,907] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-35 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,907] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-6 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,907] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-6 in 80 milliseconds, of which 80 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,907] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-6 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,907] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-39 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,908] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-39 in 79 milliseconds, of which 79 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,908] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-39 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,908] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-27 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,908] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-27 in 79 milliseconds, of which 79 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,908] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-27 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,908] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-31 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,908] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-31 in 77 milliseconds, of which 77 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,908] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-31 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,908] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-19 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,909] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-19 in 76 milliseconds, of which 76 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,909] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-19 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,909] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-23 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,909] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-23 in 76 milliseconds, of which 76 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:12:17,909] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-23 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
control-center                     | [2023-08-04 10:12:17,942] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] End offset for changelog _confluent-command-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:17,943] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-restore-consumer, groupId=null] Assigned to partition(s): _confluent-command-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:17,973] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:18,004] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-command-0 to 0 since the associated topicId changed from null to 26mdvJTUTviIQl0i3yP-hQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:18,005] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:18,154] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Finished restoring changelog _confluent-command-0 to store commander with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:18,160] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Setting topic '_confluent-command' to consume from earliest offset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:18,160] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Seeking to earliest offset of partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:18,168] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Found no committed offset for partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                             | [2023-08-04 10:12:18,167] INFO [TransactionCoordinator id=1] Initialized transactionalId default_ with producerId 3 and producer epoch 0 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
ksqldb-server                      | [2023-08-04 10:12:18,179] INFO [Producer clientId=producer-default_, transactionalId=default_] ProducerId set to 3 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
ksqldb-server                      | [2023-08-04 10:12:18,181] INFO ConsumerConfig values: 
ksqldb-server                      | 	allow.auto.create.topics = true
ksqldb-server                      | 	auto.commit.interval.ms = 5000
ksqldb-server                      | 	auto.include.jmx.reporter = true
ksqldb-server                      | 	auto.offset.reset = none
ksqldb-server                      | 	bootstrap.servers = [broker:29092]
ksqldb-server                      | 	check.crcs = true
ksqldb-server                      | 	client.dns.lookup = use_all_dns_ips
ksqldb-server                      | 	client.id = consumer-null-2
ksqldb-server                      | 	client.rack = 
ksqldb-server                      | 	confluent.proxy.protocol.client.address = null
ksqldb-server                      | 	confluent.proxy.protocol.client.port = null
ksqldb-server                      | 	confluent.proxy.protocol.client.version = NONE
ksqldb-server                      | 	connections.max.idle.ms = 540000
ksqldb-server                      | 	default.api.timeout.ms = 60000
ksqldb-server                      | 	enable.auto.commit = true
ksqldb-server                      | 	exclude.internal.topics = true
ksqldb-server                      | 	fetch.max.bytes = 52428800
ksqldb-server                      | 	fetch.max.wait.ms = 500
ksqldb-server                      | 	fetch.min.bytes = 1
ksqldb-server                      | 	group.id = null
ksqldb-server                      | 	group.instance.id = null
ksqldb-server                      | 	heartbeat.interval.ms = 3000
ksqldb-server                      | 	interceptor.classes = []
ksqldb-server                      | 	internal.leave.group.on.close = true
ksqldb-server                      | 	internal.throw.on.fetch.stable.offset.unsupported = false
ksqldb-server                      | 	isolation.level = read_committed
ksqldb-server                      | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
ksqldb-server                      | 	max.partition.fetch.bytes = 1048576
ksqldb-server                      | 	max.poll.interval.ms = 300000
ksqldb-server                      | 	max.poll.records = 500
ksqldb-server                      | 	metadata.max.age.ms = 300000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	metrics.num.samples = 2
ksqldb-server                      | 	metrics.recording.level = INFO
ksqldb-server                      | 	metrics.sample.window.ms = 30000
ksqldb-server                      | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
ksqldb-server                      | 	receive.buffer.bytes = 65536
ksqldb-server                      | 	reconnect.backoff.max.ms = 1000
ksqldb-server                      | 	reconnect.backoff.ms = 50
ksqldb-server                      | 	request.timeout.ms = 30000
ksqldb-server                      | 	retry.backoff.ms = 100
ksqldb-server                      | 	sasl.client.callback.handler.class = null
ksqldb-server                      | 	sasl.jaas.config = null
ksqldb-server                      | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
ksqldb-server                      | 	sasl.kerberos.min.time.before.relogin = 60000
ksqldb-server                      | 	sasl.kerberos.service.name = null
ksqldb-server                      | 	sasl.kerberos.ticket.renew.jitter = 0.05
ksqldb-server                      | 	sasl.kerberos.ticket.renew.window.factor = 0.8
ksqldb-server                      | 	sasl.login.callback.handler.class = null
ksqldb-server                      | 	sasl.login.class = null
ksqldb-server                      | 	sasl.login.connect.timeout.ms = null
ksqldb-server                      | 	sasl.login.read.timeout.ms = null
ksqldb-server                      | 	sasl.login.refresh.buffer.seconds = 300
ksqldb-server                      | 	sasl.login.refresh.min.period.seconds = 60
ksqldb-server                      | 	sasl.login.refresh.window.factor = 0.8
ksqldb-server                      | 	sasl.login.refresh.window.jitter = 0.05
ksqldb-server                      | 	sasl.login.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.login.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.mechanism = GSSAPI
ksqldb-server                      | 	sasl.oauthbearer.clock.skew.seconds = 30
ksqldb-server                      | 	sasl.oauthbearer.expected.audience = null
ksqldb-server                      | 	sasl.oauthbearer.expected.issuer = null
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.url = null
ksqldb-server                      | 	sasl.oauthbearer.scope.claim.name = scope
ksqldb-server                      | 	sasl.oauthbearer.sub.claim.name = sub
ksqldb-server                      | 	sasl.oauthbearer.token.endpoint.url = null
ksqldb-server                      | 	security.protocol = PLAINTEXT
ksqldb-server                      | 	security.providers = null
ksqldb-server                      | 	send.buffer.bytes = 131072
ksqldb-server                      | 	session.timeout.ms = 45000
ksqldb-server                      | 	socket.connection.setup.timeout.max.ms = 30000
ksqldb-server                      | 	socket.connection.setup.timeout.ms = 10000
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
ksqldb-server                      |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:18,190] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] task [0_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:18,191] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Restoration took 2439 ms for all tasks [0_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:18,194] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:18,226] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71] State transition from REBALANCING to RUNNING (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:12:18,236] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Requesting the log end offset for _confluent-command-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
ksqldb-server                      | [2023-08-04 10:12:18,287] WARN These configurations '[awt.toolkit, log4j.configuration, producer.interceptor.classes, metrics.context.resource.version, metrics.context.resource.kafka.cluster.id, metrics.context.resource.type, metrics.context.resource.commit.id, ksql.server.install.dir, com.sun.management.jmxremote.authenticate, ksql.schema.registry.url, ksql.connect.url, file.encoding, cache.max.bytes.buffering, host.name, config.dir, com.sun.management.jmxremote.ssl, metrics.context.resource.cluster.id, ksql.logging.processing.stream.auto.create, com.sun.management.jmxremote, jdk.debug, listeners, classpath, ksql.logging.processing.topic.auto.create, ksql.log.dir, jdk.vendor.version, ksql.logging.processing.topic.replication.factor, consumer.interceptor.classes]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig)
ksqldb-server                      | [2023-08-04 10:12:18,287] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:18,287] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:18,287] INFO Kafka startTimeMs: 1691143938287 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:18,289] INFO [Consumer clientId=consumer-null-2, groupId=null] Assigned to partition(s): _confluent-ksql-default__command_topic-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
ksqldb-server                      | [2023-08-04 10:12:18,328] INFO [Consumer clientId=consumer-null-2, groupId=null] Resetting the last seen epoch of partition _confluent-ksql-default__command_topic-0 to 0 since the associated topicId changed from null to qszbKrAmRm-xGzAdJbM2PQ (org.apache.kafka.clients.Metadata)
ksqldb-server                      | [2023-08-04 10:12:18,329] INFO [Consumer clientId=consumer-null-2, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
ksqldb-server                      | [2023-08-04 10:12:18,346] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
ksqldb-server                      | [2023-08-04 10:12:18,357] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
ksqldb-server                      | [2023-08-04 10:12:18,358] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:12:18,377] INFO Streams state is RUNNING (io.confluent.command.kafka.CommandStore)
ksqldb-server                      | [2023-08-04 10:12:18,385] INFO App info kafka.consumer for consumer-null-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:18,392] INFO There are no queries assigned to Gen 2 runtimes yet. (io.confluent.ksql.engine.RuntimeAssignor)
ksqldb-server                      | [2023-08-04 10:12:18,399] INFO ProcessingLogConfig values: 
ksqldb-server                      | 	ksql.logging.processing.rows.include = false
ksqldb-server                      | 	ksql.logging.processing.stream.auto.create = false
ksqldb-server                      | 	ksql.logging.processing.stream.name = KSQL_PROCESSING_LOG
ksqldb-server                      | 	ksql.logging.processing.topic.auto.create = false
ksqldb-server                      | 	ksql.logging.processing.topic.name = 
ksqldb-server                      | 	ksql.logging.processing.topic.partitions = 1
ksqldb-server                      | 	ksql.logging.processing.topic.replication.factor = 1
ksqldb-server                      |  (io.confluent.ksql.logging.processing.ProcessingLogConfig)
ksqldb-server                      | [2023-08-04 10:12:18,406] INFO KsqlConfig values: 
ksqldb-server                      | 	ksql.access.validator.enable = auto
ksqldb-server                      | 	ksql.assert.schema.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.assert.topic.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.authorization.cache.expiry.time.secs = 30
ksqldb-server                      | 	ksql.authorization.cache.max.entries = 10000
ksqldb-server                      | 	ksql.cast.strings.preserve.nulls = true
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.file = 
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.reload = false
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.source = NONE
ksqldb-server                      | 	ksql.connect.request.headers.plugin = null
ksqldb-server                      | 	ksql.connect.request.timeout.ms = 5000
ksqldb-server                      | 	ksql.connect.url = http://connect:8083
ksqldb-server                      | 	ksql.connect.worker.config = 
ksqldb-server                      | 	ksql.create.or.replace.enabled = true
ksqldb-server                      | 	ksql.endpoint.migrate.query = true
ksqldb-server                      | 	ksql.error.classifier.regex = 
ksqldb-server                      | 	ksql.extension.dir = ext
ksqldb-server                      | 	ksql.headers.columns.enabled = true
ksqldb-server                      | 	ksql.hidden.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.insert.into.values.enabled = true
ksqldb-server                      | 	ksql.internal.topic.min.insync.replicas = 1
ksqldb-server                      | 	ksql.internal.topic.replicas = 1
ksqldb-server                      | 	ksql.json_sr.converter.deserializer.enabled = true
ksqldb-server                      | 	ksql.lambdas.enabled = true
ksqldb-server                      | 	ksql.metastore.backup.location = 
ksqldb-server                      | 	ksql.metrics.extension = null
ksqldb-server                      | 	ksql.metrics.tags.custom = 
ksqldb-server                      | 	ksql.nested.error.set.null = true
ksqldb-server                      | 	ksql.new.query.planner.enabled = false
ksqldb-server                      | 	ksql.output.topic.name.prefix = 
ksqldb-server                      | 	ksql.persistence.default.format.key = KAFKA
ksqldb-server                      | 	ksql.persistence.default.format.value = null
ksqldb-server                      | 	ksql.persistence.wrap.single.values = null
ksqldb-server                      | 	ksql.persistent.prefix = query_
ksqldb-server                      | 	ksql.properties.overrides.denylist = []
ksqldb-server                      | 	ksql.pull.queries.enable = true
ksqldb-server                      | 	ksql.query.cleanup.shutdown.timeout.ms = 30000
ksqldb-server                      | 	ksql.query.error.max.queue.size = 10
ksqldb-server                      | 	ksql.query.persistent.active.limit = 2147483647
ksqldb-server                      | 	ksql.query.persistent.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.query.pull.enable.standby.reads = false
ksqldb-server                      | 	ksql.query.pull.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.pull.limit.clause.enabled = true
ksqldb-server                      | 	ksql.query.pull.max.allowed.offset.lag = 9223372036854775807
ksqldb-server                      | 	ksql.query.pull.max.concurrent.requests = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.qps = 2147483647
ksqldb-server                      | 	ksql.query.pull.metrics.enabled = true
ksqldb-server                      | 	ksql.query.pull.range.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.router.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.pull.stream.enabled = true
ksqldb-server                      | 	ksql.query.pull.table.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.push.v2.alos.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.catchup.consumer.msg.window = 50
ksqldb-server                      | 	ksql.query.push.v2.continuation.tokens.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.latest.reset.age.ms = 30000
ksqldb-server                      | 	ksql.query.push.v2.max.catchup.consumers = 5
ksqldb-server                      | 	ksql.query.push.v2.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.push.v2.metrics.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.new.latest.delay.ms = 5000
ksqldb-server                      | 	ksql.query.push.v2.registry.installed = false
ksqldb-server                      | 	ksql.query.retry.backoff.initial.ms = 15000
ksqldb-server                      | 	ksql.query.retry.backoff.max.ms = 900000
ksqldb-server                      | 	ksql.query.status.running.threshold.seconds = 300
ksqldb-server                      | 	ksql.query.transient.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.queryanonymizer.cluster_namespace = null
ksqldb-server                      | 	ksql.queryanonymizer.logs_enabled = true
ksqldb-server                      | 	ksql.readonly.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.runtime.feature.shared.enabled = false
ksqldb-server                      | 	ksql.schema.registry.url = http://schema-registry:8081
ksqldb-server                      | 	ksql.security.extension.class = null
ksqldb-server                      | 	ksql.service.id = default_
ksqldb-server                      | 	ksql.shared.runtimes.count = 2
ksqldb-server                      | 	ksql.sink.window.change.log.additional.retention = 1000000
ksqldb-server                      | 	ksql.source.table.materialization.enabled = true
ksqldb-server                      | 	ksql.streams.shutdown.timeout.ms = 300000
ksqldb-server                      | 	ksql.suppress.buffer.size.bytes = -1
ksqldb-server                      | 	ksql.suppress.enabled = true
ksqldb-server                      | 	ksql.timestamp.throw.on.invalid = false
ksqldb-server                      | 	ksql.transient.prefix = transient_
ksqldb-server                      | 	ksql.transient.query.cleanup.service.enable = true
ksqldb-server                      | 	ksql.transient.query.cleanup.service.initial.delay.seconds = 600
ksqldb-server                      | 	ksql.transient.query.cleanup.service.period.seconds = 600
ksqldb-server                      | 	ksql.udf.collect.metrics = false
ksqldb-server                      | 	ksql.udf.enable.security.manager = true
ksqldb-server                      | 	ksql.udfs.enabled = true
ksqldb-server                      | 	ksql.variable.substitution.enable = true
ksqldb-server                      | 	ksql.websocket.connection.max.timeout.ms = 3600000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.util.KsqlConfig)
ksqldb-server                      | [2023-08-04 10:12:18,420] INFO JsonConverterConfig values: 
ksqldb-server                      | 	converter.type = value
ksqldb-server                      | 	decimal.format = NUMERIC
ksqldb-server                      | 	schemas.cache.size = 1000
ksqldb-server                      | 	schemas.enable = false
ksqldb-server                      |  (org.apache.kafka.connect.json.JsonConverterConfig)
ksqldb-server                      | [2023-08-04 10:12:18,423] INFO Source KSQL_PROCESSING_LOG created on the metastore (io.confluent.ksql.metastore.MetaStoreImpl)
ksqldb-server                      | [2023-08-04 10:12:19,026] INFO [Producer clientId=producer-default_, transactionalId=default_] Resetting the last seen epoch of partition _confluent-ksql-default__command_topic-0 to 0 since the associated topicId changed from null to qszbKrAmRm-xGzAdJbM2PQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:19,396] INFO action=started topology=command (io.confluent.controlcenter.application.AllControlCenter)
control-center                     | [2023-08-04 10:12:19,397] INFO action=starting operation=command-migration  (io.confluent.controlcenter.application.AllControlCenter)
control-center                     | [2023-08-04 10:12:19,414] INFO action=completed operation=command-migration (io.confluent.controlcenter.application.AllControlCenter)
control-center                     | [2023-08-04 10:12:19,418] INFO action=starting topology=monitoring (io.confluent.controlcenter.application.AllControlCenter)
control-center                     | [2023-08-04 10:12:19,570] WARN Deprecated config cache.max.bytes.buffering is set, and will be used; we suggest setting the new config statestore.cache.max.bytes instead as deprecated cache.max.bytes.buffering would be removed in the future. (org.apache.kafka.streams.internals.StreamsConfigUtils)
control-center                     | [2023-08-04 10:12:19,766] INFO No process id found on disk, got fresh process id 7115cef0-0c58-4d55-8267-9dbbd81b5514 (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                     | [2023-08-04 10:12:19,790] INFO AdminClientConfig values: 
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-admin
control-center                     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	confluent.use.controller.listener = false
control-center                     | 	connections.max.idle.ms = 300000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                     | [2023-08-04 10:12:19,819] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:19,820] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:19,820] INFO Kafka startTimeMs: 1691143939819 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:19,839] INFO stream-client [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514] Kafka Streams version: 7.4.1-ce (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:12:19,840] INFO stream-client [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514] Kafka Streams commit ID: 96cc303d3f85bf31 (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:12:19,842] WARN Deprecated config cache.max.bytes.buffering is set, and will be used; we suggest setting the new config statestore.cache.max.bytes instead as deprecated cache.max.bytes.buffering would be removed in the future. (org.apache.kafka.streams.internals.StreamsConfigUtils)
control-center                     | [2023-08-04 10:12:19,842] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:19,844] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = true
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-restore-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = null
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:19,872] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:19,872] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:19,878] INFO Kafka startTimeMs: 1691143939872 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:19,880] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:19,885] INFO ProducerConfig values: 
control-center                     | 	acks = -1
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	batch.size = 16384
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffer.memory = 33554432
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-producer
control-center                     | 	compression.type = lz4
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	delivery.timeout.ms = 2147483647
control-center                     | 	enable.idempotence = true
control-center                     | 	interceptor.classes = []
control-center                     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     | 	linger.ms = 500
control-center                     | 	max.block.ms = 9223372036854775807
control-center                     | 	max.in.flight.requests.per.connection = 5
control-center                     | 	max.request.size = 10485760
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metadata.max.idle.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partitioner.adaptive.partitioning.enable = true
control-center                     | 	partitioner.availability.timeout.ms = 0
control-center                     | 	partitioner.class = null
control-center                     | 	partitioner.ignore.keys = false
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	transaction.timeout.ms = 60000
control-center                     | 	transactional.id = null
control-center                     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                     | [2023-08-04 10:12:19,887] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:12:19,929] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:19,929] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:19,929] INFO Kafka startTimeMs: 1691143939929 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:19,930] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:19,932] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = false
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:19,986] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:19,988] WARN stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:19,993] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:20,011] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-producer] ProducerId set to 4 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                     | [2023-08-04 10:12:20,004] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,012] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,012] INFO Kafka startTimeMs: 1691143940004 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,018] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:20,037] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = true
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-restore-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = null
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:20,110] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,110] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,111] INFO Kafka startTimeMs: 1691143940109 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,119] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:20,122] INFO ProducerConfig values: 
control-center                     | 	acks = -1
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	batch.size = 16384
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffer.memory = 33554432
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-producer
control-center                     | 	compression.type = lz4
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	delivery.timeout.ms = 2147483647
control-center                     | 	enable.idempotence = true
control-center                     | 	interceptor.classes = []
control-center                     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     | 	linger.ms = 500
control-center                     | 	max.block.ms = 9223372036854775807
control-center                     | 	max.in.flight.requests.per.connection = 5
control-center                     | 	max.request.size = 10485760
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metadata.max.idle.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partitioner.adaptive.partitioning.enable = true
control-center                     | 	partitioner.availability.timeout.ms = 0
control-center                     | 	partitioner.class = null
control-center                     | 	partitioner.ignore.keys = false
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	transaction.timeout.ms = 60000
control-center                     | 	transactional.id = null
control-center                     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                     | [2023-08-04 10:12:20,140] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:12:20,185] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,186] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,257] INFO Kafka startTimeMs: 1691143940185 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,265] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:20,318] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = false
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:20,354] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:20,377] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-producer] ProducerId set to 5 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                     | [2023-08-04 10:12:20,424] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:20,448] WARN stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:20,453] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:12:20,453] INFO Executing statement: stream/`KSQL_PROCESSING_LOG`/create (io.confluent.ksql.rest.server.computation.CommandRunner)
control-center                     | [2023-08-04 10:12:20,463] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,464] INFO Kafka startTimeMs: 1691143940453 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,480] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:20,481] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = true
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-restore-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = null
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
ksqldb-server                      | [2023-08-04 10:12:20,480] INFO KsqlConfig values: 
ksqldb-server                      | 	ksql.access.validator.enable = auto
ksqldb-server                      | 	ksql.assert.schema.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.assert.topic.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.authorization.cache.expiry.time.secs = 30
ksqldb-server                      | 	ksql.authorization.cache.max.entries = 10000
ksqldb-server                      | 	ksql.cast.strings.preserve.nulls = true
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.file = 
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.reload = false
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.source = NONE
ksqldb-server                      | 	ksql.connect.request.headers.plugin = null
ksqldb-server                      | 	ksql.connect.request.timeout.ms = 5000
ksqldb-server                      | 	ksql.connect.url = http://connect:8083
ksqldb-server                      | 	ksql.connect.worker.config = 
ksqldb-server                      | 	ksql.create.or.replace.enabled = true
ksqldb-server                      | 	ksql.endpoint.migrate.query = true
ksqldb-server                      | 	ksql.error.classifier.regex = 
ksqldb-server                      | 	ksql.extension.dir = ext
ksqldb-server                      | 	ksql.headers.columns.enabled = true
ksqldb-server                      | 	ksql.hidden.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.insert.into.values.enabled = true
ksqldb-server                      | 	ksql.internal.topic.min.insync.replicas = 1
ksqldb-server                      | 	ksql.internal.topic.replicas = 1
ksqldb-server                      | 	ksql.json_sr.converter.deserializer.enabled = true
ksqldb-server                      | 	ksql.lambdas.enabled = true
ksqldb-server                      | 	ksql.metastore.backup.location = 
ksqldb-server                      | 	ksql.metrics.extension = null
ksqldb-server                      | 	ksql.metrics.tags.custom = 
ksqldb-server                      | 	ksql.nested.error.set.null = true
ksqldb-server                      | 	ksql.new.query.planner.enabled = false
ksqldb-server                      | 	ksql.output.topic.name.prefix = 
ksqldb-server                      | 	ksql.persistence.default.format.key = KAFKA
ksqldb-server                      | 	ksql.persistence.default.format.value = null
ksqldb-server                      | 	ksql.persistence.wrap.single.values = null
ksqldb-server                      | 	ksql.persistent.prefix = query_
ksqldb-server                      | 	ksql.properties.overrides.denylist = []
ksqldb-server                      | 	ksql.pull.queries.enable = true
ksqldb-server                      | 	ksql.query.cleanup.shutdown.timeout.ms = 30000
ksqldb-server                      | 	ksql.query.error.max.queue.size = 10
ksqldb-server                      | 	ksql.query.persistent.active.limit = 2147483647
ksqldb-server                      | 	ksql.query.persistent.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.query.pull.enable.standby.reads = false
ksqldb-server                      | 	ksql.query.pull.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.pull.limit.clause.enabled = true
ksqldb-server                      | 	ksql.query.pull.max.allowed.offset.lag = 9223372036854775807
ksqldb-server                      | 	ksql.query.pull.max.concurrent.requests = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.qps = 2147483647
ksqldb-server                      | 	ksql.query.pull.metrics.enabled = true
ksqldb-server                      | 	ksql.query.pull.range.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.router.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.pull.stream.enabled = true
ksqldb-server                      | 	ksql.query.pull.table.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.push.v2.alos.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.catchup.consumer.msg.window = 50
ksqldb-server                      | 	ksql.query.push.v2.continuation.tokens.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.latest.reset.age.ms = 30000
ksqldb-server                      | 	ksql.query.push.v2.max.catchup.consumers = 5
ksqldb-server                      | 	ksql.query.push.v2.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.push.v2.metrics.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.new.latest.delay.ms = 5000
ksqldb-server                      | 	ksql.query.push.v2.registry.installed = false
ksqldb-server                      | 	ksql.query.retry.backoff.initial.ms = 15000
ksqldb-server                      | 	ksql.query.retry.backoff.max.ms = 900000
ksqldb-server                      | 	ksql.query.status.running.threshold.seconds = 300
ksqldb-server                      | 	ksql.query.transient.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.queryanonymizer.cluster_namespace = null
ksqldb-server                      | 	ksql.queryanonymizer.logs_enabled = true
ksqldb-server                      | 	ksql.readonly.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.runtime.feature.shared.enabled = false
ksqldb-server                      | 	ksql.schema.registry.url = http://schema-registry:8081
ksqldb-server                      | 	ksql.security.extension.class = null
ksqldb-server                      | 	ksql.service.id = default_
ksqldb-server                      | 	ksql.shared.runtimes.count = 2
ksqldb-server                      | 	ksql.sink.window.change.log.additional.retention = 1000000
ksqldb-server                      | 	ksql.source.table.materialization.enabled = true
ksqldb-server                      | 	ksql.streams.shutdown.timeout.ms = 300000
ksqldb-server                      | 	ksql.suppress.buffer.size.bytes = -1
ksqldb-server                      | 	ksql.suppress.enabled = true
ksqldb-server                      | 	ksql.timestamp.throw.on.invalid = false
ksqldb-server                      | 	ksql.transient.prefix = transient_
ksqldb-server                      | 	ksql.transient.query.cleanup.service.enable = true
ksqldb-server                      | 	ksql.transient.query.cleanup.service.initial.delay.seconds = 600
ksqldb-server                      | 	ksql.transient.query.cleanup.service.period.seconds = 600
ksqldb-server                      | 	ksql.udf.collect.metrics = false
ksqldb-server                      | 	ksql.udf.enable.security.manager = true
ksqldb-server                      | 	ksql.udfs.enabled = true
ksqldb-server                      | 	ksql.variable.substitution.enable = true
ksqldb-server                      | 	ksql.websocket.connection.max.timeout.ms = 3600000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.util.KsqlConfig)
ksqldb-server                      | [2023-08-04 10:12:20,495] INFO KsqlConfig values: 
ksqldb-server                      | 	ksql.access.validator.enable = auto
ksqldb-server                      | 	ksql.assert.schema.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.assert.topic.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.authorization.cache.expiry.time.secs = 30
ksqldb-server                      | 	ksql.authorization.cache.max.entries = 10000
ksqldb-server                      | 	ksql.cast.strings.preserve.nulls = true
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.file = 
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.reload = false
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.source = NONE
ksqldb-server                      | 	ksql.connect.request.headers.plugin = null
ksqldb-server                      | 	ksql.connect.request.timeout.ms = 5000
ksqldb-server                      | 	ksql.connect.url = http://connect:8083
ksqldb-server                      | 	ksql.connect.worker.config = 
ksqldb-server                      | 	ksql.create.or.replace.enabled = true
ksqldb-server                      | 	ksql.endpoint.migrate.query = true
ksqldb-server                      | 	ksql.error.classifier.regex = 
ksqldb-server                      | 	ksql.extension.dir = ext
ksqldb-server                      | 	ksql.headers.columns.enabled = true
ksqldb-server                      | 	ksql.hidden.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.insert.into.values.enabled = true
ksqldb-server                      | 	ksql.internal.topic.min.insync.replicas = 1
ksqldb-server                      | 	ksql.internal.topic.replicas = 1
ksqldb-server                      | 	ksql.json_sr.converter.deserializer.enabled = true
ksqldb-server                      | 	ksql.lambdas.enabled = true
ksqldb-server                      | 	ksql.metastore.backup.location = 
ksqldb-server                      | 	ksql.metrics.extension = null
ksqldb-server                      | 	ksql.metrics.tags.custom = 
ksqldb-server                      | 	ksql.nested.error.set.null = true
ksqldb-server                      | 	ksql.new.query.planner.enabled = false
ksqldb-server                      | 	ksql.output.topic.name.prefix = 
ksqldb-server                      | 	ksql.persistence.default.format.key = KAFKA
ksqldb-server                      | 	ksql.persistence.default.format.value = null
ksqldb-server                      | 	ksql.persistence.wrap.single.values = null
ksqldb-server                      | 	ksql.persistent.prefix = query_
ksqldb-server                      | 	ksql.properties.overrides.denylist = []
ksqldb-server                      | 	ksql.pull.queries.enable = true
ksqldb-server                      | 	ksql.query.cleanup.shutdown.timeout.ms = 30000
ksqldb-server                      | 	ksql.query.error.max.queue.size = 10
ksqldb-server                      | 	ksql.query.persistent.active.limit = 2147483647
ksqldb-server                      | 	ksql.query.persistent.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.query.pull.enable.standby.reads = false
ksqldb-server                      | 	ksql.query.pull.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.pull.limit.clause.enabled = true
ksqldb-server                      | 	ksql.query.pull.max.allowed.offset.lag = 9223372036854775807
ksqldb-server                      | 	ksql.query.pull.max.concurrent.requests = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.qps = 2147483647
ksqldb-server                      | 	ksql.query.pull.metrics.enabled = true
ksqldb-server                      | 	ksql.query.pull.range.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.router.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.pull.stream.enabled = true
ksqldb-server                      | 	ksql.query.pull.table.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.push.v2.alos.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.catchup.consumer.msg.window = 50
ksqldb-server                      | 	ksql.query.push.v2.continuation.tokens.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.latest.reset.age.ms = 30000
ksqldb-server                      | 	ksql.query.push.v2.max.catchup.consumers = 5
ksqldb-server                      | 	ksql.query.push.v2.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.push.v2.metrics.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.new.latest.delay.ms = 5000
ksqldb-server                      | 	ksql.query.push.v2.registry.installed = false
ksqldb-server                      | 	ksql.query.retry.backoff.initial.ms = 15000
ksqldb-server                      | 	ksql.query.retry.backoff.max.ms = 900000
ksqldb-server                      | 	ksql.query.status.running.threshold.seconds = 300
ksqldb-server                      | 	ksql.query.transient.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.queryanonymizer.cluster_namespace = null
ksqldb-server                      | 	ksql.queryanonymizer.logs_enabled = true
ksqldb-server                      | 	ksql.readonly.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.runtime.feature.shared.enabled = false
ksqldb-server                      | 	ksql.schema.registry.url = http://schema-registry:8081
ksqldb-server                      | 	ksql.security.extension.class = null
ksqldb-server                      | 	ksql.service.id = default_
ksqldb-server                      | 	ksql.shared.runtimes.count = 2
ksqldb-server                      | 	ksql.sink.window.change.log.additional.retention = 1000000
ksqldb-server                      | 	ksql.source.table.materialization.enabled = true
ksqldb-server                      | 	ksql.streams.shutdown.timeout.ms = 300000
ksqldb-server                      | 	ksql.suppress.buffer.size.bytes = -1
ksqldb-server                      | 	ksql.suppress.enabled = true
ksqldb-server                      | 	ksql.timestamp.throw.on.invalid = false
ksqldb-server                      | 	ksql.transient.prefix = transient_
ksqldb-server                      | 	ksql.transient.query.cleanup.service.enable = true
ksqldb-server                      | 	ksql.transient.query.cleanup.service.initial.delay.seconds = 600
ksqldb-server                      | 	ksql.transient.query.cleanup.service.period.seconds = 600
ksqldb-server                      | 	ksql.udf.collect.metrics = false
ksqldb-server                      | 	ksql.udf.enable.security.manager = true
ksqldb-server                      | 	ksql.udfs.enabled = true
ksqldb-server                      | 	ksql.variable.substitution.enable = true
ksqldb-server                      | 	ksql.websocket.connection.max.timeout.ms = 3600000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.util.KsqlConfig)
ksqldb-server                      | [2023-08-04 10:12:20,497] INFO Source KSQL_PROCESSING_LOG created on the metastore (io.confluent.ksql.metastore.MetaStoreImpl)
control-center                     | [2023-08-04 10:12:20,515] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,515] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,516] INFO Kafka startTimeMs: 1691143940515 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,516] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:20,522] INFO ProducerConfig values: 
control-center                     | 	acks = -1
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	batch.size = 16384
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffer.memory = 33554432
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-producer
control-center                     | 	compression.type = lz4
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	delivery.timeout.ms = 2147483647
control-center                     | 	enable.idempotence = true
control-center                     | 	interceptor.classes = []
control-center                     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     | 	linger.ms = 500
control-center                     | 	max.block.ms = 9223372036854775807
control-center                     | 	max.in.flight.requests.per.connection = 5
control-center                     | 	max.request.size = 10485760
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metadata.max.idle.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partitioner.adaptive.partitioning.enable = true
control-center                     | 	partitioner.availability.timeout.ms = 0
control-center                     | 	partitioner.class = null
control-center                     | 	partitioner.ignore.keys = false
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	transaction.timeout.ms = 60000
control-center                     | 	transactional.id = null
control-center                     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     |  (org.apache.kafka.clients.producer.ProducerConfig)
ksqldb-server                      | [2023-08-04 10:12:20,525] INFO [Producer clientId=producer-default_, transactionalId=default_] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
ksqldb-server                      | [2023-08-04 10:12:20,542] INFO Executed statement: stream/`KSQL_PROCESSING_LOG`/create (io.confluent.ksql.rest.server.computation.CommandRunner)
control-center                     | [2023-08-04 10:12:20,544] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
ksqldb-server                      | [2023-08-04 10:12:20,549] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
ksqldb-server                      | [2023-08-04 10:12:20,550] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
ksqldb-server                      | [2023-08-04 10:12:20,550] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
ksqldb-server                      | [2023-08-04 10:12:20,551] INFO App info kafka.producer for producer-default_ unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,558] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,559] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,560] INFO Kafka startTimeMs: 1691143940558 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,561] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:20,564] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = false
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
ksqldb-server                      | [2023-08-04 10:12:20,565] INFO Processed successfully: KsqlRequest{configOverrides={}, requestProperties={}, commandSequenceNumber=Optional.empty} (fb4c327f-85bb-3fe8-b8e2-45b9d778598c): CREATE STREAM stream1 (column1 VARCHAR, column2 VARCHAR, column3 BIGINT, column4 STRUCT<INT, STRUCT<VARCHAR, VARCHAR, VARCHAR, ARRAY<VARCHAR>, VARCHAR>, STRUCT<VARCHAR, VARCHAR, ARRAY<VARCHAR>>, STRUCT<VARCHAR>, STRUCT<VARCHAR, VARCHAR, VARCHAR, ARRAY<VARCHAR>, VARCHAR>, STRUCT<VARCHAR, VARCHAR, ARRAY<VARCHAR>>>) WITH (KAFKA_TOPIC='[string]', VALUE_FORMAT='[string]', KEY_FORMAT='[string]'); (io.confluent.ksql.logging.query.QueryLogger)
ksqldb-server                      | [2023-08-04 10:12:20,566] INFO Successfully created processing log stream. (io.confluent.ksql.rest.server.KsqlRestApplication)
ksqldb-server                      | [2023-08-04 10:12:20,567] INFO ksql server took PT17.344361S to become ready (io.confluent.ksql.rest.server.KsqlRestApplicationMetrics)
ksqldb-server                      | [2023-08-04 10:12:20,573] INFO No customer ID configured -- falling back to id 'anonymous' (io.confluent.support.metrics.BaseSupportConfig)
ksqldb-server                      | [2023-08-04 10:12:20,574] WARN Enforcing customer ID 'anonymous' (io.confluent.support.metrics.PhoneHomeConfig)
control-center                     | [2023-08-04 10:12:20,621] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:20,622] WARN stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
ksqldb-server                      | [2023-08-04 10:12:20,628] WARN Please note that the version check feature of KSQL is enabled.  With this enabled, this instance is configured to collect and report anonymously the version information to Confluent, Inc. ("Confluent") or its parent, subsidiaries, affiliates or service providers every 24hours.  This Metadata may be transferred to any country in which Confluent maintains facilities.  For a more in depth discussion of how Confluent processes such information, please read our Privacy Policy located at http://www.confluent.io/privacy. By proceeding with `confluent.support.metrics.enable=true`, you agree to all such collection, transfer and use of Version information by Confluent. You can turn the version check  feature off by setting `confluent.support.metrics.enable=false` in the KSQL configuration and restarting the KSQL.  See the Confluent Platform documentation for further information. (io.confluent.ksql.version.metrics.KsqlVersionCheckerAgent)
ksqldb-server                      | [2023-08-04 10:12:20,630] INFO ksqlDB API server listening on http://0.0.0.0:8088 (io.confluent.ksql.rest.server.KsqlRestApplication)
ksqldb-server                      | [2023-08-04 10:12:20,633] INFO Server up and running (io.confluent.ksql.rest.server.KsqlServerMain)
control-center                     | [2023-08-04 10:12:20,635] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,635] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,636] INFO Kafka startTimeMs: 1691143940635 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,643] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
ksqldb-server                      | [2023-08-04 10:12:20,637] INFO Waiting until monitored service is ready for metrics collection (io.confluent.support.metrics.BaseMetricsReporter)
ksqldb-server                      | [2023-08-04 10:12:20,644] INFO Monitored service is now ready (io.confluent.support.metrics.BaseMetricsReporter)
ksqldb-server                      | [2023-08-04 10:12:20,644] INFO Attempting to collect and submit metrics (io.confluent.support.metrics.BaseMetricsReporter)
control-center                     | [2023-08-04 10:12:20,643] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:20,652] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-producer] ProducerId set to 6 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                     | [2023-08-04 10:12:20,654] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = true
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-restore-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = null
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:20,674] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,675] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,676] INFO Kafka startTimeMs: 1691143940673 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,677] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:20,680] INFO ProducerConfig values: 
control-center                     | 	acks = -1
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	batch.size = 16384
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffer.memory = 33554432
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-producer
control-center                     | 	compression.type = lz4
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	delivery.timeout.ms = 2147483647
control-center                     | 	enable.idempotence = true
control-center                     | 	interceptor.classes = []
control-center                     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     | 	linger.ms = 500
control-center                     | 	max.block.ms = 9223372036854775807
control-center                     | 	max.in.flight.requests.per.connection = 5
control-center                     | 	max.request.size = 10485760
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metadata.max.idle.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partitioner.adaptive.partitioning.enable = true
control-center                     | 	partitioner.availability.timeout.ms = 0
control-center                     | 	partitioner.class = null
control-center                     | 	partitioner.ignore.keys = false
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	transaction.timeout.ms = 60000
control-center                     | 	transactional.id = null
control-center                     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                     | [2023-08-04 10:12:20,685] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:12:20,740] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,740] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,755] INFO Kafka startTimeMs: 1691143940739 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:20,756] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:20,759] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = false
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:20,812] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:20,814] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-producer] ProducerId set to 7 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                     | [2023-08-04 10:12:20,911] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:20,913] WARN stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:20,993] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,022] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,033] INFO Kafka startTimeMs: 1691143940993 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,088] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:21,100] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = true
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-restore-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = null
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:21,141] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,141] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,141] INFO Kafka startTimeMs: 1691143941141 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,142] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:21,153] INFO ProducerConfig values: 
control-center                     | 	acks = -1
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	batch.size = 16384
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffer.memory = 33554432
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-producer
control-center                     | 	compression.type = lz4
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	delivery.timeout.ms = 2147483647
control-center                     | 	enable.idempotence = true
control-center                     | 	interceptor.classes = []
control-center                     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     | 	linger.ms = 500
control-center                     | 	max.block.ms = 9223372036854775807
control-center                     | 	max.in.flight.requests.per.connection = 5
control-center                     | 	max.request.size = 10485760
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metadata.max.idle.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partitioner.adaptive.partitioning.enable = true
control-center                     | 	partitioner.availability.timeout.ms = 0
control-center                     | 	partitioner.class = null
control-center                     | 	partitioner.ignore.keys = false
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	transaction.timeout.ms = 60000
control-center                     | 	transactional.id = null
control-center                     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                     | [2023-08-04 10:12:21,155] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:12:21,181] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,183] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,184] INFO Kafka startTimeMs: 1691143941181 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,185] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:21,188] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = false
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:21,253] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:21,258] WARN stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:21,263] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,269] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,269] INFO Kafka startTimeMs: 1691143941262 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,272] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:21,274] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = true
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-restore-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = null
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:21,283] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,283] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,283] INFO Kafka startTimeMs: 1691143941283 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,284] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:21,255] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:21,291] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-producer] ProducerId set to 8 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                     | [2023-08-04 10:12:21,295] INFO ProducerConfig values: 
control-center                     | 	acks = -1
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	batch.size = 16384
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffer.memory = 33554432
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-producer
control-center                     | 	compression.type = lz4
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	delivery.timeout.ms = 2147483647
control-center                     | 	enable.idempotence = true
control-center                     | 	interceptor.classes = []
control-center                     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     | 	linger.ms = 500
control-center                     | 	max.block.ms = 9223372036854775807
control-center                     | 	max.in.flight.requests.per.connection = 5
control-center                     | 	max.request.size = 10485760
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metadata.max.idle.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partitioner.adaptive.partitioning.enable = true
control-center                     | 	partitioner.availability.timeout.ms = 0
control-center                     | 	partitioner.class = null
control-center                     | 	partitioner.ignore.keys = false
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	transaction.timeout.ms = 60000
control-center                     | 	transactional.id = null
control-center                     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                     | [2023-08-04 10:12:21,320] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:12:21,355] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,357] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,357] INFO Kafka startTimeMs: 1691143941355 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,440] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:21,442] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = false
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:21,488] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:21,505] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-producer] ProducerId set to 9 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                     | [2023-08-04 10:12:21,491] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:21,507] WARN stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:21,528] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,528] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,529] INFO Kafka startTimeMs: 1691143941528 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,545] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:21,565] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = true
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-restore-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = null
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:21,630] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,639] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,640] INFO Kafka startTimeMs: 1691143941630 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,642] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:21,644] INFO ProducerConfig values: 
control-center                     | 	acks = -1
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	batch.size = 16384
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffer.memory = 33554432
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-producer
control-center                     | 	compression.type = lz4
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	delivery.timeout.ms = 2147483647
control-center                     | 	enable.idempotence = true
control-center                     | 	interceptor.classes = []
control-center                     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     | 	linger.ms = 500
control-center                     | 	max.block.ms = 9223372036854775807
control-center                     | 	max.in.flight.requests.per.connection = 5
control-center                     | 	max.request.size = 10485760
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metadata.max.idle.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partitioner.adaptive.partitioning.enable = true
control-center                     | 	partitioner.availability.timeout.ms = 0
control-center                     | 	partitioner.class = null
control-center                     | 	partitioner.ignore.keys = false
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	transaction.timeout.ms = 60000
control-center                     | 	transactional.id = null
control-center                     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                     | [2023-08-04 10:12:21,650] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:12:21,671] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,672] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,673] INFO Kafka startTimeMs: 1691143941671 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,674] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:21,681] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = false
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:21,723] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:21,731] WARN stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:21,730] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:21,752] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-producer] ProducerId set to 10 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                     | [2023-08-04 10:12:21,736] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,754] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,755] INFO Kafka startTimeMs: 1691143941736 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,760] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:21,769] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = true
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-restore-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = null
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:21,802] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,803] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,804] INFO Kafka startTimeMs: 1691143941802 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,806] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:21,810] INFO ProducerConfig values: 
control-center                     | 	acks = -1
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	batch.size = 16384
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffer.memory = 33554432
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-producer
control-center                     | 	compression.type = lz4
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	delivery.timeout.ms = 2147483647
control-center                     | 	enable.idempotence = true
control-center                     | 	interceptor.classes = []
control-center                     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     | 	linger.ms = 500
control-center                     | 	max.block.ms = 9223372036854775807
control-center                     | 	max.in.flight.requests.per.connection = 5
control-center                     | 	max.request.size = 10485760
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metadata.max.idle.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partitioner.adaptive.partitioning.enable = true
control-center                     | 	partitioner.availability.timeout.ms = 0
control-center                     | 	partitioner.class = null
control-center                     | 	partitioner.ignore.keys = false
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	transaction.timeout.ms = 60000
control-center                     | 	transactional.id = null
control-center                     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                     | [2023-08-04 10:12:21,816] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:12:21,846] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,847] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,848] INFO Kafka startTimeMs: 1691143941846 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,849] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:21,851] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = false
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:21,872] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:21,879] WARN stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:21,900] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,901] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,901] INFO Kafka startTimeMs: 1691143941900 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,906] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:21,912] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = true
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-restore-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = null
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:21,939] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:21,963] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-producer] ProducerId set to 11 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                     | [2023-08-04 10:12:21,959] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,976] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,977] INFO Kafka startTimeMs: 1691143941959 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:21,979] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:21,981] INFO ProducerConfig values: 
control-center                     | 	acks = -1
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	batch.size = 16384
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffer.memory = 33554432
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-producer
control-center                     | 	compression.type = lz4
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	delivery.timeout.ms = 2147483647
control-center                     | 	enable.idempotence = true
control-center                     | 	interceptor.classes = []
control-center                     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     | 	linger.ms = 500
control-center                     | 	max.block.ms = 9223372036854775807
control-center                     | 	max.in.flight.requests.per.connection = 5
control-center                     | 	max.request.size = 10485760
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metadata.max.idle.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partitioner.adaptive.partitioning.enable = true
control-center                     | 	partitioner.availability.timeout.ms = 0
control-center                     | 	partitioner.class = null
control-center                     | 	partitioner.ignore.keys = false
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	transaction.timeout.ms = 60000
control-center                     | 	transactional.id = null
control-center                     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                     | [2023-08-04 10:12:22,000] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:12:22,044] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,045] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,046] INFO Kafka startTimeMs: 1691143942044 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,048] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:22,056] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = false
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:22,103] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:22,111] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-producer] ProducerId set to 12 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                     | [2023-08-04 10:12:22,119] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:22,130] WARN stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:22,134] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,143] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,150] INFO Kafka startTimeMs: 1691143942134 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,155] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:22,186] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = true
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-restore-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = null
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:22,608] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,615] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,616] INFO Kafka startTimeMs: 1691143942607 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,618] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:22,631] INFO ProducerConfig values: 
control-center                     | 	acks = -1
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	batch.size = 16384
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffer.memory = 33554432
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-producer
control-center                     | 	compression.type = lz4
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	delivery.timeout.ms = 2147483647
control-center                     | 	enable.idempotence = true
control-center                     | 	interceptor.classes = []
control-center                     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     | 	linger.ms = 500
control-center                     | 	max.block.ms = 9223372036854775807
control-center                     | 	max.in.flight.requests.per.connection = 5
control-center                     | 	max.request.size = 10485760
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metadata.max.idle.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partitioner.adaptive.partitioning.enable = true
control-center                     | 	partitioner.availability.timeout.ms = 0
control-center                     | 	partitioner.class = null
control-center                     | 	partitioner.ignore.keys = false
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	transaction.timeout.ms = 60000
control-center                     | 	transactional.id = null
control-center                     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                     | [2023-08-04 10:12:22,638] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:12:22,683] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,690] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,691] INFO Kafka startTimeMs: 1691143942683 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,723] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:22,725] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = false
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:22,755] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:22,769] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-producer] ProducerId set to 13 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                     | [2023-08-04 10:12:22,800] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:22,810] WARN stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:22,815] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,824] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,824] INFO Kafka startTimeMs: 1691143942815 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,828] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:22,830] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = true
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-restore-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = null
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:22,838] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,839] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,839] INFO Kafka startTimeMs: 1691143942838 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,839] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:22,841] INFO ProducerConfig values: 
control-center                     | 	acks = -1
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	batch.size = 16384
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffer.memory = 33554432
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-producer
control-center                     | 	compression.type = lz4
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	delivery.timeout.ms = 2147483647
control-center                     | 	enable.idempotence = true
control-center                     | 	interceptor.classes = []
control-center                     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     | 	linger.ms = 500
control-center                     | 	max.block.ms = 9223372036854775807
control-center                     | 	max.in.flight.requests.per.connection = 5
control-center                     | 	max.request.size = 10485760
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metadata.max.idle.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partitioner.adaptive.partitioning.enable = true
control-center                     | 	partitioner.availability.timeout.ms = 0
control-center                     | 	partitioner.class = null
control-center                     | 	partitioner.ignore.keys = false
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	transaction.timeout.ms = 60000
control-center                     | 	transactional.id = null
control-center                     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                     | [2023-08-04 10:12:22,842] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:12:22,851] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,851] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,851] INFO Kafka startTimeMs: 1691143942851 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,852] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:22,853] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = false
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:22,862] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:22,862] WARN stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:22,866] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,866] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,866] INFO Kafka startTimeMs: 1691143942866 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,869] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:22,871] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = true
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-restore-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = null
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:22,882] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,882] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,882] INFO Kafka startTimeMs: 1691143942882 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,883] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:22,884] INFO ProducerConfig values: 
control-center                     | 	acks = -1
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	batch.size = 16384
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	buffer.memory = 33554432
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-producer
control-center                     | 	compression.type = lz4
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	delivery.timeout.ms = 2147483647
control-center                     | 	enable.idempotence = true
control-center                     | 	interceptor.classes = []
control-center                     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     | 	linger.ms = 500
control-center                     | 	max.block.ms = 9223372036854775807
control-center                     | 	max.in.flight.requests.per.connection = 5
control-center                     | 	max.request.size = 10485760
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metadata.max.idle.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partitioner.adaptive.partitioning.enable = true
control-center                     | 	partitioner.availability.timeout.ms = 0
control-center                     | 	partitioner.class = null
control-center                     | 	partitioner.ignore.keys = false
control-center                     | 	receive.buffer.bytes = 32768
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	transaction.timeout.ms = 60000
control-center                     | 	transactional.id = null
control-center                     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                     |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                     | [2023-08-04 10:12:22,886] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:12:22,932] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:22,932] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,942] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,943] INFO Kafka startTimeMs: 1691143942932 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,944] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:22,945] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-producer] ProducerId set to 14 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                     | [2023-08-04 10:12:22,955] INFO ConsumerConfig values: 
control-center                     | 	allow.auto.create.topics = false
control-center                     | 	auto.commit.interval.ms = 5000
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	auto.offset.reset = none
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	check.crcs = true
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer
control-center                     | 	client.rack = 
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	connections.max.idle.ms = 540000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	enable.auto.commit = false
control-center                     | 	exclude.internal.topics = true
control-center                     | 	fetch.max.bytes = 52428800
control-center                     | 	fetch.max.wait.ms = 500
control-center                     | 	fetch.min.bytes = 1
control-center                     | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                     | 	group.instance.id = null
control-center                     | 	heartbeat.interval.ms = 3000
control-center                     | 	interceptor.classes = []
control-center                     | 	internal.leave.group.on.close = false
control-center                     | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                     | 	isolation.level = read_uncommitted
control-center                     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     | 	max.partition.fetch.bytes = 1048576
control-center                     | 	max.poll.interval.ms = 21600000
control-center                     | 	max.poll.records = 100
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	session.timeout.ms = 60000
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                     | [2023-08-04 10:12:22,972] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:22,972] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-producer] ProducerId set to 15 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                     | [2023-08-04 10:12:22,981] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:22,983] WARN stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                     | [2023-08-04 10:12:22,987] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,987] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:22,991] INFO Kafka startTimeMs: 1691143942987 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:23,004] INFO stream-client [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514] State transition from CREATED to REBALANCING (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:12:23,011] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,011] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,028] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,029] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,030] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,030] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,033] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,034] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,034] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,034] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,040] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,040] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,041] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,041] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,050] INFO stream-client [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514] Started 12 stream threads (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:12:23,050] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,049] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,052] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,053] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,054] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                     | [2023-08-04 10:12:23,055] INFO tocheck=[Store{name=group-aggregate-store, rollup=true}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=TriggerEventsStore, rollup=false}, Store{name=Group, rollup=true}, Store{name=AlertHistoryStore, rollup=false}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                     | [2023-08-04 10:12:23,063] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                     | [2023-08-04 10:12:23,047] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,088] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,046] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:23,087] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:23,087] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:23,087] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:23,087] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:23,087] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:23,087] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:23,087] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:23,087] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:23,059] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,283] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,054] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,313] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,313] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:23,306] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 6A1t9PL6S9ibcqZ61sSDAQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,277] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 6A1t9PL6S9ibcqZ61sSDAQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,324] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to 5zxMVZraQkS0mMTEISnuCA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,324] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,324] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to mNO1aiiEQ1aUPUFDBwdR-w (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,273] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 6A1t9PL6S9ibcqZ61sSDAQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,331] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to 5zxMVZraQkS0mMTEISnuCA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,238] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 6A1t9PL6S9ibcqZ61sSDAQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,181] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 6A1t9PL6S9ibcqZ61sSDAQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,177] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 6A1t9PL6S9ibcqZ61sSDAQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,173] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 6A1t9PL6S9ibcqZ61sSDAQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,162] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 6A1t9PL6S9ibcqZ61sSDAQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,334] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to 5zxMVZraQkS0mMTEISnuCA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,334] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to 5zxMVZraQkS0mMTEISnuCA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,108] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:23,334] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to 5zxMVZraQkS0mMTEISnuCA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,334] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to 5zxMVZraQkS0mMTEISnuCA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,334] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to 5zxMVZraQkS0mMTEISnuCA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,335] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,334] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,331] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to RTYUY2zsS26iu53cLq8UPg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,331] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to 5zxMVZraQkS0mMTEISnuCA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,337] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to mNO1aiiEQ1aUPUFDBwdR-w (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,337] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to RTYUY2zsS26iu53cLq8UPg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,338] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,338] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,338] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,338] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,339] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,339] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,339] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,340] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,340] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,340] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,340] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,341] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,341] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to I5wV8QWRRJuTL-yY1xI20g (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,341] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to TL8DPoV0R9OJhSSwRHqD6A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,342] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to jGx_3gCrTDmXXO7DlfFZkw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,342] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to _zkEEKNOSh-BwqO66EWICw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,343] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to g31qr5BRRCeyT-Ctg5GhYw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,343] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to wbsqiFi3RFiBKXg6x7JVQg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,343] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to qrRJJzmBRu6JLAarYC2FXQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,344] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to z2ogKsUbRf6X0CRLuVUTWw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,344] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to NGyXVef5Tbicz4WJpOfTdg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,345] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,349] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,351] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,324] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 6A1t9PL6S9ibcqZ61sSDAQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,323] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:23,337] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,337] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,355] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to mNO1aiiEQ1aUPUFDBwdR-w (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,337] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to mNO1aiiEQ1aUPUFDBwdR-w (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,355] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to RTYUY2zsS26iu53cLq8UPg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,355] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to RTYUY2zsS26iu53cLq8UPg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,355] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to 5zxMVZraQkS0mMTEISnuCA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,336] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,355] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,355] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to mNO1aiiEQ1aUPUFDBwdR-w (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,355] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to mNO1aiiEQ1aUPUFDBwdR-w (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,355] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to RTYUY2zsS26iu53cLq8UPg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,355] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to RTYUY2zsS26iu53cLq8UPg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,355] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,355] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,336] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,336] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,334] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,358] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to mNO1aiiEQ1aUPUFDBwdR-w (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,359] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to RTYUY2zsS26iu53cLq8UPg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,359] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,360] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,360] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,360] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,361] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,362] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,362] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,363] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,364] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,364] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,366] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,366] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,367] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to I5wV8QWRRJuTL-yY1xI20g (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,369] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to TL8DPoV0R9OJhSSwRHqD6A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,370] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to jGx_3gCrTDmXXO7DlfFZkw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,371] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to _zkEEKNOSh-BwqO66EWICw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,371] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to g31qr5BRRCeyT-Ctg5GhYw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,372] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to wbsqiFi3RFiBKXg6x7JVQg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,372] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to qrRJJzmBRu6JLAarYC2FXQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,373] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to z2ogKsUbRf6X0CRLuVUTWw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,373] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to NGyXVef5Tbicz4WJpOfTdg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,374] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,375] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,356] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,355] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,381] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,355] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,383] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,383] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,384] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,385] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,387] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,355] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,355] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,390] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,390] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,391] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,391] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,392] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,393] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,393] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,394] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,395] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,386] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,396] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,396] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,408] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,409] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,409] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,410] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,410] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,410] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,411] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to I5wV8QWRRJuTL-yY1xI20g (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,411] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to TL8DPoV0R9OJhSSwRHqD6A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,411] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to jGx_3gCrTDmXXO7DlfFZkw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,412] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to _zkEEKNOSh-BwqO66EWICw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,376] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,358] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to mNO1aiiEQ1aUPUFDBwdR-w (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,358] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to mNO1aiiEQ1aUPUFDBwdR-w (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,413] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to RTYUY2zsS26iu53cLq8UPg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,413] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to RTYUY2zsS26iu53cLq8UPg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,414] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,414] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,414] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,415] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,415] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,415] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,415] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,415] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,416] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,416] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,416] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,416] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,416] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,416] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to I5wV8QWRRJuTL-yY1xI20g (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,417] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to TL8DPoV0R9OJhSSwRHqD6A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,412] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to g31qr5BRRCeyT-Ctg5GhYw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,405] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,405] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,394] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,394] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,419] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,420] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,420] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,421] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,422] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to I5wV8QWRRJuTL-yY1xI20g (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,423] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to TL8DPoV0R9OJhSSwRHqD6A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,423] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to jGx_3gCrTDmXXO7DlfFZkw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,423] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to _zkEEKNOSh-BwqO66EWICw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,424] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to g31qr5BRRCeyT-Ctg5GhYw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,424] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to wbsqiFi3RFiBKXg6x7JVQg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,424] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to qrRJJzmBRu6JLAarYC2FXQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,425] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to z2ogKsUbRf6X0CRLuVUTWw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,418] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to jGx_3gCrTDmXXO7DlfFZkw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,414] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,426] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,426] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,427] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,427] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,427] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,428] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,429] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,430] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,430] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,431] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to NGyXVef5Tbicz4WJpOfTdg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,432] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,436] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,438] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,425] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to wbsqiFi3RFiBKXg6x7JVQg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,422] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,419] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,442] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,443] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,443] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,443] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,444] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,444] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to I5wV8QWRRJuTL-yY1xI20g (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,444] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to TL8DPoV0R9OJhSSwRHqD6A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,445] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to jGx_3gCrTDmXXO7DlfFZkw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,445] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to _zkEEKNOSh-BwqO66EWICw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,445] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to g31qr5BRRCeyT-Ctg5GhYw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,446] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to wbsqiFi3RFiBKXg6x7JVQg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,433] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,431] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to _zkEEKNOSh-BwqO66EWICw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,431] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,447] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,444] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,448] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to I5wV8QWRRJuTL-yY1xI20g (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,449] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to qrRJJzmBRu6JLAarYC2FXQ (org.apache.kafka.clients.Metadata)
broker                             | [2023-08-04 10:12:23,452] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer-a8ed36a1-4563-481b-a92c-9c234f0c0d0a and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:23,448] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to g31qr5BRRCeyT-Ctg5GhYw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,446] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to qrRJJzmBRu6JLAarYC2FXQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,447] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,454] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to z2ogKsUbRf6X0CRLuVUTWw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,453] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,455] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,456] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,456] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to I5wV8QWRRJuTL-yY1xI20g (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,457] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to TL8DPoV0R9OJhSSwRHqD6A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,457] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to jGx_3gCrTDmXXO7DlfFZkw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,457] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to z2ogKsUbRf6X0CRLuVUTWw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,458] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to NGyXVef5Tbicz4WJpOfTdg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,458] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to NGyXVef5Tbicz4WJpOfTdg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,457] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,458] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to wbsqiFi3RFiBKXg6x7JVQg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,458] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to TL8DPoV0R9OJhSSwRHqD6A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,458] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to _zkEEKNOSh-BwqO66EWICw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,460] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to qrRJJzmBRu6JLAarYC2FXQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,506] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to z2ogKsUbRf6X0CRLuVUTWw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,505] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to jGx_3gCrTDmXXO7DlfFZkw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,506] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to _zkEEKNOSh-BwqO66EWICw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,507] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to g31qr5BRRCeyT-Ctg5GhYw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,507] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to wbsqiFi3RFiBKXg6x7JVQg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,508] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to qrRJJzmBRu6JLAarYC2FXQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,509] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to z2ogKsUbRf6X0CRLuVUTWw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,505] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,503] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,460] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,511] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,512] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,512] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,496] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,515] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,515] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,492] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer-a8ed36a1-4563-481b-a92c-9c234f0c0d0a (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,515] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to NGyXVef5Tbicz4WJpOfTdg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,491] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,518] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 6A1t9PL6S9ibcqZ61sSDAQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,519] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to 5zxMVZraQkS0mMTEISnuCA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,520] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,520] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to mNO1aiiEQ1aUPUFDBwdR-w (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,524] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to RTYUY2zsS26iu53cLq8UPg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,460] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,490] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,486] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,460] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to g31qr5BRRCeyT-Ctg5GhYw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,527] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to wbsqiFi3RFiBKXg6x7JVQg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,532] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to qrRJJzmBRu6JLAarYC2FXQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,532] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to z2ogKsUbRf6X0CRLuVUTWw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,532] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to NGyXVef5Tbicz4WJpOfTdg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,534] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,535] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,536] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,526] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,516] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to I5wV8QWRRJuTL-yY1xI20g (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,542] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,542] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to TL8DPoV0R9OJhSSwRHqD6A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,542] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to jGx_3gCrTDmXXO7DlfFZkw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,542] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to _zkEEKNOSh-BwqO66EWICw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,543] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to g31qr5BRRCeyT-Ctg5GhYw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,543] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to wbsqiFi3RFiBKXg6x7JVQg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,543] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to qrRJJzmBRu6JLAarYC2FXQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,543] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to z2ogKsUbRf6X0CRLuVUTWw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,543] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to NGyXVef5Tbicz4WJpOfTdg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,545] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                             | [2023-08-04 10:12:23,546] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer-9a87f21e-0ffe-49cc-bddd-d3eff7c842d3 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:23,508] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to NGyXVef5Tbicz4WJpOfTdg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,548] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,559] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,541] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,538] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,537] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,532] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 6A1t9PL6S9ibcqZ61sSDAQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,530] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                             | [2023-08-04 10:12:23,562] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer-df05ac14-bbec-4941-8a2a-676e6eae0166 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:23,529] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 6A1t9PL6S9ibcqZ61sSDAQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,564] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to 5zxMVZraQkS0mMTEISnuCA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,565] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,566] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to mNO1aiiEQ1aUPUFDBwdR-w (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,567] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,568] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,568] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,569] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,569] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to 5zxMVZraQkS0mMTEISnuCA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,570] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,570] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to mNO1aiiEQ1aUPUFDBwdR-w (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,571] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to RTYUY2zsS26iu53cLq8UPg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,572] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,572] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,573] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,573] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,573] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,549] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer-9a87f21e-0ffe-49cc-bddd-d3eff7c842d3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,569] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,568] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to RTYUY2zsS26iu53cLq8UPg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,575] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,576] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
broker                             | [2023-08-04 10:12:23,580] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 0 (__consumer_offsets-5) (reason: Adding new member _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer-a8ed36a1-4563-481b-a92c-9c234f0c0d0a with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:23,579] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,580] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,580] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,580] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,580] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,581] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,581] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,581] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,582] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,583] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,583] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
broker                             | [2023-08-04 10:12:23,584] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1 generation 1 (__consumer_offsets-5) with 1 members (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:23,583] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,584] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,584] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,584] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to I5wV8QWRRJuTL-yY1xI20g (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,585] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to TL8DPoV0R9OJhSSwRHqD6A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,585] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to jGx_3gCrTDmXXO7DlfFZkw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,584] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,586] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,586] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,587] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,587] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to _zkEEKNOSh-BwqO66EWICw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,588] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to I5wV8QWRRJuTL-yY1xI20g (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,588] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to TL8DPoV0R9OJhSSwRHqD6A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,589] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to jGx_3gCrTDmXXO7DlfFZkw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,590] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to _zkEEKNOSh-BwqO66EWICw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,594] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to g31qr5BRRCeyT-Ctg5GhYw (org.apache.kafka.clients.Metadata)
broker                             | [2023-08-04 10:12:23,595] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in CompletingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer-fc7e04c9-507a-4f4d-bc6e-afaddd9f6818 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:23,597] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to wbsqiFi3RFiBKXg6x7JVQg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,600] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to qrRJJzmBRu6JLAarYC2FXQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,588] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to g31qr5BRRCeyT-Ctg5GhYw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,601] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to wbsqiFi3RFiBKXg6x7JVQg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,601] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to qrRJJzmBRu6JLAarYC2FXQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,599] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,599] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,596] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=1, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer-a8ed36a1-4563-481b-a92c-9c234f0c0d0a', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,589] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,604] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,605] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,606] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,606] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,602] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to z2ogKsUbRf6X0CRLuVUTWw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,608] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,608] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to NGyXVef5Tbicz4WJpOfTdg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,608] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,608] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to z2ogKsUbRf6X0CRLuVUTWw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,609] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,615] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,615] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to NGyXVef5Tbicz4WJpOfTdg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,615] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer-fc7e04c9-507a-4f4d-bc6e-afaddd9f6818 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,618] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,618] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,618] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to I5wV8QWRRJuTL-yY1xI20g (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,620] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer-df05ac14-bbec-4941-8a2a-676e6eae0166 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,622] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to TL8DPoV0R9OJhSSwRHqD6A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,621] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,624] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,620] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,629] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,625] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,625] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,625] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to jGx_3gCrTDmXXO7DlfFZkw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,625] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,643] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to _zkEEKNOSh-BwqO66EWICw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,644] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,644] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to g31qr5BRRCeyT-Ctg5GhYw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,648] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to wbsqiFi3RFiBKXg6x7JVQg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,648] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to qrRJJzmBRu6JLAarYC2FXQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,649] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,645] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,650] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to z2ogKsUbRf6X0CRLuVUTWw (org.apache.kafka.clients.Metadata)
broker                             | [2023-08-04 10:12:23,651] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 1 (__consumer_offsets-5) (reason: Adding new member _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer-9a87f21e-0ffe-49cc-bddd-d3eff7c842d3 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:23,651] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to NGyXVef5Tbicz4WJpOfTdg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:23,654] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:23,664] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
broker                             | [2023-08-04 10:12:23,665] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer-f9a123c7-cd0e-44fb-a431-232976a16d29 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:23,665] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,667] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,676] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer-f9a123c7-cd0e-44fb-a431-232976a16d29 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,677] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,677] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                             | [2023-08-04 10:12:23,688] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer-6ad7c37e-7d89-49d1-ae00-40a920173199 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:23,697] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer-6ad7c37e-7d89-49d1-ae00-40a920173199 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,706] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,707] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,722] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,746] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                             | [2023-08-04 10:12:23,755] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer-1d3c1642-2e7f-4de0-ae96-6bc906c9a146 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:23,770] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer-1d3c1642-2e7f-4de0-ae96-6bc906c9a146 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,772] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,773] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,782] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,789] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,790] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                             | [2023-08-04 10:12:23,798] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer-db36686e-c2fa-4f3f-b32c-f46fff38ebde and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:12:23,844] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer-f872c169-bcc8-4d34-b85e-0cbd983f608d and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:12:23,852] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer-73e8bccf-ffe4-47cf-b879-69c384de9268 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:23,872] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer-db36686e-c2fa-4f3f-b32c-f46fff38ebde (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,873] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,873] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,859] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer-73e8bccf-ffe4-47cf-b879-69c384de9268 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,899] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,909] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                             | [2023-08-04 10:12:23,921] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer-9647d00a-ef67-4863-8743-a793e5ddb72c and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:23,850] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer-f872c169-bcc8-4d34-b85e-0cbd983f608d (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,942] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,942] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,952] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer-9647d00a-ef67-4863-8743-a793e5ddb72c (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,953] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                             | [2023-08-04 10:12:23,956] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer-dbde5110-3cab-4fe6-97fc-a0e3c47b902b and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:23,963] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:23,956] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,035] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer-dbde5110-3cab-4fe6-97fc-a0e3c47b902b (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,045] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,059] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,069] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                     | [2023-08-04 10:12:24,070] INFO tocheck=[Store{name=group-aggregate-store, rollup=true}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=TriggerEventsStore, rollup=false}, Store{name=Group, rollup=true}, Store{name=AlertHistoryStore, rollup=false}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                     | [2023-08-04 10:12:24,070] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                     | [2023-08-04 10:12:24,173] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer] All members participating in this rebalance: 
control-center                     | 7115cef0-0c58-4d55-8267-9dbbd81b5514: [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer-a8ed36a1-4563-481b-a92c-9c234f0c0d0a]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,217] INFO Decided on assignment: {7115cef0-0c58-4d55-8267-9dbbd81b5514=[activeTasks: ([0_0, 1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 10_0, 10_1, 10_2, 10_3, 10_4, 10_5, 10_6, 10_7, 10_8, 10_9, 10_10, 10_11, 11_0, 12_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([1_0=0, 2_0=0, 3_0=0, 4_0=0, 5_0=0, 6_0=0, 7_0=0, 8_0=0, 9_0=0, 11_0=0, 12_0=0]) clientTags: ([]) capacity: 1 assigned: 24]} with no followup probing rebalance. (org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor)
control-center                     | [2023-08-04 10:12:24,218] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer] Assigned tasks [1_0, 10_9, 10_8, 2_0, 10_7, 3_0, 10_6, 4_0, 10_5, 5_0, 10_4, 6_0, 10_3, 7_0, 10_2, 8_0, 10_1, 9_0, 10_0, 11_0, 12_0, 10_11, 0_0, 10_10] including stateful [1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 11_0, 12_0] to clients as: 
control-center                     | 7115cef0-0c58-4d55-8267-9dbbd81b5514=[activeTasks: ([0_0, 1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 10_0, 10_1, 10_2, 10_3, 10_4, 10_5, 10_6, 10_7, 10_8, 10_9, 10_10, 10_11, 11_0, 12_0]) standbyTasks: ([])]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,221] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer] Client 7115cef0-0c58-4d55-8267-9dbbd81b5514 per-consumer assignment:
control-center                     | 	prev owned active {}
control-center                     | 	prev owned standby {_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer-a8ed36a1-4563-481b-a92c-9c234f0c0d0a=[]}
control-center                     | 	assigned active {_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer-a8ed36a1-4563-481b-a92c-9c234f0c0d0a=[1_0, 10_9, 2_0, 10_8, 3_0, 10_7, 4_0, 10_6, 5_0, 10_5, 6_0, 10_4, 7_0, 10_3, 8_0, 10_2, 9_0, 10_1, 10_0, 11_0, 12_0, 10_11, 0_0, 10_10]}
control-center                     | 	revoking active {}
control-center                     | 	assigned standby {}
control-center                     |  (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,224] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,224] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Finished assignment for group at generation 1: {_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer-a8ed36a1-4563-481b-a92c-9c234f0c0d0a=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, _confluent-metrics-0, _confluent-metrics-1, _confluent-metrics-2, _confluent-metrics-3, _confluent-metrics-4, _confluent-metrics-5, _confluent-metrics-6, _confluent-metrics-7, _confluent-metrics-8, _confluent-metrics-9, _confluent-metrics-10, _confluent-metrics-11, _confluent-monitoring-0], userDataSize=352)} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,264] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=1, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer-a8ed36a1-4563-481b-a92c-9c234f0c0d0a', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,265] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group is rebalancing, so a rejoin is needed.' (RebalanceInProgressException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,265] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                             | [2023-08-04 10:12:24,274] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1 generation 2 (__consumer_offsets-5) with 12 members (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:24,281] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer-dbde5110-3cab-4fe6-97fc-a0e3c47b902b', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,282] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer-6ad7c37e-7d89-49d1-ae00-40a920173199', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,283] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer-73e8bccf-ffe4-47cf-b879-69c384de9268', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,285] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer-9a87f21e-0ffe-49cc-bddd-d3eff7c842d3', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,287] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer-fc7e04c9-507a-4f4d-bc6e-afaddd9f6818', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,303] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer-df05ac14-bbec-4941-8a2a-676e6eae0166', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,304] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer-f9a123c7-cd0e-44fb-a431-232976a16d29', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,319] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer-1d3c1642-2e7f-4de0-ae96-6bc906c9a146', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,322] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer-db36686e-c2fa-4f3f-b32c-f46fff38ebde', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,324] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer-a8ed36a1-4563-481b-a92c-9c234f0c0d0a', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,322] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer-9647d00a-ef67-4863-8743-a793e5ddb72c', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,330] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer-f872c169-bcc8-4d34-b85e-0cbd983f608d', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,489] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer] All members participating in this rebalance: 
control-center                     | 7115cef0-0c58-4d55-8267-9dbbd81b5514: [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer-a8ed36a1-4563-481b-a92c-9c234f0c0d0a, _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer-1d3c1642-2e7f-4de0-ae96-6bc906c9a146, _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer-9647d00a-ef67-4863-8743-a793e5ddb72c, _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer-db36686e-c2fa-4f3f-b32c-f46fff38ebde, _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer-6ad7c37e-7d89-49d1-ae00-40a920173199, _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer-fc7e04c9-507a-4f4d-bc6e-afaddd9f6818, _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer-f9a123c7-cd0e-44fb-a431-232976a16d29, _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer-73e8bccf-ffe4-47cf-b879-69c384de9268, _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer-9a87f21e-0ffe-49cc-bddd-d3eff7c842d3, _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer-df05ac14-bbec-4941-8a2a-676e6eae0166, _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer-dbde5110-3cab-4fe6-97fc-a0e3c47b902b, _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer-f872c169-bcc8-4d34-b85e-0cbd983f608d]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,492] INFO Decided on assignment: {7115cef0-0c58-4d55-8267-9dbbd81b5514=[activeTasks: ([0_0, 1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 10_0, 10_1, 10_2, 10_3, 10_4, 10_5, 10_6, 10_7, 10_8, 10_9, 10_10, 10_11, 11_0, 12_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([1_0=0, 2_0=0, 3_0=0, 4_0=0, 5_0=0, 6_0=0, 7_0=0, 8_0=0, 9_0=0, 11_0=0, 12_0=0]) clientTags: ([]) capacity: 12 assigned: 24]} with no followup probing rebalance. (org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor)
control-center                     | [2023-08-04 10:12:24,492] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer] Assigned tasks [1_0, 10_9, 10_8, 2_0, 10_7, 3_0, 10_6, 4_0, 10_5, 5_0, 10_4, 6_0, 10_3, 7_0, 10_2, 8_0, 10_1, 9_0, 10_0, 11_0, 12_0, 10_11, 0_0, 10_10] including stateful [1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 11_0, 12_0] to clients as: 
control-center                     | 7115cef0-0c58-4d55-8267-9dbbd81b5514=[activeTasks: ([0_0, 1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 10_0, 10_1, 10_2, 10_3, 10_4, 10_5, 10_6, 10_7, 10_8, 10_9, 10_10, 10_11, 11_0, 12_0]) standbyTasks: ([])]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,510] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer] Client 7115cef0-0c58-4d55-8267-9dbbd81b5514 per-consumer assignment:
control-center                     | 	prev owned active {}
control-center                     | 	prev owned standby {_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer-a8ed36a1-4563-481b-a92c-9c234f0c0d0a=[], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer-1d3c1642-2e7f-4de0-ae96-6bc906c9a146=[], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer-9647d00a-ef67-4863-8743-a793e5ddb72c=[], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer-db36686e-c2fa-4f3f-b32c-f46fff38ebde=[], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer-6ad7c37e-7d89-49d1-ae00-40a920173199=[], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer-fc7e04c9-507a-4f4d-bc6e-afaddd9f6818=[], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer-f9a123c7-cd0e-44fb-a431-232976a16d29=[], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer-73e8bccf-ffe4-47cf-b879-69c384de9268=[], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer-9a87f21e-0ffe-49cc-bddd-d3eff7c842d3=[], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer-df05ac14-bbec-4941-8a2a-676e6eae0166=[], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer-dbde5110-3cab-4fe6-97fc-a0e3c47b902b=[], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer-f872c169-bcc8-4d34-b85e-0cbd983f608d=[]}
control-center                     | 	assigned active {_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer-a8ed36a1-4563-481b-a92c-9c234f0c0d0a=[1_0, 0_0], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer-1d3c1642-2e7f-4de0-ae96-6bc906c9a146=[2_0, 10_0], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer-9647d00a-ef67-4863-8743-a793e5ddb72c=[3_0, 10_1], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer-db36686e-c2fa-4f3f-b32c-f46fff38ebde=[4_0, 10_2], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer-6ad7c37e-7d89-49d1-ae00-40a920173199=[5_0, 10_3], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer-fc7e04c9-507a-4f4d-bc6e-afaddd9f6818=[6_0, 10_4], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer-f9a123c7-cd0e-44fb-a431-232976a16d29=[10_5, 7_0], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer-73e8bccf-ffe4-47cf-b879-69c384de9268=[10_6, 8_0], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer-9a87f21e-0ffe-49cc-bddd-d3eff7c842d3=[10_7, 9_0], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer-df05ac14-bbec-4941-8a2a-676e6eae0166=[10_8, 11_0], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer-dbde5110-3cab-4fe6-97fc-a0e3c47b902b=[10_9, 12_0], _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer-f872c169-bcc8-4d34-b85e-0cbd983f608d=[10_11, 10_10]}
control-center                     | 	revoking active {}
control-center                     | 	assigned standby {}
control-center                     |  (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,518] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,519] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Finished assignment for group at generation 2: {_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer-6ad7c37e-7d89-49d1-ae00-40a920173199=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, _confluent-metrics-3], userDataSize=64), _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer-9a87f21e-0ffe-49cc-bddd-d3eff7c842d3=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, _confluent-metrics-7], userDataSize=88), _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer-9647d00a-ef67-4863-8743-a793e5ddb72c=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, _confluent-metrics-1], userDataSize=64), _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer-1d3c1642-2e7f-4de0-ae96-6bc906c9a146=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-metrics-0], userDataSize=64), _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer-73e8bccf-ffe4-47cf-b879-69c384de9268=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, _confluent-metrics-6], userDataSize=64), _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer-dbde5110-3cab-4fe6-97fc-a0e3c47b902b=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, _confluent-metrics-9], userDataSize=64), _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer-db36686e-c2fa-4f3f-b32c-f46fff38ebde=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-metrics-2], userDataSize=64), _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer-fc7e04c9-507a-4f4d-bc6e-afaddd9f6818=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, _confluent-metrics-4], userDataSize=64), _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer-df05ac14-bbec-4941-8a2a-676e6eae0166=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, _confluent-metrics-8], userDataSize=64), _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer-a8ed36a1-4563-481b-a92c-9c234f0c0d0a=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, _confluent-monitoring-0], userDataSize=64), _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer-f9a123c7-cd0e-44fb-a431-232976a16d29=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, _confluent-metrics-5], userDataSize=64), _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer-f872c169-bcc8-4d34-b85e-0cbd983f608d=Assignment(partitions=[_confluent-metrics-10, _confluent-metrics-11], userDataSize=64)} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                             | [2023-08-04 10:12:24,525] INFO [GroupCoordinator 1]: Assignment received from leader _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer-a8ed36a1-4563-481b-a92c-9c234f0c0d0a for group _confluent-controlcenter-7-4-1-1 for generation 2. The group has 12 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
control-center                     | [2023-08-04 10:12:24,537] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer-73e8bccf-ffe4-47cf-b879-69c384de9268', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,540] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, _confluent-metrics-6], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,548] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,539] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer-9647d00a-ef67-4863-8743-a793e5ddb72c', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,539] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer-fc7e04c9-507a-4f4d-bc6e-afaddd9f6818', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,559] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, _confluent-metrics-4], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,558] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, _confluent-metrics-1], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,561] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,551] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Handle new assignment with:
control-center                     | 	New active tasks: [10_6, 8_0]
control-center                     | 	New standby tasks: []
control-center                     | 	Existing active tasks: []
control-center                     | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                     | [2023-08-04 10:12:24,556] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer-a8ed36a1-4563-481b-a92c-9c234f0c0d0a', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,592] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, _confluent-monitoring-0], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,548] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer-db36686e-c2fa-4f3f-b32c-f46fff38ebde', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,548] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer-df05ac14-bbec-4941-8a2a-676e6eae0166', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,546] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer-f9a123c7-cd0e-44fb-a431-232976a16d29', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,543] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer-9a87f21e-0ffe-49cc-bddd-d3eff7c842d3', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,543] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer-1d3c1642-2e7f-4de0-ae96-6bc906c9a146', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,542] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer-dbde5110-3cab-4fe6-97fc-a0e3c47b902b', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,542] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer-6ad7c37e-7d89-49d1-ae00-40a920173199', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,595] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, _confluent-metrics-5], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,595] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, _confluent-metrics-3], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,595] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,595] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,595] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Handle new assignment with:
control-center                     | 	New active tasks: [10_5, 7_0]
control-center                     | 	New standby tasks: []
control-center                     | 	Existing active tasks: []
control-center                     | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                     | [2023-08-04 10:12:24,595] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Handle new assignment with:
control-center                     | 	New active tasks: [5_0, 10_3]
control-center                     | 	New standby tasks: []
control-center                     | 	Existing active tasks: []
control-center                     | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                     | [2023-08-04 10:12:24,584] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Handle new assignment with:
control-center                     | 	New active tasks: [3_0, 10_1]
control-center                     | 	New standby tasks: []
control-center                     | 	Existing active tasks: []
control-center                     | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                     | [2023-08-04 10:12:24,561] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,618] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Handle new assignment with:
control-center                     | 	New active tasks: [10_4, 6_0]
control-center                     | 	New standby tasks: []
control-center                     | 	Existing active tasks: []
control-center                     | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                     | [2023-08-04 10:12:24,613] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, _confluent-metrics-7], userDataSize=88) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,598] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-metrics-0], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,596] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-metrics-2], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,619] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,620] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,598] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,621] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Handle new assignment with:
control-center                     | 	New active tasks: [1_0, 0_0]
control-center                     | 	New standby tasks: []
control-center                     | 	Existing active tasks: []
control-center                     | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                     | [2023-08-04 10:12:24,622] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,597] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, _confluent-metrics-9], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,623] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,624] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Handle new assignment with:
control-center                     | 	New active tasks: [10_9, 12_0]
control-center                     | 	New standby tasks: []
control-center                     | 	Existing active tasks: []
control-center                     | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                     | [2023-08-04 10:12:24,597] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, _confluent-metrics-8], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,597] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer-f872c169-bcc8-4d34-b85e-0cbd983f608d', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,643] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-metrics-10, _confluent-metrics-11], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,644] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,644] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Handle new assignment with:
control-center                     | 	New active tasks: [10_11, 10_10]
control-center                     | 	New standby tasks: []
control-center                     | 	Existing active tasks: []
control-center                     | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                     | [2023-08-04 10:12:24,644] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                     | [2023-08-04 10:12:24,645] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Handle new assignment with:
control-center                     | 	New active tasks: [10_8, 11_0]
control-center                     | 	New standby tasks: []
control-center                     | 	Existing active tasks: []
control-center                     | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                     | [2023-08-04 10:12:24,625] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Handle new assignment with:
control-center                     | 	New active tasks: [10_7, 9_0]
control-center                     | 	New standby tasks: []
control-center                     | 	Existing active tasks: []
control-center                     | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                     | [2023-08-04 10:12:24,621] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Handle new assignment with:
control-center                     | 	New active tasks: [2_0, 10_0]
control-center                     | 	New standby tasks: []
control-center                     | 	Existing active tasks: []
control-center                     | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                     | [2023-08-04 10:12:24,620] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Handle new assignment with:
control-center                     | 	New active tasks: [4_0, 10_2]
control-center                     | 	New standby tasks: []
control-center                     | 	Existing active tasks: []
control-center                     | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                     | [2023-08-04 10:12:24,666] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, _confluent-metrics-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,667] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:24,718] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
data-agrigator-taskmanager-1       | 2023-08-04 10:12:24,784 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:12:24,808 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                     | [2023-08-04 10:12:24,836] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, _confluent-metrics-6 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,836] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:24,844] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-metrics-10, _confluent-metrics-11 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,847] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:24,852] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, _confluent-monitoring-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,854] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:24,854] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,868] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] task [10_11] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:24,868] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] task [10_10] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:24,855] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] task [10_6] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:24,886] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,922] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-metrics-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,934] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:24,934] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, _confluent-metrics-5 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,937] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:24,938] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] task [10_11] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:24,927] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, _confluent-metrics-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,940] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:24,942] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] task [10_1] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:24,932] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, _confluent-metrics-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,954] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:24,932] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, _confluent-metrics-8 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,958] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:24,970] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] task [10_10] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:24,971] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Restoration took 124 ms for all tasks [10_11, 10_10] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:24,991] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:24,996] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-10 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:24,998] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-11 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:25,012] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,017] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] stream-task [11_0] State store MetricsAggregateStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:25,040] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] task [11_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,040] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] task [10_8] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:24,979] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] stream-task [6_0] State store MonitoringStream-ONE_MINUTE did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:25,042] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] task [6_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,043] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] task [10_4] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:24,983] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,982] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,977] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, _confluent-metrics-9 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:24,973] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, _confluent-metrics-7 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,047] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:25,038] INFO ControlCenterBoundedMemoryConfig values: 
control-center                     | 	rocksdb.cache.limit.strict = false
control-center                     | 	rocksdb.cache.size = 16106127360
control-center                     | 	rocksdb.cache.size.limit.enabled = false
control-center                     | 	rocksdb.index.filter.block.ratio = 0.0
control-center                     | 	rocksdb.write.buffer.cache.use = false
control-center                     | 	rocksdb.write.buffer.size = 5368709120
control-center                     |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center                     | [2023-08-04 10:12:25,012] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] stream-task [3_0] State store MonitoringMessageAggregatorWindows-ONE_MINUTE did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:25,057] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] task [3_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:24,998] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-metrics-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,058] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:24,996] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,062] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] task [10_5] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:24,993] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] task [10_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:24,993] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,070] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                     | [2023-08-04 10:12:25,083] INFO tocheck=[Store{name=group-aggregate-store, rollup=true}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=TriggerEventsStore, rollup=false}, Store{name=Group, rollup=true}, Store{name=AlertHistoryStore, rollup=false}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                     | [2023-08-04 10:12:25,084] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                     | [2023-08-04 10:12:25,065] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] task [10_8] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,057] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] task [10_4] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,049] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:25,087] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,092] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,093] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,084] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-10 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,104] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-11 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,109] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] End offset for changelog _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:25,118] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:25,113] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:25,127] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:25,129] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:25,142] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,145] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] task [10_2] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,180] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:25,182] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,206] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] stream-task [7_0] State store MonitoringStream-THREE_HOURS did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:25,219] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] task [7_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,216] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] task [10_9] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,214] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] task [10_7] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,207] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] stream-task [8_0] State store Group-ONE_MINUTE did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:25,225] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] stream-task [8_0] State store Group-THREE_HOURS did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:25,226] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] task [8_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,240] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 to 0 since the associated topicId changed from null to Gcm9HudQQIePkLpcnSZatA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:25,243] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] task [10_1] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,261] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:25,266] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:25,276] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:25,289] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:25,291] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] task [10_5] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,292] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] task [10_6] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,300] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to ZjoO_4HfTKWmowjlTjE6sw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:25,303] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:25,331] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:25,332] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:25,341] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] stream-task [5_0] State store MonitoringMessageAggregatorWindows-THREE_HOURS did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:25,341] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] task [5_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,341] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] task [10_3] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,368] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to HFZ8vq7-RZyhNR6Sk4e1ng (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:25,359] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] End offset for changelog _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:25,371] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] End offset for changelog _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:25,345] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:25,320] INFO ControlCenterBoundedMemoryConfig values: 
control-center                     | 	rocksdb.cache.limit.strict = false
control-center                     | 	rocksdb.cache.size = 16106127360
control-center                     | 	rocksdb.cache.size.limit.enabled = false
control-center                     | 	rocksdb.index.filter.block.ratio = 0.0
control-center                     | 	rocksdb.write.buffer.cache.use = false
control-center                     | 	rocksdb.write.buffer.size = 5368709120
control-center                     |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center                     | [2023-08-04 10:12:25,388] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:25,371] INFO Restore started for store [MetricsAggregateStore] with topic-partition [_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:25,371] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:25,442] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to 8kdvQ-ncQWWnL9X7ZnXiBA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:25,431] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:25,448] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:25,449] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:25,499] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] task [10_3] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,506] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] stream-task [2_0] State store aggregatedTopicPartitionTableWindows-ONE_MINUTE did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:25,517] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] stream-task [2_0] Initializing to the starting offset for changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 of in-memory state store group-aggregate-store-ONE_MINUTE (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:25,517] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] task [2_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,496] INFO Restore started for store [MonitoringStream-ONE_MINUTE] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:25,506] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to Ly7r4MYYR8iZXnKz11AfxQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:25,530] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to UbhlmVOjQAusw3dsEKYqeQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:25,531] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:25,533] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 to store MetricsAggregateStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:25,534] INFO Restore completed for store [MetricsAggregateStore] and topic-partition [_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:25,541] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] stream-task [4_0] State store aggregatedTopicPartitionTableWindows-THREE_HOURS did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:25,541] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] stream-task [4_0] Initializing to the starting offset for changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 of in-memory state store group-aggregate-store-THREE_HOURS (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:25,541] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] task [4_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,543] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,501] INFO Restore started for store [MonitoringMessageAggregatorWindows-ONE_MINUTE] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:25,544] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:25,571] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:25,573] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] task [10_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,574] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:25,573] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,482] INFO Restore started for store [MonitoringStream-THREE_HOURS] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:25,592] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] End offset for changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:25,592] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] End offset for changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:25,594] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:25,599] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:25,600] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:25,602] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] stream-task [1_0] State store MonitoringVerifierStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:25,602] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] stream-task [1_0] Initializing to the starting offset for changelog _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 of in-memory state store aggregate-topic-partition-store (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:25,627] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] stream-task [1_0] Initializing to the starting offset for changelog _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 of in-memory state store monitoring-aggregate-rekey-store (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:25,628] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] task [1_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,629] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] task [0_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,630] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to tnG6SGoITzy4Bdh8lieE1g (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:25,592] INFO Restore started for store [Group-ONE_MINUTE] with topic-partition [_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:25,640] INFO Restore started for store [Group-THREE_HOURS] with topic-partition [_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:25,638] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 to store MonitoringStream-ONE_MINUTE with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:25,651] INFO Restore completed for store [MonitoringStream-ONE_MINUTE] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:25,653] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] task [11_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,653] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Restoration took 694 ms for all tasks [10_8, 11_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:25,625] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to xgvCZkJ3RMurzOd2tG9eqg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:25,613] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] task [10_2] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,700] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:25,700] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to gpn2CyU5S3CepZalgg_6ug (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:25,707] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:25,716] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 to store MonitoringStream-THREE_HOURS with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:25,719] INFO Restore completed for store [MonitoringStream-THREE_HOURS] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:25,695] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 to store MonitoringMessageAggregatorWindows-ONE_MINUTE with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:25,663] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:25,663] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,737] INFO Restore completed for store [MonitoringMessageAggregatorWindows-ONE_MINUTE] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:25,750] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,724] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-8 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:25,960] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,961] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:25,990] INFO Restore started for store [MonitoringMessageAggregatorWindows-THREE_HOURS] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:25,957] INFO Restore started for store [aggregatedTopicPartitionTableWindows-ONE_MINUTE] with topic-partition [_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,015] INFO Restore started for store [group-aggregate-store-ONE_MINUTE] with topic-partition [_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:25,952] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,945] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] task [0_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:25,940] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] End offset for changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:25,746] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:25,743] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Finished restoring changelog _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 to store Group-ONE_MINUTE with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,023] INFO Restore completed for store [Group-ONE_MINUTE] and topic-partition [_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,025] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Finished restoring changelog _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 to store Group-THREE_HOURS with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,026] INFO Restore completed for store [Group-THREE_HOURS] and topic-partition [_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,030] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,050] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] End offset for changelog _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,051] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,054] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] End offset for changelog _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,055] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,056] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,057] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,058] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,006] INFO ControlCenterBoundedMemoryConfig values: 
control-center                     | 	rocksdb.cache.limit.strict = false
control-center                     | 	rocksdb.cache.size = 16106127360
control-center                     | 	rocksdb.cache.size.limit.enabled = false
control-center                     | 	rocksdb.index.filter.block.ratio = 0.0
control-center                     | 	rocksdb.write.buffer.cache.use = false
control-center                     | 	rocksdb.write.buffer.size = 5368709120
control-center                     |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center                     | [2023-08-04 10:12:26,003] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,044] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,070] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-8 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,073] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,039] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,036] INFO ControlCenterBoundedMemoryConfig values: 
control-center                     | 	rocksdb.cache.limit.strict = false
control-center                     | 	rocksdb.cache.size = 16106127360
control-center                     | 	rocksdb.cache.size.limit.enabled = false
control-center                     | 	rocksdb.index.filter.block.ratio = 0.0
control-center                     | 	rocksdb.write.buffer.cache.use = false
control-center                     | 	rocksdb.write.buffer.size = 5368709120
control-center                     |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center                     | [2023-08-04 10:12:26,085] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 to 0 since the associated topicId changed from null to 27wkC6UWTRWT8JkErrPTLw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:26,034] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] task [6_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:26,086] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Restoration took 1419 ms for all tasks [6_0, 10_4] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,086] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,033] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] task [7_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:26,087] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Restoration took 1149 ms for all tasks [10_5, 7_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,087] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,088] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,089] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-5 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,091] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,027] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] End offset for changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,104] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,104] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,105] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,095] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] task [8_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:26,089] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,085] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                     | [2023-08-04 10:12:26,130] INFO tocheck=[Store{name=group-aggregate-store, rollup=true}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=TriggerEventsStore, rollup=false}, Store{name=Group, rollup=true}, Store{name=AlertHistoryStore, rollup=false}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                     | [2023-08-04 10:12:26,130] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                     | [2023-08-04 10:12:26,076] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] task [3_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:26,132] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Restoration took 1191 ms for all tasks [10_1, 3_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,133] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,134] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,135] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-1 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,142] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Restoration took 1306 ms for all tasks [8_0, 10_6] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,143] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,144] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,150] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,151] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-1 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,152] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,153] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,130] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,154] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-5 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,154] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,155] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,127] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 to 0 since the associated topicId changed from null to tkBvdtlJRWily0KDogf0lg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:26,119] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Finished restoring changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 to store aggregatedTopicPartitionTableWindows-ONE_MINUTE with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,153] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] stream-task [9_0] State store KSTREAM-OUTERTHIS-0000000105-store did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:26,151] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-6 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,150] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to P4lKo9gcStGZyM8e3fWDyw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:26,166] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] stream-task [9_0] State store KSTREAM-OUTEROTHER-0000000106-store did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:26,166] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to xYBgTiSgQeeJ02q7jNXaDg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:26,148] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 to store MonitoringMessageAggregatorWindows-THREE_HOURS with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,135] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-4 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,168] INFO Restore completed for store [MonitoringMessageAggregatorWindows-THREE_HOURS] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,166] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] stream-task [9_0] State store MonitoringTriggerStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:26,170] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] task [9_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:26,166] INFO Restore completed for store [aggregatedTopicPartitionTableWindows-ONE_MINUTE] and topic-partition [_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,172] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Finished restoring changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 to store group-aggregate-store-ONE_MINUTE with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,172] INFO Restore completed for store [group-aggregate-store-ONE_MINUTE] and topic-partition [_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,186] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,195] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-4 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,198] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,170] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 to 0 since the associated topicId changed from null to adG-RfPuSaSjxyV6nKsuDw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:26,187] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,172] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:26,202] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] task [10_7] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:26,210] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,216] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:26,221] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,227] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,229] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] task [5_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:26,233] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Restoration took 1279 ms for all tasks [5_0, 10_3] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,231] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,240] INFO Restore started for store [group-aggregate-store-THREE_HOURS] with topic-partition [_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,234] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,240] INFO Restore started for store [aggregatedTopicPartitionTableWindows-THREE_HOURS] with topic-partition [_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,241] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-6 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,244] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] task [2_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:26,244] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,245] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,259] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Restoration took 1324 ms for all tasks [2_0, 10_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,259] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,260] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,260] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,261] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] End offset for changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,262] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] End offset for changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,263] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,265] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,268] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,268] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,269] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,265] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,275] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,276] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-3 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,293] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,297] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 to 0 since the associated topicId changed from null to Hsu6BGHpQ3KehegKKf6yyg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:26,299] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 to 0 since the associated topicId changed from null to 3CHyfz78QgSOj8D2PYTkuA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:26,308] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 to 0 since the associated topicId changed from null to CyEPox4GQNy8kLG7WEQWrQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:26,314] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:26,314] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,316] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,312] INFO Restore started for store [monitoring-aggregate-rekey-store] with topic-partition [_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,318] INFO Restore started for store [aggregate-topic-partition-store] with topic-partition [_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,331] INFO Restore started for store [MonitoringVerifierStore] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,339] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,342] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Finished restoring changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 to store aggregatedTopicPartitionTableWindows-THREE_HOURS with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,348] INFO Restore completed for store [aggregatedTopicPartitionTableWindows-THREE_HOURS] and topic-partition [_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,349] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Finished restoring changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 to store group-aggregate-store-THREE_HOURS with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,349] INFO Restore completed for store [group-aggregate-store-THREE_HOURS] and topic-partition [_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,395] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,396] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,399] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] task [4_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:26,400] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Restoration took 1338 ms for all tasks [4_0, 10_2] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,400] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,401] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,403] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-2 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,412] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,424] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-3 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,425] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,426] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,437] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,439] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-2 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,440] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,443] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,423] INFO ControlCenterBoundedMemoryConfig values: 
control-center                     | 	rocksdb.cache.limit.strict = false
control-center                     | 	rocksdb.cache.size = 16106127360
control-center                     | 	rocksdb.cache.size.limit.enabled = false
control-center                     | 	rocksdb.index.filter.block.ratio = 0.0
control-center                     | 	rocksdb.write.buffer.cache.use = false
control-center                     | 	rocksdb.write.buffer.size = 5368709120
control-center                     |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center                     | [2023-08-04 10:12:26,453] INFO Restore started for store [KSTREAM-OUTEROTHER-0000000106-store] with topic-partition [_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,456] INFO Restore started for store [KSTREAM-OUTERTHIS-0000000105-store] with topic-partition [_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,458] INFO Restore started for store [MonitoringTriggerStore] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,483] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 to store MonitoringVerifierStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,484] INFO Restore completed for store [MonitoringVerifierStore] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,485] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Finished restoring changelog _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 to store monitoring-aggregate-rekey-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,486] INFO Restore completed for store [monitoring-aggregate-rekey-store] and topic-partition [_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,486] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Finished restoring changelog _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 to store aggregate-topic-partition-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,487] INFO Restore completed for store [aggregate-topic-partition-store] and topic-partition [_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,493] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,495] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,516] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:26,544] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] task [1_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:26,545] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Restoration took 1690 ms for all tasks [1_0, 0_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,547] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,548] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-monitoring-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,549] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,561] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Finished restoring changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 to store KSTREAM-OUTEROTHER-0000000106-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,562] INFO Restore completed for store [KSTREAM-OUTEROTHER-0000000106-store] and topic-partition [_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,564] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Finished restoring changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 to store KSTREAM-OUTERTHIS-0000000105-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,565] INFO Restore completed for store [KSTREAM-OUTERTHIS-0000000105-store] and topic-partition [_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,566] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 to store MonitoringTriggerStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,567] INFO Restore completed for store [MonitoringTriggerStore] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,571] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,572] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,573] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,574] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,575] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,576] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,579] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] task [9_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:26,582] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Restoration took 1532 ms for all tasks [9_0, 10_7] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,584] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,588] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,592] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-7 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,591] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,598] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-monitoring-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,599] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,600] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,632] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] stream-task [12_0] State store TriggerActionsStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:26,632] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] stream-task [12_0] State store TriggerEventsStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:26,632] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] stream-task [12_0] State store AlertHistoryStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                     | [2023-08-04 10:12:26,632] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] task [12_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:26,649] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,653] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,654] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,676] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-7 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,676] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,676] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,677] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,677] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,677] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,677] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,651] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] task [10_9] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:26,718] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] End offset for changelog _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,718] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] End offset for changelog _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,718] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] End offset for changelog _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,718] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,719] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,719] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,719] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:26,737] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 to 0 since the associated topicId changed from null to 1LLF15hWQe-xqLOpsOrbpA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:26,738] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 to 0 since the associated topicId changed from null to Qlt_SM49Sl6zBrVxalhSRw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:26,739] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 to 0 since the associated topicId changed from null to dxHSpn7BThGwIYeZu2BHmA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:26,747] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:26,781] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,794] INFO Restore started for store [TriggerEventsStore] with topic-partition [_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,794] INFO Restore started for store [AlertHistoryStore] with topic-partition [_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,798] INFO Restore started for store [TriggerActionsStore] with topic-partition [_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,899] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Finished restoring changelog _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 to store TriggerActionsStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,899] INFO Restore completed for store [TriggerActionsStore] and topic-partition [_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,899] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Finished restoring changelog _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 to store TriggerEventsStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,900] INFO Restore completed for store [TriggerEventsStore] and topic-partition [_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,900] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Finished restoring changelog _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 to store AlertHistoryStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                     | [2023-08-04 10:12:26,900] INFO Restore completed for store [AlertHistoryStore] and topic-partition [_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                     | [2023-08-04 10:12:26,906] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,907] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,909] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] task [12_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:12:26,909] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Restoration took 1821 ms for all tasks [10_9, 12_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,909] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,910] INFO stream-client [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514] State transition from REBALANCING to RUNNING (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:12:26,910] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-9 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,911] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:12:26,922] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,923] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-9 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                     | [2023-08-04 10:12:26,924] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:12:26,926] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                     | [2023-08-04 10:12:27,143] INFO streams in state=RUNNING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                     | [2023-08-04 10:12:27,144] INFO tocheck=[Store{name=group-aggregate-store, rollup=true}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=TriggerEventsStore, rollup=false}, Store{name=Group, rollup=true}, Store{name=AlertHistoryStore, rollup=false}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                     | [2023-08-04 10:12:27,368] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
ksqldb-server                      | [2023-08-04 10:12:27,820] INFO Successfully submitted metrics to Confluent via secure endpoint (io.confluent.support.metrics.submitters.ConfluentSubmitter)
control-center                     | [2023-08-04 10:12:28,149] INFO action=started topology=monitoring (io.confluent.controlcenter.application.AllControlCenter)
control-center                     | [2023-08-04 10:12:28,203] INFO AdminClientConfig values: 
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = 
control-center                     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	confluent.use.controller.listener = false
control-center                     | 	connections.max.idle.ms = 300000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                     | [2023-08-04 10:12:28,233] WARN These configurations '[consumer.session.timeout.ms, producer.max.block.ms, producer.retries, upgrade.from, producer.retry.backoff.ms, producer.linger.ms, producer.delivery.timeout.ms, task.timeout.ms, cache.max.bytes.buffering, producer.compression.type, num.stream.threads]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                     | [2023-08-04 10:12:28,233] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:28,233] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:28,233] INFO Kafka startTimeMs: 1691143948233 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:28,295] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:28,322] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:12:28,323] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:12:28,324] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:12:28,331] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                     | 	metrics.jmx.prefix = confluent.controlcenter
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = true
control-center                     | 	port = 9021
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
control-center                     | [2023-08-04 10:12:28,345] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                     | 	metrics.jmx.prefix = confluent.controlcenter
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = true
control-center                     | 	port = 9021
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
control-center                     | [2023-08-04 10:12:28,351] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                     | 	metrics.jmx.prefix = confluent.controlcenter
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = true
control-center                     | 	port = 9021
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
control-center                     | [2023-08-04 10:12:28,357] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                     | 	metrics.jmx.prefix = confluent.controlcenter
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = true
control-center                     | 	port = 9021
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
control-center                     | [2023-08-04 10:12:28,378] INFO EventEmitterConfig values: 
control-center                     |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
control-center                     | [2023-08-04 10:12:28,379] INFO EventEmitterConfig values: 
control-center                     |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
control-center                     | [2023-08-04 10:12:28,396] INFO ConfluentTelemetryConfig values: 
control-center                     | 	confluent.telemetry.api.key = null
control-center                     | 	confluent.telemetry.api.secret = null
control-center                     | 	confluent.telemetry.debug.enabled = false
control-center                     | 	confluent.telemetry.enabled = false
control-center                     | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
control-center                     | 	confluent.telemetry.events.enable = true
control-center                     | 	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*
control-center                     | 	confluent.telemetry.metrics.collector.interval.ms = 60000
control-center                     | 	confluent.telemetry.metrics.collector.slo.enabled = false
control-center                     | 	confluent.telemetry.proxy.password = null
control-center                     | 	confluent.telemetry.proxy.url = null
control-center                     | 	confluent.telemetry.proxy.username = null
control-center                     |  (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                     | [2023-08-04 10:12:28,398] INFO VolumeMetricsCollectorConfig values: 
control-center                     | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
control-center                     |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
control-center                     | [2023-08-04 10:12:28,400] INFO HttpExporterConfig values: 
control-center                     | 	api.key = null
control-center                     | 	api.secret = null
control-center                     | 	buffer.batch.duration.max.ms = null
control-center                     | 	buffer.batch.items.max = null
control-center                     | 	buffer.inflight.submissions.max = null
control-center                     | 	buffer.pending.batches.max = null
control-center                     | 	client.attempts.max = null
control-center                     | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center                     | 	client.compression = null
control-center                     | 	client.connect.timeout.ms = null
control-center                     | 	client.contentType = null
control-center                     | 	client.request.timeout.ms = null
control-center                     | 	client.retry.delay.seconds = null
control-center                     | 	enabled = false
control-center                     | 	events.enabled = true
control-center                     | 	metrics.enabled = true
control-center                     | 	metrics.include = null
control-center                     | 	proxy.password = null
control-center                     | 	proxy.url = null
control-center                     | 	proxy.username = null
control-center                     | 	type = http
control-center                     |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
control-center                     | [2023-08-04 10:12:28,402] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                     | [2023-08-04 10:12:28,404] INFO RemoteConfigConfiguration values: 
control-center                     | 	enabled = true
control-center                     | 	polling.interval.ms = 60000
control-center                     |  (io.confluent.shaded.io.confluent.telemetry.config.remote.RemoteConfigConfiguration)
control-center                     | [2023-08-04 10:12:28,405] WARN Ignoring redefinition of existing telemetry label controlcenter.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
control-center                     | [2023-08-04 10:12:28,425] INFO ConfluentTelemetryConfig values: 
control-center                     | 	confluent.telemetry.api.key = null
control-center                     | 	confluent.telemetry.api.secret = null
control-center                     | 	confluent.telemetry.debug.enabled = false
control-center                     | 	confluent.telemetry.enabled = false
control-center                     | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
control-center                     | 	confluent.telemetry.events.enable = true
control-center                     | 	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.controlcenter/.*(metrics_input_topic_progress|monitoring_input_topic_progress|misconfigured_topics|missing_topic_configurations|broker_log_persistent_dir|cluster_offline|streams_status|total_lag|request_latency|response_size|response_rate).*
control-center                     | 	confluent.telemetry.metrics.collector.interval.ms = 60000
control-center                     | 	confluent.telemetry.metrics.collector.slo.enabled = false
control-center                     | 	confluent.telemetry.proxy.password = null
control-center                     | 	confluent.telemetry.proxy.url = null
control-center                     | 	confluent.telemetry.proxy.username = null
control-center                     |  (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                     | [2023-08-04 10:12:28,426] INFO VolumeMetricsCollectorConfig values: 
control-center                     | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
control-center                     |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
control-center                     | [2023-08-04 10:12:28,431] INFO HttpExporterConfig values: 
control-center                     | 	api.key = null
control-center                     | 	api.secret = null
control-center                     | 	buffer.batch.duration.max.ms = null
control-center                     | 	buffer.batch.items.max = null
control-center                     | 	buffer.inflight.submissions.max = null
control-center                     | 	buffer.pending.batches.max = null
control-center                     | 	client.attempts.max = null
control-center                     | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center                     | 	client.compression = null
control-center                     | 	client.connect.timeout.ms = null
control-center                     | 	client.contentType = null
control-center                     | 	client.request.timeout.ms = null
control-center                     | 	client.retry.delay.seconds = null
control-center                     | 	enabled = false
control-center                     | 	events.enabled = true
control-center                     | 	metrics.enabled = true
control-center                     | 	metrics.include = null
control-center                     | 	proxy.password = null
control-center                     | 	proxy.url = null
control-center                     | 	proxy.username = null
control-center                     | 	type = http
control-center                     |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
control-center                     | [2023-08-04 10:12:28,433] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                     | [2023-08-04 10:12:28,434] INFO RemoteConfigConfiguration values: 
control-center                     | 	enabled = true
control-center                     | 	polling.interval.ms = 60000
control-center                     |  (io.confluent.shaded.io.confluent.telemetry.config.remote.RemoteConfigConfiguration)
control-center                     | [2023-08-04 10:12:28,435] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
control-center                     | [2023-08-04 10:12:28,437] INFO EventLoggerConfig values: 
control-center                     | 	event.logger.cloudevent.codec = structured
control-center                     | 	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter
control-center                     |  (io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig)
control-center                     | [2023-08-04 10:12:28,439] INFO HttpExporterConfig values: 
control-center                     | 	api.key = null
control-center                     | 	api.secret = null
control-center                     | 	buffer.batch.duration.max.ms = null
control-center                     | 	buffer.batch.items.max = null
control-center                     | 	buffer.inflight.submissions.max = null
control-center                     | 	buffer.pending.batches.max = null
control-center                     | 	client.attempts.max = null
control-center                     | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center                     | 	client.compression = null
control-center                     | 	client.connect.timeout.ms = null
control-center                     | 	client.request.timeout.ms = null
control-center                     | 	client.retry.delay.seconds = null
control-center                     | 	enabled = false
control-center                     | 	events.enabled = true
control-center                     | 	metrics.enabled = true
control-center                     | 	proxy.password = null
control-center                     | 	proxy.url = null
control-center                     | 	proxy.username = null
control-center                     | 	type = http
control-center                     |  (io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig)
control-center                     | [2023-08-04 10:12:28,468] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter)
control-center                     | [2023-08-04 10:12:29,032] INFO [Producer clientId=c3-command] Resetting the last seen epoch of partition _confluent-command-0 to 0 since the associated topicId changed from null to 26mdvJTUTviIQl0i3yP-hQ (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:29,320] INFO KafkaRestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	advertised.listeners = []
control-center                     | 	api.endpoints.allowlist = []
control-center                     | 	api.endpoints.blocklist = []
control-center                     | 	api.v2.enable = true
control-center                     | 	api.v3.enable = true
control-center                     | 	api.v3.produce.rate.limit.cache.expiry.ms = 3600000
control-center                     | 	api.v3.produce.rate.limit.enabled = false
control-center                     | 	api.v3.produce.rate.limit.max.bytes.global.per.sec = 10000000
control-center                     | 	api.v3.produce.rate.limit.max.bytes.per.sec = 10000000
control-center                     | 	api.v3.produce.rate.limit.max.requests.global.per.sec = 10000
control-center                     | 	api.v3.produce.rate.limit.max.requests.per.sec = 10000
control-center                     | 	api.v3.produce.response.thread.pool.size = 4
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	bootstrap.servers = broker:29092
control-center                     | 	client.init.timeout.ms = 60000
control-center                     | 	client.sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	client.sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	client.sasl.kerberos.service.name = 
control-center                     | 	client.sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	client.sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	client.sasl.mechanism = GSSAPI
control-center                     | 	client.security.protocol = PLAINTEXT
control-center                     | 	client.ssl.cipher.suites = 
control-center                     | 	client.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
control-center                     | 	client.ssl.endpoint.identification.algorithm = 
control-center                     | 	client.ssl.key.password = [hidden]
control-center                     | 	client.ssl.keymanager.algorithm = SunX509
control-center                     | 	client.ssl.keystore.location = 
control-center                     | 	client.ssl.keystore.password = [hidden]
control-center                     | 	client.ssl.keystore.type = JKS
control-center                     | 	client.ssl.protocol = TLS
control-center                     | 	client.ssl.provider = 
control-center                     | 	client.ssl.trustmanager.algorithm = PKIX
control-center                     | 	client.ssl.truststore.location = 
control-center                     | 	client.ssl.truststore.password = [hidden]
control-center                     | 	client.ssl.truststore.type = JKS
control-center                     | 	client.timeout.ms = 500
control-center                     | 	client.zk.session.timeout.ms = 30000
control-center                     | 	compression.enable = true
control-center                     | 	confluent.resource.name.authority = 
control-center                     | 	connector.connection.limit = 0
control-center                     | 	consumer.instance.timeout.ms = 300000
control-center                     | 	consumer.iterator.backoff.ms = 50
control-center                     | 	consumer.iterator.timeout.ms = 1
control-center                     | 	consumer.request.max.bytes = 67108864
control-center                     | 	consumer.request.timeout.ms = 1000
control-center                     | 	consumer.threads = 50
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	fetch.min.bytes = -1
control-center                     | 	host.name = 
control-center                     | 	http2.enabled = true
control-center                     | 	id = 
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	kafka.rest.resource.extension.class = [io.confluent.kafkarest.KafkaRestResourceExtension]
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.jmx.prefix = kafka.rest
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = false
control-center                     | 	port = 8082
control-center                     | 	producer.threads = 5
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	rate.limit.backend = guava
control-center                     | 	rate.limit.costs = 
control-center                     | 	rate.limit.default.cost = 1
control-center                     | 	rate.limit.enable = false
control-center                     | 	rate.limit.per.cluster.cache.expiry.ms = 3600000
control-center                     | 	rate.limit.per.cluster.permits.per.sec = 50
control-center                     | 	rate.limit.permits.per.sec = 50
control-center                     | 	rate.limit.timeout.ms = 0
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json, application/vnd.kafka.v2+json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	schema.registry.url = http://localhost:8081
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	simpleconsumer.pool.size.max = 25
control-center                     | 	simpleconsumer.pool.timeout.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	streaming.connection.max.duration.grace.period.ms = 500
control-center                     | 	streaming.connection.max.duration.ms = 86400000
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /api/kafka-rest-ws/MkU3OEVBNTcwNTJENDM2Qg
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     | 	zookeeper.connect = 
control-center                     |  (io.confluent.kafkarest.KafkaRestConfig)
control-center                     | [2023-08-04 10:12:30,639] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
control-center                     | [2023-08-04 10:12:30,640] INFO SchemaRegistryConfig values: 
control-center                     | 	auto.register.schemas = false
control-center                     | 	basic.auth.credentials.source = URL
control-center                     | 	basic.auth.user.info = [hidden]
control-center                     | 	bearer.auth.cache.expiry.buffer.seconds = 300
control-center                     | 	bearer.auth.client.id = null
control-center                     | 	bearer.auth.client.secret = null
control-center                     | 	bearer.auth.credentials.source = STATIC_TOKEN
control-center                     | 	bearer.auth.custom.provider.class = null
control-center                     | 	bearer.auth.identity.pool.id = null
control-center                     | 	bearer.auth.issuer.endpoint.url = null
control-center                     | 	bearer.auth.logical.cluster = null
control-center                     | 	bearer.auth.scope = null
control-center                     | 	bearer.auth.scope.claim.name = scope
control-center                     | 	bearer.auth.sub.claim.name = sub
control-center                     | 	bearer.auth.token = [hidden]
control-center                     | 	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
control-center                     | 	http.connect.timeout.ms = 60000
control-center                     | 	http.read.timeout.ms = 60000
control-center                     | 	id.compatibility.strict = true
control-center                     | 	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
control-center                     | 	latest.cache.size = 1000
control-center                     | 	latest.cache.ttl.sec = -1
control-center                     | 	latest.compatibility.strict = true
control-center                     | 	max.schemas.per.subject = 1000
control-center                     | 	normalize.schemas = false
control-center                     | 	proxy.host = 
control-center                     | 	proxy.port = -1
control-center                     | 	rule.actions = []
control-center                     | 	rule.executors = []
control-center                     | 	rule.service.loader.enable = true
control-center                     | 	schema.format = null
control-center                     | 	schema.reflection = false
control-center                     | 	schema.registry.basic.auth.user.info = [hidden]
control-center                     | 	schema.registry.ssl.cipher.suites = null
control-center                     | 	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	schema.registry.ssl.endpoint.identification.algorithm = https
control-center                     | 	schema.registry.ssl.engine.factory.class = null
control-center                     | 	schema.registry.ssl.key.password = null
control-center                     | 	schema.registry.ssl.keymanager.algorithm = SunX509
control-center                     | 	schema.registry.ssl.keystore.certificate.chain = null
control-center                     | 	schema.registry.ssl.keystore.key = null
control-center                     | 	schema.registry.ssl.keystore.location = null
control-center                     | 	schema.registry.ssl.keystore.password = null
control-center                     | 	schema.registry.ssl.keystore.type = JKS
control-center                     | 	schema.registry.ssl.protocol = TLSv1.3
control-center                     | 	schema.registry.ssl.provider = null
control-center                     | 	schema.registry.ssl.secure.random.implementation = null
control-center                     | 	schema.registry.ssl.trustmanager.algorithm = PKIX
control-center                     | 	schema.registry.ssl.truststore.certificates = null
control-center                     | 	schema.registry.ssl.truststore.location = null
control-center                     | 	schema.registry.ssl.truststore.password = null
control-center                     | 	schema.registry.ssl.truststore.type = JKS
control-center                     | 	schema.registry.url = [http://localhost:8081]
control-center                     | 	use.latest.version = false
control-center                     | 	use.latest.with.metadata = null
control-center                     | 	use.schema.id = -1
control-center                     | 	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
control-center                     |  (io.confluent.kafkarest.config.SchemaRegistryConfig)
control-center                     | [2023-08-04 10:12:31,205] INFO Binding EmbeddedKafkaRestApplication to all listeners. (io.confluent.rest.Application)
control-center                     | [2023-08-04 10:12:32,118] INFO Starting Health Check (io.confluent.controlcenter.application.AllControlCenter)
control-center                     | [2023-08-04 10:12:32,119] INFO Starting Alert Manager (io.confluent.controlcenter.application.AllControlCenter)
control-center                     | [2023-08-04 10:12:32,124] INFO Starting Consumer Offsets Fetch (io.confluent.controlcenter.application.AllControlCenter)
control-center                     | [2023-08-04 10:12:32,132] INFO current clusterId=MkU3OEVBNTcwNTJENDM2Qg (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:12:32,132] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                     | 	metrics.jmx.prefix = confluent.controlcenter
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = true
control-center                     | 	port = 9021
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
control-center                     | [2023-08-04 10:12:32,153] INFO Initial capacity 128, increased by 64, maximum capacity 2147483647. (io.confluent.rest.ApplicationServer)
control-center                     | [2023-08-04 10:12:32,207] INFO AdminClientConfig values: 
control-center                     | 	auto.include.jmx.reporter = true
control-center                     | 	bootstrap.servers = [broker:29092]
control-center                     | 	client.dns.lookup = use_all_dns_ips
control-center                     | 	client.id = 
control-center                     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                     | 	confluent.proxy.protocol.client.address = null
control-center                     | 	confluent.proxy.protocol.client.port = null
control-center                     | 	confluent.proxy.protocol.client.version = NONE
control-center                     | 	confluent.use.controller.listener = false
control-center                     | 	connections.max.idle.ms = 300000
control-center                     | 	default.api.timeout.ms = 60000
control-center                     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                     | 	metadata.max.age.ms = 300000
control-center                     | 	metric.reporters = []
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.recording.level = INFO
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	receive.buffer.bytes = 65536
control-center                     | 	reconnect.backoff.max.ms = 1000
control-center                     | 	reconnect.backoff.ms = 50
control-center                     | 	request.timeout.ms = 30000
control-center                     | 	retries = 2147483647
control-center                     | 	retry.backoff.ms = 100
control-center                     | 	sasl.client.callback.handler.class = null
control-center                     | 	sasl.jaas.config = null
control-center                     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                     | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                     | 	sasl.kerberos.service.name = null
control-center                     | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                     | 	sasl.login.callback.handler.class = null
control-center                     | 	sasl.login.class = null
control-center                     | 	sasl.login.connect.timeout.ms = null
control-center                     | 	sasl.login.read.timeout.ms = null
control-center                     | 	sasl.login.refresh.buffer.seconds = 300
control-center                     | 	sasl.login.refresh.min.period.seconds = 60
control-center                     | 	sasl.login.refresh.window.factor = 0.8
control-center                     | 	sasl.login.refresh.window.jitter = 0.05
control-center                     | 	sasl.login.retry.backoff.max.ms = 10000
control-center                     | 	sasl.login.retry.backoff.ms = 100
control-center                     | 	sasl.mechanism = GSSAPI
control-center                     | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                     | 	sasl.oauthbearer.expected.audience = null
control-center                     | 	sasl.oauthbearer.expected.issuer = null
control-center                     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                     | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                     | 	sasl.oauthbearer.scope.claim.name = scope
control-center                     | 	sasl.oauthbearer.sub.claim.name = sub
control-center                     | 	sasl.oauthbearer.token.endpoint.url = null
control-center                     | 	security.protocol = PLAINTEXT
control-center                     | 	security.providers = null
control-center                     | 	send.buffer.bytes = 131072
control-center                     | 	socket.connection.setup.timeout.max.ms = 30000
control-center                     | 	socket.connection.setup.timeout.ms = 10000
control-center                     | 	ssl.cipher.suites = null
control-center                     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                     | 	ssl.endpoint.identification.algorithm = https
control-center                     | 	ssl.engine.factory.class = null
control-center                     | 	ssl.key.password = null
control-center                     | 	ssl.keymanager.algorithm = SunX509
control-center                     | 	ssl.keystore.certificate.chain = null
control-center                     | 	ssl.keystore.key = null
control-center                     | 	ssl.keystore.location = null
control-center                     | 	ssl.keystore.password = null
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.protocol = TLSv1.3
control-center                     | 	ssl.provider = null
control-center                     | 	ssl.secure.random.implementation = null
control-center                     | 	ssl.trustmanager.algorithm = PKIX
control-center                     | 	ssl.truststore.certificates = null
control-center                     | 	ssl.truststore.location = null
control-center                     | 	ssl.truststore.password = null
control-center                     | 	ssl.truststore.type = JKS
control-center                     |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                     | [2023-08-04 10:12:32,238] WARN These configurations '[consumer.session.timeout.ms, producer.max.block.ms, producer.retries, upgrade.from, producer.retry.backoff.ms, producer.linger.ms, producer.delivery.timeout.ms, task.timeout.ms, cache.max.bytes.buffering, producer.compression.type, num.stream.threads]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                     | [2023-08-04 10:12:32,238] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:32,238] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:32,238] INFO Kafka startTimeMs: 1691143952238 (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:12:32,285] INFO broker id set has changed new={1=[broker:29092 (id: 1 rack: null tags: [])]} removed={} (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:12:32,287] INFO new controller=broker:29092 (id: 1 rack: null tags: []) (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:12:32,379] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:12:32,379] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:12:32,879] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to 5zxMVZraQkS0mMTEISnuCA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:32,893] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to TL8DPoV0R9OJhSSwRHqD6A (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:32,901] INFO Adding listener with HTTP/2: NamedURI{uri=http://0.0.0.0:9021, name='null'} (io.confluent.rest.ApplicationServer)
control-center                     | [2023-08-04 10:12:33,551] INFO name=monitoring-input-topic-progress-.count type=monitoring cluster= value=0.0 (io.confluent.controlcenter.util.StreamProgressReporter)
control-center                     | [2023-08-04 10:12:33,560] INFO name=monitoring-input-topic-progress-.rate type=monitoring cluster= value=0.0 (io.confluent.controlcenter.util.StreamProgressReporter)
control-center                     | [2023-08-04 10:12:33,569] INFO name=monitoring-input-topic-progress-.timestamp type=monitoring cluster= value=NaN (io.confluent.controlcenter.util.StreamProgressReporter)
control-center                     | [2023-08-04 10:12:33,578] INFO name=monitoring-input-topic-progress-.min type=monitoring cluster= value=1.7976931348623157E308 (io.confluent.controlcenter.util.StreamProgressReporter)
control-center                     | [2023-08-04 10:12:33,628] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to RTYUY2zsS26iu53cLq8UPg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:33,753] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to z2ogKsUbRf6X0CRLuVUTWw (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:33,789] INFO ControlCenterBoundedMemoryConfig values: 
control-center                     | 	rocksdb.cache.limit.strict = false
control-center                     | 	rocksdb.cache.size = 16106127360
control-center                     | 	rocksdb.cache.size.limit.enabled = false
control-center                     | 	rocksdb.index.filter.block.ratio = 0.0
control-center                     | 	rocksdb.write.buffer.cache.use = false
control-center                     | 	rocksdb.write.buffer.size = 5368709120
control-center                     |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center                     | [2023-08-04 10:12:33,856] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 to 0 since the associated topicId changed from null to 3CHyfz78QgSOj8D2PYTkuA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:33,884] INFO ControlCenterBoundedMemoryConfig values: 
control-center                     | 	rocksdb.cache.limit.strict = false
control-center                     | 	rocksdb.cache.size = 16106127360
control-center                     | 	rocksdb.cache.size.limit.enabled = false
control-center                     | 	rocksdb.index.filter.block.ratio = 0.0
control-center                     | 	rocksdb.write.buffer.cache.use = false
control-center                     | 	rocksdb.write.buffer.size = 5368709120
control-center                     |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center                     | [2023-08-04 10:12:34,071] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 to 0 since the associated topicId changed from null to Hsu6BGHpQ3KehegKKf6yyg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:12:34,500] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                     | 	metrics.jmx.prefix = confluent.controlcenter
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = true
control-center                     | 	port = 9021
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
data-agrigator-taskmanager-1       | 2023-08-04 10:12:34,892 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:12:34,894 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                     | [2023-08-04 10:12:37,292] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                     | 	metrics.jmx.prefix = confluent.controlcenter
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = true
control-center                     | 	port = 9021
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
control-center                     | [2023-08-04 10:12:37,408] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                     | 	metrics.jmx.prefix = confluent.controlcenter
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = true
control-center                     | 	port = 9021
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
control-center                     | [2023-08-04 10:12:37,645] INFO Binding ControlCenterApplication to all listeners. (io.confluent.rest.Application)
control-center                     | [2023-08-04 10:12:37,965] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                     | 	metrics.jmx.prefix = confluent.controlcenter
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = true
control-center                     | 	port = 9021
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
control-center                     | [2023-08-04 10:12:38,236] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                     | 	metrics.jmx.prefix = confluent.controlcenter
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = true
control-center                     | 	port = 9021
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
control-center                     | [2023-08-04 10:12:39,202] WARN [creqId=215e06b2][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T10:12:37.375Z(1691143957375000), length=0B, duration=1650ms(1650070309ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                     | [2023-08-04 10:12:39,205] WARN [creqId=215e06b2][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T10:12:39.044Z(1691143959044000), length=0B, duration=0ns, totalDuration=1669ms(1669631757ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                     | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083
control-center                     | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                     | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                     | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083
control-center                     | Caused by: java.net.ConnectException: Connection refused
control-center                     | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center                     | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center                     | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                     | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                     | [2023-08-04 10:12:39,820] INFO RestConfig values: 
control-center                     | 	access.control.allow.headers = 
control-center                     | 	access.control.allow.methods = 
control-center                     | 	access.control.allow.origin = 
control-center                     | 	access.control.skip.options = true
control-center                     | 	authentication.method = NONE
control-center                     | 	authentication.realm = 
control-center                     | 	authentication.roles = [*]
control-center                     | 	authentication.skip.paths = []
control-center                     | 	compression.enable = true
control-center                     | 	connector.connection.limit = 0
control-center                     | 	csrf.prevention.enable = false
control-center                     | 	csrf.prevention.token.endpoint = /csrf
control-center                     | 	csrf.prevention.token.expiration.minutes = 30
control-center                     | 	csrf.prevention.token.max.entries = 10000
control-center                     | 	debug = false
control-center                     | 	dos.filter.delay.ms = 100
control-center                     | 	dos.filter.enabled = false
control-center                     | 	dos.filter.insert.headers = true
control-center                     | 	dos.filter.ip.whitelist = []
control-center                     | 	dos.filter.managed.attr = false
control-center                     | 	dos.filter.max.idle.tracker.ms = 30000
control-center                     | 	dos.filter.max.requests.ms = 30000
control-center                     | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                     | 	dos.filter.max.requests.per.sec = 25
control-center                     | 	dos.filter.max.wait.ms = 50
control-center                     | 	dos.filter.throttle.ms = 30000
control-center                     | 	dos.filter.throttled.requests = 5
control-center                     | 	http2.enabled = true
control-center                     | 	idle.timeout.ms = 30000
control-center                     | 	listener.protocol.map = []
control-center                     | 	listeners = []
control-center                     | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                     | 	metrics.jmx.prefix = confluent.controlcenter
control-center                     | 	metrics.num.samples = 2
control-center                     | 	metrics.sample.window.ms = 30000
control-center                     | 	metrics.tag.map = []
control-center                     | 	nosniff.prevention.enable = true
control-center                     | 	port = 9021
control-center                     | 	proxy.protocol.enabled = false
control-center                     | 	reject.options.request = false
control-center                     | 	request.logger.name = io.confluent.rest-utils.requests
control-center                     | 	request.queue.capacity = 2147483647
control-center                     | 	request.queue.capacity.growby = 64
control-center                     | 	request.queue.capacity.init = 128
control-center                     | 	resource.extension.classes = []
control-center                     | 	response.http.headers.config = 
control-center                     | 	response.mediatype.default = application/json
control-center                     | 	response.mediatype.preferred = [application/json]
control-center                     | 	rest.servlet.initializor.classes = []
control-center                     | 	server.connection.limit = 0
control-center                     | 	shutdown.graceful.ms = 1000
control-center                     | 	ssl.cipher.suites = []
control-center                     | 	ssl.client.auth = false
control-center                     | 	ssl.client.authentication = NONE
control-center                     | 	ssl.enabled.protocols = []
control-center                     | 	ssl.endpoint.identification.algorithm = null
control-center                     | 	ssl.key.password = [hidden]
control-center                     | 	ssl.keymanager.algorithm = 
control-center                     | 	ssl.keystore.location = 
control-center                     | 	ssl.keystore.password = [hidden]
control-center                     | 	ssl.keystore.reload = false
control-center                     | 	ssl.keystore.type = JKS
control-center                     | 	ssl.keystore.watch.location = 
control-center                     | 	ssl.protocol = TLS
control-center                     | 	ssl.provider = 
control-center                     | 	ssl.trustmanager.algorithm = 
control-center                     | 	ssl.truststore.location = 
control-center                     | 	ssl.truststore.password = [hidden]
control-center                     | 	ssl.truststore.type = JKS
control-center                     | 	suppress.stack.trace.response = true
control-center                     | 	thread.pool.max = 200
control-center                     | 	thread.pool.min = 8
control-center                     | 	websocket.path.prefix = /ws
control-center                     | 	websocket.servlet.initializor.classes = []
control-center                     |  (io.confluent.rest.RestConfig)
control-center                     | [2023-08-04 10:12:40,500] INFO adding websocket endpoint ProducerResource (io.confluent.controlcenter.rest.ControlCenterApplication)
control-center                     | [2023-08-04 10:12:40,528] INFO adding websocket endpoint ConsumerResource (io.confluent.controlcenter.rest.ControlCenterApplication)
control-center                     | [2023-08-04 10:12:41,760] INFO jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.18+10-LTS (org.eclipse.jetty.server.Server)
ksqldb-server                      | [2023-08-04 10:12:43,133] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:12:43 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:12:43,433] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
control-center                     | [2023-08-04 10:12:43,437] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
control-center                     | [2023-08-04 10:12:43,448] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session)
schema-registry                    | [2023-08-04 10:12:44,345] INFO 192.168.160.13 - - [04/Aug/2023:10:12:41 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 2655 (io.confluent.rest-utils.requests)
data-agrigator-taskmanager-1       | 2023-08-04 10:12:45,063 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1       | 2023-08-04 10:12:45,059 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
control-center                     | [2023-08-04 10:12:47,228] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
control-center                     | [2023-08-04 10:12:47,263] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
control-center                     | [2023-08-04 10:12:47,296] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
control-center                     | [2023-08-04 10:12:47,340] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
data-agrigator-taskmanager-1       | 2023-08-04 10:12:55,110 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:12:55,113 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                     | [2023-08-04 10:12:55,491] INFO Started o.e.j.s.ServletContextHandler@28d2afd8{/api/kafka-rest/MkU3OEVBNTcwNTJENDM2Qg/kafka,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
control-center                     | Aug 04, 2023 10:12:56 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                     | WARNING: A provider io.confluent.controlcenter.rest.CachedConsumerOffsetsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.CachedConsumerOffsetsResource will be ignored. 
control-center                     | Aug 04, 2023 10:12:56 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                     | WARNING: A provider io.confluent.controlcenter.rest.CommandResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.CommandResource will be ignored. 
control-center                     | Aug 04, 2023 10:12:56 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                     | WARNING: A provider io.confluent.controlcenter.rest.MessageDeliveryResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.MessageDeliveryResource will be ignored. 
control-center                     | Aug 04, 2023 10:12:56 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                     | WARNING: A provider io.confluent.controlcenter.rest.PermissionsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.PermissionsResource will be ignored. 
control-center                     | Aug 04, 2023 10:12:56 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                     | WARNING: A provider io.confluent.controlcenter.rest.AuthResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.AuthResource will be ignored. 
control-center                     | Aug 04, 2023 10:12:56 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                     | WARNING: A provider io.confluent.controlcenter.rest.MetricsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.MetricsResource will be ignored. 
control-center                     | Aug 04, 2023 10:12:56 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                     | WARNING: A provider io.confluent.controlcenter.rest.LicenseResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.LicenseResource will be ignored. 
control-center                     | Aug 04, 2023 10:12:56 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                     | WARNING: A provider io.confluent.controlcenter.rest.KafkaResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.KafkaResource will be ignored. 
control-center                     | Aug 04, 2023 10:12:56 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                     | WARNING: A provider io.confluent.controlcenter.rest.ClusterResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.ClusterResource will be ignored. 
control-center                     | Aug 04, 2023 10:12:56 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                     | WARNING: A provider io.confluent.controlcenter.rest.StatusResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.StatusResource will be ignored. 
control-center                     | Aug 04, 2023 10:12:56 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                     | WARNING: A provider io.confluent.controlcenter.rest.AlertsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.AlertsResource will be ignored. 
control-center                     | Aug 04, 2023 10:12:56 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                     | WARNING: A provider io.confluent.controlcenter.rest.ServiceHealthCheckResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.ServiceHealthCheckResource will be ignored. 
control-center                     | Aug 04, 2023 10:12:56 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                     | WARNING: A provider io.confluent.controlcenter.rest.HealthCheckResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.HealthCheckResource will be ignored. 
control-center                     | Aug 04, 2023 10:12:56 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                     | WARNING: A provider io.confluent.controlcenter.rest.FeatureFlagResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.FeatureFlagResource will be ignored. 
control-center                     | [2023-08-04 10:12:58,208] INFO Started o.e.j.s.ServletContextHandler@77a35b2f{/,[io.confluent.controlcenter.rest.ModifiableResource@74cf0dd9],AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
control-center                     | [2023-08-04 10:12:58,265] INFO Started o.e.j.s.ServletContextHandler@755057c7{/api/kafka-rest-ws/MkU3OEVBNTcwNTJENDM2Qg,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
control-center                     | [2023-08-04 10:12:58,322] INFO Started o.e.j.s.ServletContextHandler@4d62bb8b{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
control-center                     | [2023-08-04 10:12:58,468] INFO Started NetworkTrafficServerConnector@586af46{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:9021} (org.eclipse.jetty.server.AbstractConnector)
control-center                     | [2023-08-04 10:12:58,470] INFO Started @156439ms (org.eclipse.jetty.server.Server)
control-center                     | [2023-08-04 10:12:59,713] WARN [creqId=70979642][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T10:12:59.656Z(1691143979656000), length=0B, duration=56417s(56417447ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                     | [2023-08-04 10:12:59,717] WARN [creqId=70979642][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T10:12:59.712Z(1691143979712000), length=0B, duration=0ns, totalDuration=56556s(56556502ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                     | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083
control-center                     | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                     | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                     | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083
control-center                     | Caused by: java.net.ConnectException: Connection refused
control-center                     | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center                     | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center                     | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                     | 	at java.base/java.lang.Thread.run(Thread.java:829)
connect                            | [2023-08-04 10:13:01,738] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/acl/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:01,744] INFO Loading plugin from: /usr/share/java/kafka-serde-tools (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:04,064] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/kafka-serde-tools/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:04,067] INFO Loading plugin from: /usr/share/java/confluent-hub-client (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
ksqldb-server                      | [2023-08-04 10:13:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
data-agrigator-taskmanager-1       | 2023-08-04 10:13:05,198 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:13:05,207 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
connect                            | [2023-08-04 10:13:05,382] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-hub-client/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:05,388] INFO Loading plugin from: /usr/share/java/monitoring-interceptors (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
schema-registry                    | [2023-08-04 10:13:06,308] INFO 192.168.160.13 - - [04/Aug/2023:10:13:06 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 36 (io.confluent.rest-utils.requests)
connect                            | [2023-08-04 10:13:06,669] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/monitoring-interceptors/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:06,672] INFO Loading plugin from: /usr/share/java/ce-kafka-rest-servlet (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:06,688] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/ce-kafka-rest-servlet/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:06,691] INFO Loading plugin from: /usr/share/java/confluent-rebalancer (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
ksqldb-server                      | [2023-08-04 10:13:06,883] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:13:06 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
connect                            | [2023-08-04 10:13:14,727] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-rebalancer/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:14,731] INFO Loading plugin from: /usr/share/java/confluent-security (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
control-center                     | [2023-08-04 10:13:19,435] WARN [creqId=18f6eaa6][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T10:13:19.418Z(1691143999418000), length=0B, duration=16027s(16027868ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                     | [2023-08-04 10:13:19,436] WARN [creqId=18f6eaa6][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T10:13:19.434Z(1691143999434000), length=0B, duration=0ns, totalDuration=16169s(16169322ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                     | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083
control-center                     | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                     | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                     | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083
control-center                     | Caused by: java.net.ConnectException: Connection refused
control-center                     | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center                     | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center                     | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                     | 	at java.base/java.lang.Thread.run(Thread.java:829)
data-agrigator-taskmanager-1       | 2023-08-04 10:13:20,351 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:13:20,354 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
schema-registry                    | [2023-08-04 10:13:25,199] INFO 192.168.160.13 - - [04/Aug/2023:10:13:25 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 49 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:13:28,650] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:13:28 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
data-agrigator-taskmanager-1       | 2023-08-04 10:13:30,441 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:13:30,447 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                     | [2023-08-04 10:13:32,237] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:13:32,238] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
postgres_db                        | 2023-08-04 10:13:35.579 UTC [22] LOG:  checkpoint starting: time
postgres_db                        | 2023-08-04 10:13:35.606 UTC [22] LOG:  checkpoint complete: wrote 3 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.009 s, sync=0.003 s, total=0.027 s; sync files=2, longest=0.002 s, average=0.002 s; distance=0 kB, estimate=0 kB
control-center                     | [2023-08-04 10:13:39,530] WARN [creqId=85ed77fa][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T10:13:39.523Z(1691144019523000), length=0B, duration=5482s(5482153ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                     | [2023-08-04 10:13:39,530] WARN [creqId=85ed77fa][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T10:13:39.529Z(1691144019529000), length=0B, duration=0ns, totalDuration=5695s(5695013ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                     | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083
control-center                     | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                     | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                     | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083
control-center                     | Caused by: java.net.ConnectException: Connection refused
control-center                     | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center                     | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center                     | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                     | 	at java.base/java.lang.Thread.run(Thread.java:829)
data-agrigator-taskmanager-1       | 2023-08-04 10:13:40,491 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:13:40,496 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
connect                            | [2023-08-04 10:13:42,312] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-security/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:42,316] INFO Loading plugin from: /usr/share/java/confluent-telemetry (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:44,465] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-telemetry/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:44,467] INFO Loading plugin from: /usr/share/java/rest-utils (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
schema-registry                    | [2023-08-04 10:13:45,553] INFO 192.168.160.13 - - [04/Aug/2023:10:13:45 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 21 (io.confluent.rest-utils.requests)
connect                            | [2023-08-04 10:13:46,367] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/rest-utils/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:46,370] INFO Loading plugin from: /usr/share/java/confluent-common (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:46,392] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-common/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:46,394] INFO Loading plugin from: /usr/share/java/ce-kafka-rest-extensions (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
ksqldb-server                      | [2023-08-04 10:13:46,760] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:13:46 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
connect                            | [2023-08-04 10:13:47,050] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/ce-kafka-rest-extensions/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:47,053] INFO Loading plugin from: /usr/share/java/kafka-rest-lib (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
data-agrigator-taskmanager-1       | 2023-08-04 10:13:50,527 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:13:50,530 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
connect                            | [2023-08-04 10:13:50,588] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/kafka-rest-lib/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:50,591] INFO Loading plugin from: /usr/share/java/confluent-metadata-service (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:53,561] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-metadata-service/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:53,563] INFO Loading plugin from: /usr/share/java/ce-kafka-http-server (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:55,686] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/ce-kafka-http-server/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:13:55,688] INFO Loading plugin from: /usr/share/java/kafka (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
data-agrigator-taskmanager-1       | 2023-08-04 10:14:00,586 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:14:00,596 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                     | [2023-08-04 10:14:01,591] WARN [creqId=911f8dfb][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T10:14:01.584Z(1691144041584000), length=0B, duration=6074s(6074975ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                     | [2023-08-04 10:14:01,592] WARN [creqId=911f8dfb][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T10:14:01.590Z(1691144041590000), length=0B, duration=0ns, totalDuration=6203s(6203821ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                     | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083
control-center                     | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                     | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                     | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083
control-center                     | Caused by: java.net.ConnectException: Connection refused
control-center                     | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center                     | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center                     | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                     | 	at java.base/java.lang.Thread.run(Thread.java:829)
ksqldb-server                      | [2023-08-04 10:14:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
schema-registry                    | [2023-08-04 10:14:06,557] INFO 192.168.160.13 - - [04/Aug/2023:10:14:06 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 9 (io.confluent.rest-utils.requests)
connect                            | [2023-08-04 10:14:07,003] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/kafka/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:07,003] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:07,004] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:07,004] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:07,005] INFO Added plugin 'io.confluent.connect.rest.datapreview.extension.util.PreviewRecordTransformer' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:07,005] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:07,005] INFO Added plugin 'io.confluent.connect.rest.datapreview.extension.ConnectorDataPreviewRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:07,009] INFO Loading plugin from: /usr/share/java/kafka-rest-bin (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:07,027] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/kafka-rest-bin/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:07,031] INFO Loading plugin from: /usr/share/confluent-hub-components/confluentinc-kafka-connect-datagen (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:07,860] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/confluent-hub-components/confluentinc-kafka-connect-datagen/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:07,860] INFO Added plugin 'io.confluent.kafka.connect.datagen.DatagenConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
ksqldb-server                      | [2023-08-04 10:14:09,161] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:14:09 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
data-agrigator-taskmanager-1       | 2023-08-04 10:14:10,637 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:14:10,642 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                     | [2023-08-04 10:14:14,937] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Processed 1 total records, ran 0 punctuators, and committed 1 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:14:18,011] WARN [creqId=8b282570][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T10:14:17.997Z(1691144057997000), length=0B, duration=12813s(12813909ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                     | [2023-08-04 10:14:18,013] WARN [creqId=8b282570][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T10:14:18.010Z(1691144058010000), length=0B, duration=0ns, totalDuration=12940s(12940718ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                     | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083
control-center                     | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                     | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                     | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083
control-center                     | Caused by: java.net.ConnectException: Connection refused
control-center                     | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center                     | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center                     | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                     | 	at java.base/java.lang.Thread.run(Thread.java:829)
data-agrigator-taskmanager-1       | 2023-08-04 10:14:20,683 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:14:20,694 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                     | [2023-08-04 10:14:23,436] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Processed 16 total records, ran 0 punctuators, and committed 6 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:14:23,451] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:14:23,536] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:14:23,537] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Processed 16 total records, ran 0 punctuators, and committed 3 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:14:23,545] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Processed 16 total records, ran 0 punctuators, and committed 3 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:14:23,579] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:14:23,615] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:14:23,615] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:14:23,626] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:14:23,636] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:14:23,683] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:14:23,723] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
schema-registry                    | [2023-08-04 10:14:27,422] INFO 192.168.160.13 - - [04/Aug/2023:10:14:27 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 40 (io.confluent.rest-utils.requests)
data-agrigator-taskmanager-1       | 2023-08-04 10:14:30,747 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:14:30,750 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                     | [2023-08-04 10:14:32,133] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:14:32,134] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
ksqldb-server                      | [2023-08-04 10:14:32,792] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:14:32 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:14:40,457] WARN [creqId=62d63e3a][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T10:14:40.451Z(1691144080451000), length=0B, duration=4626s(4626663ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                     | [2023-08-04 10:14:40,458] WARN [creqId=62d63e3a][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T10:14:40.456Z(1691144080456000), length=0B, duration=0ns, totalDuration=4733s(4733814ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                     | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083
control-center                     | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                     | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                     | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                     | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                     | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.160.8:8083
control-center                     | Caused by: java.net.ConnectException: Connection refused
control-center                     | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center                     | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center                     | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center                     | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                     | 	at java.base/java.lang.Thread.run(Thread.java:829)
data-agrigator-taskmanager-1       | 2023-08-04 10:14:40,811 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:14:40,820 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
schema-registry                    | [2023-08-04 10:14:45,018] INFO 192.168.160.13 - - [04/Aug/2023:10:14:44 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 28 (io.confluent.rest-utils.requests)
connect                            | [2023-08-04 10:14:46,504] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@2c13da15 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,509] INFO Added aliases 'DatagenConnector' and 'Datagen' to plugin 'io.confluent.kafka.connect.datagen.DatagenConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,509] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,510] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,510] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,510] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,510] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,511] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,511] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,511] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,511] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,512] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,512] INFO Added aliases 'JsonSchemaConverter' and 'JsonSchema' to plugin 'io.confluent.connect.json.JsonSchemaConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,512] INFO Added aliases 'ProtobufConverter' and 'Protobuf' to plugin 'io.confluent.connect.protobuf.ProtobufConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,513] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,513] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,513] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,514] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,514] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,514] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,514] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,514] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,515] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,515] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,515] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,515] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,515] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,516] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,516] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,516] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,516] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,517] INFO Added alias 'PreviewRecordTransformer' to plugin 'io.confluent.connect.rest.datapreview.extension.util.PreviewRecordTransformer' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,517] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,518] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,518] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,518] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,519] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,519] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,520] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,520] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,520] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,520] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,520] INFO Added alias 'ConnectorDataPreviewRestExtension' to plugin 'io.confluent.connect.rest.datapreview.extension.ConnectorDataPreviewRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,520] INFO Added alias 'ConnectSecurityExtension' to plugin 'io.confluent.connect.security.ConnectSecurityExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,521] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,521] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,521] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,521] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect                            | [2023-08-04 10:14:46,741] INFO DistributedConfig values: 
connect                            | 	access.control.allow.methods = 
connect                            | 	access.control.allow.origin = 
connect                            | 	admin.listeners = null
connect                            | 	bootstrap.servers = [broker:29092]
connect                            | 	client.dns.lookup = use_all_dns_ips
connect                            | 	client.id = 
connect                            | 	config.providers = []
connect                            | 	config.storage.replication.factor = 1
connect                            | 	config.storage.topic = docker-connect-configs
connect                            | 	confluent.connector.task.status.metrics = false
connect                            | 	confluent.license = [hidden]
connect                            | 	confluent.license.inject.into.connectors = true
connect                            | 	confluent.topic = _confluent-command
connect                            | 	confluent.topic.bootstrap.servers = []
connect                            | 	confluent.topic.client.dns.lookup = use_all_dns_ips
connect                            | 	confluent.topic.client.id = 
connect                            | 	confluent.topic.connections.max.idle.ms = 540000
connect                            | 	confluent.topic.consumer.allow.auto.create.topics = true
connect                            | 	confluent.topic.consumer.auto.commit.interval.ms = 5000
connect                            | 	confluent.topic.consumer.auto.offset.reset = latest
connect                            | 	confluent.topic.consumer.check.crcs = true
connect                            | 	confluent.topic.consumer.client.dns.lookup = use_all_dns_ips
connect                            | 	confluent.topic.consumer.client.id = 
connect                            | 	confluent.topic.consumer.client.rack = 
connect                            | 	confluent.topic.consumer.connections.max.idle.ms = 540000
connect                            | 	confluent.topic.consumer.default.api.timeout.ms = 60000
connect                            | 	confluent.topic.consumer.enable.auto.commit = true
connect                            | 	confluent.topic.consumer.exclude.internal.topics = true
connect                            | 	confluent.topic.consumer.fetch.max.bytes = 52428800
connect                            | 	confluent.topic.consumer.fetch.max.wait.ms = 500
connect                            | 	confluent.topic.consumer.fetch.min.bytes = 1
connect                            | 	confluent.topic.consumer.group.id = null
connect                            | 	confluent.topic.consumer.group.instance.id = null
connect                            | 	confluent.topic.consumer.heartbeat.interval.ms = 3000
connect                            | 	confluent.topic.consumer.interceptor.classes = []
connect                            | 	confluent.topic.consumer.internal.leave.group.on.close = true
connect                            | 	confluent.topic.consumer.internal.throw.on.fetch.stable.offset.unsupported = false
connect                            | 	confluent.topic.consumer.isolation.level = read_uncommitted
connect                            | 	confluent.topic.consumer.max.partition.fetch.bytes = 1048576
connect                            | 	confluent.topic.consumer.max.poll.interval.ms = 300000
connect                            | 	confluent.topic.consumer.max.poll.records = 500
connect                            | 	confluent.topic.consumer.metadata.max.age.ms = 300000
connect                            | 	confluent.topic.consumer.metric.reporters = []
connect                            | 	confluent.topic.consumer.metrics.num.samples = 2
connect                            | 	confluent.topic.consumer.metrics.recording.level = INFO
connect                            | 	confluent.topic.consumer.metrics.sample.window.ms = 30000
connect                            | 	confluent.topic.consumer.partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
connect                            | 	confluent.topic.consumer.receive.buffer.bytes = 65536
connect                            | 	confluent.topic.consumer.reconnect.backoff.max.ms = 1000
connect                            | 	confluent.topic.consumer.reconnect.backoff.ms = 50
connect                            | 	confluent.topic.consumer.request.timeout.ms = 30000
connect                            | 	confluent.topic.consumer.retry.backoff.ms = 100
connect                            | 	confluent.topic.consumer.sasl.client.callback.handler.class = null
connect                            | 	confluent.topic.consumer.sasl.jaas.config = null
connect                            | 	confluent.topic.consumer.sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	confluent.topic.consumer.sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	confluent.topic.consumer.sasl.kerberos.service.name = null
connect                            | 	confluent.topic.consumer.sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	confluent.topic.consumer.sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	confluent.topic.consumer.sasl.login.callback.handler.class = null
connect                            | 	confluent.topic.consumer.sasl.login.class = null
connect                            | 	confluent.topic.consumer.sasl.login.connect.timeout.ms = null
connect                            | 	confluent.topic.consumer.sasl.login.read.timeout.ms = null
connect                            | 	confluent.topic.consumer.sasl.login.refresh.buffer.seconds = 300
connect                            | 	confluent.topic.consumer.sasl.login.refresh.min.period.seconds = 60
connect                            | 	confluent.topic.consumer.sasl.login.refresh.window.factor = 0.8
connect                            | 	confluent.topic.consumer.sasl.login.refresh.window.jitter = 0.05
connect                            | 	confluent.topic.consumer.sasl.login.retry.backoff.max.ms = 10000
connect                            | 	confluent.topic.consumer.sasl.login.retry.backoff.ms = 100
connect                            | 	confluent.topic.consumer.sasl.mechanism = GSSAPI
connect                            | 	confluent.topic.consumer.sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	confluent.topic.consumer.sasl.oauthbearer.expected.audience = null
connect                            | 	confluent.topic.consumer.sasl.oauthbearer.expected.issuer = null
connect                            | 	confluent.topic.consumer.sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	confluent.topic.consumer.sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	confluent.topic.consumer.sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	confluent.topic.consumer.sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	confluent.topic.consumer.sasl.oauthbearer.scope.claim.name = scope
connect                            | 	confluent.topic.consumer.sasl.oauthbearer.sub.claim.name = sub
connect                            | 	confluent.topic.consumer.sasl.oauthbearer.token.endpoint.url = null
connect                            | 	confluent.topic.consumer.security.protocol = PLAINTEXT
connect                            | 	confluent.topic.consumer.security.providers = null
connect                            | 	confluent.topic.consumer.send.buffer.bytes = 131072
connect                            | 	confluent.topic.consumer.session.timeout.ms = 45000
connect                            | 	confluent.topic.consumer.socket.connection.setup.timeout.max.ms = 30000
connect                            | 	confluent.topic.consumer.socket.connection.setup.timeout.ms = 10000
connect                            | 	confluent.topic.consumer.ssl.cipher.suites = null
connect                            | 	confluent.topic.consumer.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	confluent.topic.consumer.ssl.endpoint.identification.algorithm = https
connect                            | 	confluent.topic.consumer.ssl.engine.factory.class = null
connect                            | 	confluent.topic.consumer.ssl.key.password = null
connect                            | 	confluent.topic.consumer.ssl.keymanager.algorithm = SunX509
connect                            | 	confluent.topic.consumer.ssl.keystore.certificate.chain = null
connect                            | 	confluent.topic.consumer.ssl.keystore.key = null
connect                            | 	confluent.topic.consumer.ssl.keystore.location = null
connect                            | 	confluent.topic.consumer.ssl.keystore.password = null
connect                            | 	confluent.topic.consumer.ssl.keystore.type = JKS
connect                            | 	confluent.topic.consumer.ssl.protocol = TLSv1.3
connect                            | 	confluent.topic.consumer.ssl.provider = null
connect                            | 	confluent.topic.consumer.ssl.secure.random.implementation = null
connect                            | 	confluent.topic.consumer.ssl.trustmanager.algorithm = PKIX
connect                            | 	confluent.topic.consumer.ssl.truststore.certificates = null
connect                            | 	confluent.topic.consumer.ssl.truststore.location = null
connect                            | 	confluent.topic.consumer.ssl.truststore.password = null
connect                            | 	confluent.topic.consumer.ssl.truststore.type = JKS
connect                            | 	confluent.topic.interceptor.classes = []
connect                            | 	confluent.topic.metadata.max.age.ms = 300000
connect                            | 	confluent.topic.metric.reporters = []
connect                            | 	confluent.topic.metrics.num.samples = 2
connect                            | 	confluent.topic.metrics.recording.level = INFO
connect                            | 	confluent.topic.metrics.sample.window.ms = 30000
connect                            | 	confluent.topic.producer.acks = all
connect                            | 	confluent.topic.producer.batch.size = 16384
connect                            | 	confluent.topic.producer.buffer.memory = 33554432
connect                            | 	confluent.topic.producer.client.dns.lookup = use_all_dns_ips
connect                            | 	confluent.topic.producer.client.id = 
connect                            | 	confluent.topic.producer.compression.type = none
connect                            | 	confluent.topic.producer.connections.max.idle.ms = 540000
connect                            | 	confluent.topic.producer.delivery.timeout.ms = 120000
connect                            | 	confluent.topic.producer.enable.idempotence = true
connect                            | 	confluent.topic.producer.interceptor.classes = []
connect                            | 	confluent.topic.producer.linger.ms = 0
connect                            | 	confluent.topic.producer.max.block.ms = 60000
connect                            | 	confluent.topic.producer.max.in.flight.requests.per.connection = 5
connect                            | 	confluent.topic.producer.max.request.size = 1048576
connect                            | 	confluent.topic.producer.metadata.max.age.ms = 300000
connect                            | 	confluent.topic.producer.metadata.max.idle.ms = 300000
connect                            | 	confluent.topic.producer.metric.reporters = []
connect                            | 	confluent.topic.producer.metrics.num.samples = 2
connect                            | 	confluent.topic.producer.metrics.recording.level = INFO
connect                            | 	confluent.topic.producer.metrics.sample.window.ms = 30000
connect                            | 	confluent.topic.producer.partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
connect                            | 	confluent.topic.producer.receive.buffer.bytes = 32768
connect                            | 	confluent.topic.producer.reconnect.backoff.max.ms = 1000
connect                            | 	confluent.topic.producer.reconnect.backoff.ms = 50
connect                            | 	confluent.topic.producer.request.timeout.ms = 30000
connect                            | 	confluent.topic.producer.retry.backoff.ms = 100
connect                            | 	confluent.topic.producer.sasl.client.callback.handler.class = null
connect                            | 	confluent.topic.producer.sasl.jaas.config = null
connect                            | 	confluent.topic.producer.sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	confluent.topic.producer.sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	confluent.topic.producer.sasl.kerberos.service.name = null
connect                            | 	confluent.topic.producer.sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	confluent.topic.producer.sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	confluent.topic.producer.sasl.login.callback.handler.class = null
connect                            | 	confluent.topic.producer.sasl.login.class = null
connect                            | 	confluent.topic.producer.sasl.login.connect.timeout.ms = null
connect                            | 	confluent.topic.producer.sasl.login.read.timeout.ms = null
connect                            | 	confluent.topic.producer.sasl.login.refresh.buffer.seconds = 300
connect                            | 	confluent.topic.producer.sasl.login.refresh.min.period.seconds = 60
connect                            | 	confluent.topic.producer.sasl.login.refresh.window.factor = 0.8
connect                            | 	confluent.topic.producer.sasl.login.refresh.window.jitter = 0.05
connect                            | 	confluent.topic.producer.sasl.login.retry.backoff.max.ms = 10000
connect                            | 	confluent.topic.producer.sasl.login.retry.backoff.ms = 100
connect                            | 	confluent.topic.producer.sasl.mechanism = GSSAPI
connect                            | 	confluent.topic.producer.sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	confluent.topic.producer.sasl.oauthbearer.expected.audience = null
connect                            | 	confluent.topic.producer.sasl.oauthbearer.expected.issuer = null
connect                            | 	confluent.topic.producer.sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	confluent.topic.producer.sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	confluent.topic.producer.sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	confluent.topic.producer.sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	confluent.topic.producer.sasl.oauthbearer.scope.claim.name = scope
connect                            | 	confluent.topic.producer.sasl.oauthbearer.sub.claim.name = sub
connect                            | 	confluent.topic.producer.sasl.oauthbearer.token.endpoint.url = null
connect                            | 	confluent.topic.producer.security.protocol = PLAINTEXT
connect                            | 	confluent.topic.producer.security.providers = null
connect                            | 	confluent.topic.producer.send.buffer.bytes = 131072
connect                            | 	confluent.topic.producer.socket.connection.setup.timeout.max.ms = 30000
connect                            | 	confluent.topic.producer.socket.connection.setup.timeout.ms = 10000
connect                            | 	confluent.topic.producer.ssl.cipher.suites = null
connect                            | 	confluent.topic.producer.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	confluent.topic.producer.ssl.endpoint.identification.algorithm = https
connect                            | 	confluent.topic.producer.ssl.engine.factory.class = null
connect                            | 	confluent.topic.producer.ssl.key.password = null
connect                            | 	confluent.topic.producer.ssl.keymanager.algorithm = SunX509
connect                            | 	confluent.topic.producer.ssl.keystore.certificate.chain = null
connect                            | 	confluent.topic.producer.ssl.keystore.key = null
connect                            | 	confluent.topic.producer.ssl.keystore.location = null
connect                            | 	confluent.topic.producer.ssl.keystore.password = null
connect                            | 	confluent.topic.producer.ssl.keystore.type = JKS
connect                            | 	confluent.topic.producer.ssl.protocol = TLSv1.3
connect                            | 	confluent.topic.producer.ssl.provider = null
connect                            | 	confluent.topic.producer.ssl.secure.random.implementation = null
connect                            | 	confluent.topic.producer.ssl.trustmanager.algorithm = PKIX
connect                            | 	confluent.topic.producer.ssl.truststore.certificates = null
connect                            | 	confluent.topic.producer.ssl.truststore.location = null
connect                            | 	confluent.topic.producer.ssl.truststore.password = null
connect                            | 	confluent.topic.producer.ssl.truststore.type = JKS
connect                            | 	confluent.topic.producer.transaction.timeout.ms = 60000
connect                            | 	confluent.topic.producer.transactional.id = null
connect                            | 	confluent.topic.receive.buffer.bytes = 32768
connect                            | 	confluent.topic.reconnect.backoff.max.ms = 1000
connect                            | 	confluent.topic.reconnect.backoff.ms = 50
connect                            | 	confluent.topic.replication.factor = 3
connect                            | 	confluent.topic.request.timeout.ms = 30000
connect                            | 	confluent.topic.retry.backoff.ms = 100
connect                            | 	confluent.topic.sasl.client.callback.handler.class = null
connect                            | 	confluent.topic.sasl.jaas.config = null
connect                            | 	confluent.topic.sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	confluent.topic.sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	confluent.topic.sasl.kerberos.service.name = null
connect                            | 	confluent.topic.sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	confluent.topic.sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	confluent.topic.sasl.login.callback.handler.class = null
connect                            | 	confluent.topic.sasl.login.class = null
connect                            | 	confluent.topic.sasl.login.connect.timeout.ms = null
connect                            | 	confluent.topic.sasl.login.read.timeout.ms = null
connect                            | 	confluent.topic.sasl.login.refresh.buffer.seconds = 300
connect                            | 	confluent.topic.sasl.login.refresh.min.period.seconds = 60
connect                            | 	confluent.topic.sasl.login.refresh.window.factor = 0.8
connect                            | 	confluent.topic.sasl.login.refresh.window.jitter = 0.05
connect                            | 	confluent.topic.sasl.login.retry.backoff.max.ms = 10000
connect                            | 	confluent.topic.sasl.login.retry.backoff.ms = 100
connect                            | 	confluent.topic.sasl.mechanism = GSSAPI
connect                            | 	confluent.topic.sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	confluent.topic.sasl.oauthbearer.expected.audience = null
connect                            | 	confluent.topic.sasl.oauthbearer.expected.issuer = null
connect                            | 	confluent.topic.sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	confluent.topic.sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	confluent.topic.sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	confluent.topic.sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	confluent.topic.sasl.oauthbearer.scope.claim.name = scope
connect                            | 	confluent.topic.sasl.oauthbearer.sub.claim.name = sub
connect                            | 	confluent.topic.sasl.oauthbearer.token.endpoint.url = null
connect                            | 	confluent.topic.security.protocol = PLAINTEXT
connect                            | 	confluent.topic.security.providers = null
connect                            | 	confluent.topic.send.buffer.bytes = 131072
connect                            | 	confluent.topic.socket.connection.setup.timeout.max.ms = 30000
connect                            | 	confluent.topic.socket.connection.setup.timeout.ms = 10000
connect                            | 	confluent.topic.ssl.cipher.suites = null
connect                            | 	confluent.topic.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	confluent.topic.ssl.endpoint.identification.algorithm = https
connect                            | 	confluent.topic.ssl.engine.factory.class = null
connect                            | 	confluent.topic.ssl.key.password = null
connect                            | 	confluent.topic.ssl.keymanager.algorithm = SunX509
connect                            | 	confluent.topic.ssl.keystore.certificate.chain = null
connect                            | 	confluent.topic.ssl.keystore.key = null
connect                            | 	confluent.topic.ssl.keystore.location = null
connect                            | 	confluent.topic.ssl.keystore.password = null
connect                            | 	confluent.topic.ssl.keystore.type = JKS
connect                            | 	confluent.topic.ssl.protocol = TLSv1.3
connect                            | 	confluent.topic.ssl.provider = null
connect                            | 	confluent.topic.ssl.secure.random.implementation = null
connect                            | 	confluent.topic.ssl.trustmanager.algorithm = PKIX
connect                            | 	confluent.topic.ssl.truststore.certificates = null
connect                            | 	confluent.topic.ssl.truststore.location = null
connect                            | 	confluent.topic.ssl.truststore.password = null
connect                            | 	confluent.topic.ssl.truststore.type = JKS
connect                            | 	connect.protocol = sessioned
connect                            | 	connections.max.idle.ms = 540000
connect                            | 	connector.client.config.override.policy = All
connect                            | 	group.id = compose-connect-group
connect                            | 	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
connect                            | 	heartbeat.interval.ms = 3000
connect                            | 	inter.worker.key.generation.algorithm = HmacSHA256
connect                            | 	inter.worker.key.size = null
connect                            | 	inter.worker.key.ttl.ms = 3600000
connect                            | 	inter.worker.signature.algorithm = HmacSHA256
connect                            | 	inter.worker.verification.algorithms = [HmacSHA256]
connect                            | 	key.converter = class org.apache.kafka.connect.storage.StringConverter
connect                            | 	listeners = [http://:8083]
connect                            | 	metadata.max.age.ms = 300000
connect                            | 	metric.reporters = []
connect                            | 	metrics.num.samples = 2
connect                            | 	metrics.recording.level = INFO
connect                            | 	metrics.sample.window.ms = 30000
connect                            | 	offset.flush.interval.ms = 10000
connect                            | 	offset.flush.timeout.ms = 5000
connect                            | 	offset.storage.partitions = 25
connect                            | 	offset.storage.replication.factor = 1
connect                            | 	offset.storage.topic = docker-connect-offsets
connect                            | 	plugin.path = [/usr/share/java, /usr/share/confluent-hub-components]
connect                            | 	rebalance.timeout.ms = 60000
connect                            | 	receive.buffer.bytes = 32768
connect                            | 	reconnect.backoff.max.ms = 1000
connect                            | 	reconnect.backoff.ms = 50
connect                            | 	request.timeout.ms = 40000
connect                            | 	response.http.headers.config = 
connect                            | 	rest.advertised.host.name = connect
connect                            | 	rest.advertised.listener = null
connect                            | 	rest.advertised.port = null
connect                            | 	rest.extension.classes = []
connect                            | 	rest.servlet.initializor.classes = []
connect                            | 	retry.backoff.ms = 100
connect                            | 	sasl.client.callback.handler.class = null
connect                            | 	sasl.jaas.config = null
connect                            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	sasl.kerberos.service.name = null
connect                            | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	sasl.login.callback.handler.class = null
connect                            | 	sasl.login.class = null
connect                            | 	sasl.login.connect.timeout.ms = null
connect                            | 	sasl.login.read.timeout.ms = null
connect                            | 	sasl.login.refresh.buffer.seconds = 300
connect                            | 	sasl.login.refresh.min.period.seconds = 60
connect                            | 	sasl.login.refresh.window.factor = 0.8
connect                            | 	sasl.login.refresh.window.jitter = 0.05
connect                            | 	sasl.login.retry.backoff.max.ms = 10000
connect                            | 	sasl.login.retry.backoff.ms = 100
connect                            | 	sasl.mechanism = GSSAPI
connect                            | 	sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	sasl.oauthbearer.expected.audience = null
connect                            | 	sasl.oauthbearer.expected.issuer = null
connect                            | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	sasl.oauthbearer.scope.claim.name = scope
connect                            | 	sasl.oauthbearer.sub.claim.name = sub
connect                            | 	sasl.oauthbearer.token.endpoint.url = null
connect                            | 	scheduled.rebalance.max.delay.ms = 300000
connect                            | 	security.protocol = PLAINTEXT
connect                            | 	send.buffer.bytes = 131072
connect                            | 	session.timeout.ms = 10000
connect                            | 	socket.connection.setup.timeout.max.ms = 30000
connect                            | 	socket.connection.setup.timeout.ms = 10000
connect                            | 	ssl.cipher.suites = null
connect                            | 	ssl.client.auth = none
connect                            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	ssl.endpoint.identification.algorithm = https
connect                            | 	ssl.engine.factory.class = null
connect                            | 	ssl.key.password = null
connect                            | 	ssl.keymanager.algorithm = SunX509
connect                            | 	ssl.keystore.certificate.chain = null
connect                            | 	ssl.keystore.key = null
connect                            | 	ssl.keystore.location = null
connect                            | 	ssl.keystore.password = null
connect                            | 	ssl.keystore.type = JKS
connect                            | 	ssl.protocol = TLSv1.3
connect                            | 	ssl.provider = null
connect                            | 	ssl.secure.random.implementation = null
connect                            | 	ssl.trustmanager.algorithm = PKIX
connect                            | 	ssl.truststore.certificates = null
connect                            | 	ssl.truststore.location = null
connect                            | 	ssl.truststore.password = null
connect                            | 	ssl.truststore.type = JKS
connect                            | 	status.storage.partitions = 5
connect                            | 	status.storage.replication.factor = 1
connect                            | 	status.storage.topic = docker-connect-status
connect                            | 	task.shutdown.graceful.timeout.ms = 5000
connect                            | 	topic.creation.enable = true
connect                            | 	topic.tracking.allow.reset = true
connect                            | 	topic.tracking.enable = true
connect                            | 	value.converter = class io.confluent.connect.avro.AvroConverter
connect                            | 	worker.sync.timeout.ms = 3000
connect                            | 	worker.unsync.backoff.ms = 300000
connect                            |  (org.apache.kafka.connect.runtime.distributed.DistributedConfig)
connect                            | [2023-08-04 10:14:46,756] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
connect                            | [2023-08-04 10:14:46,772] INFO AdminClientConfig values: 
connect                            | 	bootstrap.servers = [broker:29092]
connect                            | 	client.dns.lookup = use_all_dns_ips
connect                            | 	client.id = 
connect                            | 	connections.max.idle.ms = 300000
connect                            | 	default.api.timeout.ms = 60000
connect                            | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
connect                            | 	metadata.max.age.ms = 300000
connect                            | 	metric.reporters = []
connect                            | 	metrics.num.samples = 2
connect                            | 	metrics.recording.level = INFO
connect                            | 	metrics.sample.window.ms = 30000
connect                            | 	receive.buffer.bytes = 65536
connect                            | 	reconnect.backoff.max.ms = 1000
connect                            | 	reconnect.backoff.ms = 50
connect                            | 	request.timeout.ms = 30000
connect                            | 	retries = 2147483647
connect                            | 	retry.backoff.ms = 100
connect                            | 	sasl.client.callback.handler.class = null
connect                            | 	sasl.jaas.config = null
connect                            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	sasl.kerberos.service.name = null
connect                            | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	sasl.login.callback.handler.class = null
connect                            | 	sasl.login.class = null
connect                            | 	sasl.login.connect.timeout.ms = null
connect                            | 	sasl.login.read.timeout.ms = null
connect                            | 	sasl.login.refresh.buffer.seconds = 300
connect                            | 	sasl.login.refresh.min.period.seconds = 60
connect                            | 	sasl.login.refresh.window.factor = 0.8
connect                            | 	sasl.login.refresh.window.jitter = 0.05
connect                            | 	sasl.login.retry.backoff.max.ms = 10000
connect                            | 	sasl.login.retry.backoff.ms = 100
connect                            | 	sasl.mechanism = GSSAPI
connect                            | 	sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	sasl.oauthbearer.expected.audience = null
connect                            | 	sasl.oauthbearer.expected.issuer = null
connect                            | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	sasl.oauthbearer.scope.claim.name = scope
connect                            | 	sasl.oauthbearer.sub.claim.name = sub
connect                            | 	sasl.oauthbearer.token.endpoint.url = null
connect                            | 	security.protocol = PLAINTEXT
connect                            | 	security.providers = null
connect                            | 	send.buffer.bytes = 131072
connect                            | 	socket.connection.setup.timeout.max.ms = 30000
connect                            | 	socket.connection.setup.timeout.ms = 10000
connect                            | 	ssl.cipher.suites = null
connect                            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	ssl.endpoint.identification.algorithm = https
connect                            | 	ssl.engine.factory.class = null
connect                            | 	ssl.key.password = null
connect                            | 	ssl.keymanager.algorithm = SunX509
connect                            | 	ssl.keystore.certificate.chain = null
connect                            | 	ssl.keystore.key = null
connect                            | 	ssl.keystore.location = null
connect                            | 	ssl.keystore.password = null
connect                            | 	ssl.keystore.type = JKS
connect                            | 	ssl.protocol = TLSv1.3
connect                            | 	ssl.provider = null
connect                            | 	ssl.secure.random.implementation = null
connect                            | 	ssl.trustmanager.algorithm = PKIX
connect                            | 	ssl.truststore.certificates = null
connect                            | 	ssl.truststore.location = null
connect                            | 	ssl.truststore.password = null
connect                            | 	ssl.truststore.type = JKS
connect                            |  (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,196] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,196] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,196] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,196] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,196] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,196] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,197] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,197] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,197] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,197] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,197] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,198] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,198] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,198] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,198] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,198] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,199] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,199] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:47,202] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:47,202] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:47,202] INFO Kafka startTimeMs: 1691144087200 (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:49,112] INFO Kafka cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.connect.util.ConnectUtils)
connect                            | [2023-08-04 10:14:49,115] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:49,154] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:49,155] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:49,155] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:49,209] INFO Logging initialized @275910ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
connect                            | [2023-08-04 10:14:49,441] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer)
connect                            | [2023-08-04 10:14:49,443] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer)
connect                            | [2023-08-04 10:14:49,470] INFO jetty-9.4.44.v20210927; built: 2021-09-27T23:02:44.612Z; git: 8da83308eeca865e495e53ef315a249d63ba9332; jvm 11.0.14.1+1-LTS (org.eclipse.jetty.server.Server)
connect                            | [2023-08-04 10:14:49,561] INFO Started http_8083@19b4dbb0{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector)
connect                            | [2023-08-04 10:14:49,563] INFO Started @276262ms (org.eclipse.jetty.server.Server)
connect                            | [2023-08-04 10:14:49,657] INFO Advertised URI: http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
connect                            | [2023-08-04 10:14:49,657] INFO REST server listening at http://192.168.160.8:8083/, advertising URL http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
connect                            | [2023-08-04 10:14:49,658] INFO Advertised URI: http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
connect                            | [2023-08-04 10:14:49,658] INFO REST admin endpoints at http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
connect                            | [2023-08-04 10:14:49,659] INFO Advertised URI: http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
connect                            | [2023-08-04 10:14:49,671] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
connect                            | [2023-08-04 10:14:49,672] INFO AdminClientConfig values: 
connect                            | 	bootstrap.servers = [broker:29092]
connect                            | 	client.dns.lookup = use_all_dns_ips
connect                            | 	client.id = 
connect                            | 	connections.max.idle.ms = 300000
connect                            | 	default.api.timeout.ms = 60000
connect                            | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
connect                            | 	metadata.max.age.ms = 300000
connect                            | 	metric.reporters = []
connect                            | 	metrics.num.samples = 2
connect                            | 	metrics.recording.level = INFO
connect                            | 	metrics.sample.window.ms = 30000
connect                            | 	receive.buffer.bytes = 65536
connect                            | 	reconnect.backoff.max.ms = 1000
connect                            | 	reconnect.backoff.ms = 50
connect                            | 	request.timeout.ms = 30000
connect                            | 	retries = 2147483647
connect                            | 	retry.backoff.ms = 100
connect                            | 	sasl.client.callback.handler.class = null
connect                            | 	sasl.jaas.config = null
connect                            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	sasl.kerberos.service.name = null
connect                            | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	sasl.login.callback.handler.class = null
connect                            | 	sasl.login.class = null
connect                            | 	sasl.login.connect.timeout.ms = null
connect                            | 	sasl.login.read.timeout.ms = null
connect                            | 	sasl.login.refresh.buffer.seconds = 300
connect                            | 	sasl.login.refresh.min.period.seconds = 60
connect                            | 	sasl.login.refresh.window.factor = 0.8
connect                            | 	sasl.login.refresh.window.jitter = 0.05
connect                            | 	sasl.login.retry.backoff.max.ms = 10000
connect                            | 	sasl.login.retry.backoff.ms = 100
connect                            | 	sasl.mechanism = GSSAPI
connect                            | 	sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	sasl.oauthbearer.expected.audience = null
connect                            | 	sasl.oauthbearer.expected.issuer = null
connect                            | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	sasl.oauthbearer.scope.claim.name = scope
connect                            | 	sasl.oauthbearer.sub.claim.name = sub
connect                            | 	sasl.oauthbearer.token.endpoint.url = null
connect                            | 	security.protocol = PLAINTEXT
connect                            | 	security.providers = null
connect                            | 	send.buffer.bytes = 131072
connect                            | 	socket.connection.setup.timeout.max.ms = 30000
connect                            | 	socket.connection.setup.timeout.ms = 10000
connect                            | 	ssl.cipher.suites = null
connect                            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	ssl.endpoint.identification.algorithm = https
connect                            | 	ssl.engine.factory.class = null
connect                            | 	ssl.key.password = null
connect                            | 	ssl.keymanager.algorithm = SunX509
connect                            | 	ssl.keystore.certificate.chain = null
connect                            | 	ssl.keystore.key = null
connect                            | 	ssl.keystore.location = null
connect                            | 	ssl.keystore.password = null
connect                            | 	ssl.keystore.type = JKS
connect                            | 	ssl.protocol = TLSv1.3
connect                            | 	ssl.provider = null
connect                            | 	ssl.secure.random.implementation = null
connect                            | 	ssl.trustmanager.algorithm = PKIX
connect                            | 	ssl.truststore.certificates = null
connect                            | 	ssl.truststore.location = null
connect                            | 	ssl.truststore.password = null
connect                            | 	ssl.truststore.type = JKS
connect                            |  (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,694] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,694] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,694] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,694] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,694] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,695] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,695] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,695] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,695] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,695] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,695] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,695] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,695] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,696] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,696] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,696] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,696] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,696] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,696] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:49,697] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:49,697] INFO Kafka startTimeMs: 1691144089696 (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:49,730] INFO Kafka cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.connect.util.ConnectUtils)
connect                            | [2023-08-04 10:14:49,732] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:49,745] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:49,746] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:49,746] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:49,766] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy)
connect                            | [2023-08-04 10:14:49,799] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
connect                            | [2023-08-04 10:14:49,801] INFO AdminClientConfig values: 
connect                            | 	bootstrap.servers = [broker:29092]
connect                            | 	client.dns.lookup = use_all_dns_ips
connect                            | 	client.id = 
connect                            | 	connections.max.idle.ms = 300000
connect                            | 	default.api.timeout.ms = 60000
connect                            | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
connect                            | 	metadata.max.age.ms = 300000
connect                            | 	metric.reporters = []
connect                            | 	metrics.num.samples = 2
connect                            | 	metrics.recording.level = INFO
connect                            | 	metrics.sample.window.ms = 30000
connect                            | 	receive.buffer.bytes = 65536
connect                            | 	reconnect.backoff.max.ms = 1000
connect                            | 	reconnect.backoff.ms = 50
connect                            | 	request.timeout.ms = 30000
connect                            | 	retries = 2147483647
connect                            | 	retry.backoff.ms = 100
connect                            | 	sasl.client.callback.handler.class = null
connect                            | 	sasl.jaas.config = null
connect                            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	sasl.kerberos.service.name = null
connect                            | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	sasl.login.callback.handler.class = null
connect                            | 	sasl.login.class = null
connect                            | 	sasl.login.connect.timeout.ms = null
connect                            | 	sasl.login.read.timeout.ms = null
connect                            | 	sasl.login.refresh.buffer.seconds = 300
connect                            | 	sasl.login.refresh.min.period.seconds = 60
connect                            | 	sasl.login.refresh.window.factor = 0.8
connect                            | 	sasl.login.refresh.window.jitter = 0.05
connect                            | 	sasl.login.retry.backoff.max.ms = 10000
connect                            | 	sasl.login.retry.backoff.ms = 100
connect                            | 	sasl.mechanism = GSSAPI
connect                            | 	sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	sasl.oauthbearer.expected.audience = null
connect                            | 	sasl.oauthbearer.expected.issuer = null
connect                            | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	sasl.oauthbearer.scope.claim.name = scope
connect                            | 	sasl.oauthbearer.sub.claim.name = sub
connect                            | 	sasl.oauthbearer.token.endpoint.url = null
connect                            | 	security.protocol = PLAINTEXT
connect                            | 	security.providers = null
connect                            | 	send.buffer.bytes = 131072
connect                            | 	socket.connection.setup.timeout.max.ms = 30000
connect                            | 	socket.connection.setup.timeout.ms = 10000
connect                            | 	ssl.cipher.suites = null
connect                            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	ssl.endpoint.identification.algorithm = https
connect                            | 	ssl.engine.factory.class = null
connect                            | 	ssl.key.password = null
connect                            | 	ssl.keymanager.algorithm = SunX509
connect                            | 	ssl.keystore.certificate.chain = null
connect                            | 	ssl.keystore.key = null
connect                            | 	ssl.keystore.location = null
connect                            | 	ssl.keystore.password = null
connect                            | 	ssl.keystore.type = JKS
connect                            | 	ssl.protocol = TLSv1.3
connect                            | 	ssl.provider = null
connect                            | 	ssl.secure.random.implementation = null
connect                            | 	ssl.trustmanager.algorithm = PKIX
connect                            | 	ssl.truststore.certificates = null
connect                            | 	ssl.truststore.location = null
connect                            | 	ssl.truststore.password = null
connect                            | 	ssl.truststore.type = JKS
connect                            |  (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,819] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,819] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,819] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,819] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,819] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,819] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,819] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,820] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,820] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,820] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,820] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,820] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,820] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,820] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,820] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,820] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,820] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,820] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:49,821] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:49,821] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:49,821] INFO Kafka startTimeMs: 1691144089821 (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:49,850] INFO Kafka cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.connect.util.ConnectUtils)
connect                            | [2023-08-04 10:14:49,852] INFO App info kafka.admin.client for adminclient-3 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:49,862] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:49,862] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:49,863] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:49,877] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:49,877] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:49,877] INFO Kafka startTimeMs: 1691144089877 (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,407] INFO JsonConverterConfig values: 
connect                            | 	converter.type = key
connect                            | 	decimal.format = BASE64
connect                            | 	schemas.cache.size = 1000
connect                            | 	schemas.enable = false
connect                            |  (org.apache.kafka.connect.json.JsonConverterConfig)
connect                            | [2023-08-04 10:14:50,411] INFO JsonConverterConfig values: 
connect                            | 	converter.type = value
connect                            | 	decimal.format = BASE64
connect                            | 	schemas.cache.size = 1000
connect                            | 	schemas.enable = false
connect                            |  (org.apache.kafka.connect.json.JsonConverterConfig)
connect                            | [2023-08-04 10:14:50,411] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
connect                            | [2023-08-04 10:14:50,412] INFO AdminClientConfig values: 
connect                            | 	bootstrap.servers = [broker:29092]
connect                            | 	client.dns.lookup = use_all_dns_ips
connect                            | 	client.id = 
connect                            | 	connections.max.idle.ms = 300000
connect                            | 	default.api.timeout.ms = 60000
connect                            | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
connect                            | 	metadata.max.age.ms = 300000
connect                            | 	metric.reporters = []
connect                            | 	metrics.num.samples = 2
connect                            | 	metrics.recording.level = INFO
connect                            | 	metrics.sample.window.ms = 30000
connect                            | 	receive.buffer.bytes = 65536
connect                            | 	reconnect.backoff.max.ms = 1000
connect                            | 	reconnect.backoff.ms = 50
connect                            | 	request.timeout.ms = 30000
connect                            | 	retries = 2147483647
connect                            | 	retry.backoff.ms = 100
connect                            | 	sasl.client.callback.handler.class = null
connect                            | 	sasl.jaas.config = null
connect                            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	sasl.kerberos.service.name = null
connect                            | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	sasl.login.callback.handler.class = null
connect                            | 	sasl.login.class = null
connect                            | 	sasl.login.connect.timeout.ms = null
connect                            | 	sasl.login.read.timeout.ms = null
connect                            | 	sasl.login.refresh.buffer.seconds = 300
connect                            | 	sasl.login.refresh.min.period.seconds = 60
connect                            | 	sasl.login.refresh.window.factor = 0.8
connect                            | 	sasl.login.refresh.window.jitter = 0.05
connect                            | 	sasl.login.retry.backoff.max.ms = 10000
connect                            | 	sasl.login.retry.backoff.ms = 100
connect                            | 	sasl.mechanism = GSSAPI
connect                            | 	sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	sasl.oauthbearer.expected.audience = null
connect                            | 	sasl.oauthbearer.expected.issuer = null
connect                            | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	sasl.oauthbearer.scope.claim.name = scope
connect                            | 	sasl.oauthbearer.sub.claim.name = sub
connect                            | 	sasl.oauthbearer.token.endpoint.url = null
connect                            | 	security.protocol = PLAINTEXT
connect                            | 	security.providers = null
connect                            | 	send.buffer.bytes = 131072
connect                            | 	socket.connection.setup.timeout.max.ms = 30000
connect                            | 	socket.connection.setup.timeout.ms = 10000
connect                            | 	ssl.cipher.suites = null
connect                            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	ssl.endpoint.identification.algorithm = https
connect                            | 	ssl.engine.factory.class = null
connect                            | 	ssl.key.password = null
connect                            | 	ssl.keymanager.algorithm = SunX509
connect                            | 	ssl.keystore.certificate.chain = null
connect                            | 	ssl.keystore.key = null
connect                            | 	ssl.keystore.location = null
connect                            | 	ssl.keystore.password = null
connect                            | 	ssl.keystore.type = JKS
connect                            | 	ssl.protocol = TLSv1.3
connect                            | 	ssl.provider = null
connect                            | 	ssl.secure.random.implementation = null
connect                            | 	ssl.trustmanager.algorithm = PKIX
connect                            | 	ssl.truststore.certificates = null
connect                            | 	ssl.truststore.location = null
connect                            | 	ssl.truststore.password = null
connect                            | 	ssl.truststore.type = JKS
connect                            |  (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,428] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,428] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,428] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,428] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,428] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,430] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,430] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,430] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,430] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,430] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,431] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,431] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,431] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,431] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,432] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,432] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,432] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,432] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,432] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,433] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,433] INFO Kafka startTimeMs: 1691144090432 (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,461] INFO Kafka cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.connect.util.ConnectUtils)
connect                            | [2023-08-04 10:14:50,463] INFO App info kafka.admin.client for adminclient-4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,470] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:50,470] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:50,470] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:50,499] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
connect                            | [2023-08-04 10:14:50,500] INFO AdminClientConfig values: 
connect                            | 	bootstrap.servers = [broker:29092]
connect                            | 	client.dns.lookup = use_all_dns_ips
connect                            | 	client.id = 
connect                            | 	connections.max.idle.ms = 300000
connect                            | 	default.api.timeout.ms = 60000
connect                            | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
connect                            | 	metadata.max.age.ms = 300000
connect                            | 	metric.reporters = []
connect                            | 	metrics.num.samples = 2
connect                            | 	metrics.recording.level = INFO
connect                            | 	metrics.sample.window.ms = 30000
connect                            | 	receive.buffer.bytes = 65536
connect                            | 	reconnect.backoff.max.ms = 1000
connect                            | 	reconnect.backoff.ms = 50
connect                            | 	request.timeout.ms = 30000
connect                            | 	retries = 2147483647
connect                            | 	retry.backoff.ms = 100
connect                            | 	sasl.client.callback.handler.class = null
connect                            | 	sasl.jaas.config = null
connect                            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	sasl.kerberos.service.name = null
connect                            | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	sasl.login.callback.handler.class = null
connect                            | 	sasl.login.class = null
connect                            | 	sasl.login.connect.timeout.ms = null
connect                            | 	sasl.login.read.timeout.ms = null
connect                            | 	sasl.login.refresh.buffer.seconds = 300
connect                            | 	sasl.login.refresh.min.period.seconds = 60
connect                            | 	sasl.login.refresh.window.factor = 0.8
connect                            | 	sasl.login.refresh.window.jitter = 0.05
connect                            | 	sasl.login.retry.backoff.max.ms = 10000
connect                            | 	sasl.login.retry.backoff.ms = 100
connect                            | 	sasl.mechanism = GSSAPI
connect                            | 	sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	sasl.oauthbearer.expected.audience = null
connect                            | 	sasl.oauthbearer.expected.issuer = null
connect                            | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	sasl.oauthbearer.scope.claim.name = scope
connect                            | 	sasl.oauthbearer.sub.claim.name = sub
connect                            | 	sasl.oauthbearer.token.endpoint.url = null
connect                            | 	security.protocol = PLAINTEXT
connect                            | 	security.providers = null
connect                            | 	send.buffer.bytes = 131072
connect                            | 	socket.connection.setup.timeout.max.ms = 30000
connect                            | 	socket.connection.setup.timeout.ms = 10000
connect                            | 	ssl.cipher.suites = null
connect                            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	ssl.endpoint.identification.algorithm = https
connect                            | 	ssl.engine.factory.class = null
connect                            | 	ssl.key.password = null
connect                            | 	ssl.keymanager.algorithm = SunX509
connect                            | 	ssl.keystore.certificate.chain = null
connect                            | 	ssl.keystore.key = null
connect                            | 	ssl.keystore.location = null
connect                            | 	ssl.keystore.password = null
connect                            | 	ssl.keystore.type = JKS
connect                            | 	ssl.protocol = TLSv1.3
connect                            | 	ssl.provider = null
connect                            | 	ssl.secure.random.implementation = null
connect                            | 	ssl.trustmanager.algorithm = PKIX
connect                            | 	ssl.truststore.certificates = null
connect                            | 	ssl.truststore.location = null
connect                            | 	ssl.truststore.password = null
connect                            | 	ssl.truststore.type = JKS
connect                            |  (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,514] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,514] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,515] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,516] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,516] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,516] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,516] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,517] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,517] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,517] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,517] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,517] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,517] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,517] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,517] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,517] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,517] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,518] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,518] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,518] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,518] INFO Kafka startTimeMs: 1691144090518 (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,544] INFO Kafka cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.connect.util.ConnectUtils)
connect                            | [2023-08-04 10:14:50,546] INFO App info kafka.admin.client for adminclient-5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,553] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:50,553] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:50,554] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:50,568] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
connect                            | [2023-08-04 10:14:50,569] INFO AdminClientConfig values: 
connect                            | 	bootstrap.servers = [broker:29092]
connect                            | 	client.dns.lookup = use_all_dns_ips
connect                            | 	client.id = 
connect                            | 	connections.max.idle.ms = 300000
connect                            | 	default.api.timeout.ms = 60000
connect                            | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
connect                            | 	metadata.max.age.ms = 300000
connect                            | 	metric.reporters = []
connect                            | 	metrics.num.samples = 2
connect                            | 	metrics.recording.level = INFO
connect                            | 	metrics.sample.window.ms = 30000
connect                            | 	receive.buffer.bytes = 65536
connect                            | 	reconnect.backoff.max.ms = 1000
connect                            | 	reconnect.backoff.ms = 50
connect                            | 	request.timeout.ms = 30000
connect                            | 	retries = 2147483647
connect                            | 	retry.backoff.ms = 100
connect                            | 	sasl.client.callback.handler.class = null
connect                            | 	sasl.jaas.config = null
connect                            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	sasl.kerberos.service.name = null
connect                            | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	sasl.login.callback.handler.class = null
connect                            | 	sasl.login.class = null
connect                            | 	sasl.login.connect.timeout.ms = null
connect                            | 	sasl.login.read.timeout.ms = null
connect                            | 	sasl.login.refresh.buffer.seconds = 300
connect                            | 	sasl.login.refresh.min.period.seconds = 60
connect                            | 	sasl.login.refresh.window.factor = 0.8
connect                            | 	sasl.login.refresh.window.jitter = 0.05
connect                            | 	sasl.login.retry.backoff.max.ms = 10000
connect                            | 	sasl.login.retry.backoff.ms = 100
connect                            | 	sasl.mechanism = GSSAPI
connect                            | 	sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	sasl.oauthbearer.expected.audience = null
connect                            | 	sasl.oauthbearer.expected.issuer = null
connect                            | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	sasl.oauthbearer.scope.claim.name = scope
connect                            | 	sasl.oauthbearer.sub.claim.name = sub
connect                            | 	sasl.oauthbearer.token.endpoint.url = null
connect                            | 	security.protocol = PLAINTEXT
connect                            | 	security.providers = null
connect                            | 	send.buffer.bytes = 131072
connect                            | 	socket.connection.setup.timeout.max.ms = 30000
connect                            | 	socket.connection.setup.timeout.ms = 10000
connect                            | 	ssl.cipher.suites = null
connect                            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	ssl.endpoint.identification.algorithm = https
connect                            | 	ssl.engine.factory.class = null
connect                            | 	ssl.key.password = null
connect                            | 	ssl.keymanager.algorithm = SunX509
connect                            | 	ssl.keystore.certificate.chain = null
connect                            | 	ssl.keystore.key = null
connect                            | 	ssl.keystore.location = null
connect                            | 	ssl.keystore.password = null
connect                            | 	ssl.keystore.type = JKS
connect                            | 	ssl.protocol = TLSv1.3
connect                            | 	ssl.provider = null
connect                            | 	ssl.secure.random.implementation = null
connect                            | 	ssl.trustmanager.algorithm = PKIX
connect                            | 	ssl.truststore.certificates = null
connect                            | 	ssl.truststore.location = null
connect                            | 	ssl.truststore.password = null
connect                            | 	ssl.truststore.type = JKS
connect                            |  (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,581] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,582] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,582] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,582] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,582] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,582] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,582] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,582] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,582] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,583] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,583] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,583] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,583] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,583] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,583] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,583] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,583] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,583] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,584] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,584] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,584] INFO Kafka startTimeMs: 1691144090583 (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,607] INFO Kafka cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.connect.util.ConnectUtils)
connect                            | [2023-08-04 10:14:50,608] INFO App info kafka.admin.client for adminclient-6 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,613] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:50,613] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:50,613] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:50,660] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
connect                            | [2023-08-04 10:14:50,662] INFO AdminClientConfig values: 
connect                            | 	bootstrap.servers = [broker:29092]
connect                            | 	client.dns.lookup = use_all_dns_ips
connect                            | 	client.id = 
connect                            | 	connections.max.idle.ms = 300000
connect                            | 	default.api.timeout.ms = 60000
connect                            | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
connect                            | 	metadata.max.age.ms = 300000
connect                            | 	metric.reporters = []
connect                            | 	metrics.num.samples = 2
connect                            | 	metrics.recording.level = INFO
connect                            | 	metrics.sample.window.ms = 30000
connect                            | 	receive.buffer.bytes = 65536
connect                            | 	reconnect.backoff.max.ms = 1000
connect                            | 	reconnect.backoff.ms = 50
connect                            | 	request.timeout.ms = 30000
connect                            | 	retries = 2147483647
connect                            | 	retry.backoff.ms = 100
connect                            | 	sasl.client.callback.handler.class = null
connect                            | 	sasl.jaas.config = null
connect                            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	sasl.kerberos.service.name = null
connect                            | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	sasl.login.callback.handler.class = null
connect                            | 	sasl.login.class = null
connect                            | 	sasl.login.connect.timeout.ms = null
connect                            | 	sasl.login.read.timeout.ms = null
connect                            | 	sasl.login.refresh.buffer.seconds = 300
connect                            | 	sasl.login.refresh.min.period.seconds = 60
connect                            | 	sasl.login.refresh.window.factor = 0.8
connect                            | 	sasl.login.refresh.window.jitter = 0.05
connect                            | 	sasl.login.retry.backoff.max.ms = 10000
connect                            | 	sasl.login.retry.backoff.ms = 100
connect                            | 	sasl.mechanism = GSSAPI
connect                            | 	sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	sasl.oauthbearer.expected.audience = null
connect                            | 	sasl.oauthbearer.expected.issuer = null
connect                            | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	sasl.oauthbearer.scope.claim.name = scope
connect                            | 	sasl.oauthbearer.sub.claim.name = sub
connect                            | 	sasl.oauthbearer.token.endpoint.url = null
connect                            | 	security.protocol = PLAINTEXT
connect                            | 	security.providers = null
connect                            | 	send.buffer.bytes = 131072
connect                            | 	socket.connection.setup.timeout.max.ms = 30000
connect                            | 	socket.connection.setup.timeout.ms = 10000
connect                            | 	ssl.cipher.suites = null
connect                            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	ssl.endpoint.identification.algorithm = https
connect                            | 	ssl.engine.factory.class = null
connect                            | 	ssl.key.password = null
connect                            | 	ssl.keymanager.algorithm = SunX509
connect                            | 	ssl.keystore.certificate.chain = null
connect                            | 	ssl.keystore.key = null
connect                            | 	ssl.keystore.location = null
connect                            | 	ssl.keystore.password = null
connect                            | 	ssl.keystore.type = JKS
connect                            | 	ssl.protocol = TLSv1.3
connect                            | 	ssl.provider = null
connect                            | 	ssl.secure.random.implementation = null
connect                            | 	ssl.trustmanager.algorithm = PKIX
connect                            | 	ssl.truststore.certificates = null
connect                            | 	ssl.truststore.location = null
connect                            | 	ssl.truststore.password = null
connect                            | 	ssl.truststore.type = JKS
connect                            |  (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,677] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,678] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,679] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,679] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,680] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,681] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,681] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,682] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,683] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,683] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,684] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,685] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,685] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,686] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,686] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,687] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,688] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,688] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:50,689] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,690] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,690] INFO Kafka startTimeMs: 1691144090689 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:14:50,738] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:14:50 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
connect                            | [2023-08-04 10:14:50,751] INFO Kafka cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.connect.util.ConnectUtils)
connect                            | [2023-08-04 10:14:50,754] INFO App info kafka.admin.client for adminclient-7 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,763] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:50,764] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:14:50,765] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
data-agrigator-taskmanager-1       | 2023-08-04 10:14:50,896 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:14:50,913 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
connect                            | [2023-08-04 10:14:50,959] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,960] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,961] INFO Kafka startTimeMs: 1691144090953 (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:50,992] INFO Kafka Connect distributed worker initialization took 261277ms (org.apache.kafka.connect.cli.ConnectDistributed)
connect                            | [2023-08-04 10:14:51,001] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect)
connect                            | [2023-08-04 10:14:51,015] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer)
connect                            | [2023-08-04 10:14:51,019] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
connect                            | [2023-08-04 10:14:51,019] INFO Worker starting (org.apache.kafka.connect.runtime.Worker)
connect                            | [2023-08-04 10:14:51,019] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore)
connect                            | [2023-08-04 10:14:51,020] INFO Starting KafkaBasedLog with topic docker-connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
connect                            | [2023-08-04 10:14:51,024] INFO AdminClientConfig values: 
connect                            | 	bootstrap.servers = [broker:29092]
connect                            | 	client.dns.lookup = use_all_dns_ips
connect                            | 	client.id = 
connect                            | 	connections.max.idle.ms = 300000
connect                            | 	default.api.timeout.ms = 60000
connect                            | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
connect                            | 	metadata.max.age.ms = 300000
connect                            | 	metric.reporters = []
connect                            | 	metrics.num.samples = 2
connect                            | 	metrics.recording.level = INFO
connect                            | 	metrics.sample.window.ms = 30000
connect                            | 	receive.buffer.bytes = 65536
connect                            | 	reconnect.backoff.max.ms = 1000
connect                            | 	reconnect.backoff.ms = 50
connect                            | 	request.timeout.ms = 30000
connect                            | 	retries = 2147483647
connect                            | 	retry.backoff.ms = 100
connect                            | 	sasl.client.callback.handler.class = null
connect                            | 	sasl.jaas.config = null
connect                            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	sasl.kerberos.service.name = null
connect                            | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	sasl.login.callback.handler.class = null
connect                            | 	sasl.login.class = null
connect                            | 	sasl.login.connect.timeout.ms = null
connect                            | 	sasl.login.read.timeout.ms = null
connect                            | 	sasl.login.refresh.buffer.seconds = 300
connect                            | 	sasl.login.refresh.min.period.seconds = 60
connect                            | 	sasl.login.refresh.window.factor = 0.8
connect                            | 	sasl.login.refresh.window.jitter = 0.05
connect                            | 	sasl.login.retry.backoff.max.ms = 10000
connect                            | 	sasl.login.retry.backoff.ms = 100
connect                            | 	sasl.mechanism = GSSAPI
connect                            | 	sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	sasl.oauthbearer.expected.audience = null
connect                            | 	sasl.oauthbearer.expected.issuer = null
connect                            | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	sasl.oauthbearer.scope.claim.name = scope
connect                            | 	sasl.oauthbearer.sub.claim.name = sub
connect                            | 	sasl.oauthbearer.token.endpoint.url = null
connect                            | 	security.protocol = PLAINTEXT
connect                            | 	security.providers = null
connect                            | 	send.buffer.bytes = 131072
connect                            | 	socket.connection.setup.timeout.max.ms = 30000
connect                            | 	socket.connection.setup.timeout.ms = 10000
connect                            | 	ssl.cipher.suites = null
connect                            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	ssl.endpoint.identification.algorithm = https
connect                            | 	ssl.engine.factory.class = null
connect                            | 	ssl.key.password = null
connect                            | 	ssl.keymanager.algorithm = SunX509
connect                            | 	ssl.keystore.certificate.chain = null
connect                            | 	ssl.keystore.key = null
connect                            | 	ssl.keystore.location = null
connect                            | 	ssl.keystore.password = null
connect                            | 	ssl.keystore.type = JKS
connect                            | 	ssl.protocol = TLSv1.3
connect                            | 	ssl.provider = null
connect                            | 	ssl.secure.random.implementation = null
connect                            | 	ssl.trustmanager.algorithm = PKIX
connect                            | 	ssl.truststore.certificates = null
connect                            | 	ssl.truststore.location = null
connect                            | 	ssl.truststore.password = null
connect                            | 	ssl.truststore.type = JKS
connect                            |  (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,050] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,050] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,050] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,051] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,051] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,051] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,051] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,051] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,051] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,051] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,051] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,051] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,051] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,051] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,051] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,052] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,052] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,052] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,052] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,052] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect                            | [2023-08-04 10:14:51,052] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:51,052] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:51,052] INFO Kafka startTimeMs: 1691144091052 (org.apache.kafka.common.utils.AppInfoParser)
broker                             | [2023-08-04 10:14:51,158] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='docker-connect-offsets', numPartitions=25, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,159] INFO [Controller 1] Created topic docker-connect-offsets with topic ID XlZu4UyKTamPRyQ7VaDLgA. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,165] INFO [Controller 1] ConfigResource(type=TOPIC, name='docker-connect-offsets'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:14:51,167] INFO [Controller 1] Created partition docker-connect-offsets-0 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,168] INFO [Controller 1] Created partition docker-connect-offsets-1 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,168] INFO [Controller 1] Created partition docker-connect-offsets-2 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,169] INFO [Controller 1] Created partition docker-connect-offsets-3 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,170] INFO [Controller 1] Created partition docker-connect-offsets-4 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,171] INFO [Controller 1] Created partition docker-connect-offsets-5 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,171] INFO [Controller 1] Created partition docker-connect-offsets-6 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,172] INFO [Controller 1] Created partition docker-connect-offsets-7 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,173] INFO [Controller 1] Created partition docker-connect-offsets-8 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,179] INFO [Controller 1] Created partition docker-connect-offsets-9 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,180] INFO [Controller 1] Created partition docker-connect-offsets-10 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,181] INFO [Controller 1] Created partition docker-connect-offsets-11 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,181] INFO [Controller 1] Created partition docker-connect-offsets-12 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,182] INFO [Controller 1] Created partition docker-connect-offsets-13 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,183] INFO [Controller 1] Created partition docker-connect-offsets-14 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,184] INFO [Controller 1] Created partition docker-connect-offsets-15 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,184] INFO [Controller 1] Created partition docker-connect-offsets-16 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,185] INFO [Controller 1] Created partition docker-connect-offsets-17 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,186] INFO [Controller 1] Created partition docker-connect-offsets-18 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,187] INFO [Controller 1] Created partition docker-connect-offsets-19 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,187] INFO [Controller 1] Created partition docker-connect-offsets-20 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,188] INFO [Controller 1] Created partition docker-connect-offsets-21 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,189] INFO [Controller 1] Created partition docker-connect-offsets-22 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,190] INFO [Controller 1] Created partition docker-connect-offsets-23 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,191] INFO [Controller 1] Created partition docker-connect-offsets-24 with topic ID XlZu4UyKTamPRyQ7VaDLgA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:51,220] INFO [Broker id=1] Transitioning 25 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:14:51,220] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(docker-connect-offsets-11, docker-connect-offsets-13, docker-connect-offsets-15, docker-connect-offsets-17, docker-connect-offsets-3, docker-connect-offsets-5, docker-connect-offsets-7, docker-connect-offsets-9, docker-connect-offsets-1, docker-connect-offsets-18, docker-connect-offsets-20, docker-connect-offsets-22, docker-connect-offsets-24, docker-connect-offsets-10, docker-connect-offsets-12, docker-connect-offsets-14, docker-connect-offsets-16, docker-connect-offsets-2, docker-connect-offsets-4, docker-connect-offsets-6, docker-connect-offsets-8, docker-connect-offsets-0, docker-connect-offsets-19, docker-connect-offsets-21, docker-connect-offsets-23) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:14:51,221] INFO [Broker id=1] Creating new partition docker-connect-offsets-11 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:51,250] INFO [LogLoader partition=docker-connect-offsets-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:51,252] INFO Created log for partition docker-connect-offsets-11 in /tmp/kraft-combined-logs/docker-connect-offsets-11 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,269] INFO [Partition docker-connect-offsets-11 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-11 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,269] INFO [Partition docker-connect-offsets-11 broker=1] Log loaded for partition docker-connect-offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,269] INFO [Broker id=1] Leader docker-connect-offsets-11 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect                            | [2023-08-04 10:14:51,277] INFO Created topic (name=docker-connect-offsets, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at broker:29092 (org.apache.kafka.connect.util.TopicAdmin)
broker                             | [2023-08-04 10:14:51,282] INFO [Broker id=1] Creating new partition docker-connect-offsets-13 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:51,298] INFO [LogLoader partition=docker-connect-offsets-13, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:51,300] INFO Created log for partition docker-connect-offsets-13 in /tmp/kraft-combined-logs/docker-connect-offsets-13 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,302] INFO [Partition docker-connect-offsets-13 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-13 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,302] INFO [Partition docker-connect-offsets-13 broker=1] Log loaded for partition docker-connect-offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,302] INFO [Broker id=1] Leader docker-connect-offsets-13 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:51,323] INFO [Broker id=1] Creating new partition docker-connect-offsets-15 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:51,343] INFO [LogLoader partition=docker-connect-offsets-15, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:51,349] INFO Created log for partition docker-connect-offsets-15 in /tmp/kraft-combined-logs/docker-connect-offsets-15 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,352] INFO [Partition docker-connect-offsets-15 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-15 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,353] INFO [Partition docker-connect-offsets-15 broker=1] Log loaded for partition docker-connect-offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,354] INFO [Broker id=1] Leader docker-connect-offsets-15 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:51,367] INFO [Broker id=1] Creating new partition docker-connect-offsets-17 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
connect                            | [2023-08-04 10:14:51,381] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer)
connect                            | [2023-08-04 10:14:51,389] INFO ProducerConfig values: 
connect                            | 	acks = -1
connect                            | 	batch.size = 16384
connect                            | 	bootstrap.servers = [broker:29092]
connect                            | 	buffer.memory = 33554432
connect                            | 	client.dns.lookup = use_all_dns_ips
connect                            | 	client.id = producer-1
connect                            | 	compression.type = none
connect                            | 	connections.max.idle.ms = 540000
connect                            | 	delivery.timeout.ms = 2147483647
connect                            | 	enable.idempotence = false
connect                            | 	interceptor.classes = []
connect                            | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
connect                            | 	linger.ms = 0
connect                            | 	max.block.ms = 60000
connect                            | 	max.in.flight.requests.per.connection = 1
connect                            | 	max.request.size = 1048576
connect                            | 	metadata.max.age.ms = 300000
connect                            | 	metadata.max.idle.ms = 300000
connect                            | 	metric.reporters = []
connect                            | 	metrics.num.samples = 2
connect                            | 	metrics.recording.level = INFO
connect                            | 	metrics.sample.window.ms = 30000
connect                            | 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
connect                            | 	receive.buffer.bytes = 32768
connect                            | 	reconnect.backoff.max.ms = 1000
connect                            | 	reconnect.backoff.ms = 50
connect                            | 	request.timeout.ms = 30000
connect                            | 	retries = 2147483647
connect                            | 	retry.backoff.ms = 100
connect                            | 	sasl.client.callback.handler.class = null
connect                            | 	sasl.jaas.config = null
connect                            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	sasl.kerberos.service.name = null
connect                            | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	sasl.login.callback.handler.class = null
connect                            | 	sasl.login.class = null
connect                            | 	sasl.login.connect.timeout.ms = null
connect                            | 	sasl.login.read.timeout.ms = null
connect                            | 	sasl.login.refresh.buffer.seconds = 300
connect                            | 	sasl.login.refresh.min.period.seconds = 60
connect                            | 	sasl.login.refresh.window.factor = 0.8
connect                            | 	sasl.login.refresh.window.jitter = 0.05
connect                            | 	sasl.login.retry.backoff.max.ms = 10000
connect                            | 	sasl.login.retry.backoff.ms = 100
connect                            | 	sasl.mechanism = GSSAPI
connect                            | 	sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	sasl.oauthbearer.expected.audience = null
connect                            | 	sasl.oauthbearer.expected.issuer = null
connect                            | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	sasl.oauthbearer.scope.claim.name = scope
connect                            | 	sasl.oauthbearer.sub.claim.name = sub
connect                            | 	sasl.oauthbearer.token.endpoint.url = null
connect                            | 	security.protocol = PLAINTEXT
connect                            | 	security.providers = null
connect                            | 	send.buffer.bytes = 131072
connect                            | 	socket.connection.setup.timeout.max.ms = 30000
connect                            | 	socket.connection.setup.timeout.ms = 10000
connect                            | 	ssl.cipher.suites = null
connect                            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	ssl.endpoint.identification.algorithm = https
connect                            | 	ssl.engine.factory.class = null
connect                            | 	ssl.key.password = null
connect                            | 	ssl.keymanager.algorithm = SunX509
connect                            | 	ssl.keystore.certificate.chain = null
connect                            | 	ssl.keystore.key = null
connect                            | 	ssl.keystore.location = null
connect                            | 	ssl.keystore.password = null
connect                            | 	ssl.keystore.type = JKS
connect                            | 	ssl.protocol = TLSv1.3
connect                            | 	ssl.provider = null
connect                            | 	ssl.secure.random.implementation = null
connect                            | 	ssl.trustmanager.algorithm = PKIX
connect                            | 	ssl.truststore.certificates = null
connect                            | 	ssl.truststore.location = null
connect                            | 	ssl.truststore.password = null
connect                            | 	ssl.truststore.type = JKS
connect                            | 	transaction.timeout.ms = 60000
connect                            | 	transactional.id = null
connect                            | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
connect                            |  (org.apache.kafka.clients.producer.ProducerConfig)
broker                             | [2023-08-04 10:14:51,395] INFO [LogLoader partition=docker-connect-offsets-17, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:51,410] INFO Created log for partition docker-connect-offsets-17 in /tmp/kraft-combined-logs/docker-connect-offsets-17 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,422] INFO [Partition docker-connect-offsets-17 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-17 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,423] INFO [Partition docker-connect-offsets-17 broker=1] Log loaded for partition docker-connect-offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,424] INFO [Broker id=1] Leader docker-connect-offsets-17 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:51,434] INFO [Broker id=1] Creating new partition docker-connect-offsets-3 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:51,453] INFO [LogLoader partition=docker-connect-offsets-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:51,456] INFO Created log for partition docker-connect-offsets-3 in /tmp/kraft-combined-logs/docker-connect-offsets-3 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,459] INFO [Partition docker-connect-offsets-3 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-3 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,459] INFO [Partition docker-connect-offsets-3 broker=1] Log loaded for partition docker-connect-offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,460] INFO [Broker id=1] Leader docker-connect-offsets-3 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:51,470] INFO [Broker id=1] Creating new partition docker-connect-offsets-5 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:51,493] INFO [LogLoader partition=docker-connect-offsets-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:51,495] INFO Created log for partition docker-connect-offsets-5 in /tmp/kraft-combined-logs/docker-connect-offsets-5 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,496] INFO [Partition docker-connect-offsets-5 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-5 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,496] INFO [Partition docker-connect-offsets-5 broker=1] Log loaded for partition docker-connect-offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,496] INFO [Broker id=1] Leader docker-connect-offsets-5 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:51,514] INFO [Broker id=1] Creating new partition docker-connect-offsets-7 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:51,532] INFO [LogLoader partition=docker-connect-offsets-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:51,540] INFO Created log for partition docker-connect-offsets-7 in /tmp/kraft-combined-logs/docker-connect-offsets-7 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,541] INFO [Partition docker-connect-offsets-7 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-7 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,541] INFO [Partition docker-connect-offsets-7 broker=1] Log loaded for partition docker-connect-offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,541] INFO [Broker id=1] Leader docker-connect-offsets-7 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:51,554] INFO [Broker id=1] Creating new partition docker-connect-offsets-9 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:51,570] INFO [LogLoader partition=docker-connect-offsets-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:51,580] INFO Created log for partition docker-connect-offsets-9 in /tmp/kraft-combined-logs/docker-connect-offsets-9 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,587] INFO [Partition docker-connect-offsets-9 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-9 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,588] INFO [Partition docker-connect-offsets-9 broker=1] Log loaded for partition docker-connect-offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,589] INFO [Broker id=1] Leader docker-connect-offsets-9 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:51,597] INFO [Broker id=1] Creating new partition docker-connect-offsets-1 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:51,622] INFO [LogLoader partition=docker-connect-offsets-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:51,624] INFO Created log for partition docker-connect-offsets-1 in /tmp/kraft-combined-logs/docker-connect-offsets-1 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,624] INFO [Partition docker-connect-offsets-1 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-1 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,624] INFO [Partition docker-connect-offsets-1 broker=1] Log loaded for partition docker-connect-offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,624] INFO [Broker id=1] Leader docker-connect-offsets-1 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:51,634] INFO [Broker id=1] Creating new partition docker-connect-offsets-18 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:51,646] INFO [LogLoader partition=docker-connect-offsets-18, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
connect                            | [2023-08-04 10:14:51,648] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,648] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,648] WARN The configuration 'metrics.context.resource.version' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,648] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,648] WARN The configuration 'metrics.context.resource.type' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,648] WARN The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,648] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,648] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,648] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,649] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,649] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,649] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,649] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,649] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,649] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,649] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,650] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,650] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,650] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,650] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,650] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,651] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:51,651] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
broker                             | [2023-08-04 10:14:51,652] INFO Created log for partition docker-connect-offsets-18 in /tmp/kraft-combined-logs/docker-connect-offsets-18 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,652] INFO [Partition docker-connect-offsets-18 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-18 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,652] INFO [Partition docker-connect-offsets-18 broker=1] Log loaded for partition docker-connect-offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,652] INFO [Broker id=1] Leader docker-connect-offsets-18 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect                            | [2023-08-04 10:14:51,653] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:51,655] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:51,655] INFO Kafka startTimeMs: 1691144091651 (org.apache.kafka.common.utils.AppInfoParser)
broker                             | [2023-08-04 10:14:51,670] INFO [Broker id=1] Creating new partition docker-connect-offsets-20 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:51,693] INFO [LogLoader partition=docker-connect-offsets-20, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:51,702] INFO Created log for partition docker-connect-offsets-20 in /tmp/kraft-combined-logs/docker-connect-offsets-20 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,702] INFO [Partition docker-connect-offsets-20 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-20 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,702] INFO [Partition docker-connect-offsets-20 broker=1] Log loaded for partition docker-connect-offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,703] INFO [Broker id=1] Leader docker-connect-offsets-20 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect                            | [2023-08-04 10:14:51,722] INFO ConsumerConfig values: 
connect                            | 	allow.auto.create.topics = true
connect                            | 	auto.commit.interval.ms = 5000
connect                            | 	auto.offset.reset = earliest
connect                            | 	bootstrap.servers = [broker:29092]
connect                            | 	check.crcs = true
connect                            | 	client.dns.lookup = use_all_dns_ips
connect                            | 	client.id = consumer-compose-connect-group-1
connect                            | 	client.rack = 
connect                            | 	connections.max.idle.ms = 540000
connect                            | 	default.api.timeout.ms = 60000
connect                            | 	enable.auto.commit = false
connect                            | 	exclude.internal.topics = true
connect                            | 	fetch.max.bytes = 52428800
connect                            | 	fetch.max.wait.ms = 500
connect                            | 	fetch.min.bytes = 1
connect                            | 	group.id = compose-connect-group
connect                            | 	group.instance.id = null
connect                            | 	heartbeat.interval.ms = 3000
connect                            | 	interceptor.classes = []
connect                            | 	internal.leave.group.on.close = true
connect                            | 	internal.throw.on.fetch.stable.offset.unsupported = false
connect                            | 	isolation.level = read_uncommitted
connect                            | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
connect                            | 	max.partition.fetch.bytes = 1048576
connect                            | 	max.poll.interval.ms = 300000
connect                            | 	max.poll.records = 500
connect                            | 	metadata.max.age.ms = 300000
connect                            | 	metric.reporters = []
connect                            | 	metrics.num.samples = 2
connect                            | 	metrics.recording.level = INFO
connect                            | 	metrics.sample.window.ms = 30000
connect                            | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
connect                            | 	receive.buffer.bytes = 65536
connect                            | 	reconnect.backoff.max.ms = 1000
connect                            | 	reconnect.backoff.ms = 50
connect                            | 	request.timeout.ms = 30000
connect                            | 	retry.backoff.ms = 100
connect                            | 	sasl.client.callback.handler.class = null
connect                            | 	sasl.jaas.config = null
connect                            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	sasl.kerberos.service.name = null
connect                            | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	sasl.login.callback.handler.class = null
connect                            | 	sasl.login.class = null
connect                            | 	sasl.login.connect.timeout.ms = null
connect                            | 	sasl.login.read.timeout.ms = null
connect                            | 	sasl.login.refresh.buffer.seconds = 300
connect                            | 	sasl.login.refresh.min.period.seconds = 60
connect                            | 	sasl.login.refresh.window.factor = 0.8
connect                            | 	sasl.login.refresh.window.jitter = 0.05
connect                            | 	sasl.login.retry.backoff.max.ms = 10000
connect                            | 	sasl.login.retry.backoff.ms = 100
connect                            | 	sasl.mechanism = GSSAPI
connect                            | 	sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	sasl.oauthbearer.expected.audience = null
connect                            | 	sasl.oauthbearer.expected.issuer = null
connect                            | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	sasl.oauthbearer.scope.claim.name = scope
connect                            | 	sasl.oauthbearer.sub.claim.name = sub
connect                            | 	sasl.oauthbearer.token.endpoint.url = null
connect                            | 	security.protocol = PLAINTEXT
connect                            | 	security.providers = null
connect                            | 	send.buffer.bytes = 131072
connect                            | 	session.timeout.ms = 45000
connect                            | 	socket.connection.setup.timeout.max.ms = 30000
connect                            | 	socket.connection.setup.timeout.ms = 10000
connect                            | 	ssl.cipher.suites = null
connect                            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	ssl.endpoint.identification.algorithm = https
connect                            | 	ssl.engine.factory.class = null
connect                            | 	ssl.key.password = null
connect                            | 	ssl.keymanager.algorithm = SunX509
connect                            | 	ssl.keystore.certificate.chain = null
connect                            | 	ssl.keystore.key = null
connect                            | 	ssl.keystore.location = null
connect                            | 	ssl.keystore.password = null
connect                            | 	ssl.keystore.type = JKS
connect                            | 	ssl.protocol = TLSv1.3
connect                            | 	ssl.provider = null
connect                            | 	ssl.secure.random.implementation = null
connect                            | 	ssl.trustmanager.algorithm = PKIX
connect                            | 	ssl.truststore.certificates = null
connect                            | 	ssl.truststore.location = null
connect                            | 	ssl.truststore.password = null
connect                            | 	ssl.truststore.type = JKS
connect                            | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
connect                            |  (org.apache.kafka.clients.consumer.ConsumerConfig)
broker                             | [2023-08-04 10:14:51,726] INFO [Broker id=1] Creating new partition docker-connect-offsets-22 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:51,753] INFO [LogLoader partition=docker-connect-offsets-22, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:51,755] INFO Created log for partition docker-connect-offsets-22 in /tmp/kraft-combined-logs/docker-connect-offsets-22 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,758] INFO [Partition docker-connect-offsets-22 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-22 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,758] INFO [Partition docker-connect-offsets-22 broker=1] Log loaded for partition docker-connect-offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,759] INFO [Broker id=1] Leader docker-connect-offsets-22 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:51,766] INFO [Broker id=1] Creating new partition docker-connect-offsets-24 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
connect                            | [2023-08-04 10:14:51,761] INFO [Producer clientId=producer-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
broker                             | [2023-08-04 10:14:51,781] INFO [LogLoader partition=docker-connect-offsets-24, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:51,784] INFO Created log for partition docker-connect-offsets-24 in /tmp/kraft-combined-logs/docker-connect-offsets-24 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,784] INFO [Partition docker-connect-offsets-24 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-24 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,784] INFO [Partition docker-connect-offsets-24 broker=1] Log loaded for partition docker-connect-offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,788] INFO [Broker id=1] Leader docker-connect-offsets-24 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:51,795] INFO [Broker id=1] Creating new partition docker-connect-offsets-10 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:51,830] INFO [LogLoader partition=docker-connect-offsets-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:51,846] INFO Created log for partition docker-connect-offsets-10 in /tmp/kraft-combined-logs/docker-connect-offsets-10 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,847] INFO [Partition docker-connect-offsets-10 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-10 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,848] INFO [Partition docker-connect-offsets-10 broker=1] Log loaded for partition docker-connect-offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,851] INFO [Broker id=1] Leader docker-connect-offsets-10 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:51,870] INFO [Broker id=1] Creating new partition docker-connect-offsets-12 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:51,878] INFO [LogLoader partition=docker-connect-offsets-12, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:51,881] INFO Created log for partition docker-connect-offsets-12 in /tmp/kraft-combined-logs/docker-connect-offsets-12 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,881] INFO [Partition docker-connect-offsets-12 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-12 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,882] INFO [Partition docker-connect-offsets-12 broker=1] Log loaded for partition docker-connect-offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,882] INFO [Broker id=1] Leader docker-connect-offsets-12 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:51,891] INFO [Broker id=1] Creating new partition docker-connect-offsets-14 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:51,925] INFO [LogLoader partition=docker-connect-offsets-14, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:51,936] INFO Created log for partition docker-connect-offsets-14 in /tmp/kraft-combined-logs/docker-connect-offsets-14 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,940] INFO [Partition docker-connect-offsets-14 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-14 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,941] INFO [Partition docker-connect-offsets-14 broker=1] Log loaded for partition docker-connect-offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,942] INFO [Broker id=1] Leader docker-connect-offsets-14 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:51,963] INFO [Broker id=1] Creating new partition docker-connect-offsets-16 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:51,982] INFO [LogLoader partition=docker-connect-offsets-16, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:51,990] INFO Created log for partition docker-connect-offsets-16 in /tmp/kraft-combined-logs/docker-connect-offsets-16 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:51,995] INFO [Partition docker-connect-offsets-16 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-16 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,996] INFO [Partition docker-connect-offsets-16 broker=1] Log loaded for partition docker-connect-offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:51,997] INFO [Broker id=1] Leader docker-connect-offsets-16 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect                            | [2023-08-04 10:14:51,995] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,004] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,005] WARN The configuration 'metrics.context.resource.version' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,006] WARN The configuration 'metrics.context.resource.type' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,006] WARN The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,007] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
broker                             | [2023-08-04 10:14:52,007] INFO [Broker id=1] Creating new partition docker-connect-offsets-2 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
connect                            | [2023-08-04 10:14:52,008] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,009] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,010] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,011] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,012] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,012] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,013] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,014] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,015] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,016] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,017] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,018] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,018] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,019] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,020] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,021] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,022] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:52,023] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:52,023] INFO Kafka startTimeMs: 1691144092022 (org.apache.kafka.common.utils.AppInfoParser)
broker                             | [2023-08-04 10:14:52,029] INFO [LogLoader partition=docker-connect-offsets-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:52,031] INFO Created log for partition docker-connect-offsets-2 in /tmp/kraft-combined-logs/docker-connect-offsets-2 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:52,032] INFO [Partition docker-connect-offsets-2 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-2 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,038] INFO [Partition docker-connect-offsets-2 broker=1] Log loaded for partition docker-connect-offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,039] INFO [Broker id=1] Leader docker-connect-offsets-2 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:52,051] INFO [Broker id=1] Creating new partition docker-connect-offsets-4 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:52,060] INFO [LogLoader partition=docker-connect-offsets-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:52,066] INFO Created log for partition docker-connect-offsets-4 in /tmp/kraft-combined-logs/docker-connect-offsets-4 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:52,066] INFO [Partition docker-connect-offsets-4 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-4 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,066] INFO [Partition docker-connect-offsets-4 broker=1] Log loaded for partition docker-connect-offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,066] INFO [Broker id=1] Leader docker-connect-offsets-4 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:52,075] INFO [Broker id=1] Creating new partition docker-connect-offsets-6 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:52,086] INFO [LogLoader partition=docker-connect-offsets-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:52,088] INFO Created log for partition docker-connect-offsets-6 in /tmp/kraft-combined-logs/docker-connect-offsets-6 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:52,088] INFO [Partition docker-connect-offsets-6 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-6 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,089] INFO [Partition docker-connect-offsets-6 broker=1] Log loaded for partition docker-connect-offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,089] INFO [Broker id=1] Leader docker-connect-offsets-6 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect                            | [2023-08-04 10:14:52,092] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
broker                             | [2023-08-04 10:14:52,100] INFO [Broker id=1] Creating new partition docker-connect-offsets-8 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:52,111] INFO [LogLoader partition=docker-connect-offsets-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:52,122] INFO Created log for partition docker-connect-offsets-8 in /tmp/kraft-combined-logs/docker-connect-offsets-8 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:52,122] INFO [Partition docker-connect-offsets-8 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-8 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,122] INFO [Partition docker-connect-offsets-8 broker=1] Log loaded for partition docker-connect-offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,123] INFO [Broker id=1] Leader docker-connect-offsets-8 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect                            | [2023-08-04 10:14:52,130] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Subscribed to partition(s): docker-connect-offsets-17, docker-connect-offsets-20, docker-connect-offsets-11, docker-connect-offsets-23, docker-connect-offsets-14, docker-connect-offsets-5, docker-connect-offsets-0, docker-connect-offsets-8, docker-connect-offsets-7, docker-connect-offsets-4, docker-connect-offsets-1, docker-connect-offsets-10, docker-connect-offsets-13, docker-connect-offsets-24, docker-connect-offsets-21, docker-connect-offsets-16, docker-connect-offsets-3, docker-connect-offsets-9, docker-connect-offsets-15, docker-connect-offsets-18, docker-connect-offsets-19, docker-connect-offsets-22, docker-connect-offsets-6, docker-connect-offsets-2, docker-connect-offsets-12 (org.apache.kafka.clients.consumer.KafkaConsumer)
broker                             | [2023-08-04 10:14:52,132] INFO [Broker id=1] Creating new partition docker-connect-offsets-0 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:52,143] INFO [LogLoader partition=docker-connect-offsets-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:52,149] INFO Created log for partition docker-connect-offsets-0 in /tmp/kraft-combined-logs/docker-connect-offsets-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:52,151] INFO [Partition docker-connect-offsets-0 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,151] INFO [Partition docker-connect-offsets-0 broker=1] Log loaded for partition docker-connect-offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,152] INFO [Broker id=1] Leader docker-connect-offsets-0 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:52,168] INFO [Broker id=1] Creating new partition docker-connect-offsets-19 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:52,181] INFO [LogLoader partition=docker-connect-offsets-19, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
connect                            | [2023-08-04 10:14:52,181] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,184] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,185] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,185] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,185] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,185] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,185] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,186] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,186] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,186] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,186] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,186] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,187] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,187] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,187] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
broker                             | [2023-08-04 10:14:52,190] INFO Created log for partition docker-connect-offsets-19 in /tmp/kraft-combined-logs/docker-connect-offsets-19 with properties {cleanup.policy=compact} (kafka.log.LogManager)
connect                            | [2023-08-04 10:14:52,190] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,190] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,191] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,191] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,191] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,191] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,192] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,192] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,192] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:52,192] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
broker                             | [2023-08-04 10:14:52,193] INFO [Partition docker-connect-offsets-19 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-19 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,196] INFO [Partition docker-connect-offsets-19 broker=1] Log loaded for partition docker-connect-offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,197] INFO [Broker id=1] Leader docker-connect-offsets-19 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:52,209] INFO [Broker id=1] Creating new partition docker-connect-offsets-21 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:52,222] INFO [LogLoader partition=docker-connect-offsets-21, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:52,224] INFO Created log for partition docker-connect-offsets-21 in /tmp/kraft-combined-logs/docker-connect-offsets-21 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:52,225] INFO [Partition docker-connect-offsets-21 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-21 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,225] INFO [Partition docker-connect-offsets-21 broker=1] Log loaded for partition docker-connect-offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,225] INFO [Broker id=1] Leader docker-connect-offsets-21 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:52,241] INFO [Broker id=1] Creating new partition docker-connect-offsets-23 with topic id XlZu4UyKTamPRyQ7VaDLgA. (state.change.logger)
broker                             | [2023-08-04 10:14:52,252] INFO [LogLoader partition=docker-connect-offsets-23, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:52,254] INFO Created log for partition docker-connect-offsets-23 in /tmp/kraft-combined-logs/docker-connect-offsets-23 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:52,255] INFO [Partition docker-connect-offsets-23 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-23 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,255] INFO [Partition docker-connect-offsets-23 broker=1] Log loaded for partition docker-connect-offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,255] INFO [Broker id=1] Leader docker-connect-offsets-23 with topic id Some(XlZu4UyKTamPRyQ7VaDLgA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:52,264] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic docker-connect-offsets with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
connect                            | [2023-08-04 10:14:52,293] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
connect                            | [2023-08-04 10:14:52,300] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
connect                            | [2023-08-04 10:14:52,305] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session)
connect                            | [2023-08-04 10:14:52,445] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-17 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,447] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-20 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,448] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-11 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,449] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-23 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,453] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-14 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,454] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-5 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,455] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-0 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,456] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-8 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,456] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-7 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,457] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-4 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,459] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-1 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,460] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-10 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,462] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-13 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,464] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-24 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,465] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-21 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,466] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-16 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,467] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-3 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,468] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-9 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,469] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-15 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,473] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-18 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,474] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-19 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,475] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-22 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,476] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-6 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,477] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-2 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,478] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-12 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:52,530] INFO Finished reading KafkaBasedLog for topic docker-connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
connect                            | [2023-08-04 10:14:52,532] INFO Started KafkaBasedLog for topic docker-connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
connect                            | [2023-08-04 10:14:52,532] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore)
connect                            | [2023-08-04 10:14:52,686] INFO LogEventsConfig values: 
connect                            | 	confluent.event.logger.cloudevent.codec = binary
connect                            | 	confluent.event.logger.enable = false
connect                            | 	confluent.event.logger.exporter.class = class io.confluent.telemetry.events.exporter.kafka.EventKafkaExporter
connect                            | 	confluent.event.logger.exporter.kafka.producer.bootstrap.servers = 
connect                            | 	confluent.event.logger.exporter.kafka.producer.client.id = confluent-connect-log-events-emitter-compose-connect-group
connect                            | 	confluent.event.logger.exporter.kafka.topic.create = true
connect                            | 	confluent.event.logger.exporter.kafka.topic.name = confluent-connect-log-events
connect                            | 	confluent.event.logger.exporter.kafka.type = kafka
connect                            |  (io.confluent.logevents.connect.LogEventsConfig)
connect                            | [2023-08-04 10:14:52,688] INFO Connect Log Events aren't enabled. (io.confluent.logevents.connect.LogEventsKafkaEmitter)
connect                            | [2023-08-04 10:14:52,689] INFO Worker started (org.apache.kafka.connect.runtime.Worker)
connect                            | [2023-08-04 10:14:52,690] INFO Starting KafkaBasedLog with topic docker-connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
broker                             | [2023-08-04 10:14:52,696] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='docker-connect-status', numPartitions=5, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:52,696] INFO [Controller 1] Created topic docker-connect-status with topic ID FZMPJSlQRhuancFPWWYdoQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:52,696] INFO [Controller 1] ConfigResource(type=TOPIC, name='docker-connect-status'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:14:52,696] INFO [Controller 1] Created partition docker-connect-status-0 with topic ID FZMPJSlQRhuancFPWWYdoQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:52,696] INFO [Controller 1] Created partition docker-connect-status-1 with topic ID FZMPJSlQRhuancFPWWYdoQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:52,696] INFO [Controller 1] Created partition docker-connect-status-2 with topic ID FZMPJSlQRhuancFPWWYdoQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:52,696] INFO [Controller 1] Created partition docker-connect-status-3 with topic ID FZMPJSlQRhuancFPWWYdoQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:52,696] INFO [Controller 1] Created partition docker-connect-status-4 with topic ID FZMPJSlQRhuancFPWWYdoQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:52,725] INFO [Broker id=1] Transitioning 5 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:14:52,726] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(docker-connect-status-1, docker-connect-status-0, docker-connect-status-3, docker-connect-status-2, docker-connect-status-4) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:14:52,726] INFO [Broker id=1] Creating new partition docker-connect-status-1 with topic id FZMPJSlQRhuancFPWWYdoQ. (state.change.logger)
broker                             | [2023-08-04 10:14:52,734] INFO [LogLoader partition=docker-connect-status-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
connect                            | [2023-08-04 10:14:52,744] INFO Created topic (name=docker-connect-status, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at broker:29092 (org.apache.kafka.connect.util.TopicAdmin)
broker                             | [2023-08-04 10:14:52,745] INFO Created log for partition docker-connect-status-1 in /tmp/kraft-combined-logs/docker-connect-status-1 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:52,749] INFO [Partition docker-connect-status-1 broker=1] No checkpointed highwatermark is found for partition docker-connect-status-1 (kafka.cluster.Partition)
connect                            | [2023-08-04 10:14:52,749] INFO ProducerConfig values: 
connect                            | 	acks = -1
connect                            | 	batch.size = 16384
connect                            | 	bootstrap.servers = [broker:29092]
connect                            | 	buffer.memory = 33554432
connect                            | 	client.dns.lookup = use_all_dns_ips
connect                            | 	client.id = producer-2
connect                            | 	compression.type = none
connect                            | 	connections.max.idle.ms = 540000
connect                            | 	delivery.timeout.ms = 120000
connect                            | 	enable.idempotence = false
connect                            | 	interceptor.classes = []
connect                            | 	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
connect                            | 	linger.ms = 0
connect                            | 	max.block.ms = 60000
connect                            | 	max.in.flight.requests.per.connection = 1
connect                            | 	max.request.size = 1048576
connect                            | 	metadata.max.age.ms = 300000
connect                            | 	metadata.max.idle.ms = 300000
connect                            | 	metric.reporters = []
connect                            | 	metrics.num.samples = 2
connect                            | 	metrics.recording.level = INFO
connect                            | 	metrics.sample.window.ms = 30000
connect                            | 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
connect                            | 	receive.buffer.bytes = 32768
connect                            | 	reconnect.backoff.max.ms = 1000
connect                            | 	reconnect.backoff.ms = 50
connect                            | 	request.timeout.ms = 30000
connect                            | 	retries = 0
connect                            | 	retry.backoff.ms = 100
connect                            | 	sasl.client.callback.handler.class = null
connect                            | 	sasl.jaas.config = null
connect                            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	sasl.kerberos.service.name = null
connect                            | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	sasl.login.callback.handler.class = null
connect                            | 	sasl.login.class = null
connect                            | 	sasl.login.connect.timeout.ms = null
connect                            | 	sasl.login.read.timeout.ms = null
connect                            | 	sasl.login.refresh.buffer.seconds = 300
connect                            | 	sasl.login.refresh.min.period.seconds = 60
connect                            | 	sasl.login.refresh.window.factor = 0.8
connect                            | 	sasl.login.refresh.window.jitter = 0.05
connect                            | 	sasl.login.retry.backoff.max.ms = 10000
connect                            | 	sasl.login.retry.backoff.ms = 100
connect                            | 	sasl.mechanism = GSSAPI
connect                            | 	sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	sasl.oauthbearer.expected.audience = null
connect                            | 	sasl.oauthbearer.expected.issuer = null
connect                            | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	sasl.oauthbearer.scope.claim.name = scope
connect                            | 	sasl.oauthbearer.sub.claim.name = sub
connect                            | 	sasl.oauthbearer.token.endpoint.url = null
connect                            | 	security.protocol = PLAINTEXT
connect                            | 	security.providers = null
connect                            | 	send.buffer.bytes = 131072
connect                            | 	socket.connection.setup.timeout.max.ms = 30000
connect                            | 	socket.connection.setup.timeout.ms = 10000
connect                            | 	ssl.cipher.suites = null
connect                            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	ssl.endpoint.identification.algorithm = https
connect                            | 	ssl.engine.factory.class = null
connect                            | 	ssl.key.password = null
connect                            | 	ssl.keymanager.algorithm = SunX509
connect                            | 	ssl.keystore.certificate.chain = null
connect                            | 	ssl.keystore.key = null
connect                            | 	ssl.keystore.location = null
connect                            | 	ssl.keystore.password = null
connect                            | 	ssl.keystore.type = JKS
connect                            | 	ssl.protocol = TLSv1.3
connect                            | 	ssl.provider = null
connect                            | 	ssl.secure.random.implementation = null
connect                            | 	ssl.trustmanager.algorithm = PKIX
connect                            | 	ssl.truststore.certificates = null
connect                            | 	ssl.truststore.location = null
connect                            | 	ssl.truststore.password = null
connect                            | 	ssl.truststore.type = JKS
connect                            | 	transaction.timeout.ms = 60000
connect                            | 	transactional.id = null
connect                            | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
connect                            |  (org.apache.kafka.clients.producer.ProducerConfig)
broker                             | [2023-08-04 10:14:52,749] INFO [Partition docker-connect-status-1 broker=1] Log loaded for partition docker-connect-status-1 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,752] INFO [Broker id=1] Leader docker-connect-status-1 with topic id Some(FZMPJSlQRhuancFPWWYdoQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:52,760] INFO [Broker id=1] Creating new partition docker-connect-status-0 with topic id FZMPJSlQRhuancFPWWYdoQ. (state.change.logger)
connect                            | [2023-08-04 10:14:52,804] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,806] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,806] WARN The configuration 'metrics.context.resource.version' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,808] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,809] WARN The configuration 'metrics.context.resource.type' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,809] WARN The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,810] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,811] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,812] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,813] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,813] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,814] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,815] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,816] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,817] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
broker                             | [2023-08-04 10:14:52,817] INFO [LogLoader partition=docker-connect-status-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
connect                            | [2023-08-04 10:14:52,818] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,820] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,821] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
broker                             | [2023-08-04 10:14:52,824] INFO Created log for partition docker-connect-status-0 in /tmp/kraft-combined-logs/docker-connect-status-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:52,825] INFO [Partition docker-connect-status-0 broker=1] No checkpointed highwatermark is found for partition docker-connect-status-0 (kafka.cluster.Partition)
connect                            | [2023-08-04 10:14:52,827] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
broker                             | [2023-08-04 10:14:52,828] INFO [Partition docker-connect-status-0 broker=1] Log loaded for partition docker-connect-status-0 with initial high watermark 0 (kafka.cluster.Partition)
connect                            | [2023-08-04 10:14:52,828] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
broker                             | [2023-08-04 10:14:52,829] INFO [Broker id=1] Leader docker-connect-status-0 with topic id Some(FZMPJSlQRhuancFPWWYdoQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect                            | [2023-08-04 10:14:52,829] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,831] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,831] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:52,832] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:52,834] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:52,834] INFO Kafka startTimeMs: 1691144092832 (org.apache.kafka.common.utils.AppInfoParser)
broker                             | [2023-08-04 10:14:52,845] INFO [Broker id=1] Creating new partition docker-connect-status-3 with topic id FZMPJSlQRhuancFPWWYdoQ. (state.change.logger)
broker                             | [2023-08-04 10:14:52,874] INFO [LogLoader partition=docker-connect-status-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:52,881] INFO Created log for partition docker-connect-status-3 in /tmp/kraft-combined-logs/docker-connect-status-3 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:52,882] INFO [Partition docker-connect-status-3 broker=1] No checkpointed highwatermark is found for partition docker-connect-status-3 (kafka.cluster.Partition)
connect                            | [2023-08-04 10:14:52,885] INFO ConsumerConfig values: 
connect                            | 	allow.auto.create.topics = true
connect                            | 	auto.commit.interval.ms = 5000
connect                            | 	auto.offset.reset = earliest
connect                            | 	bootstrap.servers = [broker:29092]
connect                            | 	check.crcs = true
connect                            | 	client.dns.lookup = use_all_dns_ips
connect                            | 	client.id = consumer-compose-connect-group-2
connect                            | 	client.rack = 
connect                            | 	connections.max.idle.ms = 540000
connect                            | 	default.api.timeout.ms = 60000
connect                            | 	enable.auto.commit = false
connect                            | 	exclude.internal.topics = true
connect                            | 	fetch.max.bytes = 52428800
connect                            | 	fetch.max.wait.ms = 500
connect                            | 	fetch.min.bytes = 1
connect                            | 	group.id = compose-connect-group
connect                            | 	group.instance.id = null
connect                            | 	heartbeat.interval.ms = 3000
connect                            | 	interceptor.classes = []
connect                            | 	internal.leave.group.on.close = true
connect                            | 	internal.throw.on.fetch.stable.offset.unsupported = false
connect                            | 	isolation.level = read_uncommitted
connect                            | 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
connect                            | 	max.partition.fetch.bytes = 1048576
connect                            | 	max.poll.interval.ms = 300000
connect                            | 	max.poll.records = 500
connect                            | 	metadata.max.age.ms = 300000
connect                            | 	metric.reporters = []
connect                            | 	metrics.num.samples = 2
connect                            | 	metrics.recording.level = INFO
connect                            | 	metrics.sample.window.ms = 30000
connect                            | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
connect                            | 	receive.buffer.bytes = 65536
connect                            | 	reconnect.backoff.max.ms = 1000
connect                            | 	reconnect.backoff.ms = 50
connect                            | 	request.timeout.ms = 30000
connect                            | 	retry.backoff.ms = 100
connect                            | 	sasl.client.callback.handler.class = null
connect                            | 	sasl.jaas.config = null
connect                            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	sasl.kerberos.service.name = null
connect                            | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	sasl.login.callback.handler.class = null
connect                            | 	sasl.login.class = null
connect                            | 	sasl.login.connect.timeout.ms = null
connect                            | 	sasl.login.read.timeout.ms = null
connect                            | 	sasl.login.refresh.buffer.seconds = 300
connect                            | 	sasl.login.refresh.min.period.seconds = 60
connect                            | 	sasl.login.refresh.window.factor = 0.8
connect                            | 	sasl.login.refresh.window.jitter = 0.05
connect                            | 	sasl.login.retry.backoff.max.ms = 10000
connect                            | 	sasl.login.retry.backoff.ms = 100
connect                            | 	sasl.mechanism = GSSAPI
connect                            | 	sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	sasl.oauthbearer.expected.audience = null
connect                            | 	sasl.oauthbearer.expected.issuer = null
connect                            | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	sasl.oauthbearer.scope.claim.name = scope
connect                            | 	sasl.oauthbearer.sub.claim.name = sub
connect                            | 	sasl.oauthbearer.token.endpoint.url = null
connect                            | 	security.protocol = PLAINTEXT
connect                            | 	security.providers = null
connect                            | 	send.buffer.bytes = 131072
connect                            | 	session.timeout.ms = 45000
connect                            | 	socket.connection.setup.timeout.max.ms = 30000
connect                            | 	socket.connection.setup.timeout.ms = 10000
connect                            | 	ssl.cipher.suites = null
connect                            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	ssl.endpoint.identification.algorithm = https
connect                            | 	ssl.engine.factory.class = null
connect                            | 	ssl.key.password = null
connect                            | 	ssl.keymanager.algorithm = SunX509
connect                            | 	ssl.keystore.certificate.chain = null
connect                            | 	ssl.keystore.key = null
connect                            | 	ssl.keystore.location = null
connect                            | 	ssl.keystore.password = null
connect                            | 	ssl.keystore.type = JKS
connect                            | 	ssl.protocol = TLSv1.3
connect                            | 	ssl.provider = null
connect                            | 	ssl.secure.random.implementation = null
connect                            | 	ssl.trustmanager.algorithm = PKIX
connect                            | 	ssl.truststore.certificates = null
connect                            | 	ssl.truststore.location = null
connect                            | 	ssl.truststore.password = null
connect                            | 	ssl.truststore.type = JKS
connect                            | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
connect                            |  (org.apache.kafka.clients.consumer.ConsumerConfig)
broker                             | [2023-08-04 10:14:52,897] INFO [Partition docker-connect-status-3 broker=1] Log loaded for partition docker-connect-status-3 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,898] INFO [Broker id=1] Leader docker-connect-status-3 with topic id Some(FZMPJSlQRhuancFPWWYdoQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:52,917] INFO [Broker id=1] Creating new partition docker-connect-status-2 with topic id FZMPJSlQRhuancFPWWYdoQ. (state.change.logger)
broker                             | [2023-08-04 10:14:52,929] INFO [LogLoader partition=docker-connect-status-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
connect                            | [2023-08-04 10:14:52,922] INFO [Producer clientId=producer-2] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
broker                             | [2023-08-04 10:14:52,948] INFO Created log for partition docker-connect-status-2 in /tmp/kraft-combined-logs/docker-connect-status-2 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:52,949] INFO [Partition docker-connect-status-2 broker=1] No checkpointed highwatermark is found for partition docker-connect-status-2 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,955] INFO [Partition docker-connect-status-2 broker=1] Log loaded for partition docker-connect-status-2 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,956] INFO [Broker id=1] Leader docker-connect-status-2 with topic id Some(FZMPJSlQRhuancFPWWYdoQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:52,968] INFO [Broker id=1] Creating new partition docker-connect-status-4 with topic id FZMPJSlQRhuancFPWWYdoQ. (state.change.logger)
broker                             | [2023-08-04 10:14:52,980] INFO [LogLoader partition=docker-connect-status-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:52,982] INFO Created log for partition docker-connect-status-4 in /tmp/kraft-combined-logs/docker-connect-status-4 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:52,982] INFO [Partition docker-connect-status-4 broker=1] No checkpointed highwatermark is found for partition docker-connect-status-4 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,983] INFO [Partition docker-connect-status-4 broker=1] Log loaded for partition docker-connect-status-4 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:52,984] INFO [Broker id=1] Leader docker-connect-status-4 with topic id Some(FZMPJSlQRhuancFPWWYdoQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect                            | [2023-08-04 10:14:52,978] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,990] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,991] WARN The configuration 'metrics.context.resource.version' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,992] WARN The configuration 'metrics.context.resource.type' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,993] WARN The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,993] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,994] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,995] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,997] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:52,998] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,000] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
broker                             | [2023-08-04 10:14:53,002] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic docker-connect-status with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
connect                            | [2023-08-04 10:14:53,001] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,012] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,013] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,013] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,014] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,014] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,014] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,015] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,015] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,016] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,016] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,017] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:53,017] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:53,018] INFO Kafka startTimeMs: 1691144093017 (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:53,057] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,061] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Subscribed to partition(s): docker-connect-status-1, docker-connect-status-3, docker-connect-status-2, docker-connect-status-0, docker-connect-status-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
connect                            | [2023-08-04 10:14:53,061] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:53,062] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:53,062] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:53,062] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:53,062] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:53,098] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-1 to 0 since the associated topicId changed from null to FZMPJSlQRhuancFPWWYdoQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,098] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-3 to 0 since the associated topicId changed from null to FZMPJSlQRhuancFPWWYdoQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,098] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-2 to 0 since the associated topicId changed from null to FZMPJSlQRhuancFPWWYdoQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,098] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-0 to 0 since the associated topicId changed from null to FZMPJSlQRhuancFPWWYdoQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,098] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-4 to 0 since the associated topicId changed from null to FZMPJSlQRhuancFPWWYdoQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,128] INFO Finished reading KafkaBasedLog for topic docker-connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
connect                            | [2023-08-04 10:14:53,139] INFO Started KafkaBasedLog for topic docker-connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
connect                            | [2023-08-04 10:14:53,199] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
connect                            | [2023-08-04 10:14:53,206] INFO Starting KafkaBasedLog with topic docker-connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)
broker                             | [2023-08-04 10:14:53,227] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='docker-connect-configs', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:53,228] INFO [Controller 1] Created topic docker-connect-configs with topic ID rW3LqdQcRZq6zws5HiUfgw. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:53,229] INFO [Controller 1] ConfigResource(type=TOPIC, name='docker-connect-configs'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:14:53,230] INFO [Controller 1] Created partition docker-connect-configs-0 with topic ID rW3LqdQcRZq6zws5HiUfgw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:14:53,259] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:14:53,259] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(docker-connect-configs-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:14:53,259] INFO [Broker id=1] Creating new partition docker-connect-configs-0 with topic id rW3LqdQcRZq6zws5HiUfgw. (state.change.logger)
connect                            | [2023-08-04 10:14:53,262] INFO Created topic (name=docker-connect-configs, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at broker:29092 (org.apache.kafka.connect.util.TopicAdmin)
connect                            | [2023-08-04 10:14:53,264] INFO ProducerConfig values: 
connect                            | 	acks = -1
connect                            | 	batch.size = 16384
connect                            | 	bootstrap.servers = [broker:29092]
connect                            | 	buffer.memory = 33554432
connect                            | 	client.dns.lookup = use_all_dns_ips
connect                            | 	client.id = producer-3
connect                            | 	compression.type = none
connect                            | 	connections.max.idle.ms = 540000
connect                            | 	delivery.timeout.ms = 2147483647
connect                            | 	enable.idempotence = false
connect                            | 	interceptor.classes = []
connect                            | 	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
connect                            | 	linger.ms = 0
connect                            | 	max.block.ms = 60000
connect                            | 	max.in.flight.requests.per.connection = 1
connect                            | 	max.request.size = 1048576
connect                            | 	metadata.max.age.ms = 300000
connect                            | 	metadata.max.idle.ms = 300000
connect                            | 	metric.reporters = []
connect                            | 	metrics.num.samples = 2
connect                            | 	metrics.recording.level = INFO
connect                            | 	metrics.sample.window.ms = 30000
connect                            | 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
connect                            | 	receive.buffer.bytes = 32768
connect                            | 	reconnect.backoff.max.ms = 1000
connect                            | 	reconnect.backoff.ms = 50
connect                            | 	request.timeout.ms = 30000
connect                            | 	retries = 2147483647
connect                            | 	retry.backoff.ms = 100
connect                            | 	sasl.client.callback.handler.class = null
connect                            | 	sasl.jaas.config = null
connect                            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	sasl.kerberos.service.name = null
connect                            | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	sasl.login.callback.handler.class = null
connect                            | 	sasl.login.class = null
connect                            | 	sasl.login.connect.timeout.ms = null
connect                            | 	sasl.login.read.timeout.ms = null
connect                            | 	sasl.login.refresh.buffer.seconds = 300
connect                            | 	sasl.login.refresh.min.period.seconds = 60
connect                            | 	sasl.login.refresh.window.factor = 0.8
connect                            | 	sasl.login.refresh.window.jitter = 0.05
connect                            | 	sasl.login.retry.backoff.max.ms = 10000
connect                            | 	sasl.login.retry.backoff.ms = 100
connect                            | 	sasl.mechanism = GSSAPI
connect                            | 	sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	sasl.oauthbearer.expected.audience = null
connect                            | 	sasl.oauthbearer.expected.issuer = null
connect                            | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	sasl.oauthbearer.scope.claim.name = scope
connect                            | 	sasl.oauthbearer.sub.claim.name = sub
connect                            | 	sasl.oauthbearer.token.endpoint.url = null
connect                            | 	security.protocol = PLAINTEXT
connect                            | 	security.providers = null
connect                            | 	send.buffer.bytes = 131072
connect                            | 	socket.connection.setup.timeout.max.ms = 30000
connect                            | 	socket.connection.setup.timeout.ms = 10000
connect                            | 	ssl.cipher.suites = null
connect                            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	ssl.endpoint.identification.algorithm = https
connect                            | 	ssl.engine.factory.class = null
connect                            | 	ssl.key.password = null
connect                            | 	ssl.keymanager.algorithm = SunX509
connect                            | 	ssl.keystore.certificate.chain = null
connect                            | 	ssl.keystore.key = null
connect                            | 	ssl.keystore.location = null
connect                            | 	ssl.keystore.password = null
connect                            | 	ssl.keystore.type = JKS
connect                            | 	ssl.protocol = TLSv1.3
connect                            | 	ssl.provider = null
connect                            | 	ssl.secure.random.implementation = null
connect                            | 	ssl.trustmanager.algorithm = PKIX
connect                            | 	ssl.truststore.certificates = null
connect                            | 	ssl.truststore.location = null
connect                            | 	ssl.truststore.password = null
connect                            | 	ssl.truststore.type = JKS
connect                            | 	transaction.timeout.ms = 60000
connect                            | 	transactional.id = null
connect                            | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
connect                            |  (org.apache.kafka.clients.producer.ProducerConfig)
broker                             | [2023-08-04 10:14:53,275] INFO [LogLoader partition=docker-connect-configs-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:14:53,285] INFO Created log for partition docker-connect-configs-0 in /tmp/kraft-combined-logs/docker-connect-configs-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker                             | [2023-08-04 10:14:53,290] INFO [Partition docker-connect-configs-0 broker=1] No checkpointed highwatermark is found for partition docker-connect-configs-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:53,295] INFO [Partition docker-connect-configs-0 broker=1] Log loaded for partition docker-connect-configs-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:14:53,296] INFO [Broker id=1] Leader docker-connect-configs-0 with topic id Some(rW3LqdQcRZq6zws5HiUfgw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:14:53,306] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic docker-connect-configs with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
connect                            | [2023-08-04 10:14:53,327] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,327] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,327] WARN The configuration 'metrics.context.resource.version' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,327] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,327] WARN The configuration 'metrics.context.resource.type' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,327] WARN The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,327] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,327] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,328] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,328] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,328] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,328] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,328] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,328] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,328] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,328] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,328] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,328] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,328] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,328] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,328] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,328] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,328] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect                            | [2023-08-04 10:14:53,328] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:53,329] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:53,329] INFO Kafka startTimeMs: 1691144093328 (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:53,339] INFO ConsumerConfig values: 
connect                            | 	allow.auto.create.topics = true
connect                            | 	auto.commit.interval.ms = 5000
connect                            | 	auto.offset.reset = earliest
connect                            | 	bootstrap.servers = [broker:29092]
connect                            | 	check.crcs = true
connect                            | 	client.dns.lookup = use_all_dns_ips
connect                            | 	client.id = consumer-compose-connect-group-3
connect                            | 	client.rack = 
connect                            | 	connections.max.idle.ms = 540000
connect                            | 	default.api.timeout.ms = 60000
connect                            | 	enable.auto.commit = false
connect                            | 	exclude.internal.topics = true
connect                            | 	fetch.max.bytes = 52428800
connect                            | 	fetch.max.wait.ms = 500
connect                            | 	fetch.min.bytes = 1
connect                            | 	group.id = compose-connect-group
connect                            | 	group.instance.id = null
connect                            | 	heartbeat.interval.ms = 3000
connect                            | 	interceptor.classes = []
connect                            | 	internal.leave.group.on.close = true
connect                            | 	internal.throw.on.fetch.stable.offset.unsupported = false
connect                            | 	isolation.level = read_uncommitted
connect                            | 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
connect                            | 	max.partition.fetch.bytes = 1048576
connect                            | 	max.poll.interval.ms = 300000
connect                            | 	max.poll.records = 500
connect                            | 	metadata.max.age.ms = 300000
connect                            | 	metric.reporters = []
connect                            | 	metrics.num.samples = 2
connect                            | 	metrics.recording.level = INFO
connect                            | 	metrics.sample.window.ms = 30000
connect                            | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
connect                            | 	receive.buffer.bytes = 65536
connect                            | 	reconnect.backoff.max.ms = 1000
connect                            | 	reconnect.backoff.ms = 50
connect                            | 	request.timeout.ms = 30000
connect                            | 	retry.backoff.ms = 100
connect                            | 	sasl.client.callback.handler.class = null
connect                            | 	sasl.jaas.config = null
connect                            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect                            | 	sasl.kerberos.min.time.before.relogin = 60000
connect                            | 	sasl.kerberos.service.name = null
connect                            | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect                            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect                            | 	sasl.login.callback.handler.class = null
connect                            | 	sasl.login.class = null
connect                            | 	sasl.login.connect.timeout.ms = null
connect                            | 	sasl.login.read.timeout.ms = null
connect                            | 	sasl.login.refresh.buffer.seconds = 300
connect                            | 	sasl.login.refresh.min.period.seconds = 60
connect                            | 	sasl.login.refresh.window.factor = 0.8
connect                            | 	sasl.login.refresh.window.jitter = 0.05
connect                            | 	sasl.login.retry.backoff.max.ms = 10000
connect                            | 	sasl.login.retry.backoff.ms = 100
connect                            | 	sasl.mechanism = GSSAPI
connect                            | 	sasl.oauthbearer.clock.skew.seconds = 30
connect                            | 	sasl.oauthbearer.expected.audience = null
connect                            | 	sasl.oauthbearer.expected.issuer = null
connect                            | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect                            | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect                            | 	sasl.oauthbearer.jwks.endpoint.url = null
connect                            | 	sasl.oauthbearer.scope.claim.name = scope
connect                            | 	sasl.oauthbearer.sub.claim.name = sub
connect                            | 	sasl.oauthbearer.token.endpoint.url = null
connect                            | 	security.protocol = PLAINTEXT
connect                            | 	security.providers = null
connect                            | 	send.buffer.bytes = 131072
connect                            | 	session.timeout.ms = 45000
connect                            | 	socket.connection.setup.timeout.max.ms = 30000
connect                            | 	socket.connection.setup.timeout.ms = 10000
connect                            | 	ssl.cipher.suites = null
connect                            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect                            | 	ssl.endpoint.identification.algorithm = https
connect                            | 	ssl.engine.factory.class = null
connect                            | 	ssl.key.password = null
connect                            | 	ssl.keymanager.algorithm = SunX509
connect                            | 	ssl.keystore.certificate.chain = null
connect                            | 	ssl.keystore.key = null
connect                            | 	ssl.keystore.location = null
connect                            | 	ssl.keystore.password = null
connect                            | 	ssl.keystore.type = JKS
connect                            | 	ssl.protocol = TLSv1.3
connect                            | 	ssl.provider = null
connect                            | 	ssl.secure.random.implementation = null
connect                            | 	ssl.trustmanager.algorithm = PKIX
connect                            | 	ssl.truststore.certificates = null
connect                            | 	ssl.truststore.location = null
connect                            | 	ssl.truststore.password = null
connect                            | 	ssl.truststore.type = JKS
connect                            | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
connect                            |  (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,432] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,441] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,442] WARN The configuration 'metrics.context.resource.version' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,443] WARN The configuration 'metrics.context.resource.type' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,444] WARN The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,444] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,445] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,448] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,450] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,451] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,454] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,459] INFO [Producer clientId=producer-3] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,468] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,475] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,484] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,485] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,491] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,494] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,495] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,496] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,498] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,500] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,501] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect                            | [2023-08-04 10:14:53,502] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:53,503] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:53,504] INFO Kafka startTimeMs: 1691144093502 (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:14:53,531] INFO [Consumer clientId=consumer-compose-connect-group-3, groupId=compose-connect-group] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,537] INFO [Consumer clientId=consumer-compose-connect-group-3, groupId=compose-connect-group] Subscribed to partition(s): docker-connect-configs-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
connect                            | [2023-08-04 10:14:53,539] INFO [Consumer clientId=consumer-compose-connect-group-3, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect                            | [2023-08-04 10:14:53,570] INFO [Consumer clientId=consumer-compose-connect-group-3, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-configs-0 to 0 since the associated topicId changed from null to rW3LqdQcRZq6zws5HiUfgw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,575] INFO Finished reading KafkaBasedLog for topic docker-connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)
connect                            | [2023-08-04 10:14:53,576] INFO Started KafkaBasedLog for topic docker-connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)
connect                            | [2023-08-04 10:14:53,577] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
connect                            | [2023-08-04 10:14:53,578] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
connect                            | [2023-08-04 10:14:53,617] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 to 0 since the associated topicId changed from null to STq7PyopRJWACFWKlYOr2A (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,622] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to ZjoO_4HfTKWmowjlTjE6sw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,623] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 to 0 since the associated topicId changed from null to Qlt_SM49Sl6zBrVxalhSRw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,624] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to UbhlmVOjQAusw3dsEKYqeQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,625] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to qrRJJzmBRu6JLAarYC2FXQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,626] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 to 0 since the associated topicId changed from null to dxHSpn7BThGwIYeZu2BHmA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,627] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to tnG6SGoITzy4Bdh8lieE1g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,628] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 to 0 since the associated topicId changed from null to _vYRjW7IRXK7apsabDMiKQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,629] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to RTYUY2zsS26iu53cLq8UPg (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,630] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 to 0 since the associated topicId changed from null to Hsu6BGHpQ3KehegKKf6yyg (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,631] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 to 0 since the associated topicId changed from null to 27wkC6UWTRWT8JkErrPTLw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,632] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 to 0 since the associated topicId changed from null to zgm-W2fpQnOGMZXkBmgLQw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,633] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,634] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,635] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,636] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,637] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,638] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,639] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,644] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,645] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,647] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,648] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,649] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,650] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to P4lKo9gcStGZyM8e3fWDyw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,651] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 to 0 since the associated topicId changed from null to w4L152RES8qYs0ThphAjlg (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,652] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-ksql-default__command_topic-0 to 0 since the associated topicId changed from null to qszbKrAmRm-xGzAdJbM2PQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,653] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-configs-0 to 0 since the associated topicId changed from null to rW3LqdQcRZq6zws5HiUfgw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,655] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 to 0 since the associated topicId changed from null to RPkOqg4kSBaqVbVHl5cH9w (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,657] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 to 0 since the associated topicId changed from null to 3CHyfz78QgSOj8D2PYTkuA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,658] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-command-0 to 0 since the associated topicId changed from null to 26mdvJTUTviIQl0i3yP-hQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,659] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to wbsqiFi3RFiBKXg6x7JVQg (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,660] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 to 0 since the associated topicId changed from null to CyEPox4GQNy8kLG7WEQWrQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,660] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to 8kdvQ-ncQWWnL9X7ZnXiBA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,661] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to c1c8GxRtS4u85J4-7gRYng (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,662] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to jGx_3gCrTDmXXO7DlfFZkw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,663] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to mNO1aiiEQ1aUPUFDBwdR-w (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,665] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to 5zxMVZraQkS0mMTEISnuCA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,666] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to I5wV8QWRRJuTL-yY1xI20g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,667] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to gpn2CyU5S3CepZalgg_6ug (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,668] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 to 0 since the associated topicId changed from null to 7CG1vSjFRcuvphmKW10WEA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,669] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-1 to 0 since the associated topicId changed from null to FZMPJSlQRhuancFPWWYdoQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,670] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-3 to 0 since the associated topicId changed from null to FZMPJSlQRhuancFPWWYdoQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,671] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-2 to 0 since the associated topicId changed from null to FZMPJSlQRhuancFPWWYdoQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,671] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-0 to 0 since the associated topicId changed from null to FZMPJSlQRhuancFPWWYdoQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,671] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-4 to 0 since the associated topicId changed from null to FZMPJSlQRhuancFPWWYdoQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,671] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition default_ksql_processing_log-0 to 0 since the associated topicId changed from null to NaFdOZr7T36qpM3L26ausQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,671] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to g31qr5BRRCeyT-Ctg5GhYw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,672] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 to 0 since the associated topicId changed from null to 1LLF15hWQe-xqLOpsOrbpA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,672] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to igf_bdToRC6wz8tLXU8ZWA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,672] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to _zkEEKNOSh-BwqO66EWICw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,672] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to xgvCZkJ3RMurzOd2tG9eqg (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,672] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 to 0 since the associated topicId changed from null to tkBvdtlJRWily0KDogf0lg (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,672] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 to 0 since the associated topicId changed from null to Gcm9HudQQIePkLpcnSZatA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,672] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to TL8DPoV0R9OJhSSwRHqD6A (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,672] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 5MSfzhgUTOCsJsKEWsUfUQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,672] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to z2ogKsUbRf6X0CRLuVUTWw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,672] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 to 0 since the associated topicId changed from null to qbl7Kw1mTrKwZUmrs21Ogw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,672] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 6A1t9PL6S9ibcqZ61sSDAQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,672] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _schemas-0 to 0 since the associated topicId changed from null to Tj_6_gizR86RoKJiuNP8Fw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,673] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to xYBgTiSgQeeJ02q7jNXaDg (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,673] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 to 0 since the associated topicId changed from null to bKHlhox8TzeDMS5qRnHymg (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,673] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to Ly7r4MYYR8iZXnKz11AfxQ (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,673] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to HFZ8vq7-RZyhNR6Sk4e1ng (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,673] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-17 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,673] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-11 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,673] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-23 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,673] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-40 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,673] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-5 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,673] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-0 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,673] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-29 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,673] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-46 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,674] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-30 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,674] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-4 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,674] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-39 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,674] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-42 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,674] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-36 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,674] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-48 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,674] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-10 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,674] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-13 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,674] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-45 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,674] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-16 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,674] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-28 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,674] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-34 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,674] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-19 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,675] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-22 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,675] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-31 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,675] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-2 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,675] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-25 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,675] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-20 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,675] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-26 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,675] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-14 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,675] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-32 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,675] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-37 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,675] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-8 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,675] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-43 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,675] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-7 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,675] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-49 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,676] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-33 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,676] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-1 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,676] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-27 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,676] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-24 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,676] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-21 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,676] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-47 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,676] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-3 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,676] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-9 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,676] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-15 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,676] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-18 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,676] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-44 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,676] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-6 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,677] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-35 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,677] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-38 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,677] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-41 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,677] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __transaction_state-12 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,687] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-17 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,687] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-20 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,688] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-11 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,688] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-23 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,688] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-14 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,688] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-5 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,688] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-0 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,688] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-8 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,688] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-7 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,688] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-4 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,688] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-1 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,688] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-10 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,689] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-13 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,689] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-24 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,689] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-21 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,689] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-16 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,689] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-3 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,689] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-9 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,689] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-15 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,689] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-18 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,689] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-19 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,689] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-22 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,689] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-6 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,689] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-2 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,690] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-12 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,690] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 to 0 since the associated topicId changed from null to adG-RfPuSaSjxyV6nKsuDw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,690] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,690] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 to 0 since the associated topicId changed from null to LV4LlNFGSOuelBMsyB9XFw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,690] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to NGyXVef5Tbicz4WJpOfTdg (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,690] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 to 0 since the associated topicId changed from null to EnSw6NjFQNmCXav_Zub-bw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,690] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-17 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,690] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-11 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,690] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-23 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,690] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-40 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,690] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-5 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,690] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-0 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,690] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-29 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,690] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-46 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,690] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-30 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,691] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-4 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,691] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-39 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,691] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-42 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,691] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-36 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,691] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-48 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,691] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-10 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,691] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-13 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,691] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-45 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,691] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-16 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,691] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-28 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,691] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-34 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,691] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-19 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,691] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-22 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,692] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-31 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,692] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-2 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,692] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-25 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,692] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-20 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,692] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-26 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,692] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-14 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,692] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-32 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,692] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-37 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,692] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-8 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,692] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-43 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,692] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-7 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,692] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-49 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,692] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-33 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,692] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-1 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,692] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-27 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,692] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-24 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,693] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-21 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,693] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-47 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,693] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-3 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,693] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-9 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,693] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-15 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,693] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-18 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,693] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-44 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,693] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-6 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,693] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-35 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,693] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-38 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,693] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-41 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,693] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-12 to 0 since the associated topicId changed from null to En_RJ3y3THKwRrol6fr06Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,693] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to UORTMA9aQXuH9QFrp1lC6Q (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,724] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:53,726] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
connect                            | [2023-08-04 10:14:53,742] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
connect                            | [2023-08-04 10:14:53,742] INFO [Worker clientId=connect-1, groupId=compose-connect-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
broker                             | [2023-08-04 10:14:53,771] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group compose-connect-group in Empty state. Created a new member id connect-1-31527bae-23d9-47a6-b1a9-d2e5692b0443 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
connect                            | [2023-08-04 10:14:53,775] INFO [Worker clientId=connect-1, groupId=compose-connect-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
broker                             | [2023-08-04 10:14:53,778] INFO [GroupCoordinator 1]: Preparing to rebalance group compose-connect-group in state PreparingRebalance with old generation 0 (__consumer_offsets-37) (reason: Adding new member connect-1-31527bae-23d9-47a6-b1a9-d2e5692b0443 with group instance id None; client reason: not provided) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:14:53,780] INFO [GroupCoordinator 1]: Stabilized group compose-connect-group generation 1 (__consumer_offsets-37) with 1 members (kafka.coordinator.group.GroupCoordinator)
connect                            | [2023-08-04 10:14:53,784] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Successfully joined group with generation Generation{generationId=1, memberId='connect-1-31527bae-23d9-47a6-b1a9-d2e5692b0443', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
broker                             | [2023-08-04 10:14:53,853] INFO [GroupCoordinator 1]: Assignment received from leader connect-1-31527bae-23d9-47a6-b1a9-d2e5692b0443 for group compose-connect-group for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
connect                            | [2023-08-04 10:14:53,859] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Successfully synced group in generation Generation{generationId=1, memberId='connect-1-31527bae-23d9-47a6-b1a9-d2e5692b0443', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
connect                            | [2023-08-04 10:14:53,864] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31527bae-23d9-47a6-b1a9-d2e5692b0443', leaderUrl='http://connect:8083/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
connect                            | [2023-08-04 10:14:53,866] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Starting connectors and tasks using config offset -1 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
connect                            | [2023-08-04 10:14:53,867] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
connect                            | [2023-08-04 10:14:53,939] INFO [Producer clientId=producer-3] Resetting the last seen epoch of partition docker-connect-configs-0 to 0 since the associated topicId changed from null to rW3LqdQcRZq6zws5HiUfgw (org.apache.kafka.clients.Metadata)
connect                            | [2023-08-04 10:14:54,309] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
connect                            | Aug 04, 2023 10:14:55 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
connect                            | WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
connect                            | Aug 04, 2023 10:14:55 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
connect                            | WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
connect                            | Aug 04, 2023 10:14:55 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
connect                            | WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
connect                            | Aug 04, 2023 10:14:55 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
connect                            | WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConfluentV1MetadataResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConfluentV1MetadataResource will be ignored. 
connect                            | Aug 04, 2023 10:14:55 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
connect                            | WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
connect                            | [2023-08-04 10:14:55,763] INFO HV000001: Hibernate Validator 6.1.7.Final (org.hibernate.validator.internal.util.Version)
connect                            | Aug 04, 2023 10:14:56 AM org.glassfish.jersey.internal.Errors logErrors
connect                            | WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
connect                            | WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
connect                            | WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
connect                            | WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
connect                            | WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.
connect                            | 
connect                            | 
connect                            | [2023-08-04 10:14:56,805] INFO Started o.e.j.s.ServletContextHandler@181584c{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
connect                            | [2023-08-04 10:14:56,805] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer)
connect                            | [2023-08-04 10:14:56,805] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect)
data-agrigator-taskmanager-1       | 2023-08-04 10:15:00,940 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:15:00,942 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
ksqldb-server                      | [2023-08-04 10:15:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
schema-registry                    | [2023-08-04 10:15:07,754] INFO 192.168.160.13 - - [04/Aug/2023:10:15:07 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 20 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:15:08,724] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:15:08 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
data-agrigator-taskmanager-1       | 2023-08-04 10:15:10,988 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:15:10,993 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:21,033 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:15:21,034 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
schema-registry                    | [2023-08-04 10:15:25,290] INFO 192.168.160.13 - - [04/Aug/2023:10:15:25 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 9 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:15:30,418] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:15:30 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
data-agrigator-taskmanager-1       | 2023-08-04 10:15:31,074 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:31,075 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
control-center                     | [2023-08-04 10:15:32,134] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:15:32,134] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
data-agrigator-taskmanager-1       | 2023-08-04 10:15:41,112 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:15:41,114 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
schema-registry                    | [2023-08-04 10:15:45,440] INFO 192.168.160.13 - - [04/Aug/2023:10:15:45 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 9 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:15:47,697] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:15:47 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
data-agrigator-taskmanager-1       | 2023-08-04 10:15:51,147 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:15:51,149 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:53,882 ERROR org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Fatal error occurred in TaskExecutor akka.tcp://flink@192.168.160.7:42073/user/rpc/taskmanager_0.
data-agrigator-taskmanager-1       | org.apache.flink.runtime.taskexecutor.exceptions.RegistrationTimeoutException: Could not register at the ResourceManager within the specified maximum registration duration PT5M. This indicates a problem with this instance. Terminating now.
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.registrationTimeout(TaskExecutor.java:1552) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$startRegistrationTimeout$18(TaskExecutor.java:1537) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:453) ~[flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:453) ~[flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:218) ~[flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
data-agrigator-taskmanager-1       | 	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
data-agrigator-taskmanager-1       | 	at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
data-agrigator-taskmanager-1       | 	at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
data-agrigator-taskmanager-1       | 	at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
data-agrigator-taskmanager-1       | 2023-08-04 10:15:53,893 ERROR org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Fatal error occurred while executing the TaskManager. Shutting it down...
data-agrigator-taskmanager-1       | org.apache.flink.runtime.taskexecutor.exceptions.RegistrationTimeoutException: Could not register at the ResourceManager within the specified maximum registration duration PT5M. This indicates a problem with this instance. Terminating now.
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.registrationTimeout(TaskExecutor.java:1552) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$startRegistrationTimeout$18(TaskExecutor.java:1537) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:453) ~[flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:453) ~[flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:218) ~[flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_571cee3c-815d-4611-afb4-27fa4ef0c9e8.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
data-agrigator-taskmanager-1       | 	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
data-agrigator-taskmanager-1       | 	at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
data-agrigator-taskmanager-1       | 	at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
data-agrigator-taskmanager-1       | 	at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
data-agrigator-taskmanager-1       | 2023-08-04 10:15:53,938 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Stopping TaskExecutor akka.tcp://flink@192.168.160.7:42073/user/rpc/taskmanager_0.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:53,939 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Terminating registration attempts towards ResourceManager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:53,941 INFO  org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManager [] - Shutting down TaskExecutorStateChangelogStoragesManager.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:53,942 INFO  org.apache.flink.runtime.state.TaskExecutorChannelStateExecutorFactoryManager [] - Shutting down TaskExecutorChannelStateExecutorFactoryManager.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:53,967 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Stop job leader service.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:53,969 INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,002 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /tmp/flink-io-58a08308-98a5-45b6-80af-dc8791d35ee5
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,003 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Shutting down the network environment and its components.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,007 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful shutdown (took 3 ms).
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,014 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful shutdown (took 6 ms).
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,027 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /tmp/flink-netty-shuffle-aa5d3cac-0b38-4df2-a8b5-f4650ab95f41
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,028 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Shutting down the kvState service and its components.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,028 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Stop job leader service.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,031 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - removed file cache directory /tmp/flink-dist-cache-17656963-a210-4786-949f-f819bf4cc4e7
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,039 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Stopped TaskExecutor akka.tcp://flink@192.168.160.7:42073/user/rpc/taskmanager_0.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,051 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,059 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,062 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,091 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,170 INFO  akka.actor.CoordinatedShutdown                               [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,170 INFO  akka.actor.CoordinatedShutdown                               [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,222 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,229 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,230 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,234 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,315 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,323 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,359 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,382 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
data-agrigator-taskmanager-1       | 2023-08-04 10:15:54,386 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Terminating TaskManagerRunner with exit code 1.
data-agrigator-taskmanager-1 exited with code 1
schema-registry                    | [2023-08-04 10:16:03,310] INFO 192.168.160.13 - - [04/Aug/2023:10:16:03 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 10 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:16:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
control-center                     | [2023-08-04 10:16:07,097] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to 5zxMVZraQkS0mMTEISnuCA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:16:07,098] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:16:07,158] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:16:09,717] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 to 0 since the associated topicId changed from null to STq7PyopRJWACFWKlYOr2A (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,718] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to ZjoO_4HfTKWmowjlTjE6sw (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,718] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 to 0 since the associated topicId changed from null to Qlt_SM49Sl6zBrVxalhSRw (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,719] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to UbhlmVOjQAusw3dsEKYqeQ (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,720] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to qrRJJzmBRu6JLAarYC2FXQ (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,722] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 to 0 since the associated topicId changed from null to dxHSpn7BThGwIYeZu2BHmA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,724] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to tnG6SGoITzy4Bdh8lieE1g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,726] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 to 0 since the associated topicId changed from null to _vYRjW7IRXK7apsabDMiKQ (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,726] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to RTYUY2zsS26iu53cLq8UPg (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,727] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 to 0 since the associated topicId changed from null to Hsu6BGHpQ3KehegKKf6yyg (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,727] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 to 0 since the associated topicId changed from null to 27wkC6UWTRWT8JkErrPTLw (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,728] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 to 0 since the associated topicId changed from null to zgm-W2fpQnOGMZXkBmgLQw (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,728] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,728] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,728] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,729] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,729] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,729] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,729] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,729] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,729] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,730] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,730] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,730] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to WXyI51KeTHCt18UI0bi43A (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,730] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to P4lKo9gcStGZyM8e3fWDyw (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,730] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 to 0 since the associated topicId changed from null to w4L152RES8qYs0ThphAjlg (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,730] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-ksql-default__command_topic-0 to 0 since the associated topicId changed from null to qszbKrAmRm-xGzAdJbM2PQ (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,731] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-configs-0 to 0 since the associated topicId changed from null to rW3LqdQcRZq6zws5HiUfgw (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,731] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 to 0 since the associated topicId changed from null to RPkOqg4kSBaqVbVHl5cH9w (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,731] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 to 0 since the associated topicId changed from null to 3CHyfz78QgSOj8D2PYTkuA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,731] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-command-0 to 0 since the associated topicId changed from null to 26mdvJTUTviIQl0i3yP-hQ (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,733] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to wbsqiFi3RFiBKXg6x7JVQg (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,733] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 to 0 since the associated topicId changed from null to CyEPox4GQNy8kLG7WEQWrQ (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,735] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to 8kdvQ-ncQWWnL9X7ZnXiBA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,735] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to c1c8GxRtS4u85J4-7gRYng (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,736] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to jGx_3gCrTDmXXO7DlfFZkw (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,736] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to mNO1aiiEQ1aUPUFDBwdR-w (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,737] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to 5zxMVZraQkS0mMTEISnuCA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,737] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to I5wV8QWRRJuTL-yY1xI20g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,737] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to gpn2CyU5S3CepZalgg_6ug (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,737] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 to 0 since the associated topicId changed from null to 7CG1vSjFRcuvphmKW10WEA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,738] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-status-1 to 0 since the associated topicId changed from null to FZMPJSlQRhuancFPWWYdoQ (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,738] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-status-3 to 0 since the associated topicId changed from null to FZMPJSlQRhuancFPWWYdoQ (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,738] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-status-2 to 0 since the associated topicId changed from null to FZMPJSlQRhuancFPWWYdoQ (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,738] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-status-0 to 0 since the associated topicId changed from null to FZMPJSlQRhuancFPWWYdoQ (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,739] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-status-4 to 0 since the associated topicId changed from null to FZMPJSlQRhuancFPWWYdoQ (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,739] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition default_ksql_processing_log-0 to 0 since the associated topicId changed from null to NaFdOZr7T36qpM3L26ausQ (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,740] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to g31qr5BRRCeyT-Ctg5GhYw (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,740] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 to 0 since the associated topicId changed from null to 1LLF15hWQe-xqLOpsOrbpA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,740] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to igf_bdToRC6wz8tLXU8ZWA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,740] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to _zkEEKNOSh-BwqO66EWICw (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,741] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to xgvCZkJ3RMurzOd2tG9eqg (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,741] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 to 0 since the associated topicId changed from null to tkBvdtlJRWily0KDogf0lg (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,741] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 to 0 since the associated topicId changed from null to Gcm9HudQQIePkLpcnSZatA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,741] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to TL8DPoV0R9OJhSSwRHqD6A (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,742] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 5MSfzhgUTOCsJsKEWsUfUQ (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,742] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to z2ogKsUbRf6X0CRLuVUTWw (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,742] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 to 0 since the associated topicId changed from null to qbl7Kw1mTrKwZUmrs21Ogw (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,743] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 6A1t9PL6S9ibcqZ61sSDAQ (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,743] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to xYBgTiSgQeeJ02q7jNXaDg (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,744] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 to 0 since the associated topicId changed from null to bKHlhox8TzeDMS5qRnHymg (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,744] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to Ly7r4MYYR8iZXnKz11AfxQ (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,744] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to HFZ8vq7-RZyhNR6Sk4e1ng (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,745] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-17 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,745] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-11 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,745] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-23 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,746] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-40 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,746] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-5 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,746] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-0 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,747] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-29 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,747] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-46 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,748] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-30 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,748] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-4 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,748] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-39 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,749] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-42 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,749] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-36 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,749] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-48 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,749] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-10 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,750] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-13 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,750] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-45 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,750] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-16 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,750] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-28 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,751] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-34 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,751] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-19 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,751] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-22 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,752] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-31 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,752] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-2 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,752] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-25 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,753] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-20 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,754] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-26 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,755] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-14 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,755] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-32 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,755] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-37 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,756] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-8 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,756] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-43 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,757] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-7 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,758] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-49 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,758] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-33 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,759] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-1 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,759] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-27 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,759] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-24 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,760] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-21 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,760] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-47 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,760] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-3 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,760] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-9 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,761] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-15 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,761] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-18 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,761] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-44 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,762] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-6 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,762] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-35 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,762] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-38 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,763] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-41 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,763] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __transaction_state-12 to 0 since the associated topicId changed from null to Z9JbXZcBRjCWlzBoXwvf4g (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,763] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-17 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,764] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-20 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,764] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-11 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,765] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-23 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,765] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-14 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,765] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-5 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,766] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-0 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,766] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-8 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,766] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-7 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,767] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-4 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,767] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-1 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,767] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-10 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,768] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-13 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,768] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-24 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,768] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-21 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,768] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-16 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,769] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-3 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,769] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-9 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,769] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-15 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,769] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-18 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,770] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-19 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,770] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-22 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,770] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-6 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,770] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-2 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,771] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition docker-connect-offsets-12 to 0 since the associated topicId changed from null to XlZu4UyKTamPRyQ7VaDLgA (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,771] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 to 0 since the associated topicId changed from null to adG-RfPuSaSjxyV6nKsuDw (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,772] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to RjGiTb4XQTq3XkcLgjDFew (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,773] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 to 0 since the associated topicId changed from null to LV4LlNFGSOuelBMsyB9XFw (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,773] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to NGyXVef5Tbicz4WJpOfTdg (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,773] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 to 0 since the associated topicId changed from null to EnSw6NjFQNmCXav_Zub-bw (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:16:09,774] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to UORTMA9aQXuH9QFrp1lC6Q (org.apache.kafka.clients.Metadata)
ksqldb-server                      | [2023-08-04 10:16:11,072] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:16:11 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:16:14,997] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
schema-registry                    | [2023-08-04 10:16:19,456] INFO 192.168.160.13 - - [04/Aug/2023:10:16:19 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 10 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:16:23,478] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:16:23,478] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Processed 14 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:16:23,577] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Processed 14 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:16:23,579] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Processed 14 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:16:23,580] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:16:23,624] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:16:23,632] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:16:23,655] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:16:23,663] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:16:23,664] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:16:23,748] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:16:23,748] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:16:32,136] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:16:32,137] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:16:33,108] INFO [AdminClient clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-admin] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:16:33,521] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:16:33 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:16:41,431] INFO 192.168.160.13 - - [04/Aug/2023:10:16:41 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 14 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:16:55,826] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:16:55 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:17:01,952] INFO 192.168.160.13 - - [04/Aug/2023:10:17:01 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 9 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:17:04,345] INFO [AdminClient clientId=adminclient-2] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:17:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
ksqldb-server                      | [2023-08-04 10:17:05,051] INFO [AdminClient clientId=adminclient-3] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:17:05,779] INFO [AdminClient clientId=adminclient-4] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:17:14,971] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:17:14 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:17:18,721] INFO 192.168.160.1 - - [04/Aug/2023:10:17:18 +0000] "GET / HTTP/1.1" 200 1151 "-" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 439 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:18,746] INFO 192.168.160.1 - - [04/Aug/2023:10:17:18 +0000] "GET / HTTP/1.1" 200 1151 "-" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 148 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:18,951] INFO 192.168.160.1 - - [04/Aug/2023:10:17:18 +0000] "GET /dist/bootstrap-local.76a65c8.js HTTP/1.1" 200 61728 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 94 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:19,300] INFO 192.168.160.1 - - [04/Aug/2023:10:17:19 +0000] "GET /dist/manifest.json HTTP/1.1" 200 389 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 6 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:19,307] INFO 192.168.160.1 - - [04/Aug/2023:10:17:19 +0000] "GET /dist/favicon.ico HTTP/1.1" 200 33310 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 11 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:19,456] INFO 192.168.160.1 - - [04/Aug/2023:10:17:19 +0000] "GET /dist/dist/android-chrome-144x144.png HTTP/1.1" 404 399 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 113 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:19,752] INFO 192.168.160.1 - - [04/Aug/2023:10:17:19 +0000] "GET /3.0/license/payload HTTP/1.1" 200 172 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 492 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:19,791] INFO 192.168.160.1 - - [04/Aug/2023:10:17:19 +0000] "GET /2.0/feature/flags HTTP/1.1" 200 429 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 532 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:19,992] INFO [AdminClient clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-admin] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:17:21,497] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 to 0 since the associated topicId changed from null to 3CHyfz78QgSOj8D2PYTkuA (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:17:21,497] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to z2ogKsUbRf6X0CRLuVUTWw (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:17:25,166] INFO 192.168.160.13 - - [04/Aug/2023:10:17:25 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 25 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:27,839] INFO 192.168.160.1 - - [04/Aug/2023:10:17:26 +0000] "GET /dist/c3.chunk-e71fc146570941633bfc.js HTTP/1.1" 200 1077929 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 1273 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:27,874] INFO 192.168.160.1 - - [04/Aug/2023:10:17:26 +0000] "GET /dist/c3.chunk-71c32f43e29ac8156892.js HTTP/1.1" 200 1116319 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 1308 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:30,346] INFO 192.168.160.1 - - [04/Aug/2023:10:17:30 +0000] "GET /2.0/feature/flags HTTP/1.1" 200 429 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 17 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:30,590] INFO 192.168.160.1 - - [04/Aug/2023:10:17:30 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 74 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:31,085] INFO 192.168.160.1 - - [04/Aug/2023:10:17:30 +0000] "GET /3.0/auth/principal HTTP/1.1" 200 30 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 234 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:31,085] INFO 192.168.160.1 - - [04/Aug/2023:10:17:30 +0000] "GET /2.0/clusters/kafka/display/cluster_management HTTP/1.1" 200 110 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 266 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:31,135] INFO 192.168.160.1 - - [04/Aug/2023:10:17:30 +0000] "GET /2.0/clusters/kafka/display/stream-monitoring HTTP/1.1" 200 110 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 303 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:31,155] INFO 192.168.160.1 - - [04/Aug/2023:10:17:30 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 223 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:31,247] INFO 192.168.160.1 - - [04/Aug/2023:10:17:30 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 462 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:31,252] INFO 192.168.160.1 - - [04/Aug/2023:10:17:31 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 136 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:31,264] INFO 192.168.160.1 - - [04/Aug/2023:10:17:31 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 107 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:31,265] INFO 192.168.160.1 - - [04/Aug/2023:10:17:30 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 525 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:31,269] INFO 192.168.160.1 - - [04/Aug/2023:10:17:31 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 154 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:31,308] INFO 192.168.160.1 - - [04/Aug/2023:10:17:31 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 13 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:31,741] INFO 192.168.160.1 - - [04/Aug/2023:10:17:31 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/controller HTTP/1.1" 200 67 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 217 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:32,167] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:17:32,171] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:17:32,213] INFO 192.168.160.1 - - [04/Aug/2023:10:17:31 +0000] "GET /api/connect/connect-default/ HTTP/1.1" 200 110 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 857 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:32,274] INFO [AdminClient clientId=adminclient-3] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:17:32,444] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:17:32 GMT] "GET /info HTTP/1.1" 200 132 "-" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 0 (io.confluent.ksql.api.server.LoggingHandler)
ksqldb-server                      | [2023-08-04 10:17:32,520] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:17:32 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:17:32,591] INFO 192.168.160.1 - - [04/Aug/2023:10:17:31 +0000] "GET /api/ksql/ksqldb1/info HTTP/1.1" 200 133 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 1183 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:33,032] INFO 192.168.160.1 - - [04/Aug/2023:10:17:31 +0000] "GET /2.0/metrics/maxtime HTTP/1.1" 200 2 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 1539 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:33,286] INFO 192.168.160.1 - - [04/Aug/2023:10:17:31 +0000] "GET /2.0/metrics/clusters/status HTTP/1.1" 200 217 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 1785 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:35,308] INFO 192.168.160.1 - - [04/Aug/2023:10:17:35 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/brokers/config HTTP/1.1" 200 4496 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 99 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:35,378] INFO 192.168.160.1 - - [04/Aug/2023:10:17:35 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/broker/status HTTP/1.1" 200 162 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 34 (io.confluent.rest-utils.requests)
schema-registry                    | [2023-08-04 10:17:43,742] INFO 192.168.160.13 - - [04/Aug/2023:10:17:43 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 29 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:43,945] INFO 192.168.160.1 - - [04/Aug/2023:10:17:43 +0000] "GET /2.0/metrics/maxtime HTTP/1.1" 200 2 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 108 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:43,953] INFO 192.168.160.1 - - [04/Aug/2023:10:17:43 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/controller HTTP/1.1" 200 67 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 61 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:43,989] INFO 192.168.160.1 - - [04/Aug/2023:10:17:43 +0000] "GET /2.0/metrics/clusters/status HTTP/1.1" 200 217 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 137 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:17:44,017] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:17:44 GMT] "GET /info HTTP/1.1" 200 132 "-" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:17:44,103] INFO 192.168.160.1 - - [04/Aug/2023:10:17:44 +0000] "GET /api/ksql/ksqldb1/info HTTP/1.1" 200 133 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 99 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:17:48,809] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:17:48 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:17:50,153] INFO 192.168.160.1 - - [04/Aug/2023:10:17:50 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 30 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:50,199] INFO 192.168.160.1 - - [04/Aug/2023:10:17:50 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 64 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:50,203] INFO 192.168.160.1 - - [04/Aug/2023:10:17:50 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 61 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:50,444] INFO 192.168.160.1 - - [04/Aug/2023:10:17:50 +0000] "GET /2.0/clusters/kafka/display/cluster_management HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:50,448] INFO 192.168.160.1 - - [04/Aug/2023:10:17:50 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 15 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:50,534] INFO 192.168.160.1 - - [04/Aug/2023:10:17:50 +0000] "GET /2.0/clusters/kafka/display/stream-monitoring HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 74 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:50,538] INFO 192.168.160.1 - - [04/Aug/2023:10:17:50 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/controller HTTP/1.1" 200 67 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 44 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:50,556] INFO 192.168.160.1 - - [04/Aug/2023:10:17:50 +0000] "GET /3.0/auth/principal HTTP/1.1" 200 30 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 69 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:50,771] INFO 192.168.160.1 - - [04/Aug/2023:10:17:50 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/maxtime HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 293 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:50,868] INFO 192.168.160.1 - - [04/Aug/2023:10:17:50 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/broker/status HTTP/1.1" 200 162 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 361 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:50,992] INFO 192.168.160.1 - - [04/Aug/2023:10:17:50 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 122 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:50,998] INFO 192.168.160.1 - - [04/Aug/2023:10:17:50 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/topic/status HTTP/1.1" 200 17 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 424 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:51,046] INFO 192.168.160.1 - - [04/Aug/2023:10:17:51 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 41 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:51,092] INFO 192.168.160.1 - - [04/Aug/2023:10:17:51 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 76 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:51,110] INFO 192.168.160.1 - - [04/Aug/2023:10:17:51 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 46 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:51,135] INFO 192.168.160.1 - - [04/Aug/2023:10:17:51 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 32 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:51,184] INFO 192.168.160.1 - - [04/Aug/2023:10:17:51 +0000] "GET /api/kafka-rest/MkU3OEVBNTcwNTJENDM2Qg/v1/metadata/schemaRegistryUrls HTTP/1.1" 403 486 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 64 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:17:51,294] INFO Received: KsqlRequest{configOverrides={}, requestProperties={}, commandSequenceNumber=Optional.empty} (4d570d79-3bff-3bc6-a991-98e4ad80eeb9): DESCRIBE table1;
ksqldb-server                      | DESCRIBE STREAMS ;
ksqldb-server                      | show TOPICS;
ksqldb-server                      | show QUERIES EXTENDED;
ksqldb-server                      | show PROPERTIES; (io.confluent.ksql.logging.query.QueryLogger)
ksqldb-server                      | [2023-08-04 10:17:51,326] INFO There are no queries assigned to Gen 2 runtimes yet. (io.confluent.ksql.engine.RuntimeAssignor)
ksqldb-server                      | [2023-08-04 10:17:51,338] INFO ProcessingLogConfig values: 
ksqldb-server                      | 	ksql.logging.processing.rows.include = false
ksqldb-server                      | 	ksql.logging.processing.stream.auto.create = false
ksqldb-server                      | 	ksql.logging.processing.stream.name = KSQL_PROCESSING_LOG
ksqldb-server                      | 	ksql.logging.processing.topic.auto.create = false
ksqldb-server                      | 	ksql.logging.processing.topic.name = 
ksqldb-server                      | 	ksql.logging.processing.topic.partitions = 1
ksqldb-server                      | 	ksql.logging.processing.topic.replication.factor = 1
ksqldb-server                      |  (io.confluent.ksql.logging.processing.ProcessingLogConfig)
control-center                     | [2023-08-04 10:17:51,352] INFO 192.168.160.1 - - [04/Aug/2023:10:17:50 +0000] "GET /api/connect/connect-default/connectors?expand=status&expand=info HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 777 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:17:51,364] INFO Query created (69d3bf71-02ef-3367-93fc-1e29d9f2d414): DESCRIBE table1; (io.confluent.ksql.logging.query.QueryLogger)
ksqldb-server                      | [2023-08-04 10:17:51,373] INFO Query created (3f63a2ea-5da8-3653-a8de-1ab9040f2fdc): DESCRIBE STREAMS ; (io.confluent.ksql.logging.query.QueryLogger)
ksqldb-server                      | [2023-08-04 10:17:51,380] INFO Query created (64370dfd-0107-31b0-b29a-f495a8241f06): show TOPICS; (io.confluent.ksql.logging.query.QueryLogger)
ksqldb-server                      | [2023-08-04 10:17:51,382] INFO Query created (3dc59e22-3bd6-3936-bcf5-a3deca39fff9): show QUERIES EXTENDED; (io.confluent.ksql.logging.query.QueryLogger)
ksqldb-server                      | [2023-08-04 10:17:51,393] INFO Query created (2008326e-c8a2-3c7d-8908-9a1ab025f143): show PROPERTIES; (io.confluent.ksql.logging.query.QueryLogger)
control-center                     | [2023-08-04 10:17:51,416] INFO 192.168.160.1 - - [04/Aug/2023:10:17:50 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/broker/requests?interval=60000&end=1691144220000&start=1691129820000 HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 873 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:51,422] INFO 192.168.160.1 - - [04/Aug/2023:10:17:50 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/broker/detail HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 919 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:17:51,569] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:17:51,569] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:17:51,580] INFO Kafka startTimeMs: 1691144271569 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:17:51,586] INFO AdminClientConfig values: 
ksqldb-server                      | 	auto.include.jmx.reporter = true
ksqldb-server                      | 	bootstrap.servers = [broker:29092]
ksqldb-server                      | 	client.dns.lookup = use_all_dns_ips
ksqldb-server                      | 	client.id = 
ksqldb-server                      | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
ksqldb-server                      | 	confluent.proxy.protocol.client.address = null
ksqldb-server                      | 	confluent.proxy.protocol.client.port = null
ksqldb-server                      | 	confluent.proxy.protocol.client.version = NONE
ksqldb-server                      | 	confluent.use.controller.listener = false
ksqldb-server                      | 	connections.max.idle.ms = 300000
ksqldb-server                      | 	default.api.timeout.ms = 60000
ksqldb-server                      | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
ksqldb-server                      | 	metadata.max.age.ms = 300000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	metrics.num.samples = 2
ksqldb-server                      | 	metrics.recording.level = INFO
ksqldb-server                      | 	metrics.sample.window.ms = 30000
ksqldb-server                      | 	receive.buffer.bytes = 65536
ksqldb-server                      | 	reconnect.backoff.max.ms = 1000
ksqldb-server                      | 	reconnect.backoff.ms = 50
ksqldb-server                      | 	request.timeout.ms = 30000
ksqldb-server                      | 	retries = 2147483647
ksqldb-server                      | 	retry.backoff.ms = 100
ksqldb-server                      | 	sasl.client.callback.handler.class = null
ksqldb-server                      | 	sasl.jaas.config = null
ksqldb-server                      | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
ksqldb-server                      | 	sasl.kerberos.min.time.before.relogin = 60000
ksqldb-server                      | 	sasl.kerberos.service.name = null
ksqldb-server                      | 	sasl.kerberos.ticket.renew.jitter = 0.05
ksqldb-server                      | 	sasl.kerberos.ticket.renew.window.factor = 0.8
ksqldb-server                      | 	sasl.login.callback.handler.class = null
ksqldb-server                      | 	sasl.login.class = null
ksqldb-server                      | 	sasl.login.connect.timeout.ms = null
ksqldb-server                      | 	sasl.login.read.timeout.ms = null
ksqldb-server                      | 	sasl.login.refresh.buffer.seconds = 300
ksqldb-server                      | 	sasl.login.refresh.min.period.seconds = 60
ksqldb-server                      | 	sasl.login.refresh.window.factor = 0.8
ksqldb-server                      | 	sasl.login.refresh.window.jitter = 0.05
ksqldb-server                      | 	sasl.login.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.login.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.mechanism = GSSAPI
ksqldb-server                      | 	sasl.oauthbearer.clock.skew.seconds = 30
ksqldb-server                      | 	sasl.oauthbearer.expected.audience = null
ksqldb-server                      | 	sasl.oauthbearer.expected.issuer = null
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
ksqldb-server                      | 	sasl.oauthbearer.jwks.endpoint.url = null
ksqldb-server                      | 	sasl.oauthbearer.scope.claim.name = scope
ksqldb-server                      | 	sasl.oauthbearer.sub.claim.name = sub
ksqldb-server                      | 	sasl.oauthbearer.token.endpoint.url = null
ksqldb-server                      | 	security.protocol = PLAINTEXT
ksqldb-server                      | 	security.providers = null
ksqldb-server                      | 	send.buffer.bytes = 131072
ksqldb-server                      | 	socket.connection.setup.timeout.max.ms = 30000
ksqldb-server                      | 	socket.connection.setup.timeout.ms = 10000
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (org.apache.kafka.clients.admin.AdminClientConfig)
ksqldb-server                      | [2023-08-04 10:17:51,682] WARN These configurations '[metrics.context.resource.cluster.id, metrics.context.resource.version, metrics.context.resource.kafka.cluster.id, metrics.context.resource.type, metrics.context.resource.commit.id]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
ksqldb-server                      | [2023-08-04 10:17:51,685] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:17:51,685] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:17:51,687] INFO Kafka startTimeMs: 1691144271682 (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:17:52,074] INFO ProcessingLogConfig values: 
ksqldb-server                      | 	ksql.logging.processing.rows.include = false
ksqldb-server                      | 	ksql.logging.processing.stream.auto.create = false
ksqldb-server                      | 	ksql.logging.processing.stream.name = KSQL_PROCESSING_LOG
ksqldb-server                      | 	ksql.logging.processing.topic.auto.create = false
ksqldb-server                      | 	ksql.logging.processing.topic.name = 
ksqldb-server                      | 	ksql.logging.processing.topic.partitions = 1
ksqldb-server                      | 	ksql.logging.processing.topic.replication.factor = 1
ksqldb-server                      |  (io.confluent.ksql.logging.processing.ProcessingLogConfig)
ksqldb-server                      | [2023-08-04 10:17:52,235] INFO KsqlConfig values: 
ksqldb-server                      | 	ksql.access.validator.enable = auto
ksqldb-server                      | 	ksql.assert.schema.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.assert.topic.default.timeout.ms = 1000
ksqldb-server                      | 	ksql.authorization.cache.expiry.time.secs = 30
ksqldb-server                      | 	ksql.authorization.cache.max.entries = 10000
ksqldb-server                      | 	ksql.cast.strings.preserve.nulls = true
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.file = 
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.reload = false
ksqldb-server                      | 	ksql.connect.basic.auth.credentials.source = NONE
ksqldb-server                      | 	ksql.connect.request.headers.plugin = null
ksqldb-server                      | 	ksql.connect.request.timeout.ms = 5000
ksqldb-server                      | 	ksql.connect.url = http://connect:8083
ksqldb-server                      | 	ksql.connect.worker.config = 
ksqldb-server                      | 	ksql.create.or.replace.enabled = true
ksqldb-server                      | 	ksql.endpoint.migrate.query = true
ksqldb-server                      | 	ksql.error.classifier.regex = 
ksqldb-server                      | 	ksql.extension.dir = ext
ksqldb-server                      | 	ksql.headers.columns.enabled = true
ksqldb-server                      | 	ksql.hidden.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.insert.into.values.enabled = true
ksqldb-server                      | 	ksql.internal.topic.min.insync.replicas = 1
ksqldb-server                      | 	ksql.internal.topic.replicas = 1
ksqldb-server                      | 	ksql.json_sr.converter.deserializer.enabled = true
ksqldb-server                      | 	ksql.lambdas.enabled = true
ksqldb-server                      | 	ksql.metastore.backup.location = 
ksqldb-server                      | 	ksql.metrics.extension = null
ksqldb-server                      | 	ksql.metrics.tags.custom = 
ksqldb-server                      | 	ksql.nested.error.set.null = true
ksqldb-server                      | 	ksql.new.query.planner.enabled = false
ksqldb-server                      | 	ksql.output.topic.name.prefix = 
ksqldb-server                      | 	ksql.persistence.default.format.key = KAFKA
ksqldb-server                      | 	ksql.persistence.default.format.value = null
ksqldb-server                      | 	ksql.persistence.wrap.single.values = null
ksqldb-server                      | 	ksql.persistent.prefix = query_
ksqldb-server                      | 	ksql.properties.overrides.denylist = []
ksqldb-server                      | 	ksql.pull.queries.enable = true
ksqldb-server                      | 	ksql.query.cleanup.shutdown.timeout.ms = 30000
ksqldb-server                      | 	ksql.query.error.max.queue.size = 10
ksqldb-server                      | 	ksql.query.persistent.active.limit = 2147483647
ksqldb-server                      | 	ksql.query.persistent.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.query.pull.enable.standby.reads = false
ksqldb-server                      | 	ksql.query.pull.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.pull.limit.clause.enabled = true
ksqldb-server                      | 	ksql.query.pull.max.allowed.offset.lag = 9223372036854775807
ksqldb-server                      | 	ksql.query.pull.max.concurrent.requests = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.pull.max.qps = 2147483647
ksqldb-server                      | 	ksql.query.pull.metrics.enabled = true
ksqldb-server                      | 	ksql.query.pull.range.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.router.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.pull.stream.enabled = true
ksqldb-server                      | 	ksql.query.pull.table.scan.enabled = true
ksqldb-server                      | 	ksql.query.pull.thread.pool.size = 50
ksqldb-server                      | 	ksql.query.push.v2.alos.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.catchup.consumer.msg.window = 50
ksqldb-server                      | 	ksql.query.push.v2.continuation.tokens.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.enabled = false
ksqldb-server                      | 	ksql.query.push.v2.interpreter.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.latest.reset.age.ms = 30000
ksqldb-server                      | 	ksql.query.push.v2.max.catchup.consumers = 5
ksqldb-server                      | 	ksql.query.push.v2.max.hourly.bandwidth.megabytes = 2147483647
ksqldb-server                      | 	ksql.query.push.v2.metrics.enabled = true
ksqldb-server                      | 	ksql.query.push.v2.new.latest.delay.ms = 5000
ksqldb-server                      | 	ksql.query.push.v2.registry.installed = false
ksqldb-server                      | 	ksql.query.retry.backoff.initial.ms = 15000
ksqldb-server                      | 	ksql.query.retry.backoff.max.ms = 900000
ksqldb-server                      | 	ksql.query.status.running.threshold.seconds = 300
ksqldb-server                      | 	ksql.query.transient.max.bytes.buffering.total = -1
ksqldb-server                      | 	ksql.queryanonymizer.cluster_namespace = null
ksqldb-server                      | 	ksql.queryanonymizer.logs_enabled = true
ksqldb-server                      | 	ksql.readonly.topics = [_confluent.*, __confluent.*, _schemas, __consumer_offsets, __transaction_state, connect-configs, connect-offsets, connect-status, connect-statuses]
ksqldb-server                      | 	ksql.runtime.feature.shared.enabled = false
ksqldb-server                      | 	ksql.schema.registry.url = http://schema-registry:8081
ksqldb-server                      | 	ksql.security.extension.class = null
ksqldb-server                      | 	ksql.service.id = default_
ksqldb-server                      | 	ksql.shared.runtimes.count = 2
ksqldb-server                      | 	ksql.sink.window.change.log.additional.retention = 1000000
ksqldb-server                      | 	ksql.source.table.materialization.enabled = true
ksqldb-server                      | 	ksql.streams.shutdown.timeout.ms = 300000
ksqldb-server                      | 	ksql.suppress.buffer.size.bytes = -1
ksqldb-server                      | 	ksql.suppress.enabled = true
ksqldb-server                      | 	ksql.timestamp.throw.on.invalid = false
ksqldb-server                      | 	ksql.transient.prefix = transient_
ksqldb-server                      | 	ksql.transient.query.cleanup.service.enable = true
ksqldb-server                      | 	ksql.transient.query.cleanup.service.initial.delay.seconds = 600
ksqldb-server                      | 	ksql.transient.query.cleanup.service.period.seconds = 600
ksqldb-server                      | 	ksql.udf.collect.metrics = false
ksqldb-server                      | 	ksql.udf.enable.security.manager = true
ksqldb-server                      | 	ksql.udfs.enabled = true
ksqldb-server                      | 	ksql.variable.substitution.enable = true
ksqldb-server                      | 	ksql.websocket.connection.max.timeout.ms = 3600000
ksqldb-server                      | 	metric.reporters = []
ksqldb-server                      | 	ssl.cipher.suites = null
ksqldb-server                      | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
ksqldb-server                      | 	ssl.endpoint.identification.algorithm = https
ksqldb-server                      | 	ssl.engine.factory.class = null
ksqldb-server                      | 	ssl.key.password = null
ksqldb-server                      | 	ssl.keymanager.algorithm = SunX509
ksqldb-server                      | 	ssl.keystore.certificate.chain = null
ksqldb-server                      | 	ssl.keystore.key = null
ksqldb-server                      | 	ssl.keystore.location = null
ksqldb-server                      | 	ssl.keystore.password = null
ksqldb-server                      | 	ssl.keystore.type = JKS
ksqldb-server                      | 	ssl.protocol = TLSv1.3
ksqldb-server                      | 	ssl.provider = null
ksqldb-server                      | 	ssl.secure.random.implementation = null
ksqldb-server                      | 	ssl.trustmanager.algorithm = PKIX
ksqldb-server                      | 	ssl.truststore.certificates = null
ksqldb-server                      | 	ssl.truststore.location = null
ksqldb-server                      | 	ssl.truststore.password = null
ksqldb-server                      | 	ssl.truststore.type = JKS
ksqldb-server                      |  (io.confluent.ksql.util.KsqlConfig)
ksqldb-server                      | [2023-08-04 10:17:52,314] INFO Processed successfully: KsqlRequest{configOverrides={}, requestProperties={}, commandSequenceNumber=Optional.empty} (4d570d79-3bff-3bc6-a991-98e4ad80eeb9): DESCRIBE table1;
ksqldb-server                      | DESCRIBE STREAMS ;
ksqldb-server                      | show TOPICS;
ksqldb-server                      | show QUERIES EXTENDED;
ksqldb-server                      | show PROPERTIES; (io.confluent.ksql.logging.query.QueryLogger)
ksqldb-server                      | [2023-08-04 10:17:52,322] INFO App info kafka.admin.client for adminclient-5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:17:52,367] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
ksqldb-server                      | [2023-08-04 10:17:52,368] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
ksqldb-server                      | [2023-08-04 10:17:52,368] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
schema-registry                    | [2023-08-04 10:17:52,951] INFO 192.168.160.13 - - [04/Aug/2023:10:17:52 +0000] "GET /v1/metadata/version HTTP/1.1" 200 92 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 202 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:17:52,989] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:17:52 GMT] "POST /ksql HTTP/1.1" 200 19283 "-" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 98 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:17:53,046] INFO 192.168.160.1 - - [04/Aug/2023:10:17:52 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/brokers/config HTTP/1.1" 200 4496 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 383 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:53,049] INFO 192.168.160.1 - - [04/Aug/2023:10:17:52 +0000] "GET /api/schema-registry/8c2f8da2941df04bbb66301a6a9243a4f4768312/v1/metadata/version HTTP/1.1" 200 92 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 333 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:53,111] INFO - - - [04/Aug/2023:10:17:50 +0000] "POST /api/ksql/ksqldb1/ksql HTTP/1.1" 200 10 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/overview" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 2253 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:53,850] ERROR Async response CompletionException with error response entity of type unknown: null (javax.ws.rs.container.AsyncResponse)
control-center                     | java.util.concurrent.CompletionException: org.apache.kafka.common.errors.UnsupportedVersionException: The broker does not support LIST_MIRRORS
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:367)
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:376)
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:1019)
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
control-center                     | 	at io.confluent.kafkarest.common.KafkaFutures.lambda$toCompletableFuture$0(KafkaFutures.java:48)
control-center                     | 	at org.apache.kafka.common.internals.KafkaFutureImpl.lambda$whenComplete$2(KafkaFutureImpl.java:107)
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
control-center                     | 	at org.apache.kafka.common.internals.KafkaCompletableFuture.kafkaCompleteExceptionally(KafkaCompletableFuture.java:49)
control-center                     | 	at org.apache.kafka.common.internals.KafkaFutureImpl.completeExceptionally(KafkaFutureImpl.java:130)
control-center                     | 	at org.apache.kafka.clients.admin.KafkaAdminClient$58.handleFailure(KafkaAdminClient.java:5753)
control-center                     | 	at org.apache.kafka.clients.admin.KafkaAdminClient$Call.fail(KafkaAdminClient.java:971)
control-center                     | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.handleResponses(KafkaAdminClient.java:1401)
control-center                     | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1566)
control-center                     | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1489)
control-center                     | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                     | 	at org.apache.kafka.common.utils.KafkaThread.run(KafkaThread.java:64)
control-center                     | Caused by: org.apache.kafka.common.errors.UnsupportedVersionException: The broker does not support LIST_MIRRORS
control-center                     | [2023-08-04 10:17:54,488] INFO 192.168.160.1 - - [04/Aug/2023:10:17:52 +0000] "GET /api/kafka-rest/MkU3OEVBNTcwNTJENDM2Qg/kafka/v3/clusters/MkU3OEVBNTcwNTJENDM2Qg/links/-/mirrors HTTP/1.1" 400 73 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 1802 (io.confluent.rest-utils.requests)
broker                             | [2023-08-04 10:17:55,136] INFO [Controller 1] Validate-only CreateTopics result(s): CreatableTopic(name='adf60386-1180-4413-a422-86501d87351d', numPartitions=-1, replicationFactor=-1, assignments=[], configs=[]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:17:55,150] INFO [Controller 1] Validate-only CreateTopics result(s): CreatableTopic(name='adf60386-1180-4413-a422-86501d87351d', numPartitions=-1, replicationFactor=-1, assignments=[], configs=[]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
control-center                     | [2023-08-04 10:17:55,232] INFO 192.168.160.1 - - [04/Aug/2023:10:17:54 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/topic-defaults HTTP/1.1" 200 466 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 310 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:55,241] INFO 192.168.160.1 - - [04/Aug/2023:10:17:54 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/topic-default-config HTTP/1.1" 200 549 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 324 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:55,299] INFO 192.168.160.1 - - [04/Aug/2023:10:17:54 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/topics HTTP/1.1" 200 1275 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 387 (io.confluent.rest-utils.requests)
broker                             | [2023-08-04 10:17:58,334] INFO [Controller 1] Validate-only CreateTopics result(s): CreatableTopic(name='adf60386-1180-4413-a422-86501d87351d', numPartitions=-1, replicationFactor=-1, assignments=[], configs=[]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:17:58,357] INFO [Controller 1] Validate-only CreateTopics result(s): CreatableTopic(name='adf60386-1180-4413-a422-86501d87351d', numPartitions=-1, replicationFactor=-1, assignments=[], configs=[]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
control-center                     | [2023-08-04 10:17:58,348] INFO 192.168.160.1 - - [04/Aug/2023:10:17:58 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/topic-defaults HTTP/1.1" 200 466 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/create-topic" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 76 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:58,385] INFO 192.168.160.1 - - [04/Aug/2023:10:17:58 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/topic-default-config HTTP/1.1" 200 549 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/create-topic" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 111 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:17:58,626] INFO 192.168.160.1 - - [04/Aug/2023:10:17:58 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/topics HTTP/1.1" 200 1275 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/create-topic" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 337 (io.confluent.rest-utils.requests)
schema-registry                    | [2023-08-04 10:18:03,887] INFO 192.168.160.13 - - [04/Aug/2023:10:18:03 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 9 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:18:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
control-center                     | [2023-08-04 10:18:05,539] INFO 192.168.160.1 - - [04/Aug/2023:10:18:05 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/maxtime HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/create-topic" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 18 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:18:06,822] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:18:06 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
broker                             | [2023-08-04 10:18:13,213] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='gate_of_word', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='compression.type', value='producer'), CreateableTopicConfig(name='leader.replication.throttled.replicas', value=''), CreateableTopicConfig(name='remote.storage.enable', value='false'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='message.downconversion.enable', value='true'), CreateableTopicConfig(name='segment.jitter.ms', value='0'), CreateableTopicConfig(name='local.retention.ms', value='-2'), CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='flush.ms', value='9223372036854775807'), CreateableTopicConfig(name='follower.replication.throttled.replicas', value=''), CreateableTopicConfig(name='segment.bytes', value='1073741824'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='flush.messages', value='9223372036854775807'), CreateableTopicConfig(name='message.format.version', value='3.0-IV1'), CreateableTopicConfig(name='file.delete.delay.ms', value='60000'), CreateableTopicConfig(name='max.compaction.lag.ms', value='9223372036854775807'), CreateableTopicConfig(name='max.message.bytes', value='1048588'), CreateableTopicConfig(name='min.compaction.lag.ms', value='0'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='local.retention.bytes', value='-2'), CreateableTopicConfig(name='preallocate', value='false'), CreateableTopicConfig(name='index.interval.bytes', value='4096'), CreateableTopicConfig(name='min.cleanable.dirty.ratio', value='0.5'), CreateableTopicConfig(name='unclean.leader.election.enable', value='false'), CreateableTopicConfig(name='delete.retention.ms', value='86400000'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='segment.ms', value='604800000'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='segment.index.bytes', value='10485760')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:18:13,214] INFO [Controller 1] Created topic gate_of_word with topic ID xUE9Kc36Q_e3ecfOXf0gRg. (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:18:13,215] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration compression.type to producer (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,215] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration leader.replication.throttled.replicas to  (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,215] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration remote.storage.enable to false (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,215] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,216] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration message.downconversion.enable to true (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,216] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration segment.jitter.ms to 0 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,216] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration local.retention.ms to -2 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,216] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,216] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration flush.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,216] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration follower.replication.throttled.replicas to  (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,217] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration segment.bytes to 1073741824 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,217] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,217] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration flush.messages to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,217] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration message.format.version to 3.0-IV1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,217] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration file.delete.delay.ms to 60000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,217] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration max.compaction.lag.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,217] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration max.message.bytes to 1048588 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,218] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration min.compaction.lag.ms to 0 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,218] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,218] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration local.retention.bytes to -2 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,218] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration preallocate to false (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,218] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration index.interval.bytes to 4096 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,219] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration min.cleanable.dirty.ratio to 0.5 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,223] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration unclean.leader.election.enable to false (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,223] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration delete.retention.ms to 86400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,223] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,224] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration segment.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,224] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,224] INFO [Controller 1] ConfigResource(type=TOPIC, name='gate_of_word'): set configuration segment.index.bytes to 10485760 (org.apache.kafka.controller.ConfigurationControlManager)
broker                             | [2023-08-04 10:18:13,224] INFO [Controller 1] Created partition gate_of_word-0 with topic ID xUE9Kc36Q_e3ecfOXf0gRg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:18:13,253] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                             | [2023-08-04 10:18:13,257] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(gate_of_word-0) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:18:13,258] INFO [Broker id=1] Creating new partition gate_of_word-0 with topic id xUE9Kc36Q_e3ecfOXf0gRg. (state.change.logger)
control-center                     | [2023-08-04 10:18:13,272] INFO 192.168.160.1 - - [04/Aug/2023:10:18:12 +0000] "PUT /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/topics?validate=false HTTP/1.1" 204 0 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/create-topic" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 440 (io.confluent.rest-utils.requests)
broker                             | [2023-08-04 10:18:13,279] INFO [LogLoader partition=gate_of_word-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                             | [2023-08-04 10:18:13,282] INFO Created log for partition gate_of_word-0 in /tmp/kraft-combined-logs/gate_of_word-0 with properties {cleanup.policy=delete, compression.type="producer", delete.retention.ms=86400000, file.delete.delay.ms=60000, flush.messages=9223372036854775807, flush.ms=9223372036854775807, follower.replication.throttled.replicas=, index.interval.bytes=4096, leader.replication.throttled.replicas=, local.retention.bytes=-2, local.retention.ms=-2, max.compaction.lag.ms=9223372036854775807, max.message.bytes=1048588, message.downconversion.enable=true, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.cleanable.dirty.ratio=0.5, min.compaction.lag.ms=0, min.insync.replicas=1, preallocate=false, remote.storage.enable=false, retention.bytes=-1, retention.ms=604800000, segment.bytes=1073741824, segment.index.bytes=10485760, segment.jitter.ms=0, segment.ms=604800000, unclean.leader.election.enable=false} (kafka.log.LogManager)
broker                             | [2023-08-04 10:18:13,288] INFO [Partition gate_of_word-0 broker=1] No checkpointed highwatermark is found for partition gate_of_word-0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:18:13,289] INFO [Partition gate_of_word-0 broker=1] Log loaded for partition gate_of_word-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                             | [2023-08-04 10:18:13,289] INFO [Broker id=1] Leader gate_of_word-0 with topic id Some(xUE9Kc36Q_e3ecfOXf0gRg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                             | [2023-08-04 10:18:13,299] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic gate_of_word with new configuration : compression.type -> producer,leader.replication.throttled.replicas -> ,remote.storage.enable -> false,message.downconversion.enable -> true,min.insync.replicas -> 1,segment.jitter.ms -> 0,local.retention.ms -> -2,cleanup.policy -> delete,flush.ms -> 9223372036854775807,follower.replication.throttled.replicas -> ,segment.bytes -> 1073741824,retention.ms -> 604800000,flush.messages -> 9223372036854775807,message.format.version -> 3.0-IV1,max.compaction.lag.ms -> 9223372036854775807,file.delete.delay.ms -> 60000,max.message.bytes -> 1048588,min.compaction.lag.ms -> 0,message.timestamp.type -> CreateTime,local.retention.bytes -> -2,preallocate -> false,min.cleanable.dirty.ratio -> 0.5,index.interval.bytes -> 4096,unclean.leader.election.enable -> false,retention.bytes -> -1,delete.retention.ms -> 86400000,segment.ms -> 604800000,message.timestamp.difference.max.ms -> 9223372036854775807,segment.index.bytes -> 10485760 (kafka.server.metadata.DynamicConfigPublisher)
control-center                     | [2023-08-04 10:18:13,778] INFO 192.168.160.1 - - [04/Aug/2023:10:18:13 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/topics/gate_of_word/config HTTP/1.1" 200 798 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 77 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:14,014] INFO 192.168.160.1 - - [04/Aug/2023:10:18:13 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/topics HTTP/1.1" 200 1291 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 133 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:14,033] INFO 192.168.160.1 - - [04/Aug/2023:10:18:13 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/topics/gate_of_word/config HTTP/1.1" 200 798 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 113 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:15,004] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:18:20,092] INFO 192.168.160.1 - - [04/Aug/2023:10:18:20 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/settings/edit" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 40 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:20,193] INFO 192.168.160.1 - - [04/Aug/2023:10:18:20 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/settings/edit" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 22 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:20,490] INFO 192.168.160.1 - - [04/Aug/2023:10:18:20 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/maxtime HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/settings/edit" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 19 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:23,507] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Processed 16 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:18:23,577] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:18:23,591] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:18:23,607] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:18:23,643] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
schema-registry                    | [2023-08-04 10:18:23,660] INFO 192.168.160.13 - - [04/Aug/2023:10:18:23 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 26 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:23,673] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:18:23,692] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:18:23,718] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:18:23,731] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:18:23,744] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:18:23,773] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:18:23,845] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:18:26,932] INFO 192.168.160.1 - - [04/Aug/2023:10:18:26 +0000] "GET /2.0/clusters/kafka/display/stream-monitoring HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/monitoring/consumer/lag/consumerGroups" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:26,939] INFO 192.168.160.1 - - [04/Aug/2023:10:18:26 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/monitoring/consumer/lag/consumerGroups" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 20 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:26,939] INFO 192.168.160.1 - - [04/Aug/2023:10:18:26 +0000] "GET /2.0/clusters/kafka/display/cluster_management HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/monitoring/consumer/lag/consumerGroups" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 19 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:27,027] INFO 192.168.160.1 - - [04/Aug/2023:10:18:26 +0000] "GET /3.0/auth/principal HTTP/1.1" 200 30 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/monitoring/consumer/lag/consumerGroups" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 53 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:27,030] INFO 192.168.160.1 - - [04/Aug/2023:10:18:27 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/monitoring/consumer/lag/consumerGroups" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 13 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:27,036] INFO 192.168.160.1 - - [04/Aug/2023:10:18:27 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/monitoring/consumer/lag/consumerGroups" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 36 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:27,040] INFO 192.168.160.1 - - [04/Aug/2023:10:18:26 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/monitoring/consumer/lag/consumerGroups" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 61 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:27,050] INFO 192.168.160.1 - - [04/Aug/2023:10:18:26 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/maxtime HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/monitoring/consumer/lag/consumerGroups" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 71 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:27,114] INFO 192.168.160.1 - - [04/Aug/2023:10:18:27 +0000] "GET /2.0/consumer/offsets/MkU3OEVBNTcwNTJENDM2Qg HTTP/1.1" 200 159 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/monitoring/consumer/lag/consumerGroups" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 114 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:27,268] INFO 192.168.160.1 - - [04/Aug/2023:10:18:27 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/monitoring/consumer/lag/consumerGroups" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 25 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:27,277] INFO 192.168.160.1 - - [04/Aug/2023:10:18:27 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/monitoring/consumer/lag/consumerGroups" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 49 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:29,643] INFO 192.168.160.1 - - [04/Aug/2023:10:18:29 +0000] "GET /api/connect/connect-default/connectors?expand=status&expand=info HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/replicator" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 40 (io.confluent.rest-utils.requests)
connect                            | [2023-08-04 10:18:29,698] ERROR Uncaught exception in REST call to /ReplicatorMetrics (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper)
connect                            | javax.ws.rs.NotFoundException: HTTP 404 Not Found
connect                            | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:252)
connect                            | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
connect                            | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
connect                            | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
connect                            | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
connect                            | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
connect                            | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
connect                            | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
connect                            | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
connect                            | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
connect                            | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
connect                            | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
connect                            | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
connect                            | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
connect                            | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
connect                            | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
connect                            | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
connect                            | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
connect                            | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
connect                            | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
connect                            | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
connect                            | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
connect                            | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
connect                            | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
connect                            | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
connect                            | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
connect                            | 	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
connect                            | 	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
connect                            | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
connect                            | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
connect                            | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:400)
connect                            | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:645)
connect                            | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:392)
connect                            | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
connect                            | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
connect                            | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
connect                            | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
connect                            | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
connect                            | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
connect                            | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
connect                            | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
connect                            | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
connect                            | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
connect                            | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
connect                            | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                     | [2023-08-04 10:18:29,721] INFO 192.168.160.1 - - [04/Aug/2023:10:18:29 +0000] "GET /api/connect/connect-default/ReplicatorMetrics HTTP/1.1" 404 49 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/replicator" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 114 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:18:29,772] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:18:29 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:18:30,772] INFO 192.168.160.1 - - [04/Aug/2023:10:18:30 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/replicator" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 20 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:32,131] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:18:32,132] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:18:32,403] INFO 192.168.160.1 - - [04/Aug/2023:10:18:32 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/controller HTTP/1.1" 200 67 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 57 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:32,556] INFO 192.168.160.1 - - [04/Aug/2023:10:18:32 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/brokers/config HTTP/1.1" 200 4496 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 216 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:32,625] INFO 192.168.160.1 - - [04/Aug/2023:10:18:32 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/broker/requests?interval=60000&end=1691144280000&start=1691129880000 HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 190 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:32,652] INFO 192.168.160.1 - - [04/Aug/2023:10:18:32 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/clusterType HTTP/1.1" 200 54 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 185 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:32,656] INFO 192.168.160.1 - - [04/Aug/2023:10:18:32 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/broker/network/pool?interval=60000&end=1691144280000&start=1691129880000 HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 216 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:32,690] INFO 192.168.160.1 - - [04/Aug/2023:10:18:32 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/controller HTTP/1.1" 200 67 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 64 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:32,727] INFO 192.168.160.1 - - [04/Aug/2023:10:18:32 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/quorumInfo HTTP/1.1" 200 68 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 152 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:32,753] INFO 192.168.160.1 - - [04/Aug/2023:10:18:32 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/broker/request/pool?interval=60000&end=1691144280000&start=1691129880000 HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 279 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:34,097] INFO 192.168.160.1 - - [04/Aug/2023:10:18:32 +0000] "GET /api/kafka-rest/MkU3OEVBNTcwNTJENDM2Qg/kafka/v3/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers/1/configs HTTP/1.1" 200 7167 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 1753 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:34,195] INFO 192.168.160.1 - - [04/Aug/2023:10:18:32 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/brokers/config HTTP/1.1" 200 4496 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 1536 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:37,395] INFO 192.168.160.1 - - [04/Aug/2023:10:18:37 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/controller HTTP/1.1" 200 67 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 10 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:37,520] INFO 192.168.160.1 - - [04/Aug/2023:10:18:37 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/brokers/config HTTP/1.1" 200 4496 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 96 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:37,703] INFO 192.168.160.1 - - [04/Aug/2023:10:18:37 +0000] "GET /api/kafka-rest/MkU3OEVBNTcwNTJENDM2Qg/kafka/v3/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers/1/configs HTTP/1.1" 200 7167 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 213 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:37,816] INFO 192.168.160.1 - - [04/Aug/2023:10:18:37 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/broker/requests?interval=60000&end=1691144280000&start=1691129880000 HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 104 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:37,841] INFO 192.168.160.1 - - [04/Aug/2023:10:18:37 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/broker/detail HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 221 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:37,917] INFO 192.168.160.1 - - [04/Aug/2023:10:18:37 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/topic/status HTTP/1.1" 200 17 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 166 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:37,992] INFO 192.168.160.1 - - [04/Aug/2023:10:18:37 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/broker/status HTTP/1.1" 200 162 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 180 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:38,009] INFO 192.168.160.1 - - [04/Aug/2023:10:18:37 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/broker/request/pool?interval=60000&end=1691144280000&start=1691129880000 HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 155 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:38,017] INFO 192.168.160.1 - - [04/Aug/2023:10:18:37 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/broker/network/pool?interval=60000&end=1691144280000&start=1691129880000 HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 71 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:38,028] INFO 192.168.160.1 - - [04/Aug/2023:10:18:37 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/clusterType HTTP/1.1" 200 54 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 43 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:38,111] INFO 192.168.160.1 - - [04/Aug/2023:10:18:38 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/quorumInfo HTTP/1.1" 200 68 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/brokers" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 47 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:38,765] INFO 192.168.160.1 - - [04/Aug/2023:10:18:38 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/topics HTTP/1.1" 200 1291 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 49 (io.confluent.rest-utils.requests)
schema-registry                    | [2023-08-04 10:18:38,879] INFO 192.168.160.13 - - [04/Aug/2023:10:18:38 +0000] "GET /v1/metadata/version HTTP/1.1" 200 92 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 33 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:38,901] INFO 192.168.160.1 - - [04/Aug/2023:10:18:38 +0000] "GET /api/schema-registry/8c2f8da2941df04bbb66301a6a9243a4f4768312/v1/metadata/version HTTP/1.1" 200 92 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 83 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:38,911] ERROR Async response CompletionException with error response entity of type unknown: null (javax.ws.rs.container.AsyncResponse)
control-center                     | java.util.concurrent.CompletionException: org.apache.kafka.common.errors.UnsupportedVersionException: The broker does not support LIST_MIRRORS
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:367)
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:376)
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:1019)
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
control-center                     | 	at io.confluent.kafkarest.common.KafkaFutures.lambda$toCompletableFuture$0(KafkaFutures.java:48)
control-center                     | 	at org.apache.kafka.common.internals.KafkaFutureImpl.lambda$whenComplete$2(KafkaFutureImpl.java:107)
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                     | 	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
control-center                     | 	at org.apache.kafka.common.internals.KafkaCompletableFuture.kafkaCompleteExceptionally(KafkaCompletableFuture.java:49)
control-center                     | 	at org.apache.kafka.common.internals.KafkaFutureImpl.completeExceptionally(KafkaFutureImpl.java:130)
control-center                     | 	at org.apache.kafka.clients.admin.KafkaAdminClient$58.handleFailure(KafkaAdminClient.java:5753)
control-center                     | 	at org.apache.kafka.clients.admin.KafkaAdminClient$Call.fail(KafkaAdminClient.java:971)
control-center                     | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.handleResponses(KafkaAdminClient.java:1401)
control-center                     | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1566)
control-center                     | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1489)
control-center                     | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                     | 	at org.apache.kafka.common.utils.KafkaThread.run(KafkaThread.java:64)
control-center                     | Caused by: org.apache.kafka.common.errors.UnsupportedVersionException: The broker does not support LIST_MIRRORS
control-center                     | [2023-08-04 10:18:38,929] INFO 192.168.160.1 - - [04/Aug/2023:10:18:38 +0000] "GET /api/kafka-rest/MkU3OEVBNTcwNTJENDM2Qg/kafka/v3/clusters/MkU3OEVBNTcwNTJENDM2Qg/links/-/mirrors HTTP/1.1" 400 73 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 111 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:39,108] INFO 192.168.160.1 - - [04/Aug/2023:10:18:38 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/brokers/config HTTP/1.1" 200 4496 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 292 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:39,177] INFO 192.168.160.1 - - [04/Aug/2023:10:18:39 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/topics HTTP/1.1" 200 1291 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 95 (io.confluent.rest-utils.requests)
data-agrigator-crowler-producer-1  | Message ok
data-agrigator-crowler-producer-1 exited with code 0
schema-registry                    | [2023-08-04 10:18:41,587] INFO 192.168.160.13 - - [04/Aug/2023:10:18:41 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:41,968] INFO 192.168.160.1 - - [04/Aug/2023:10:18:41 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/maxtime HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 18 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:42,220] INFO 192.168.160.1 - - [04/Aug/2023:10:18:42 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/topics/gate_of_word/config HTTP/1.1" 200 798 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 39 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:42,260] INFO 192.168.160.1 - - [04/Aug/2023:10:18:42 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/topics/gate_of_word/config HTTP/1.1" 200 798 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 17 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:18:42,288] INFO 192.168.160.1 - - [04/Aug/2023:10:18:42 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/topics HTTP/1.1" 200 1291 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 115 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:18:53,179] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:18:53 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:18:57,005] INFO 192.168.160.1 - - [04/Aug/2023:10:18:56 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/maxtime HTTP/1.1" 200 2 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 36 (io.confluent.rest-utils.requests)
schema-registry                    | [2023-08-04 10:18:58,098] INFO 192.168.160.13 - - [04/Aug/2023:10:18:58 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 15 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:19:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
ksqldb-server                      | [2023-08-04 10:19:09,306] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:19:09 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:19:15,041] INFO 192.168.160.13 - - [04/Aug/2023:10:19:15 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 7 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:19:26,562] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:19:26 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:19:27,300] INFO 192.168.160.1 - - [04/Aug/2023:10:19:27 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:19:27,339] INFO 192.168.160.1 - - [04/Aug/2023:10:19:27 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 10 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:19:27,378] INFO 192.168.160.1 - - [04/Aug/2023:10:19:27 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 8 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:19:27,430] INFO 192.168.160.1 - - [04/Aug/2023:10:19:27 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 8 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:19:27,457] INFO 192.168.160.1 - - [04/Aug/2023:10:19:27 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 8 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:19:27,512] INFO 192.168.160.1 - - [04/Aug/2023:10:19:27 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 13 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:19:31,192] INFO 192.168.160.1 - - [04/Aug/2023:10:19:31 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:19:32,128] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:19:32,128] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
schema-registry                    | [2023-08-04 10:19:38,520] INFO 192.168.160.13 - - [04/Aug/2023:10:19:38 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 11 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:19:45,432] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:19:45 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
connect                            | [2023-08-04 10:19:51,226] INFO [AdminClient clientId=adminclient-8] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
connect                            | [2023-08-04 10:19:53,623] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition gate_of_word-0 to 0 since the associated topicId changed from null to xUE9Kc36Q_e3ecfOXf0gRg (org.apache.kafka.clients.Metadata)
schema-registry                    | [2023-08-04 10:19:57,020] INFO 192.168.160.13 - - [04/Aug/2023:10:19:57 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 8 (io.confluent.rest-utils.requests)
schema-registry                    | [2023-08-04 10:20:03,091] INFO [Producer clientId=producer-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:20:03,484] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:20:03 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:20:04,196] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:20:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
schema-registry                    | [2023-08-04 10:20:09,496] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:20:09,504] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition gate_of_word-0 to 0 since the associated topicId changed from null to xUE9Kc36Q_e3ecfOXf0gRg (org.apache.kafka.clients.Metadata)
control-center                     | [2023-08-04 10:20:15,029] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
schema-registry                    | [2023-08-04 10:20:15,806] INFO 192.168.160.13 - - [04/Aug/2023:10:20:15 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:20:18,391] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:20:22,059] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:20:22 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:20:23,526] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Processed 16 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:20:23,626] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:20:23,631] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:20:23,653] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:20:23,675] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:20:23,702] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:20:23,755] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:20:23,755] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:20:23,775] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:20:23,776] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:20:23,776] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:20:23,863] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:20:27,220] INFO 192.168.160.1 - - [04/Aug/2023:10:20:27 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 11 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:20:27,244] INFO 192.168.160.1 - - [04/Aug/2023:10:20:27 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:20:27,267] INFO 192.168.160.1 - - [04/Aug/2023:10:20:27 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:20:27,303] INFO 192.168.160.1 - - [04/Aug/2023:10:20:27 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 14 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:20:27,357] INFO 192.168.160.1 - - [04/Aug/2023:10:20:27 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 10 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:20:27,395] INFO 192.168.160.1 - - [04/Aug/2023:10:20:27 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:20:32,127] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:20:32,127] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:20:32,219] INFO 192.168.160.1 - - [04/Aug/2023:10:20:32 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 21 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:20:33,851] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:20:34,448] INFO [Producer clientId=c3-command] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:20:37,759] INFO [Producer clientId=_confluent-controlcenter-license-manager-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:20:37,988] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:20:38,215] INFO 192.168.160.13 - - [04/Aug/2023:10:20:38 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 9 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:20:40,009] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:20:40 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
ksqldb-server                      | [2023-08-04 10:20:56,534] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:20:56 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:21:00,595] INFO 192.168.160.13 - - [04/Aug/2023:10:21:00 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 13 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:21:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
ksqldb-server                      | [2023-08-04 10:21:09,245] INFO [Consumer clientId=consumer-null-1, groupId=null] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:14,593] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:21:17,620] INFO 192.168.160.13 - - [04/Aug/2023:10:21:17 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 8 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:21:19,361] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:21:19 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:21:20,620] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:20,915] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:20,967] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:21,079] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:21,527] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:21,998] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:22,371] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:23,015] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:23,199] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:23,243] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:23,375] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:23,459] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:23,534] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:23,546] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:23,569] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:23,621] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:23,627] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:23,632] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:23,670] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:23,722] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:23,722] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:23,723] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:27,246] INFO 192.168.160.1 - - [04/Aug/2023:10:21:27 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 22 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:21:27,288] INFO 192.168.160.1 - - [04/Aug/2023:10:21:27 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 30 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:21:27,322] INFO 192.168.160.1 - - [04/Aug/2023:10:21:27 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 12 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:21:27,358] INFO 192.168.160.1 - - [04/Aug/2023:10:21:27 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:21:27,389] INFO 192.168.160.1 - - [04/Aug/2023:10:21:27 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 11 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:21:27,417] INFO 192.168.160.1 - - [04/Aug/2023:10:21:27 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:21:27,688] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:27,891] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:21:32,128] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:21:32,129] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:21:33,191] INFO 192.168.160.1 - - [04/Aug/2023:10:21:33 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 6 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:21:33,211] INFO [AdminClient clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-admin] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:21:36,059] INFO 192.168.160.13 - - [04/Aug/2023:10:21:36 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 8 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:21:41,610] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:21:41 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:21:52,597] INFO 192.168.160.13 - - [04/Aug/2023:10:21:52 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 9 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:22:03,082] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:22:03 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
ksqldb-server                      | [2023-08-04 10:22:04,483] INFO [AdminClient clientId=adminclient-2] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:22:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
ksqldb-server                      | [2023-08-04 10:22:04,893] INFO Cleaning up 0 leaked topics: [] (io.confluent.ksql.engine.TransientQueryCleanupService)
ksqldb-server                      | [2023-08-04 10:22:04,895] INFO Reporting number of leaked topics: 0 (io.confluent.ksql.internal.LeakedResourcesMetrics)
ksqldb-server                      | [2023-08-04 10:22:04,896] INFO Reporting number of leaked state files: 0 (io.confluent.ksql.internal.LeakedResourcesMetrics)
ksqldb-server                      | [2023-08-04 10:22:04,896] INFO Reporting number of leaked topics after cleanup: 0 (io.confluent.ksql.internal.LeakedResourcesMetrics)
ksqldb-server                      | [2023-08-04 10:22:04,897] INFO Reporting number of leaked state directories after cleanup: 0 (io.confluent.ksql.internal.LeakedResourcesMetrics)
ksqldb-server                      | [2023-08-04 10:22:04,906] INFO Cleaning up 0 leaked state directories: [] (io.confluent.ksql.engine.TransientQueryCleanupService)
schema-registry                    | [2023-08-04 10:22:13,945] INFO 192.168.160.13 - - [04/Aug/2023:10:22:13 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 10 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:22:15,124] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
broker                             | [2023-08-04 10:22:20,014] INFO [TransactionCoordinator id=1] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:22:20,095] INFO [AdminClient clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-admin] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:22:23,589] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Processed 16 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:22:23,687] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:22:23,690] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:22:23,691] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:22:23,720] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:22:23,752] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:22:23,776] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:22:23,777] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:22:23,779] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:22:23,790] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:22:23,792] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
ksqldb-server                      | [2023-08-04 10:22:23,846] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:22:23 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:22:23,958] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:22:27,223] INFO 192.168.160.1 - - [04/Aug/2023:10:22:27 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:22:27,243] INFO 192.168.160.1 - - [04/Aug/2023:10:22:27 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 8 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:22:27,264] INFO 192.168.160.1 - - [04/Aug/2023:10:22:27 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:22:27,289] INFO 192.168.160.1 - - [04/Aug/2023:10:22:27 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:22:27,324] INFO 192.168.160.1 - - [04/Aug/2023:10:22:27 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 12 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:22:27,350] INFO 192.168.160.1 - - [04/Aug/2023:10:22:27 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:22:32,127] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:22:32,127] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
schema-registry                    | [2023-08-04 10:22:35,502] INFO 192.168.160.13 - - [04/Aug/2023:10:22:35 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 9 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:22:46,881] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:22:46 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
postgres_db                        | 2023-08-04 10:22:47.584 UTC [78] FATAL:  role "root" does not exist
schema-registry                    | [2023-08-04 10:22:51,623] INFO 192.168.160.13 - - [04/Aug/2023:10:22:51 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 13 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:23:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
ksqldb-server                      | [2023-08-04 10:23:08,477] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:23:08 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:23:14,337] INFO 192.168.160.13 - - [04/Aug/2023:10:23:14 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:23:17,202] INFO 192.168.160.1 - - [04/Aug/2023:10:23:17 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 8 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:23:30,190] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:23:30 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:23:32,128] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:23:32,129] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
schema-registry                    | [2023-08-04 10:23:32,353] INFO 192.168.160.13 - - [04/Aug/2023:10:23:32 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 14 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:23:46,616] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:23:46 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:23:48,936] INFO 192.168.160.13 - - [04/Aug/2023:10:23:48 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 9 (io.confluent.rest-utils.requests)
connect                            | [2023-08-04 10:23:51,984] INFO [Producer clientId=producer-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
connect                            | [2023-08-04 10:23:52,428] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
connect                            | [2023-08-04 10:23:53,180] INFO [Producer clientId=producer-2] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
connect                            | [2023-08-04 10:23:53,431] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
connect                            | [2023-08-04 10:23:53,719] INFO [Producer clientId=producer-3] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
connect                            | [2023-08-04 10:23:53,727] INFO [Consumer clientId=consumer-compose-connect-group-3, groupId=compose-connect-group] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
connect                            | [2023-08-04 10:23:53,761] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:24:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
ksqldb-server                      | [2023-08-04 10:24:05,997] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:24:05 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:24:07,361] INFO 192.168.160.13 - - [04/Aug/2023:10:24:07 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 15 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:24:15,169] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:24:17,262] INFO 192.168.160.1 - - [04/Aug/2023:10:24:17 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 22 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:24:17,294] INFO 192.168.160.1 - - [04/Aug/2023:10:24:17 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 10 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:24:17,328] INFO 192.168.160.1 - - [04/Aug/2023:10:24:17 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 12 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:24:17,363] INFO 192.168.160.1 - - [04/Aug/2023:10:24:17 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:24:17,409] INFO 192.168.160.1 - - [04/Aug/2023:10:24:17 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:24:17,427] INFO 192.168.160.1 - - [04/Aug/2023:10:24:17 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:24:17,472] INFO 192.168.160.1 - - [04/Aug/2023:10:24:17 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 8 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:24:23,612] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Processed 14 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:24:23,697] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Processed 14 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:24:23,702] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Processed 14 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:24:23,789] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:24:23,796] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:24:23,805] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:24:23,852] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:24:23,856] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:24:23,859] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:24:23,870] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:24:23,873] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:24:24,014] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
ksqldb-server                      | [2023-08-04 10:24:25,021] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:24:25 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:24:25,080] INFO 192.168.160.13 - - [04/Aug/2023:10:24:25 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 11 (io.confluent.rest-utils.requests)
postgres_db                        | 2023-08-04 10:24:31.674 UTC [93] FATAL:  role "myuser" does not exist
control-center                     | [2023-08-04 10:24:32,127] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:24:32,127] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
schema-registry                    | [2023-08-04 10:24:45,127] INFO 192.168.160.13 - - [04/Aug/2023:10:24:45 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 10 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:24:47,647] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:24:47 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
connect                            | [2023-08-04 10:24:51,336] INFO [AdminClient clientId=adminclient-8] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:25:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
ksqldb-server                      | [2023-08-04 10:25:05,494] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:25:05 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:25:06,193] INFO 192.168.160.13 - - [04/Aug/2023:10:25:06 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:25:17,226] INFO 192.168.160.1 - - [04/Aug/2023:10:25:17 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:25:17,252] INFO 192.168.160.1 - - [04/Aug/2023:10:25:17 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:25:17,275] INFO 192.168.160.1 - - [04/Aug/2023:10:25:17 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 6 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:25:17,302] INFO 192.168.160.1 - - [04/Aug/2023:10:25:17 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 8 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:25:17,327] INFO 192.168.160.1 - - [04/Aug/2023:10:25:17 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 6 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:25:17,369] INFO 192.168.160.1 - - [04/Aug/2023:10:25:17 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
schema-registry                    | [2023-08-04 10:25:24,968] INFO 192.168.160.13 - - [04/Aug/2023:10:25:24 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 16 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:25:27,990] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:25:27 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:25:32,125] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:25:32,125] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
schema-registry                    | [2023-08-04 10:25:44,495] INFO 192.168.160.13 - - [04/Aug/2023:10:25:44 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 9 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:25:46,409] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:25:46 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:26:03,641] INFO 192.168.160.13 - - [04/Aug/2023:10:26:03 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 8 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:26:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
ksqldb-server                      | [2023-08-04 10:26:06,530] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:26:06 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:26:15,172] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:26:17,221] INFO 192.168.160.1 - - [04/Aug/2023:10:26:17 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 10 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:26:17,286] INFO 192.168.160.1 - - [04/Aug/2023:10:26:17 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 24 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:26:17,318] INFO 192.168.160.1 - - [04/Aug/2023:10:26:17 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 11 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:26:17,344] INFO 192.168.160.1 - - [04/Aug/2023:10:26:17 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:26:17,382] INFO 192.168.160.1 - - [04/Aug/2023:10:26:17 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 11 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:26:17,404] INFO 192.168.160.1 - - [04/Aug/2023:10:26:17 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 6 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:26:17,465] INFO 192.168.160.1 - - [04/Aug/2023:10:26:17 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 14 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:26:23,630] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Processed 16 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:26:23,730] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:26:23,734] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:26:23,859] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:26:23,878] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:26:23,880] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:26:23,921] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:26:23,922] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:26:23,931] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:26:23,960] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:26:23,964] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
schema-registry                    | [2023-08-04 10:26:24,075] INFO 192.168.160.13 - - [04/Aug/2023:10:26:24 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 19 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:26:24,096] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
ksqldb-server                      | [2023-08-04 10:26:29,385] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:26:29 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:26:32,130] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:26:32,130] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:26:33,416] INFO [AdminClient clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-admin] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:26:46,585] INFO 192.168.160.13 - - [04/Aug/2023:10:26:46 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 15 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:26:48,438] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:26:48 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:27:02,904] INFO 192.168.160.13 - - [04/Aug/2023:10:27:02 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 7 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:27:04,691] INFO [AdminClient clientId=adminclient-2] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:27:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
ksqldb-server                      | [2023-08-04 10:27:05,886] INFO [AdminClient clientId=adminclient-4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:27:10,029] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:27:10 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:27:17,241] INFO 192.168.160.1 - - [04/Aug/2023:10:27:17 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 11 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:27:17,270] INFO 192.168.160.1 - - [04/Aug/2023:10:27:17 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 10 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:27:17,302] INFO 192.168.160.1 - - [04/Aug/2023:10:27:17 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:27:17,334] INFO 192.168.160.1 - - [04/Aug/2023:10:27:17 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:27:17,358] INFO 192.168.160.1 - - [04/Aug/2023:10:27:17 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:27:17,399] INFO 192.168.160.1 - - [04/Aug/2023:10:27:17 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:27:17,431] INFO 192.168.160.1 - - [04/Aug/2023:10:27:17 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 15 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:27:20,300] INFO [AdminClient clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-admin] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:27:23,889] INFO 192.168.160.13 - - [04/Aug/2023:10:27:23 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 23 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:27:31,139] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:27:31 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:27:32,125] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:27:32,125] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
schema-registry                    | [2023-08-04 10:27:47,124] INFO 192.168.160.13 - - [04/Aug/2023:10:27:47 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 14 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:27:48,304] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:27:48 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:28:03,917] INFO 192.168.160.13 - - [04/Aug/2023:10:28:03 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 15 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:28:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
ksqldb-server                      | [2023-08-04 10:28:07,732] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:28:07 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:28:15,262] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:28:17,221] INFO 192.168.160.1 - - [04/Aug/2023:10:28:17 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 6 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:28:17,242] INFO 192.168.160.1 - - [04/Aug/2023:10:28:17 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:28:17,264] INFO 192.168.160.1 - - [04/Aug/2023:10:28:17 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 5 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:28:17,288] INFO 192.168.160.1 - - [04/Aug/2023:10:28:17 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:28:17,312] INFO 192.168.160.1 - - [04/Aug/2023:10:28:17 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:28:17,342] INFO 192.168.160.1 - - [04/Aug/2023:10:28:17 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
schema-registry                    | [2023-08-04 10:28:20,675] INFO 192.168.160.13 - - [04/Aug/2023:10:28:20 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 8 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:28:23,683] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Processed 16 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:28:23,782] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:28:23,793] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:28:23,867] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:28:23,896] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:28:23,899] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:28:23,925] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:28:23,926] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:28:23,966] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:28:23,972] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:28:23,997] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:28:24,195] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
ksqldb-server                      | [2023-08-04 10:28:30,933] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:28:30 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:28:32,125] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:28:32,126] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
broker                             | [2023-08-04 10:28:38,181] INFO [BrokerToControllerChannelManager broker=1 name=forwarding] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:28:42,012] INFO 192.168.160.13 - - [04/Aug/2023:10:28:41 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 17 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:28:49,522] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:28:49 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
ksqldb-server                      | [2023-08-04 10:29:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
schema-registry                    | [2023-08-04 10:29:05,691] INFO 192.168.160.13 - - [04/Aug/2023:10:29:05 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 37 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:29:08,334] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:29:08 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:29:17,226] INFO 192.168.160.1 - - [04/Aug/2023:10:29:17 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 10 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:29:17,293] INFO 192.168.160.1 - - [04/Aug/2023:10:29:17 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:29:17,324] INFO 192.168.160.1 - - [04/Aug/2023:10:29:17 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:29:17,356] INFO 192.168.160.1 - - [04/Aug/2023:10:29:17 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 6 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:29:17,448] INFO 192.168.160.1 - - [04/Aug/2023:10:29:17 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 13 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:29:17,572] INFO 192.168.160.1 - - [04/Aug/2023:10:29:17 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 13 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:29:17,675] INFO 192.168.160.1 - - [04/Aug/2023:10:29:17 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 13 (io.confluent.rest-utils.requests)
schema-registry                    | [2023-08-04 10:29:22,997] INFO 192.168.160.13 - - [04/Aug/2023:10:29:22 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 16 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:29:31,653] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:29:31 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:29:32,124] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:29:32,125] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
schema-registry                    | [2023-08-04 10:29:44,847] INFO 192.168.160.13 - - [04/Aug/2023:10:29:44 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 13 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:29:49,857] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:29:49 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
connect                            | [2023-08-04 10:29:51,543] INFO [AdminClient clientId=adminclient-8] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:30:04,353] INFO 192.168.160.13 - - [04/Aug/2023:10:30:04 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 15 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:30:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
ksqldb-server                      | [2023-08-04 10:30:12,159] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:30:12 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:30:15,285] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:30:17,248] INFO 192.168.160.1 - - [04/Aug/2023:10:30:17 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:30:17,297] INFO 192.168.160.1 - - [04/Aug/2023:10:30:17 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:30:17,331] INFO 192.168.160.1 - - [04/Aug/2023:10:30:17 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 6 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:30:17,358] INFO 192.168.160.1 - - [04/Aug/2023:10:30:17 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 6 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:30:17,385] INFO 192.168.160.1 - - [04/Aug/2023:10:30:17 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 6 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:30:17,415] INFO 192.168.160.1 - - [04/Aug/2023:10:30:17 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:30:23,697] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Processed 14 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:30:23,796] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Processed 14 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:30:23,834] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:30:23,904] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:30:23,924] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:30:23,945] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:30:23,968] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:30:23,969] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:30:24,004] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:30:24,004] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:30:24,031] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:30:24,246] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
schema-registry                    | [2023-08-04 10:30:27,524] INFO 192.168.160.13 - - [04/Aug/2023:10:30:27 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 22 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:30:30,898] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:30:30 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:30:32,128] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:30:32,129] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
schema-registry                    | [2023-08-04 10:30:51,296] INFO 192.168.160.13 - - [04/Aug/2023:10:30:51 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 13 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:30:52,391] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:30:52 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
ksqldb-server                      | [2023-08-04 10:31:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
ksqldb-server                      | [2023-08-04 10:31:08,941] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:31:08 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:31:09,656] INFO 192.168.160.13 - - [04/Aug/2023:10:31:09 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 13 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:31:17,204] INFO 192.168.160.1 - - [04/Aug/2023:10:31:17 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 8 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:31:17,245] INFO 192.168.160.1 - - [04/Aug/2023:10:31:17 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 16 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:31:17,273] INFO 192.168.160.1 - - [04/Aug/2023:10:31:17 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:31:17,345] INFO 192.168.160.1 - - [04/Aug/2023:10:31:17 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 12 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:31:17,401] INFO 192.168.160.1 - - [04/Aug/2023:10:31:17 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 11 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:31:17,431] INFO 192.168.160.1 - - [04/Aug/2023:10:31:17 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 6 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:31:17,459] INFO 192.168.160.1 - - [04/Aug/2023:10:31:17 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 6 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:31:30,261] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:31:30 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:31:30,908] INFO 192.168.160.13 - - [04/Aug/2023:10:31:30 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:31:32,129] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:31:32,129] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:31:33,623] INFO [AdminClient clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-admin] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:31:47,478] INFO 192.168.160.13 - - [04/Aug/2023:10:31:47 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 20 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:31:49,377] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:31:49 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
ksqldb-server                      | [2023-08-04 10:32:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
ksqldb-server                      | [2023-08-04 10:32:04,889] INFO Cleaning up 0 leaked topics: [] (io.confluent.ksql.engine.TransientQueryCleanupService)
ksqldb-server                      | [2023-08-04 10:32:04,891] INFO Reporting number of leaked topics: 0 (io.confluent.ksql.internal.LeakedResourcesMetrics)
ksqldb-server                      | [2023-08-04 10:32:04,893] INFO Reporting number of leaked state files: 0 (io.confluent.ksql.internal.LeakedResourcesMetrics)
ksqldb-server                      | [2023-08-04 10:32:04,895] INFO Reporting number of leaked topics after cleanup: 0 (io.confluent.ksql.internal.LeakedResourcesMetrics)
ksqldb-server                      | [2023-08-04 10:32:04,895] INFO Reporting number of leaked state directories after cleanup: 0 (io.confluent.ksql.internal.LeakedResourcesMetrics)
ksqldb-server                      | [2023-08-04 10:32:04,898] INFO Cleaning up 0 leaked state directories: [] (io.confluent.ksql.engine.TransientQueryCleanupService)
ksqldb-server                      | [2023-08-04 10:32:04,900] INFO [AdminClient clientId=adminclient-2] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:32:05,530] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:32:05 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:32:10,968] INFO 192.168.160.13 - - [04/Aug/2023:10:32:10 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 8 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:32:15,370] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:32:17,257] INFO 192.168.160.1 - - [04/Aug/2023:10:32:17 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 30 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:32:17,284] INFO 192.168.160.1 - - [04/Aug/2023:10:32:17 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 10 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:32:17,313] INFO 192.168.160.1 - - [04/Aug/2023:10:32:17 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 13 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:32:17,338] INFO 192.168.160.1 - - [04/Aug/2023:10:32:17 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:32:17,370] INFO 192.168.160.1 - - [04/Aug/2023:10:32:17 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 12 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:32:17,409] INFO 192.168.160.1 - - [04/Aug/2023:10:32:17 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 20 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:32:17,419] INFO 192.168.160.1 - - [04/Aug/2023:10:32:17 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 13 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:32:20,506] INFO [AdminClient clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-admin] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                     | [2023-08-04 10:32:23,053] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-CleanupThread] Deleting obsolete state directory 10_3 for task 10_3 as 1198260ms has elapsed (cleanup delay is 600000ms). (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                     | [2023-08-04 10:32:23,077] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-CleanupThread] Deleting obsolete state directory 10_6 for task 10_6 as 1198516ms has elapsed (cleanup delay is 600000ms). (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                     | [2023-08-04 10:32:23,079] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-CleanupThread] Deleting obsolete state directory 10_9 for task 10_9 as 1198442ms has elapsed (cleanup delay is 600000ms). (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                     | [2023-08-04 10:32:23,080] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-CleanupThread] Deleting obsolete state directory 0_0 for task 0_0 as 1198239ms has elapsed (cleanup delay is 600000ms). (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                     | [2023-08-04 10:32:23,082] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-CleanupThread] Deleting obsolete state directory 10_5 for task 10_5 as 1198489ms has elapsed (cleanup delay is 600000ms). (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                     | [2023-08-04 10:32:23,083] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-CleanupThread] Deleting obsolete state directory 10_8 for task 10_8 as 1198434ms has elapsed (cleanup delay is 600000ms). (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                     | [2023-08-04 10:32:23,085] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-CleanupThread] Deleting obsolete state directory 10_4 for task 10_4 as 1198448ms has elapsed (cleanup delay is 600000ms). (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                     | [2023-08-04 10:32:23,086] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-CleanupThread] Deleting obsolete state directory 10_11 for task 10_11 as 1198445ms has elapsed (cleanup delay is 600000ms). (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                     | [2023-08-04 10:32:23,087] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-CleanupThread] Deleting obsolete state directory 10_1 for task 10_1 as 1198474ms has elapsed (cleanup delay is 600000ms). (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                     | [2023-08-04 10:32:23,088] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-CleanupThread] Deleting obsolete state directory 10_7 for task 10_7 as 1198199ms has elapsed (cleanup delay is 600000ms). (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                     | [2023-08-04 10:32:23,090] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-CleanupThread] Deleting obsolete state directory 10_0 for task 10_0 as 1198281ms has elapsed (cleanup delay is 600000ms). (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                     | [2023-08-04 10:32:23,091] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-CleanupThread] Deleting obsolete state directory 10_10 for task 10_10 as 1198350ms has elapsed (cleanup delay is 600000ms). (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                     | [2023-08-04 10:32:23,092] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-CleanupThread] Deleting obsolete state directory 10_2 for task 10_2 as 1198195ms has elapsed (cleanup delay is 600000ms). (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                     | [2023-08-04 10:32:23,726] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Processed 16 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:32:23,831] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:32:23,839] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Processed 14 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:32:23,956] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:32:23,988] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:32:24,002] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:32:24,013] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:32:24,013] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:32:24,102] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:32:24,102] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:32:24,126] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:32:24,257] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
ksqldb-server                      | [2023-08-04 10:32:24,922] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:32:24 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:32:29,095] INFO 192.168.160.13 - - [04/Aug/2023:10:32:29 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:32:32,128] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:32:32,129] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
ksqldb-server                      | [2023-08-04 10:32:42,239] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:32:42 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:32:47,985] INFO 192.168.160.13 - - [04/Aug/2023:10:32:47 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 21 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:32:59,378] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:32:59 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:33:04,753] INFO 192.168.160.13 - - [04/Aug/2023:10:33:04 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 8 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:33:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
control-center                     | [2023-08-04 10:33:17,303] INFO 192.168.160.1 - - [04/Aug/2023:10:33:17 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 22 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:33:17,350] INFO 192.168.160.1 - - [04/Aug/2023:10:33:17 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 8 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:33:17,403] INFO 192.168.160.1 - - [04/Aug/2023:10:33:17 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 6 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:33:17,442] INFO 192.168.160.1 - - [04/Aug/2023:10:33:17 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:33:17,505] INFO 192.168.160.1 - - [04/Aug/2023:10:33:17 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 27 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:33:17,626] INFO 192.168.160.1 - - [04/Aug/2023:10:33:17 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 14 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:33:18,197] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:33:18 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:33:24,433] INFO 192.168.160.13 - - [04/Aug/2023:10:33:24 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:33:32,127] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:33:32,127] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
ksqldb-server                      | [2023-08-04 10:33:39,691] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:33:39 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:33:43,388] INFO 192.168.160.13 - - [04/Aug/2023:10:33:43 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 7 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:34:00,476] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:34:00 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:34:03,442] INFO 192.168.160.13 - - [04/Aug/2023:10:34:03 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 12 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:34:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
control-center                     | [2023-08-04 10:34:15,464] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:34:17,202] INFO 192.168.160.1 - - [04/Aug/2023:10:34:17 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:34:17,229] INFO 192.168.160.1 - - [04/Aug/2023:10:34:17 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:34:17,258] INFO 192.168.160.1 - - [04/Aug/2023:10:34:17 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:34:17,279] INFO 192.168.160.1 - - [04/Aug/2023:10:34:17 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 8 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:34:17,305] INFO 192.168.160.1 - - [04/Aug/2023:10:34:17 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 8 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:34:17,343] INFO 192.168.160.1 - - [04/Aug/2023:10:34:17 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 13 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:34:17,383] INFO 192.168.160.1 - - [04/Aug/2023:10:34:17 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 16 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:34:17,905] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:34:17 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:34:21,341] INFO 192.168.160.13 - - [04/Aug/2023:10:34:21 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:34:23,754] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Processed 16 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:34:23,861] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:34:23,869] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:34:24,002] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:34:24,005] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:34:24,043] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:34:24,063] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:34:24,087] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:34:24,103] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:34:24,116] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:34:24,203] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:34:24,263] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:34:32,168] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:34:32,169] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
ksqldb-server                      | [2023-08-04 10:34:36,240] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:34:36 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:34:40,078] INFO 192.168.160.13 - - [04/Aug/2023:10:34:40 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 6 (io.confluent.rest-utils.requests)
connect                            | [2023-08-04 10:34:51,758] INFO [AdminClient clientId=adminclient-8] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:34:53,412] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:34:53 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:34:57,599] INFO 192.168.160.13 - - [04/Aug/2023:10:34:57 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 8 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:35:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
data-agrigator-jobmanager-1        | Starting Job Manager
data-agrigator-jobmanager-1        | Starting standalonejob as a console application on host 45393183a926.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,913 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,922 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Preconfiguration: 
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,923 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - 
data-agrigator-jobmanager-1        | 
data-agrigator-jobmanager-1        | 
data-agrigator-jobmanager-1        | RESOURCE_PARAMS extraction logs:
data-agrigator-jobmanager-1        | jvm_params: -Xmx1073741824 -Xms1073741824 -XX:MaxMetaspaceSize=268435456
data-agrigator-jobmanager-1        | dynamic_configs: -D jobmanager.memory.off-heap.size=134217728b -D jobmanager.memory.jvm-overhead.min=201326592b -D jobmanager.memory.jvm-metaspace.size=268435456b -D jobmanager.memory.heap.size=1073741824b -D jobmanager.memory.jvm-overhead.max=201326592b
data-agrigator-jobmanager-1        | logs: WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: blob.server.port, 6124
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: taskmanager.memory.process.size, 1728m
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: taskmanager.bind-host, 0.0.0.0
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: jobmanager.execution.failover-strategy, region
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: jobmanager.rpc.address, jobmanager
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: jobmanager.memory.process.size, 1600m
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: jobmanager.rpc.port, 6123
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: query.server.port, 6125
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: rest.bind-address, 0.0.0.0
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: jobmanager.bind-host, 0.0.0.0
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: parallelism.default, 2
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
data-agrigator-jobmanager-1        | INFO  [] - Loading configuration property: rest.address, 0.0.0.0
data-agrigator-jobmanager-1        | INFO  [] - The derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
data-agrigator-jobmanager-1        | INFO  [] - Final Master Memory configuration:
data-agrigator-jobmanager-1        | INFO  [] -   Total Process Memory: 1.563gb (1677721600 bytes)
data-agrigator-jobmanager-1        | INFO  [] -     Total Flink Memory: 1.125gb (1207959552 bytes)
data-agrigator-jobmanager-1        | INFO  [] -       JVM Heap:         1024.000mb (1073741824 bytes)
data-agrigator-jobmanager-1        | INFO  [] -       Off-heap:         128.000mb (134217728 bytes)
data-agrigator-jobmanager-1        | INFO  [] -     JVM Metaspace:      256.000mb (268435456 bytes)
data-agrigator-jobmanager-1        | INFO  [] -     JVM Overhead:       192.000mb (201326592 bytes)
data-agrigator-jobmanager-1        | 
data-agrigator-jobmanager-1        | 
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,924 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,925 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Starting StandaloneApplicationClusterEntryPoint (Version: 1.17.1, Scala: 2.12, Rev:2750d5c, Date:2023-05-19T10:45:46+02:00)
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,926 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  OS current user: flink
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,927 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Current Hadoop/Kerberos user: <no hadoop dependency found>
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,928 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM: OpenJDK 64-Bit Server VM - Eclipse Adoptium - 11/11.0.20+8
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,928 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Arch: amd64
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,929 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Maximum heap size: 1024 MiBytes
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,929 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JAVA_HOME: /opt/java/openjdk
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,930 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  No Hadoop Dependency available
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,931 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM Options:
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,932 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xmx1073741824
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,932 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xms1073741824
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,933 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:MaxMetaspaceSize=268435456
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,934 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog.file=/opt/flink/log/flink--standalonejob-1-45393183a926.log
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,934 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configuration=file:/opt/flink/conf/log4j-console.properties
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,935 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configurationFile=file:/opt/flink/conf/log4j-console.properties
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,936 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlogback.configurationFile=file:/opt/flink/conf/logback-console.xml
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,936 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Program Arguments:
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,940 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     --configDir
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,941 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     /opt/flink/conf
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,942 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,942 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.off-heap.size=134217728b
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,943 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,943 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.min=201326592b
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,944 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,944 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-metaspace.size=268435456b
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,945 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,946 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.heap.size=1073741824b
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,946 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,947 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.max=201326592b
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,947 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     --job-classname
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,948 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     com.example.sharov.anatoliy.DataStreamJob
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,949 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Classpath: /opt/flink/lib/flink-cep-1.17.1.jar:/opt/flink/lib/flink-connector-files-1.17.1.jar:/opt/flink/lib/flink-csv-1.17.1.jar:/opt/flink/lib/flink-json-1.17.1.jar:/opt/flink/lib/flink-scala_2.12-1.17.1.jar:/opt/flink/lib/flink-table-api-java-uber-1.17.1.jar:/opt/flink/lib/flink-table-planner-loader-1.17.1.jar:/opt/flink/lib/flink-table-runtime-1.17.1.jar:/opt/flink/lib/log4j-1.2-api-2.17.1.jar:/opt/flink/lib/log4j-api-2.17.1.jar:/opt/flink/lib/log4j-core-2.17.1.jar:/opt/flink/lib/log4j-slf4j-impl-2.17.1.jar:/opt/flink/lib/flink-dist-1.17.1.jar::::
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,949 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
data-agrigator-jobmanager-1        | 2023-08-04 10:35:14,957 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Registered UNIX signal handlers for [TERM, HUP, INT]
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,067 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: blob.server.port, 6124
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,068 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,069 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.bind-host, 0.0.0.0
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,070 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,071 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, jobmanager
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,072 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.process.size, 1600m
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,073 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,074 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: query.server.port, 6125
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,075 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.bind-address, 0.0.0.0
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,076 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.bind-host, 0.0.0.0
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,077 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 2
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,077 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,078 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.address, 0.0.0.0
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,079 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: jobmanager.memory.off-heap.size, 134217728b
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,080 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: jobmanager.memory.jvm-overhead.min, 201326592b
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,081 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: jobmanager.memory.jvm-metaspace.size, 268435456b
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,082 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: jobmanager.memory.heap.size, 1073741824b
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,083 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: jobmanager.memory.jvm-overhead.max, 201326592b
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,326 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Starting StandaloneApplicationClusterEntryPoint.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,385 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install default filesystem.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,396 INFO  org.apache.flink.core.fs.FileSystem                          [] - Hadoop is not in the classpath/dependencies. The extended set of supported File Systems via Hadoop is not available.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,458 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-statsd
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,477 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-datadog
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,478 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-influx
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,480 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-prometheus
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,481 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-jmx
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,483 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-graphite
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,485 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: external-resource-gpu
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,486 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-slf4j
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,585 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install security context.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,638 INFO  org.apache.flink.runtime.security.modules.HadoopModuleFactory [] - Cannot create Hadoop Security Module because Hadoop cannot be found in the Classpath.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,661 INFO  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file will be created as /tmp/jaas-16310783846088842112.conf.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,689 INFO  org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory [] - Cannot install HadoopSecurityContext because Hadoop cannot be found in the Classpath.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,697 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Initializing cluster services.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:15,716 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Using working directory: WorkingDirectory(/tmp/jm_888666ff37704dd2af33a54946ea63c6).
ksqldb-server                      | [2023-08-04 10:35:16,225] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:35:16 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:35:17,232] INFO 192.168.160.1 - - [04/Aug/2023:10:35:17 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 13 (io.confluent.rest-utils.requests)
data-agrigator-jobmanager-1        | 2023-08-04 10:35:17,252 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address jobmanager:6123, bind address 0.0.0.0:6123.
control-center                     | [2023-08-04 10:35:17,273] INFO 192.168.160.1 - - [04/Aug/2023:10:35:17 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 19 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:35:17,305] INFO 192.168.160.1 - - [04/Aug/2023:10:35:17 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 18 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:35:17,370] INFO 192.168.160.1 - - [04/Aug/2023:10:35:17 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 30 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:35:17,403] INFO 192.168.160.1 - - [04/Aug/2023:10:35:17 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 12 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:35:17,473] INFO 192.168.160.1 - - [04/Aug/2023:10:35:17 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 23 (io.confluent.rest-utils.requests)
schema-registry                    | [2023-08-04 10:35:18,492] INFO 192.168.160.13 - - [04/Aug/2023:10:35:18 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 7 (io.confluent.rest-utils.requests)
data-agrigator-taskmanager-1       | Starting Task Manager
data-agrigator-jobmanager-1        | 2023-08-04 10:35:21,904 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
data-agrigator-jobmanager-1        | 2023-08-04 10:35:22,051 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:22,053 INFO  akka.remote.Remoting                                         [] - Starting remoting
data-agrigator-jobmanager-1        | 2023-08-04 10:35:22,898 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink@jobmanager:6123]
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,708 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink@jobmanager:6123
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,804 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Loading delegation token providers
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,848 INFO  org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider [] - Hadoop FS is not available (not packaged with this application): NoClassDefFoundError : "org/apache/hadoop/conf/Configuration".
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,849 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Delegation token provider hadoopfs loaded and initialized
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,856 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Delegation token provider hbase loaded and initialized
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,857 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-statsd
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,858 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-datadog
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,858 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-influx
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,859 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-prometheus
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,860 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-jmx
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,860 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-graphite
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,861 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: external-resource-gpu
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,862 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-slf4j
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,867 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Delegation token providers loaded successfully
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,872 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Loading delegation token receivers
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,882 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receiver hadoopfs loaded and initialized
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,885 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receiver hbase loaded and initialized
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,885 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-statsd
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,886 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-datadog
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,887 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-influx
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,891 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-prometheus
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,892 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-jmx
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,895 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-graphite
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,896 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: external-resource-gpu
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,897 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-slf4j
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,900 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receivers loaded successfully
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,900 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Checking provider and receiver instances consistency
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,906 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Provider and receiver instances are consistent
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,907 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Obtaining delegation tokens
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,915 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Delegation tokens obtained successfully
data-agrigator-jobmanager-1        | 2023-08-04 10:35:23,919 WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - No tokens obtained so skipping notifications
data-agrigator-jobmanager-1        | 2023-08-04 10:35:24,008 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Created BLOB server storage directory /tmp/jm_888666ff37704dd2af33a54946ea63c6/blobStorage
data-agrigator-jobmanager-1        | 2023-08-04 10:35:24,021 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Started BLOB server at 0.0.0.0:6124 - max concurrent requests: 50 - max backlog: 1000
data-agrigator-jobmanager-1        | 2023-08-04 10:35:24,083 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:24,118 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address jobmanager:0, bind address 0.0.0.0:0.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:24,275 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
data-agrigator-jobmanager-1        | 2023-08-04 10:35:24,321 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:24,323 INFO  akka.remote.Remoting                                         [] - Starting remoting
data-agrigator-jobmanager-1        | 2023-08-04 10:35:24,469 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink-metrics@jobmanager:42393]
data-agrigator-jobmanager-1        | 2023-08-04 10:35:24,543 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink-metrics@jobmanager:42393
data-agrigator-jobmanager-1        | 2023-08-04 10:35:24,630 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
data-agrigator-jobmanager-1        | 2023-08-04 10:35:25,333 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Upload directory /tmp/flink-web-f459c06f-102b-40ff-a5b8-bac9c663d7a3/flink-web-upload does not exist. 
data-agrigator-jobmanager-1        | 2023-08-04 10:35:25,337 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Created directory /tmp/flink-web-f459c06f-102b-40ff-a5b8-bac9c663d7a3/flink-web-upload for file uploads.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:25,348 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Starting rest endpoint.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:27,077 INFO  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Determined location of main cluster component log file: /opt/flink/log/flink--standalonejob-1-45393183a926.log
data-agrigator-jobmanager-1        | 2023-08-04 10:35:27,077 INFO  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Determined location of main cluster component stdout file: /opt/flink/log/flink--standalonejob-1-45393183a926.out
data-agrigator-taskmanager-1       | Starting taskexecutor as a console application on host 70e59067e6a2.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:27,773 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Rest endpoint listening at 0.0.0.0:8081
data-agrigator-jobmanager-1        | 2023-08-04 10:35:27,779 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - http://0.0.0.0:8081 was granted leadership with leaderSessionID=00000000-0000-0000-0000-000000000000
data-agrigator-jobmanager-1        | 2023-08-04 10:35:27,783 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Web frontend listening at http://0.0.0.0:8081.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:27,864 INFO  org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner [] - DefaultDispatcherRunner was granted leadership with leader id 00000000-0000-0000-0000-000000000000. Creating new DispatcherLeaderProcess.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:27,894 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:27,904 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Starting resource manager service.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:27,927 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is granted leadership with session id 00000000-0000-0000-0000-000000000000.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:27,931 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs that are not finished, yet.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:27,937 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:28,112 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_0 .
data-agrigator-jobmanager-1        | 2023-08-04 10:35:28,131 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
data-agrigator-jobmanager-1        | 2023-08-04 10:35:28,251 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Starting the resource manager.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:28,347 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Starting tokens update task
data-agrigator-jobmanager-1        | 2023-08-04 10:35:28,348 WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - No tokens obtained so skipping notifications
data-agrigator-jobmanager-1        | 2023-08-04 10:35:28,349 WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Tokens update task not started because either no tokens obtained or none of the tokens specified its renewal date
data-agrigator-jobmanager-1        | 2023-08-04 10:35:28,429 INFO  org.apache.flink.client.ClientUtils                          [] - Starting program (detached: true)
data-agrigator-jobmanager-1        | 2023-08-04 10:35:28,689 WARN  org.apache.flink.connector.kafka.source.KafkaSourceBuilder   [] - Offset commit on checkpoint is disabled because group.id is not specified
data-agrigator-jobmanager-1        | WARNING: An illegal reflective access operation has occurred
data-agrigator-jobmanager-1        | WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/opt/flink/lib/flink-dist-1.17.1.jar) to field java.lang.String.value
data-agrigator-jobmanager-1        | WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner
data-agrigator-jobmanager-1        | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
data-agrigator-jobmanager-1        | WARNING: All illegal access operations will be denied in a future release
data-agrigator-jobmanager-1        | 
data-agrigator-jobmanager-1        | 
data-agrigator-jobmanager-1        | 2023-08-04 10:35:30,076 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Job d0fd1e75acc871318b01e45f45a56c83 is submitted.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:30,079 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Submitting Job with JobId=d0fd1e75acc871318b01e45f45a56c83.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:30,129 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'Flink Java API Skeleton' (d0fd1e75acc871318b01e45f45a56c83).
data-agrigator-jobmanager-1        | 2023-08-04 10:35:30,133 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'Flink Java API Skeleton' (d0fd1e75acc871318b01e45f45a56c83).
data-agrigator-jobmanager-1        | 2023-08-04 10:35:30,214 INFO  org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner [] - JobMasterServiceLeadershipRunner for job d0fd1e75acc871318b01e45f45a56c83 was granted leadership with leader id 00000000-0000-0000-0000-000000000000. Creating new JobMasterServiceProcess.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:30,355 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_2 .
data-agrigator-jobmanager-1        | 2023-08-04 10:35:30,403 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'Flink Java API Skeleton' (d0fd1e75acc871318b01e45f45a56c83).
data-agrigator-jobmanager-1        | 2023-08-04 10:35:30,498 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy NoRestartBackoffTimeStrategy for Flink Java API Skeleton (d0fd1e75acc871318b01e45f45a56c83).
data-agrigator-jobmanager-1        | 2023-08-04 10:35:30,650 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Created execution graph 28107b26cf672577ec8903d741675f1a for job d0fd1e75acc871318b01e45f45a56c83.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:30,784 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job Flink Java API Skeleton (d0fd1e75acc871318b01e45f45a56c83).
data-agrigator-jobmanager-1        | 2023-08-04 10:35:30,794 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 10 ms.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,058 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 2 new pipelined regions in 9 ms, total 2 pipelined regions currently.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,098 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@47f08104
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,099 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,109 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Checkpoint storage is set to 'jobmanager'
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,324 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,353 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@7f4d47d8 for Flink Java API Skeleton (d0fd1e75acc871318b01e45f45a56c83).
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,428 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'Flink Java API Skeleton' (d0fd1e75acc871318b01e45f45a56c83) under job master id 00000000000000000000000000000000.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,449 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Starting split enumerator for source Source: Kafka Source.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,469 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,470 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Java API Skeleton (d0fd1e75acc871318b01e45f45a56c83) switched from state CREATED to RUNNING.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,491 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2) (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from CREATED to SCHEDULED.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,590 INFO  org.apache.kafka.clients.admin.AdminClientConfig             [] - AdminClientConfig values: 
data-agrigator-jobmanager-1        | 	bootstrap.servers = [localhost:9092]
data-agrigator-jobmanager-1        | 	client.dns.lookup = use_all_dns_ips
data-agrigator-jobmanager-1        | 	client.id = KafkaSource--8073796819751080377-enumerator-admin-client
data-agrigator-jobmanager-1        | 	connections.max.idle.ms = 300000
data-agrigator-jobmanager-1        | 	default.api.timeout.ms = 60000
data-agrigator-jobmanager-1        | 	metadata.max.age.ms = 300000
data-agrigator-jobmanager-1        | 	metric.reporters = []
data-agrigator-jobmanager-1        | 	metrics.num.samples = 2
data-agrigator-jobmanager-1        | 	metrics.recording.level = INFO
data-agrigator-jobmanager-1        | 	metrics.sample.window.ms = 30000
data-agrigator-jobmanager-1        | 	receive.buffer.bytes = 65536
data-agrigator-jobmanager-1        | 	reconnect.backoff.max.ms = 1000
data-agrigator-jobmanager-1        | 	reconnect.backoff.ms = 50
data-agrigator-jobmanager-1        | 	request.timeout.ms = 30000
data-agrigator-jobmanager-1        | 	retries = 2147483647
data-agrigator-jobmanager-1        | 	retry.backoff.ms = 100
data-agrigator-jobmanager-1        | 	sasl.client.callback.handler.class = null
data-agrigator-jobmanager-1        | 	sasl.jaas.config = null
data-agrigator-jobmanager-1        | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
data-agrigator-jobmanager-1        | 	sasl.kerberos.min.time.before.relogin = 60000
data-agrigator-jobmanager-1        | 	sasl.kerberos.service.name = null
data-agrigator-jobmanager-1        | 	sasl.kerberos.ticket.renew.jitter = 0.05
data-agrigator-jobmanager-1        | 	sasl.kerberos.ticket.renew.window.factor = 0.8
data-agrigator-jobmanager-1        | 	sasl.login.callback.handler.class = null
data-agrigator-jobmanager-1        | 	sasl.login.class = null
data-agrigator-jobmanager-1        | 	sasl.login.connect.timeout.ms = null
data-agrigator-jobmanager-1        | 	sasl.login.read.timeout.ms = null
data-agrigator-jobmanager-1        | 	sasl.login.refresh.buffer.seconds = 300
data-agrigator-jobmanager-1        | 	sasl.login.refresh.min.period.seconds = 60
data-agrigator-jobmanager-1        | 	sasl.login.refresh.window.factor = 0.8
data-agrigator-jobmanager-1        | 	sasl.login.refresh.window.jitter = 0.05
data-agrigator-jobmanager-1        | 	sasl.login.retry.backoff.max.ms = 10000
data-agrigator-jobmanager-1        | 	sasl.login.retry.backoff.ms = 100
data-agrigator-jobmanager-1        | 	sasl.mechanism = GSSAPI
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.clock.skew.seconds = 30
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.expected.audience = null
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.expected.issuer = null
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.jwks.endpoint.url = null
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.scope.claim.name = scope
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.sub.claim.name = sub
data-agrigator-jobmanager-1        | 	sasl.oauthbearer.token.endpoint.url = null
data-agrigator-jobmanager-1        | 	security.protocol = PLAINTEXT
data-agrigator-jobmanager-1        | 	security.providers = null
data-agrigator-jobmanager-1        | 	send.buffer.bytes = 131072
data-agrigator-jobmanager-1        | 	socket.connection.setup.timeout.max.ms = 30000
data-agrigator-jobmanager-1        | 	socket.connection.setup.timeout.ms = 10000
data-agrigator-jobmanager-1        | 	ssl.cipher.suites = null
data-agrigator-jobmanager-1        | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
data-agrigator-jobmanager-1        | 	ssl.endpoint.identification.algorithm = https
data-agrigator-jobmanager-1        | 	ssl.engine.factory.class = null
data-agrigator-jobmanager-1        | 	ssl.key.password = null
data-agrigator-jobmanager-1        | 	ssl.keymanager.algorithm = SunX509
data-agrigator-jobmanager-1        | 	ssl.keystore.certificate.chain = null
data-agrigator-jobmanager-1        | 	ssl.keystore.key = null
data-agrigator-jobmanager-1        | 	ssl.keystore.location = null
data-agrigator-jobmanager-1        | 	ssl.keystore.password = null
data-agrigator-jobmanager-1        | 	ssl.keystore.type = JKS
data-agrigator-jobmanager-1        | 	ssl.protocol = TLSv1.3
data-agrigator-jobmanager-1        | 	ssl.provider = null
data-agrigator-jobmanager-1        | 	ssl.secure.random.implementation = null
data-agrigator-jobmanager-1        | 	ssl.trustmanager.algorithm = PKIX
data-agrigator-jobmanager-1        | 	ssl.truststore.certificates = null
data-agrigator-jobmanager-1        | 	ssl.truststore.location = null
data-agrigator-jobmanager-1        | 	ssl.truststore.password = null
data-agrigator-jobmanager-1        | 	ssl.truststore.type = JKS
data-agrigator-jobmanager-1        | 
data-agrigator-jobmanager-1        | 
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,623 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from CREATED to SCHEDULED.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,630 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,654 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,669 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager 00000000000000000000000000000000@akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job d0fd1e75acc871318b01e45f45a56c83.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,723 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 00000000000000000000000000000000@akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job d0fd1e75acc871318b01e45f45a56c83.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,737 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: 00000000000000000000000000000000.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:31,748 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job d0fd1e75acc871318b01e45f45a56c83: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,945 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,956 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Preconfiguration: 
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,957 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - 
data-agrigator-taskmanager-1       | 
data-agrigator-taskmanager-1       | 
data-agrigator-taskmanager-1       | RESOURCE_PARAMS extraction logs:
data-agrigator-taskmanager-1       | jvm_params: -Xmx536870902 -Xms536870902 -XX:MaxDirectMemorySize=268435458 -XX:MaxMetaspaceSize=268435456
data-agrigator-taskmanager-1       | dynamic_configs: -D taskmanager.memory.network.min=134217730b -D taskmanager.cpu.cores=2.0 -D taskmanager.memory.task.off-heap.size=0b -D taskmanager.memory.jvm-metaspace.size=268435456b -D external-resources=none -D taskmanager.memory.jvm-overhead.min=201326592b -D taskmanager.memory.framework.off-heap.size=134217728b -D taskmanager.memory.network.max=134217730b -D taskmanager.memory.framework.heap.size=134217728b -D taskmanager.memory.managed.size=536870920b -D taskmanager.memory.task.heap.size=402653174b -D taskmanager.numberOfTaskSlots=2 -D taskmanager.memory.jvm-overhead.max=201326592b
data-agrigator-taskmanager-1       | logs: WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: blob.server.port, 6124
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: taskmanager.memory.process.size, 1728m
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: taskmanager.bind-host, 0.0.0.0
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: jobmanager.execution.failover-strategy, region
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: jobmanager.rpc.address, jobmanager
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: jobmanager.memory.process.size, 1600m
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: jobmanager.rpc.port, 6123
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: query.server.port, 6125
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: rest.bind-address, 0.0.0.0
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: jobmanager.bind-host, 0.0.0.0
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: parallelism.default, 2
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: taskmanager.numberOfTaskSlots, 2
data-agrigator-taskmanager-1       | INFO  [] - Loading configuration property: rest.address, 0.0.0.0
data-agrigator-taskmanager-1       | INFO  [] - The derived from fraction jvm overhead memory (172.800mb (181193935 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
data-agrigator-taskmanager-1       | INFO  [] - Final TaskExecutor Memory configuration:
data-agrigator-taskmanager-1       | INFO  [] -   Total Process Memory:          1.688gb (1811939328 bytes)
data-agrigator-taskmanager-1       | INFO  [] -     Total Flink Memory:          1.250gb (1342177280 bytes)
data-agrigator-taskmanager-1       | INFO  [] -       Total JVM Heap Memory:     512.000mb (536870902 bytes)
data-agrigator-taskmanager-1       | INFO  [] -         Framework:               128.000mb (134217728 bytes)
data-agrigator-taskmanager-1       | INFO  [] -         Task:                    384.000mb (402653174 bytes)
data-agrigator-taskmanager-1       | INFO  [] -       Total Off-heap Memory:     768.000mb (805306378 bytes)
data-agrigator-taskmanager-1       | INFO  [] -         Managed:                 512.000mb (536870920 bytes)
data-agrigator-taskmanager-1       | INFO  [] -         Total JVM Direct Memory: 256.000mb (268435458 bytes)
data-agrigator-taskmanager-1       | INFO  [] -           Framework:             128.000mb (134217728 bytes)
data-agrigator-taskmanager-1       | INFO  [] -           Task:                  0 bytes
data-agrigator-taskmanager-1       | INFO  [] -           Network:               128.000mb (134217730 bytes)
data-agrigator-taskmanager-1       | INFO  [] -     JVM Metaspace:               256.000mb (268435456 bytes)
data-agrigator-taskmanager-1       | INFO  [] -     JVM Overhead:                192.000mb (201326592 bytes)
data-agrigator-taskmanager-1       | 
data-agrigator-taskmanager-1       | 
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,959 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,961 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Starting TaskManager (Version: 1.17.1, Scala: 2.12, Rev:2750d5c, Date:2023-05-19T10:45:46+02:00)
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,970 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  OS current user: flink
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,973 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Current Hadoop/Kerberos user: <no hadoop dependency found>
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,974 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  JVM: OpenJDK 64-Bit Server VM - Eclipse Adoptium - 11/11.0.20+8
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,975 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Arch: amd64
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,976 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Maximum heap size: 512 MiBytes
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,976 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  JAVA_HOME: /opt/java/openjdk
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,977 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  No Hadoop Dependency available
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,979 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  JVM Options:
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,980 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -XX:+UseG1GC
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,981 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Xmx536870902
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,981 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Xms536870902
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,982 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -XX:MaxDirectMemorySize=268435458
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,983 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -XX:MaxMetaspaceSize=268435456
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,984 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlog.file=/opt/flink/log/flink--taskexecutor-1-70e59067e6a2.log
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,984 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlog4j.configuration=file:/opt/flink/conf/log4j-console.properties
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,985 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlog4j.configurationFile=file:/opt/flink/conf/log4j-console.properties
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,986 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlogback.configurationFile=file:/opt/flink/conf/logback-console.xml
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,987 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Program Arguments:
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,994 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --configDir
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,996 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     /opt/flink/conf
data-agrigator-taskmanager-1       | 2023-08-04 10:35:31,998 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,000 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.network.min=134217730b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,002 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,002 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.cpu.cores=2.0
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,006 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,008 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.task.off-heap.size=0b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,010 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,011 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.jvm-metaspace.size=268435456b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,012 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,013 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     external-resources=none
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,013 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,014 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.jvm-overhead.min=201326592b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,015 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,016 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.framework.off-heap.size=134217728b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,017 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,018 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.network.max=134217730b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,018 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,019 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.framework.heap.size=134217728b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,020 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,021 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.managed.size=536870920b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,021 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,022 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.task.heap.size=402653174b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,023 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,024 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.numberOfTaskSlots=2
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,024 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,025 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.jvm-overhead.max=201326592b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,026 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Classpath: /opt/flink/lib/flink-cep-1.17.1.jar:/opt/flink/lib/flink-connector-files-1.17.1.jar:/opt/flink/lib/flink-csv-1.17.1.jar:/opt/flink/lib/flink-json-1.17.1.jar:/opt/flink/lib/flink-scala_2.12-1.17.1.jar:/opt/flink/lib/flink-table-api-java-uber-1.17.1.jar:/opt/flink/lib/flink-table-planner-loader-1.17.1.jar:/opt/flink/lib/flink-table-runtime-1.17.1.jar:/opt/flink/lib/log4j-1.2-api-2.17.1.jar:/opt/flink/lib/log4j-api-2.17.1.jar:/opt/flink/lib/log4j-core-2.17.1.jar:/opt/flink/lib/log4j-slf4j-impl-2.17.1.jar:/opt/flink/lib/flink-dist-1.17.1.jar::::
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,026 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,032 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Registered UNIX signal handlers for [TERM, HUP, INT]
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,043 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Maximum number of open file descriptors is 1048576.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,088 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: blob.server.port, 6124
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,090 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,092 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.bind-host, 0.0.0.0
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,094 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,095 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, jobmanager
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,096 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.process.size, 1600m
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,100 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,101 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: query.server.port, 6125
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,102 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.bind-address, 0.0.0.0
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,103 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.bind-host, 0.0.0.0
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,103 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 2
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,104 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 2
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,105 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.address, 0.0.0.0
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,105 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.network.min, 134217730b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,106 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.jvm-metaspace.size, 268435456b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,107 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.task.off-heap.size, 0b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,107 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.cpu.cores, 2.0
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,108 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: external-resources, none
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,108 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.jvm-overhead.min, 201326592b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,109 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.framework.off-heap.size, 134217728b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,115 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.network.max, 134217730b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,120 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.framework.heap.size, 134217728b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,120 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.managed.size, 536870920b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,121 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.task.heap.size, 402653174b
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,122 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.numberOfTaskSlots, 2
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,123 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.jvm-overhead.max, 201326592b
control-center                     | [2023-08-04 10:35:32,137] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:35:32,138] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,141 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'key.deserializer' was supplied but isn't a known config.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,147 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'commit.offsets.on.checkpoint' was supplied but isn't a known config.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,147 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'value.deserializer' was supplied but isn't a known config.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,148 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'enable.auto.commit' was supplied but isn't a known config.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,148 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,149 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,149 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'auto.offset.reset' was supplied but isn't a known config.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,161 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.2.3
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,162 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 50029d3ed8ba576f
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,162 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1691145332149
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,183 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Starting the KafkaSourceEnumerator for consumer group null without periodic partition discovery.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,330 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,339 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,375 INFO  org.apache.flink.core.fs.FileSystem                          [] - Hadoop is not in the classpath/dependencies. The extended set of supported File Systems via Hadoop is not available.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,434 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-statsd
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,459 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-datadog
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,461 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-influx
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,463 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-prometheus
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,463 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,465 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-jmx
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,466 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,468 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-graphite
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,469 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: external-resource-gpu
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,471 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-slf4j
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,544 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - StateChangelogStorageLoader initialized with shortcut names {memory,filesystem}.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,545 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-statsd
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,545 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-datadog
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,546 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-influx
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,547 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-prometheus
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,548 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-jmx
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,549 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-graphite
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,549 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: external-resource-gpu
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,550 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-slf4j
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,556 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - StateChangelogStorageLoader initialized with shortcut names {memory,filesystem}.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,610 INFO  org.apache.flink.runtime.security.modules.HadoopModuleFactory [] - Cannot create Hadoop Security Module because Hadoop cannot be found in the Classpath.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,660 INFO  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file will be created as /tmp/jaas-13894766394690073274.conf.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,676 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,677 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:32,733 INFO  org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory [] - Cannot install HadoopSecurityContext because Hadoop cannot be found in the Classpath.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,881 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:32,882 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:33,388 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:33,389 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:34,069 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils           [] - Trying to select the network interface and address to use by connecting to the leading JobManager.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:34,071 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils           [] - TaskManager will try to connect for PT10S before falling back to heuristics
data-agrigator-jobmanager-1        | 2023-08-04 10:35:34,196 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:34,197 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:35,015 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - TaskManager will use hostname/address '70e59067e6a2' (192.168.160.7) for communication.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:35,197 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address 192.168.160.7:0, bind address 0.0.0.0:0.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:35,309 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:35,310 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:36,522 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:36,523 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
schema-registry                    | [2023-08-04 10:35:37,226] INFO 192.168.160.13 - - [04/Aug/2023:10:35:37 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 7 (io.confluent.rest-utils.requests)
data-agrigator-taskmanager-1       | 2023-08-04 10:35:37,345 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
data-agrigator-taskmanager-1       | 2023-08-04 10:35:37,548 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:37,562 INFO  akka.remote.Remoting                                         [] - Starting remoting
data-agrigator-jobmanager-1        | 2023-08-04 10:35:37,735 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:37,736 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,055 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink@192.168.160.7:44263]
ksqldb-server                      | [2023-08-04 10:35:38,319] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:35:38 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,520 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink@192.168.160.7:44263
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,580 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Using working directory: WorkingDirectory(/tmp/tm_192.168.160.7:44263-32e813)
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,614 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,628 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address 192.168.160.7:0, bind address 0.0.0.0:0.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,697 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,715 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,718 INFO  akka.remote.Remoting                                         [] - Starting remoting
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,748 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink-metrics@192.168.160.7:33765]
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,787 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink-metrics@192.168.160.7:33765
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,831 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService_192.168.160.7:44263-32e813 .
data-agrigator-jobmanager-1        | 2023-08-04 10:35:38,848 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:38,851 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,881 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Created BLOB cache storage directory /tmp/tm_192.168.160.7:44263-32e813/blobStorage
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,897 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Created BLOB cache storage directory /tmp/tm_192.168.160.7:44263-32e813/blobStorage
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,908 INFO  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Enabled external resources: []
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,911 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Loading delegation token receivers
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,926 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receiver hadoopfs loaded and initialized
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,928 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receiver hbase loaded and initialized
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,929 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-statsd
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,930 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-datadog
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,931 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-influx
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,931 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-prometheus
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,932 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-jmx
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,932 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-graphite
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,933 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: external-resource-gpu
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,934 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-slf4j
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,935 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receivers loaded successfully
data-agrigator-taskmanager-1       | 2023-08-04 10:35:38,936 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Starting TaskManager with ResourceID: 192.168.160.7:44263-32e813
data-agrigator-taskmanager-1       | 2023-08-04 10:35:39,008 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices    [] - Temporary file directory '/tmp': total 56 GB, usable 4 GB (7.14% usable)
data-agrigator-taskmanager-1       | 2023-08-04 10:35:39,019 INFO  org.apache.flink.runtime.io.disk.iomanager.IOManager         [] - Created a new FileChannelManager for spilling of task related data to disk (joins, sorting, ...). Used directories:
data-agrigator-taskmanager-1       | 	/tmp/flink-io-1897551f-cfc5-4111-b7ef-f2ef1162b102
data-agrigator-taskmanager-1       | 2023-08-04 10:35:39,043 INFO  org.apache.flink.runtime.io.network.netty.NettyConfig        [] - NettyConfig [server address: /0.0.0.0, server port: 0, ssl enabled: false, memory segment size (bytes): 32768, transport type: AUTO, number of server threads: 2 (manual), number of client threads: 2 (manual), server connect backlog: 0 (use Netty's default), client connect timeout (sec): 120, send/receive buffer size (bytes): 0 (use Netty's default)]
data-agrigator-taskmanager-1       | 2023-08-04 10:35:39,306 INFO  org.apache.flink.runtime.io.network.NettyShuffleServiceFactory [] - Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
data-agrigator-taskmanager-1       | 	/tmp/flink-netty-shuffle-367061c2-ee6c-4579-baaf-e341eaab61cc
data-agrigator-jobmanager-1        | 2023-08-04 10:35:39,765 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:39,766 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:40,118 INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool [] - Allocated 128 MB for network buffer pool (number of memory segments: 4096, bytes per segment: 32768).
data-agrigator-taskmanager-1       | 2023-08-04 10:35:40,195 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Starting the network environment and its components.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:40,464 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Transport type 'auto': using EPOLL.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:40,470 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful initialization (took 269 ms).
data-agrigator-taskmanager-1       | 2023-08-04 10:35:40,496 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Transport type 'auto': using EPOLL.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:40,635 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful initialization (took 157 ms). Listening on SocketAddress /0.0.0.0:40999.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:40,640 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Starting the kvState service and its components.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:40,744 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
data-agrigator-taskmanager-1       | 2023-08-04 10:35:40,810 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:40,817 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - User file cache uses directory /tmp/flink-dist-cache-b8002a2d-7c3c-45eb-b3cd-67bb865ac174
data-agrigator-taskmanager-1       | 2023-08-04 10:35:40,826 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000).
data-agrigator-jobmanager-1        | 2023-08-04 10:35:40,875 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:40,881 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:41,864 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Resolved ResourceManager address, beginning registration
data-agrigator-jobmanager-1        | 2023-08-04 10:35:41,902 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:41,904 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:42,274 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 192.168.160.7:44263-32e813 (akka.tcp://flink@192.168.160.7:44263/user/rpc/taskmanager_0) at ResourceManager
data-agrigator-taskmanager-1       | WARNING: An illegal reflective access operation has occurred
data-agrigator-taskmanager-1       | WARNING: Illegal reflective access by org.jboss.netty.util.internal.ByteBufferUtil (file:/tmp/flink-rpc-akka_f8cdb79e-615b-45b8-8c53-e6ad6cd3bc9e.jar) to method java.nio.DirectByteBuffer.cleaner()
data-agrigator-taskmanager-1       | WARNING: Please consider reporting this to the maintainers of org.jboss.netty.util.internal.ByteBufferUtil
data-agrigator-taskmanager-1       | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
data-agrigator-taskmanager-1       | WARNING: All illegal access operations will be denied in a future release
data-agrigator-jobmanager-1        | 2023-08-04 10:35:42,352 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 192.168.160.7:44263-32e813 (akka.tcp://flink@192.168.160.7:44263/user/rpc/taskmanager_0) at ResourceManager
data-agrigator-taskmanager-1       | 2023-08-04 10:35:42,393 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_* under registration id 75d2e783315a99b2549ca67016287f78.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:42,570 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request f797d4bd1a47b0227c4b7b65aa4a0fa6 for job d0fd1e75acc871318b01e45f45a56c83 from resource manager with leader id 00000000000000000000000000000000.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:42,584 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for f797d4bd1a47b0227c4b7b65aa4a0fa6.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:42,588 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job d0fd1e75acc871318b01e45f45a56c83 for job leader monitoring.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:42,593 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 with leader id 00000000-0000-0000-0000-000000000000.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:42,627 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 7772969a22e3643da9544d1616336d6f for job d0fd1e75acc871318b01e45f45a56c83 from resource manager with leader id 00000000000000000000000000000000.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:42,629 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 7772969a22e3643da9544d1616336d6f.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:42,668 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
data-agrigator-taskmanager-1       | 2023-08-04 10:35:42,766 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job d0fd1e75acc871318b01e45f45a56c83.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:42,770 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job d0fd1e75acc871318b01e45f45a56c83.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:42,784 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job d0fd1e75acc871318b01e45f45a56c83.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:42,842 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2) (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from SCHEDULED to DEPLOYING.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:42,850 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2) (attempt #0) with attempt id 28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_0_0 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_0 to 192.168.160.7:44263-32e813 @ data-agrigator-taskmanager-1.data-agrigator_default (dataPort=40999) with allocation id 7772969a22e3643da9544d1616336d6f
data-agrigator-jobmanager-1        | 2023-08-04 10:35:42,871 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from SCHEDULED to DEPLOYING.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:42,873 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (attempt #0) with attempt id 28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_1_0 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_1 to 192.168.160.7:44263-32e813 @ data-agrigator-taskmanager-1.data-agrigator_default (dataPort=40999) with allocation id f797d4bd1a47b0227c4b7b65aa4a0fa6
data-agrigator-taskmanager-1       | 2023-08-04 10:35:42,944 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 7772969a22e3643da9544d1616336d6f.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:43,019 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:43,021 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,040 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,076 INFO  org.apache.flink.runtime.state.TaskExecutorChannelStateExecutorFactoryManager [] - Creating the channel state executor factory for job id d0fd1e75acc871318b01e45f45a56c83
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,102 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_0_0), deploy into slot with allocation id 7772969a22e3643da9544d1616336d6f.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,105 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from CREATED to DEPLOYING.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,114 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot f797d4bd1a47b0227c4b7b65aa4a0fa6.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,118 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_0_0) [DEPLOYING].
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,136 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_1_0), deploy into slot with allocation id f797d4bd1a47b0227c4b7b65aa4a0fa6.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,141 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 7772969a22e3643da9544d1616336d6f.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,142 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot f797d4bd1a47b0227c4b7b65aa4a0fa6.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,143 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from CREATED to DEPLOYING.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,150 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_1_0) [DEPLOYING].
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,306 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2e93b749
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,307 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@719a1a21
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,308 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,308 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,314 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,314 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,368 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from DEPLOYING to INITIALIZING.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:43,368 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from DEPLOYING to INITIALIZING.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:43,401 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2) (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from DEPLOYING to INITIALIZING.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:43,417 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from DEPLOYING to INITIALIZING.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:43,932 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:43,933 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:44,029 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:44,029 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:44,236 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:44,236 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:44,249 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from INITIALIZING to FAILED with failure cause:
data-agrigator-taskmanager-1       | java.io.IOException: unable to open JDBC writer
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at java.lang.Thread.run(Unknown Source) [?:?]
data-agrigator-taskmanager-1       | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-taskmanager-1       | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.Driver.connect(Driver.java:297) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	... 14 more
data-agrigator-taskmanager-1       | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.Driver.connect(Driver.java:297) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	... 14 more
data-agrigator-taskmanager-1       | 2023-08-04 10:35:44,251 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from INITIALIZING to FAILED with failure cause:
data-agrigator-taskmanager-1       | java.io.IOException: unable to open JDBC writer
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1       | 	at java.lang.Thread.run(Unknown Source) [?:?]
data-agrigator-taskmanager-1       | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-taskmanager-1       | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.Driver.connect(Driver.java:297) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	... 14 more
data-agrigator-taskmanager-1       | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-taskmanager-1       | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.postgresql.Driver.connect(Driver.java:297) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[my_api.jar:?]
data-agrigator-taskmanager-1       | 	... 14 more
data-agrigator-taskmanager-1       | 2023-08-04 10:35:44,281 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_1_0).
data-agrigator-taskmanager-1       | 2023-08-04 10:35:44,283 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_0_0).
data-agrigator-taskmanager-1       | 2023-08-04 10:35:44,326 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_0_0.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:44,383 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_1_0.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,483 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2) (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from INITIALIZING to FAILED on 192.168.160.7:44263-32e813 @ data-agrigator-taskmanager-1.data-agrigator_default (dataPort=40999).
data-agrigator-jobmanager-1        | java.io.IOException: unable to open JDBC writer
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1        | 	... 14 more
data-agrigator-jobmanager-1        | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-jobmanager-1        | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1        | 	... 14 more
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,537 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job d0fd1e75acc871318b01e45f45a56c83: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,550 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Removing registered reader after failure for subtask 0 (#0) of source Source: Kafka Source.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,559 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Java API Skeleton (d0fd1e75acc871318b01e45f45a56c83) switched from state RUNNING to FAILING.
data-agrigator-jobmanager-1        | org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:258) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:242) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:748) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | Caused by: java.io.IOException: unable to open JDBC writer
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.op
data-agrigator-jobmanager-1        | en(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-jobmanager-1        | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,594 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from INITIALIZING to CANCELING.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,614 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from CANCELING to CANCELED.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,619 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job d0fd1e75acc871318b01e45f45a56c83
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,618 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Java API Skeleton (d0fd1e75acc871318b01e45f45a56c83) switched from state FAILING to FAILED.
data-agrigator-jobmanager-1        | org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:258) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:242) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:748) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | Caused by: java.io.IOException: unable to open JDBC writer
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.ope
data-agrigator-jobmanager-1        | n(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-jobmanager-1        | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,648 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job d0fd1e75acc871318b01e45f45a56c83.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,676 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 28107b26cf672577ec8903d741675f1a_cbc357ccb763df2852fee8c4fc7d55f2_1_0.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,700 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job d0fd1e75acc871318b01e45f45a56c83 reached terminal state FAILED.
data-agrigator-jobmanager-1        | org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:258)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:242)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:748)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
data-agrigator-jobmanager-1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
data-agrigator-jobmanager-1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
data-agrigator-jobmanager-1        | 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
data-agrigator-jobmanager-1        | 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
data-agrigator-jobmanager-1        | 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
data-agrigator-jobmanager-1        | 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
data-agrigator-jobmanager-1        | 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
data-agrigator-jobmanager-1        | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579)
data-agrigator-jobmanager-1        | 	at akka.actor.ActorCell.invoke(ActorCell.scala:547)
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
data-agrigator-jobmanager-1        | 	at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
data-agrigator-jobmanager-1        | Caused by: java.io.IOException: unable to open JDBC writer
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142)
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52)
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101)
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46)
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734)
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709)
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
data-agrigator-jobmanager-1        | 	at java.base/java.lang.Thread.run(Unknown Source)
data-agrigator-jobmanager-1        | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342)
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443)
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297)
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121)
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140)
data-agrigator-jobmanager-1        | 	... 14 more
data-agrigator-jobmanager-1        | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-jobmanager-1        | 	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
data-agrigator-jobmanager-1        | 	at java.base/java.net.AbstractPlainSocketImpl.doConnect(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.net.AbstractPlainSocketImpl.connect(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.net.SocksSocketImpl.connect(Unknown Source)
data-agrigator-jobmanager-1        | 	at java.base/java.net.Socket.connect(Unknown Source)
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243)
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98)
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132)
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)
data-agrigator-jobmanager-1        | 	... 20 more
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,720 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job d0fd1e75acc871318b01e45f45a56c83 has been registered for cleanup in the JobResultStore after reaching a terminal state.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,746 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job 'Flink Java API Skeleton' (d0fd1e75acc871318b01e45f45a56c83).
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,766 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Closing SourceCoordinator for source Source: Kafka Source.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,790 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,800 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Disconnect TaskExecutor 192.168.160.7:44263-32e813 because: Stopping JobMaster for job 'Flink Java API Skeleton' (d0fd1e75acc871318b01e45f45a56c83).
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,803 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [7772969a22e3643da9544d1616336d6f].
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,809 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [f797d4bd1a47b0227c4b7b65aa4a0fa6].
data-agrigator-taskmanager-1       | 2023-08-04 10:35:44,833 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:1, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1, taskHeapMemory=192.000mb (201326587 bytes), taskOffHeapMemory=0 bytes, managedMemory=256.000mb (268435460 bytes), networkMemory=64.000mb (67108865 bytes)}, allocationId: 7772969a22e3643da9544d1616336d6f, jobId: d0fd1e75acc871318b01e45f45a56c83).
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,845 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 888666ff37704dd2af33a54946ea63c6: Stopping JobMaster for job 'Flink Java API Skeleton' (d0fd1e75acc871318b01e45f45a56c83).
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,849 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job d0fd1e75acc871318b01e45f45a56c83 from the resource manager.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:44,876 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1, taskHeapMemory=192.000mb (201326587 bytes), taskOffHeapMemory=0 bytes, managedMemory=256.000mb (268435460 bytes), networkMemory=64.000mb (67108865 bytes)}, allocationId: f797d4bd1a47b0227c4b7b65aa4a0fa6, jobId: d0fd1e75acc871318b01e45f45a56c83).
data-agrigator-taskmanager-1       | 2023-08-04 10:35:44,881 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job d0fd1e75acc871318b01e45f45a56c83 from job leader monitoring.
data-agrigator-taskmanager-1       | 2023-08-04 10:35:44,885 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job d0fd1e75acc871318b01e45f45a56c83.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,889 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:44,890 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,098 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,099 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource--8073796819751080377-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,213 INFO  org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap [] - Application FAILED: 
data-agrigator-jobmanager-1        | java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: FAILED
data-agrigator-jobmanager-1        | 	at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$unwrapJobResultException$7(ApplicationDispatcherBootstrap.java:403) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.postComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.complete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.client.deployment.application.JobStatusPollingUtils.lambda$null$2(JobStatusPollingUtils.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.whenComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.client.deployment.application.JobStatusPollingUtils.lambda$pollJobResultAsync$4(JobStatusPollingUtils.java:96) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.postComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.complete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:267) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.postComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.complete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1300) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.postComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.CompletableFuture.complete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.OnComplete.internal(Future.scala:300) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.OnComplete.internal(Future.scala:297) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:622) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) [flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | 	at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
data-agrigator-jobmanager-1        | Caused by: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: FAILED
data-agrigator-jobmanager-1        | 	at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:71) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	... 56 more
data-agrigator-jobmanager-1        | Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:60) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	... 56 more
data-agrigator-jobmanager-1        | Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.De
data-agrigator-jobmanager-1        | faultScheduler.recordTaskFailure(DefaultScheduler.java:258) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:242) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:748) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_f69c0460-27b7-47fd-bf40-a5d8c9d38c60.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.actor.ActorCell.invoke(ActorCell.scala:547) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
data-agrigator-jobmanager-1        | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
data-agrigator-jobmanager-1        | 	... 5 more
data-agrigator-jobmanager-1        | Caused by: java.io.IOException: unable to open JDBC writer
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-jobmanager-1        | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1        | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1        | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,243 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting StandaloneApplicationClusterEntryPoint down with application status FAILED. Diagnostics null.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,262 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shutting down rest endpoint.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,325 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Removing cache directory /tmp/flink-web-f459c06f-102b-40ff-a5b8-bac9c663d7a3/flink-web-ui
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,330 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - http://0.0.0.0:8081 lost leadership
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,332 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shut down complete.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,341 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Shut down cluster because application is in FAILED, diagnostics null.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,343 INFO  org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent [] - Closing components.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,347 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Stopping SessionDispatcherLeaderProcess.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,349 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping dispatcher akka.tcp://flink@jobmanager:6123/user/rpc/dispatcher_0.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,350 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Stopping resource manager service.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,351 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping all currently running jobs of dispatcher akka.tcp://flink@jobmanager:6123/user/rpc/dispatcher_0.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,354 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Stopping credential renewal
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,355 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Stopped credential renewal
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,356 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Closing the slot manager.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,357 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Suspending the slot manager.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,362 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is not running. Ignore revoking leadership.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,363 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka.tcp://flink@jobmanager:6123/user/rpc/dispatcher_0.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,374 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:6124
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,390 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,405 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,490 INFO  akka.actor.CoordinatedShutdown                               [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,490 INFO  akka.actor.CoordinatedShutdown                               [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,543 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,545 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,549 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,551 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,733 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,733 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,779 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,803 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
data-agrigator-jobmanager-1        | 2023-08-04 10:35:46,803 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Terminating cluster entrypoint process StandaloneApplicationClusterEntryPoint with exit code 1443.
data-agrigator-jobmanager-1 exited with code 163
schema-registry                    | [2023-08-04 10:35:56,304] INFO 192.168.160.13 - - [04/Aug/2023:10:35:56 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 6 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:36:02,286] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:36:02 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
ksqldb-server                      | [2023-08-04 10:36:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
control-center                     | [2023-08-04 10:36:15,469] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:36:17,208] INFO 192.168.160.1 - - [04/Aug/2023:10:36:17 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:36:17,233] INFO 192.168.160.1 - - [04/Aug/2023:10:36:17 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:36:17,258] INFO 192.168.160.1 - - [04/Aug/2023:10:36:17 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 6 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:36:17,306] INFO 192.168.160.1 - - [04/Aug/2023:10:36:17 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 27 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:36:17,342] INFO 192.168.160.1 - - [04/Aug/2023:10:36:17 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 18 (io.confluent.rest-utils.requests)
schema-registry                    | [2023-08-04 10:36:17,347] INFO 192.168.160.13 - - [04/Aug/2023:10:36:17 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 49 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:36:17,381] INFO 192.168.160.1 - - [04/Aug/2023:10:36:17 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 14 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:36:17,430] INFO 192.168.160.1 - - [04/Aug/2023:10:36:17 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 8 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:36:23,774] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Processed 16 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:36:23,874] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:36:23,880] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:36:24,005] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:36:24,059] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:36:24,069] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:36:24,094] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:36:24,102] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:36:24,107] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:36:24,161] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:36:24,279] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:36:24,362] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
ksqldb-server                      | [2023-08-04 10:36:26,066] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:36:26 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:36:32,125] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:36:32,125] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
data-agrigator-taskmanager-1       | 2023-08-04 10:36:32,431 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - The heartbeat of ResourceManager with id 888666ff37704dd2af33a54946ea63c6 timed out.
data-agrigator-taskmanager-1       | 2023-08-04 10:36:32,432 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close ResourceManager connection 888666ff37704dd2af33a54946ea63c6.
data-agrigator-taskmanager-1       | 2023-08-04 10:36:32,440 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000).
data-agrigator-taskmanager-1       | 2023-08-04 10:36:32,486 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:36:32,489 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                     | [2023-08-04 10:36:33,828] INFO [AdminClient clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-admin] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry                    | [2023-08-04 10:36:40,893] INFO 192.168.160.13 - - [04/Aug/2023:10:36:40 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 6 (io.confluent.rest-utils.requests)
data-agrigator-taskmanager-1       | 2023-08-04 10:36:42,552 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:36:42,575 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
ksqldb-server                      | [2023-08-04 10:36:48,145] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:36:48 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
data-agrigator-taskmanager-1       | 2023-08-04 10:36:52,622 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:36:52,627 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
schema-registry                    | [2023-08-04 10:36:59,555] INFO 192.168.160.13 - - [04/Aug/2023:10:36:59 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 7 (io.confluent.rest-utils.requests)
data-agrigator-taskmanager-1       | 2023-08-04 10:37:02,688 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:37:02,691 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
ksqldb-server                      | [2023-08-04 10:37:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
ksqldb-server                      | [2023-08-04 10:37:05,112] INFO [AdminClient clientId=adminclient-2] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:37:06,095] INFO [AdminClient clientId=adminclient-4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
ksqldb-server                      | [2023-08-04 10:37:10,995] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:37:10 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
data-agrigator-taskmanager-1       | 2023-08-04 10:37:12,725 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:37:12,727 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                     | [2023-08-04 10:37:17,242] INFO 192.168.160.1 - - [04/Aug/2023:10:37:17 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:37:17,263] INFO 192.168.160.1 - - [04/Aug/2023:10:37:17 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:37:17,288] INFO 192.168.160.1 - - [04/Aug/2023:10:37:17 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:37:17,318] INFO 192.168.160.1 - - [04/Aug/2023:10:37:17 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 6 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:37:17,339] INFO 192.168.160.1 - - [04/Aug/2023:10:37:17 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 4 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:37:17,368] INFO 192.168.160.1 - - [04/Aug/2023:10:37:17 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 8 (io.confluent.rest-utils.requests)
schema-registry                    | [2023-08-04 10:37:17,833] INFO 192.168.160.13 - - [04/Aug/2023:10:37:17 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:37:20,712] INFO [AdminClient clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-admin] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1       | 2023-08-04 10:37:27,854 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:37:27,856 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
ksqldb-server                      | [2023-08-04 10:37:29,133] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:37:29 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:37:32,125] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:37:32,126] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
schema-registry                    | [2023-08-04 10:37:37,317] INFO 192.168.160.13 - - [04/Aug/2023:10:37:37 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 8 (io.confluent.rest-utils.requests)
data-agrigator-taskmanager-1       | 2023-08-04 10:37:37,903 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:37:37,905 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1       | 2023-08-04 10:37:47,935 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:37:47,936 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
ksqldb-server                      | [2023-08-04 10:37:50,654] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:37:50 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:37:55,323] INFO 192.168.160.13 - - [04/Aug/2023:10:37:55 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 10 (io.confluent.rest-utils.requests)
data-agrigator-taskmanager-1       | 2023-08-04 10:37:57,987 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:37:57,989 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
ksqldb-server                      | [2023-08-04 10:38:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
data-agrigator-taskmanager-1       | 2023-08-04 10:38:08,025 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:38:08,031 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
ksqldb-server                      | [2023-08-04 10:38:11,685] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:38:11 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:38:14,851] INFO 192.168.160.13 - - [04/Aug/2023:10:38:14 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:38:15,555] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:38:17,233] INFO 192.168.160.1 - - [04/Aug/2023:10:38:17 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 17 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:38:17,274] INFO 192.168.160.1 - - [04/Aug/2023:10:38:17 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 10 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:38:17,324] INFO 192.168.160.1 - - [04/Aug/2023:10:38:17 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 10 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:38:17,384] INFO 192.168.160.1 - - [04/Aug/2023:10:38:17 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:38:17,414] INFO 192.168.160.1 - - [04/Aug/2023:10:38:17 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 6 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:38:17,454] INFO 192.168.160.1 - - [04/Aug/2023:10:38:17 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 18 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:38:17,492] INFO 192.168.160.1 - - [04/Aug/2023:10:38:17 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 12 (io.confluent.rest-utils.requests)
data-agrigator-taskmanager-1       | 2023-08-04 10:38:18,065 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:38:18,068 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                     | [2023-08-04 10:38:23,870] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Processed 14 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:38:23,970] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Processed 14 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:38:23,972] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Processed 14 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:38:24,072] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:38:24,075] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:38:24,093] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:38:24,109] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:38:24,110] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:38:24,175] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:38:24,195] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:38:24,287] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:38:24,377] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
data-agrigator-taskmanager-1       | 2023-08-04 10:38:28,125 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:38:28,128 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
ksqldb-server                      | [2023-08-04 10:38:29,690] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:38:29 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
control-center                     | [2023-08-04 10:38:32,124] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:38:32,125] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
schema-registry                    | [2023-08-04 10:38:36,590] INFO 192.168.160.13 - - [04/Aug/2023:10:38:36 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 6 (io.confluent.rest-utils.requests)
data-agrigator-taskmanager-1       | 2023-08-04 10:38:38,167 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:38:38,169 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1       | 2023-08-04 10:38:48,211 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:38:48,213 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
ksqldb-server                      | [2023-08-04 10:38:53,726] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:38:53 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:38:57,867] INFO 192.168.160.13 - - [04/Aug/2023:10:38:57 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 20 (io.confluent.rest-utils.requests)
data-agrigator-taskmanager-1       | 2023-08-04 10:38:58,273 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:38:58,276 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
ksqldb-server                      | [2023-08-04 10:39:04,887] INFO reporting node-level saturation 0.0 (io.confluent.ksql.utilization.PersistentQuerySaturationMetrics)
data-agrigator-taskmanager-1       | 2023-08-04 10:39:08,308 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:39:08,311 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
ksqldb-server                      | [2023-08-04 10:39:16,060] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:39:16 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
schema-registry                    | [2023-08-04 10:39:17,125] INFO 192.168.160.13 - - [04/Aug/2023:10:39:17 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 6 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:39:17,239] INFO 192.168.160.1 - - [04/Aug/2023:10:39:17 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:39:17,272] INFO 192.168.160.1 - - [04/Aug/2023:10:39:17 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 9 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:39:17,289] INFO 192.168.160.1 - - [04/Aug/2023:10:39:17 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 94 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 5 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:39:17,320] INFO 192.168.160.1 - - [04/Aug/2023:10:39:17 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 8 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:39:17,341] INFO 192.168.160.1 - - [04/Aug/2023:10:39:17 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 5 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:39:17,368] INFO 192.168.160.1 - - [04/Aug/2023:10:39:17 +0000] "GET /3.0/license HTTP/1.1" 200 448 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 5 (io.confluent.rest-utils.requests)
control-center                     | [2023-08-04 10:39:17,404] INFO 192.168.160.1 - - [04/Aug/2023:10:39:17 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters/MkU3OEVBNTcwNTJENDM2Qg/management/topics/gate_of_word/settings" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 11 (io.confluent.rest-utils.requests)
data-agrigator-taskmanager-1       | 2023-08-04 10:39:18,344 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:39:18,346 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1       | 2023-08-04 10:39:28,400 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:39:28,404 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                     | [2023-08-04 10:39:32,132] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                     | [2023-08-04 10:39:32,133] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
schema-registry                    | [2023-08-04 10:39:34,455] INFO 192.168.160.13 - - [04/Aug/2023:10:39:34 +0000] "GET / HTTP/2.0" 200 2 "-" "armeria/1.13.4" 14 (io.confluent.rest-utils.requests)
ksqldb-server                      | [2023-08-04 10:39:37,529] INFO 192.168.160.13 - - [Fri, 4 Aug 2023 10:39:37 GMT] "GET /info HTTP/2.0" 200 132 "-" "armeria/1.13.4" 0 (io.confluent.ksql.api.server.LoggingHandler)
data-agrigator-taskmanager-1       | 2023-08-04 10:39:38,442 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1       | 2023-08-04 10:39:38,448 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
postgres_db                        | 2023-08-04 10:39:39.466 UTC [1] LOG:  received fast shutdown request
postgres_db                        | 2023-08-04 10:39:39.469 UTC [1] LOG:  aborting any active transactions
postgres_db                        | 2023-08-04 10:39:39.474 UTC [102] FATAL:  terminating connection due to administrator command
postgres_db                        | 2023-08-04 10:39:39.508 UTC [1] LOG:  background worker "logical replication launcher" (PID 27) exited with exit code 1
postgres_db                        | 2023-08-04 10:39:39.576 UTC [22] LOG:  shutting down
postgres_db                        | 2023-08-04 10:39:39.580 UTC [22] LOG:  checkpoint starting: shutdown immediate
ksqldb-cli                         | exit
postgres_db                        | 2023-08-04 10:39:39.637 UTC [22] LOG:  checkpoint complete: wrote 4 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.014 s, sync=0.008 s, total=0.061 s; sync files=3, longest=0.004 s, average=0.003 s; distance=0 kB, estimate=0 kB
data-agrigator-taskmanager-1       | 2023-08-04 10:39:39,632 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.
postgres_db                        | 2023-08-04 10:39:39.717 UTC [1] LOG:  database system is shut down
control-center                     | [2023-08-04 10:39:39,693] INFO Shutting down due to shutdown hook signal (io.confluent.controlcenter.application.AllControlCenter)
control-center                     | [2023-08-04 10:39:39,776] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:39,860] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:39,861] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:39,861] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:39,862] INFO App info kafka.producer for confluent-control-center-heartbeat-sender-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:39,862] INFO Closed monitoring heartbeat producer (io.confluent.controlcenter.streams.verify.MonitoringHeartbeatSender)
control-center                     | [2023-08-04 10:39:39,862] INFO Shutting down Kafka Streams (io.confluent.controlcenter.streams.KafkaStreamsManager)
data-agrigator-taskmanager-1       | 2023-08-04 10:39:39,870 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache
data-agrigator-taskmanager-1       | 2023-08-04 10:39:39,884 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - removed file cache directory /tmp/flink-dist-cache-b8002a2d-7c3c-45eb-b3cd-67bb865ac174
control-center                     | [2023-08-04 10:39:39,905] INFO stream-client [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.KafkaStreams)
data-agrigator-taskmanager-1       | 2023-08-04 10:39:39,912 INFO  org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManager [] - Shutting down TaskExecutorStateChangelogStoragesManager.
data-agrigator-taskmanager-1       | 2023-08-04 10:39:39,915 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /tmp/flink-io-1897551f-cfc5-4111-b7ef-f2ef1162b102
data-agrigator-taskmanager-1       | 2023-08-04 10:39:39,914 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /tmp/flink-netty-shuffle-367061c2-ee6c-4579-baaf-e341eaab61cc
data-agrigator-taskmanager-1       | 2023-08-04 10:39:39,898 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Stopping TaskExecutor akka.tcp://flink@192.168.160.7:44263/user/rpc/taskmanager_0.
data-agrigator-taskmanager-1       | 2023-08-04 10:39:39,956 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Terminating registration attempts towards ResourceManager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1       | 2023-08-04 10:39:39,957 INFO  org.apache.flink.runtime.state.TaskExecutorChannelStateExecutorFactoryManager [] - Shutting down TaskExecutorChannelStateExecutorFactoryManager.
data-agrigator-taskmanager-1       | 2023-08-04 10:39:39,954 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
data-agrigator-taskmanager-1       | 2023-08-04 10:39:39,980 INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
control-center                     | [2023-08-04 10:39:40,014] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,014] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,014] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,014] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,014] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,014] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,014] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,015] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,015] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,015] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,015] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,015] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,015] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,015] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,015] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,015] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,015] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,015] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,033] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Shutting down clean (org.apache.kafka.streams.processor.internals.StreamThread)
rest-proxy                         | [2023-08-04 10:39:40,041] INFO Stopped NetworkTrafficServerConnector@2ef14fe{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8082} (org.eclipse.jetty.server.AbstractConnector)
rest-proxy                         | [2023-08-04 10:39:40,057] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session)
control-center                     | [2023-08-04 10:39:40,062] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Shutting down clean (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,063] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Shutting down clean (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,064] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Shutting down clean (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,064] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Shutting down clean (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,101] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Shutting down clean (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,115] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,116] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,116] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,116] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,116] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,116] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,116] INFO stream-client [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514] Shutting down 12 stream threads (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:39:40,118] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Shutting down clean (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,120] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Shutting down clean (org.apache.kafka.streams.processor.internals.StreamThread)
rest-proxy                         | [2023-08-04 10:39:40,125] INFO Stopped o.e.j.s.ServletContextHandler@107e5441{/ws,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
control-center                     | [2023-08-04 10:39:40,177] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Shutting down clean (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,191] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Shutting down clean (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,204] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] task [10_10] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,268] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Shutting down clean (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,272] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Shutting down clean (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:40,174] INFO Stopped NetworkTrafficServerConnector@586af46{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:9021} (org.eclipse.jetty.server.AbstractConnector)
control-center                     | [2023-08-04 10:39:40,340] INFO closing store=group-aggregate-store-THREE_HOURS (io.confluent.controlcenter.streams.verify.ThroughTopicVerifierTransformerSupplier$ThroughTopicVerifierTransformer)
control-center                     | [2023-08-04 10:39:40,340] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] task [4_0] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
rest-proxy                         | [2023-08-04 10:39:40,344] INFO Stopped o.e.j.s.ServletContextHandler@4aeaadc1{/,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
control-center                     | [2023-08-04 10:39:40,338] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session)
control-center                     | [2023-08-04 10:39:40,473] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] task [7_0] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
rest-proxy                         | [2023-08-04 10:39:40,501] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
rest-proxy                         | [2023-08-04 10:39:40,501] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
rest-proxy                         | [2023-08-04 10:39:40,502] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:40,524] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:40,526] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] stream-task [10_10] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:40,531] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:40,492] INFO Stopped o.e.j.s.ServletContextHandler@4d62bb8b{/ws,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
control-center                     | [2023-08-04 10:39:40,554] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] task [10_10] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,554] INFO Stopped o.e.j.s.ServletContextHandler@755057c7{/api/kafka-rest-ws/MkU3OEVBNTcwNTJENDM2Qg,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
control-center                     | [2023-08-04 10:39:40,590] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] task [10_8] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,595] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] task [5_0] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,616] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] task [3_0] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,634] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] task [0_0] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,637] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:40,644] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] task [10_11] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,646] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:40,714] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:40,716] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:40,718] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] task [10_9] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,733] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] task [6_0] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,728] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:40,738] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] stream-task [0_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:40,667] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] stream-task [10_8] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:40,730] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] task [8_0] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,743] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] task [10_8] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,749] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] task [11_0] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,748] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:40,750] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] stream-task [10_9] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:40,750] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] task [10_9] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,746] INFO closing store=group-aggregate-store-ONE_MINUTE (io.confluent.controlcenter.streams.verify.ThroughTopicVerifierTransformerSupplier$ThroughTopicVerifierTransformer)
control-center                     | [2023-08-04 10:39:40,797] INFO closing store=TriggerActionsStore (io.confluent.controlcenter.streams.alert.AlertTransformerSupplier)
control-center                     | [2023-08-04 10:39:40,801] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] stream-task [5_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:40,801] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] task [5_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,813] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] stream-task [7_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:40,813] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] task [7_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,820] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] task [0_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,833] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] task [12_0] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,842] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] stream-task [3_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:40,842] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] task [3_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,843] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] stream-task [4_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:40,843] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] task [4_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,844] INFO closing store=MonitoringVerifierStore (io.confluent.controlcenter.streams.verify.VerifyTransformerSupplier)
control-center                     | [2023-08-04 10:39:40,844] INFO closing store=aggregate-topic-partition-store (io.confluent.controlcenter.streams.verify.ThroughTopicVerifierTransformerSupplier$ThroughTopicVerifierTransformer)
control-center                     | [2023-08-04 10:39:40,844] INFO closing store=monitoring-aggregate-rekey-store (io.confluent.controlcenter.streams.verify.ThroughTopicVerifierTransformerSupplier$ThroughTopicVerifierTransformer)
control-center                     | [2023-08-04 10:39:40,845] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] task [1_0] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,851] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:40,855] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] stream-task [11_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:40,910] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] task [2_0] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,866] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] task [10_3] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,864] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] task [10_5] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:40,862] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:40,861] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:40,933] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] stream-task [10_11] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:40,932] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:40,944] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:40,956] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] task [10_2] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:41,020] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] task [11_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:41,020] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:41,035] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] stream-task [10_3] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:41,019] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:41,019] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:41,037] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] stream-task [10_5] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:41,015] INFO closing store=MonitoringTriggerStore (io.confluent.controlcenter.streams.alert.MonitoringTriggerTransformerSupplier)
control-center                     | [2023-08-04 10:39:41,011] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:41,010] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] task [10_1] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:41,046] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] stream-task [6_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:41,046] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] task [6_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:41,053] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] task [10_11] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:41,068] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] task [9_0] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:41,070] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] task [10_4] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:41,070] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] task [10_5] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:41,077] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] task [10_3] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:41,087] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:41,088] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] stream-task [10_2] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:41,089] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] task [10_2] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:41,089] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:41,090] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] stream-task [8_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:41,090] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] task [8_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:41,035] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:41,055] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:41,071] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:41,077] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:41,172] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,172] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,172] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,173] INFO App info kafka.producer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:41,208] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:41,248] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,257] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,257] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,257] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,257] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,227] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,257] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,257] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,257] INFO App info kafka.producer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:41,258] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,258] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,259] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,259] INFO App info kafka.producer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:41,259] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:41,284] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:41,285] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,307] INFO App info kafka.producer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:41,308] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:41,310] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,311] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,311] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,312] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,312] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,312] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,319] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] stream-task [2_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:41,319] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] task [2_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:41,319] INFO App info kafka.producer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:41,320] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:41,324] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] stream-task [1_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:41,333] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,351] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,350] INFO Stopped o.e.j.s.ServletContextHandler@77a35b2f{/,[io.confluent.controlcenter.rest.ModifiableResource@74cf0dd9],STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
control-center                     | [2023-08-04 10:39:41,402] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,412] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,413] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,415] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,663] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:41,689] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:41,692] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] stream-task [10_1] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:41,688] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:41,688] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:41,686] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] task [10_0] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:41,672] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] task [10_6] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:41,669] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] task [1_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:41,694] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:41,694] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] stream-task [10_4] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
ksqldb-cli exited with code 0
ksqldb-cli exited with code 0
control-center                     | [2023-08-04 10:39:41,965] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,987] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,965] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
data-agrigator-crowler-producer-1 exited with code 0
data-agrigator-crowler-producer-1 exited with code 143
control-center                     | [2023-08-04 10:39:42,136] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:41,979] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:41,971] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] task [10_1] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:42,137] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:42,139] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,143] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:41,968] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] task [10_4] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:42,171] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:42,150] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,150] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,207] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,140] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:42,209] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] stream-task [10_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:42,082] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:42,209] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] stream-task [10_6] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:42,223] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,224] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,334] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,334] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,271] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,334] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,334] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,270] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,334] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,334] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,352] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:42,358] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:42,367] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] task [10_6] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:42,368] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:42,367] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] task [10_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:42,369] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:42,371] INFO Stopped o.e.j.s.ServletContextHandler@28d2afd8{/api/kafka-rest/MkU3OEVBNTcwNTJENDM2Qg/kafka,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
control-center                     | [2023-08-04 10:39:42,392] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,392] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,393] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,507] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:42,508] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,508] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,524] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,536] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,537] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,552] INFO Closing reporter io.confluent.telemetry.reporter.TelemetryReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,552] INFO Stopping TelemetryReporter collectorTask (io.confluent.telemetry.reporter.TelemetryReporter)
control-center                     | [2023-08-04 10:39:42,554] INFO Closing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
control-center                     | [2023-08-04 10:39:42,565] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,651] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,652] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4-restore-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:42,653] INFO App info kafka.producer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:42,655] INFO App info kafka.producer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:42,659] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7-restore-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:42,660] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12-restore-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:42,660] INFO App info kafka.producer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:42,655] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,661] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,661] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,661] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:42,661] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:42,661] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:42,667] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9-restore-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:42,676] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] stream-task [9_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:42,793] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:42,805] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-9] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:42,793] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:42,842] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-12] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:42,793] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:42,851] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-7] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:42,792] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:42,852] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-4] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:42,770] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,853] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,853] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,743] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,863] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,864] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,778] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,874] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,875] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
data-agrigator-taskmanager-1       | 2023-08-04 10:39:42,886 INFO  akka.actor.CoordinatedShutdown                               [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
control-center                     | [2023-08-04 10:39:42,734] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,890] INFO App info kafka.producer for confluent-control-center-heartbeat-sender-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:42,741] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,734] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,733] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,731] INFO Stopping TelemetryReporter remoteConfigTask (io.confluent.telemetry.reporter.TelemetryReporter)
postgres_db exited with code 0
control-center                     | [2023-08-04 10:39:42,919] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:42,832] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] task [9_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
postgres_db exited with code 0
control-center                     | [2023-08-04 10:39:43,017] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] task [10_7] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:43,021] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:43,021] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] stream-task [10_7] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:43,022] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] task [10_7] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:43,022] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:42,832] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] stream-task [12_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:42,978] INFO App info kafka.producer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,036] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:42,977] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
data-agrigator-taskmanager-1       | 2023-08-04 10:39:43,037 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
data-agrigator-taskmanager-1       | 2023-08-04 10:39:43,039 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
control-center                     | [2023-08-04 10:39:43,060] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] task [12_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:43,062] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:43,081] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,128] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:42,891] INFO Closed monitoring heartbeat producer (io.confluent.controlcenter.streams.verify.MonitoringHeartbeatSender)
control-center                     | [2023-08-04 10:39:43,272] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,288] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,299] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,299] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,328] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,329] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
rest-proxy exited with code 0
control-center                     | [2023-08-04 10:39:43,298] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
rest-proxy exited with code 143
control-center                     | [2023-08-04 10:39:43,298] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,391] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,396] INFO Closing reporter io.confluent.telemetry.reporter.TelemetryReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,397] INFO Stopping TelemetryReporter collectorTask (io.confluent.telemetry.reporter.TelemetryReporter)
data-agrigator-taskmanager-1       | 2023-08-04 10:39:43,384 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
control-center                     | [2023-08-04 10:39:43,403] INFO Closing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
control-center                     | [2023-08-04 10:39:43,298] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,481] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,298] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,451] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,425] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2-restore-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,425] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,500] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,318] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,501] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,484] INFO Stopping TelemetryReporter remoteConfigTask (io.confluent.telemetry.reporter.TelemetryReporter)
control-center                     | [2023-08-04 10:39:43,502] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,528] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,529] INFO App info kafka.producer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,530] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,530] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,533] INFO App info kafka.producer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,537] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:43,531] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,532] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,545] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-2] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,539] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:43,538] INFO App info kafka.producer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,547] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:43,554] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,556] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,556] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11-restore-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,557] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,560] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,571] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,558] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,565] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5-restore-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,582] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,582] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-5] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,561] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,582] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,560] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,583] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-11] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,583] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,588] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1-restore-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,590] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,594] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-1] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,594] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,595] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,596] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,596] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,601] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6-restore-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,608] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,611] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-6] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,622] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,623] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,624] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,625] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,630] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,631] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,631] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,632] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,634] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,635] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,636] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,633] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,650] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3-restore-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,652] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,653] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-3] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,653] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10-restore-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,658] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,658] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-10] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,663] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8-restore-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,665] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,665] INFO stream-thread [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-StreamThread-8] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,671] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,673] INFO Closing reporter io.confluent.controlcenter.util.StreamProgressReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,674] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,679] INFO stream-client [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514] Shutdown 12 stream threads complete (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:39:43,683] INFO App info kafka.admin.client for _confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514-admin unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,701] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,701] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,702] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,705] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,706] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,706] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,707] INFO stream-client [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514] State transition from PENDING_SHUTDOWN to NOT_RUNNING (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:39:43,708] INFO stream-client [_confluent-controlcenter-7-4-1-1-7115cef0-0c58-4d55-8267-9dbbd81b5514] Streams client stopped completely (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:39:43,708] INFO Successfully closed Kafka Streams (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                     | [2023-08-04 10:39:43,708] INFO Shutting down Kafka Streams (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                     | [2023-08-04 10:39:43,710] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:39:43,719] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,720] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,721] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71] Shutting down 1 stream threads (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:39:43,795] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Shutting down clean (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,839] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] task [0_0] Suspended from RUNNING (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:43,845] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:43,894] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] stream-task [0_0] Closing record collector clean (org.apache.kafka.streams.processor.internals.RecordCollectorImpl)
control-center                     | [2023-08-04 10:39:43,894] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] task [0_0] Closed clean (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                     | [2023-08-04 10:39:43,894] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:43,901] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,901] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,901] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,902] INFO App info kafka.producer for _confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,902] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                     | [2023-08-04 10:39:43,903] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,904] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,904] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,930] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,930] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,931] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,931] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,952] INFO App info kafka.consumer for _confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1-restore-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,963] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,972] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-StreamThread-1] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                     | [2023-08-04 10:39:43,979] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71] Shutdown 1 stream threads complete (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:39:43,981] INFO App info kafka.admin.client for _confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71-admin unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:43,988] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,989] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,990] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,994] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,995] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,996] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:43,997] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71] State transition from PENDING_SHUTDOWN to NOT_RUNNING (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:39:43,998] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71] Streams client stopped completely (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:39:43,998] INFO [Producer clientId=c3-command] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:43,998] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-1b232679-b755-411f-9a4d-31973ee6bb71] Streams client is already in the terminal NOT_RUNNING state, all resources are closed and the client has stopped. (org.apache.kafka.streams.KafkaStreams)
control-center                     | [2023-08-04 10:39:43,998] INFO [Producer clientId=c3-command] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                     | [2023-08-04 10:39:44,021] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:44,027] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:44,027] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:44,027] INFO App info kafka.producer for c3-command unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                     | [2023-08-04 10:39:44,023] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:44,030] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:44,030] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                     | [2023-08-04 10:39:44,030] INFO App info kafka.producer for c3-command unregistered (org.apache.kafka.common.utils.AppInfoParser)
data-agrigator-taskmanager-1 exited with code 0
data-agrigator-taskmanager-1 exited with code 143
control-center exited with code 0
control-center exited with code 143
ksqldb-server                      | [2023-08-04 10:39:45,236] INFO Server shutting down (io.confluent.ksql.rest.server.KsqlServerMain)
ksqldb-server                      | [2023-08-04 10:39:45,236] INFO ksqlDB shutdown called (io.confluent.ksql.rest.server.KsqlRestApplication)
ksqldb-server                      | [2023-08-04 10:39:45,391] INFO Closing command store (io.confluent.ksql.rest.server.computation.CommandRunner)
ksqldb-server                      | [2023-08-04 10:39:45,394] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
ksqldb-server                      | [2023-08-04 10:39:45,394] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
ksqldb-server                      | [2023-08-04 10:39:45,396] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
ksqldb-server                      | [2023-08-04 10:39:45,414] INFO App info kafka.consumer for consumer-null-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:39:45,417] INFO App info kafka.admin.client for adminclient-4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
ksqldb-server                      | [2023-08-04 10:39:45,431] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
ksqldb-server                      | [2023-08-04 10:39:45,431] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
ksqldb-server                      | [2023-08-04 10:39:45,431] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
ksqldb-server                      | [2023-08-04 10:39:45,487] INFO API server stopped (io.confluent.ksql.api.server.Server)
ksqldb-server                      | [2023-08-04 10:39:45,549] INFO ksqlDB shutdown complete (io.confluent.ksql.rest.server.KsqlRestApplication)
ksqldb-server exited with code 0
ksqldb-server exited with code 143
connect                            | [2023-08-04 10:39:46,990] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect)
connect                            | [2023-08-04 10:39:46,993] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer)
connect                            | [2023-08-04 10:39:47,021] INFO Stopped http_8083@19b4dbb0{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector)
connect                            | [2023-08-04 10:39:47,023] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session)
connect                            | [2023-08-04 10:39:47,050] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer)
connect                            | [2023-08-04 10:39:47,051] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Herder stopping (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
connect                            | [2023-08-04 10:39:47,052] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Stopping connectors and tasks that are still assigned to this worker. (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
connect                            | [2023-08-04 10:39:47,053] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Member connect-1-31527bae-23d9-47a6-b1a9-d2e5692b0443 sending LeaveGroup request to coordinator broker:29092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
connect                            | [2023-08-04 10:39:47,057] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
connect                            | [2023-08-04 10:39:47,059] WARN [Worker clientId=connect-1, groupId=compose-connect-group] Close timed out with 1 pending requests to coordinator, terminating client connections (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
connect                            | [2023-08-04 10:39:47,060] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,061] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,062] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,090] INFO App info kafka.connect for connect-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:39:47,092] INFO Stopping KafkaBasedLog for topic docker-connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
broker                             | [2023-08-04 10:39:47,094] INFO [GroupCoordinator 1]: Preparing to rebalance group compose-connect-group in state PreparingRebalance with old generation 1 (__consumer_offsets-37) (reason: Removing member connect-1-31527bae-23d9-47a6-b1a9-d2e5692b0443 on LeaveGroup; client reason: not provided) (kafka.coordinator.group.GroupCoordinator)
connect                            | [2023-08-04 10:39:47,095] INFO [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
broker                             | [2023-08-04 10:39:47,096] INFO [GroupCoordinator 1]: Group compose-connect-group with generation 2 is now empty (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:47,109] INFO [GroupCoordinator 1]: Member MemberMetadata(memberId=connect-1-31527bae-23d9-47a6-b1a9-d2e5692b0443, groupInstanceId=None, clientId=connect-1, clientHost=/192.168.160.8, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(sessioned, compatible, default)) has left group compose-connect-group through explicit `LeaveGroup`; client reason: not provided (kafka.coordinator.group.GroupCoordinator)
connect                            | [2023-08-04 10:39:47,112] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,112] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,113] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,113] INFO App info kafka.producer for producer-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:39:47,117] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
connect                            | [2023-08-04 10:39:47,119] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
connect                            | [2023-08-04 10:39:47,121] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,122] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,123] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,132] INFO App info kafka.consumer for consumer-compose-connect-group-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:39:47,132] INFO Stopped KafkaBasedLog for topic docker-connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
connect                            | [2023-08-04 10:39:47,132] INFO Closing KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
connect                            | [2023-08-04 10:39:47,133] INFO Stopping KafkaBasedLog for topic docker-connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)
connect                            | [2023-08-04 10:39:47,135] INFO [Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
connect                            | [2023-08-04 10:39:47,144] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,145] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,146] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,147] INFO App info kafka.producer for producer-3 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:39:47,148] INFO [Consumer clientId=consumer-compose-connect-group-3, groupId=compose-connect-group] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
connect                            | [2023-08-04 10:39:47,149] INFO [Consumer clientId=consumer-compose-connect-group-3, groupId=compose-connect-group] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
connect                            | [2023-08-04 10:39:47,149] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,149] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,150] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,157] INFO App info kafka.consumer for consumer-compose-connect-group-3 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:39:47,158] INFO Stopped KafkaBasedLog for topic docker-connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)
connect                            | [2023-08-04 10:39:47,158] INFO Closed KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
connect                            | [2023-08-04 10:39:47,158] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker)
connect                            | [2023-08-04 10:39:47,159] INFO Stopping KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore)
connect                            | [2023-08-04 10:39:47,160] INFO Stopping KafkaBasedLog for topic docker-connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
connect                            | [2023-08-04 10:39:47,167] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
connect                            | [2023-08-04 10:39:47,181] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,181] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,182] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,182] INFO App info kafka.producer for producer-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:39:47,183] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
connect                            | [2023-08-04 10:39:47,184] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
connect                            | [2023-08-04 10:39:47,184] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,184] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,185] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,195] INFO App info kafka.consumer for consumer-compose-connect-group-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:39:47,197] INFO Stopped KafkaBasedLog for topic docker-connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
connect                            | [2023-08-04 10:39:47,198] INFO Stopped KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore)
connect                            | [2023-08-04 10:39:47,198] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,198] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,199] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,200] INFO App info kafka.connect for connect:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:39:47,200] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker)
connect                            | [2023-08-04 10:39:47,206] INFO App info kafka.admin.client for adminclient-8 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect                            | [2023-08-04 10:39:47,213] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,216] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,218] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect                            | [2023-08-04 10:39:47,220] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
connect                            | [2023-08-04 10:39:47,222] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
connect                            | [2023-08-04 10:39:47,223] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect)
connect exited with code 0
connect exited with code 143
schema-registry                    | [2023-08-04 10:39:48,705] INFO Stopped NetworkTrafficServerConnector@115667d{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8081} (org.eclipse.jetty.server.AbstractConnector)
schema-registry                    | [2023-08-04 10:39:48,706] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session)
schema-registry                    | [2023-08-04 10:39:48,717] INFO Stopped o.e.j.s.ServletContextHandler@716e431d{/ws,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
schema-registry                    | [2023-08-04 10:39:48,783] INFO Stopped o.e.j.s.ServletContextHandler@7e744f43{/,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
schema-registry                    | [2023-08-04 10:39:48,799] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
schema-registry                    | [2023-08-04 10:39:48,799] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
schema-registry                    | [2023-08-04 10:39:48,800] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
schema-registry                    | [2023-08-04 10:39:48,801] INFO Shutting down schema registry (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
schema-registry                    | [2023-08-04 10:39:48,805] INFO [kafka-store-reader-thread-_schemas]: Shutting down (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
schema-registry                    | [2023-08-04 10:39:48,809] INFO [kafka-store-reader-thread-_schemas]: Stopped (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
schema-registry                    | [2023-08-04 10:39:48,813] INFO [kafka-store-reader-thread-_schemas]: Shutdown completed (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
schema-registry                    | [2023-08-04 10:39:48,814] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
schema-registry                    | [2023-08-04 10:39:48,814] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
schema-registry                    | [2023-08-04 10:39:48,816] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
schema-registry                    | [2023-08-04 10:39:48,816] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
schema-registry                    | [2023-08-04 10:39:48,816] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
schema-registry                    | [2023-08-04 10:39:48,845] INFO App info kafka.consumer for KafkaStore-reader-_schemas unregistered (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:39:48,846] INFO KafkaStoreReaderThread shutdown complete. (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
schema-registry                    | [2023-08-04 10:39:48,846] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
schema-registry                    | [2023-08-04 10:39:48,870] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
schema-registry                    | [2023-08-04 10:39:48,871] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
schema-registry                    | [2023-08-04 10:39:48,871] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
schema-registry                    | [2023-08-04 10:39:48,873] INFO App info kafka.producer for producer-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
schema-registry                    | [2023-08-04 10:39:48,874] INFO Kafka store producer shut down (io.confluent.kafka.schemaregistry.storage.KafkaStore)
schema-registry                    | [2023-08-04 10:39:48,874] INFO Kafka store shut down complete (io.confluent.kafka.schemaregistry.storage.KafkaStore)
schema-registry                    | [2023-08-04 10:39:48,874] INFO Shutting down MetadataEncoderService (io.confluent.kafka.schemaregistry.storage.encoder.MetadataEncoderService)
schema-registry                    | [2023-08-04 10:39:48,879] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Member sr-1-b7ca4d5f-3414-46c5-9779-24e8259cf145 sending LeaveGroup request to coordinator broker:29092 (id: 2147483646 rack: null) due to the consumer is being closed (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:39:48,883] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting generation and member id due to: consumer pro-actively leaving the group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:39:48,884] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: consumer pro-actively leaving the group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:39:48,885] WARN [Schema registry clientId=sr-1, groupId=schema-registry] Close timed out with 1 pending requests to coordinator, terminating client connections (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry                    | [2023-08-04 10:39:48,885] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
broker                             | [2023-08-04 10:39:48,884] INFO [GroupCoordinator 1]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 1 (__consumer_offsets-29) (reason: Removing member sr-1-b7ca4d5f-3414-46c5-9779-24e8259cf145 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:48,885] INFO [GroupCoordinator 1]: Group schema-registry with generation 2 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
schema-registry                    | [2023-08-04 10:39:48,886] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
schema-registry                    | [2023-08-04 10:39:48,886] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
broker                             | [2023-08-04 10:39:48,891] INFO [GroupCoordinator 1]: Member MemberMetadata(memberId=sr-1-b7ca4d5f-3414-46c5-9779-24e8259cf145, groupInstanceId=None, clientId=sr-1, clientHost=/192.168.160.6, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
schema-registry                    | [2023-08-04 10:39:48,905] INFO App info kafka.schema.registry for sr-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
schema-registry exited with code 0
schema-registry exited with code 143
broker                             | [2023-08-04 10:39:49,980] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
broker                             | [2023-08-04 10:39:50,004] INFO [BrokerServer id=1] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
broker                             | [2023-08-04 10:39:50,015] INFO [BrokerServer id=1] shutting down (kafka.server.BrokerServer)
broker                             | [2023-08-04 10:39:50,020] INFO [BrokerLifecycleManager id=1] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
broker                             | [2023-08-04 10:39:50,023] INFO [Controller 1] Unfenced broker 1 has requested and been granted a controlled shutdown. (org.apache.kafka.controller.BrokerHeartbeatManager)
broker                             | [2023-08-04 10:39:50,091] INFO [Controller 1] enterControlledShutdown[1]: changing 196 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:39:50,158] INFO [BrokerLifecycleManager id=1] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
broker                             | [2023-08-04 10:39:50,166] INFO [Broker id=1] Transitioning 196 partition(s) to local followers. (state.change.logger)
broker                             | [2023-08-04 10:39:50,179] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,180] INFO [Broker id=1] Follower __transaction_state-42 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,180] INFO [Broker id=1] Follower docker-connect-offsets-11 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,180] INFO [Broker id=1] Follower docker-connect-status-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,180] INFO [Broker id=1] Follower __consumer_offsets-13 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,180] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,180] INFO [Broker id=1] Follower __consumer_offsets-46 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,180] INFO [Broker id=1] Follower _confluent-metrics-10 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,181] INFO [Broker id=1] Follower __transaction_state-9 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,181] INFO [Broker id=1] Follower __transaction_state-26 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,181] INFO [Broker id=1] Follower __consumer_offsets-30 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,181] INFO [Broker id=1] Follower docker-connect-status-1 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,182] INFO [Broker id=1] Follower docker-connect-offsets-12 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,183] INFO [Broker id=1] Follower _confluent-metrics-11 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,183] INFO [Broker id=1] Follower __consumer_offsets-45 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,184] INFO [Broker id=1] Follower __consumer_offsets-12 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,184] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,184] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,184] INFO [Broker id=1] Follower __transaction_state-8 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,184] INFO [Broker id=1] Follower __transaction_state-41 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,185] INFO [Broker id=1] Follower __consumer_offsets-29 starts at leader epoch 1 from offset 2 with partition epoch 1 and high watermark 2. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,185] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,185] INFO [Broker id=1] Follower __transaction_state-25 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,185] INFO [Broker id=1] Follower __consumer_offsets-15 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,185] INFO [Broker id=1] Follower __consumer_offsets-48 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,185] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,185] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,185] INFO [Broker id=1] Follower __transaction_state-7 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,186] INFO [Broker id=1] Follower __transaction_state-40 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,186] INFO [Broker id=1] Follower docker-connect-offsets-9 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,186] INFO [Broker id=1] Follower __consumer_offsets-32 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,186] INFO [Broker id=1] Follower __transaction_state-24 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,186] INFO [Broker id=1] Follower docker-connect-offsets-10 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,186] INFO [Broker id=1] Follower __consumer_offsets-47 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,186] INFO [Broker id=1] Follower _confluent-ksql-default__command_topic-0 starts at leader epoch 1 from offset 2 with partition epoch 1 and high watermark 2. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,186] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-cluster-rekey-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,186] INFO [Broker id=1] Follower __consumer_offsets-14 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,187] INFO [Broker id=1] Follower __transaction_state-6 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,187] INFO [Broker id=1] Follower __transaction_state-39 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,187] INFO [Broker id=1] Follower __consumer_offsets-31 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,187] INFO [Broker id=1] Follower __transaction_state-23 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,187] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,187] INFO [Broker id=1] Follower __transaction_state-5 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,187] INFO [Broker id=1] Follower __transaction_state-38 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,188] INFO [Broker id=1] Follower docker-connect-offsets-7 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,188] INFO [Broker id=1] Follower __consumer_offsets-17 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,188] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,188] INFO [Broker id=1] Follower _confluent-metrics-6 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,188] INFO [Broker id=1] Follower __transaction_state-22 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,188] INFO [Broker id=1] Follower docker-connect-offsets-24 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,188] INFO [Broker id=1] Follower __consumer_offsets-1 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,188] INFO [Broker id=1] Follower __consumer_offsets-34 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,189] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,189] INFO [Broker id=1] Follower __consumer_offsets-16 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,189] INFO [Broker id=1] Follower _confluent-metrics-7 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,189] INFO [Broker id=1] Follower __transaction_state-4 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,189] INFO [Broker id=1] Follower __transaction_state-37 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,189] INFO [Broker id=1] Follower docker-connect-offsets-8 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,190] INFO [Broker id=1] Follower __consumer_offsets-49 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,190] INFO [Broker id=1] Follower __consumer_offsets-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,190] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,190] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,190] INFO [Broker id=1] Follower __transaction_state-21 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,190] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,191] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,191] INFO [Broker id=1] Follower __consumer_offsets-33 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,192] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,192] INFO [Broker id=1] Follower docker-connect-status-2 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,192] INFO [Broker id=1] Follower _confluent-metrics-8 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,192] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,193] INFO [Broker id=1] Follower __transaction_state-3 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,194] INFO [Broker id=1] Follower __transaction_state-36 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,194] INFO [Broker id=1] Follower docker-connect-offsets-5 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,194] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 starts at leader epoch 1 from offset 105 with partition epoch 1 and high watermark 105. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,194] INFO [Broker id=1] Follower __consumer_offsets-19 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,195] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 starts at leader epoch 1 from offset 105 with partition epoch 1 and high watermark 105. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,195] INFO [Broker id=1] Follower __transaction_state-20 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,195] INFO [Broker id=1] Follower docker-connect-offsets-22 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,196] INFO [Broker id=1] Follower __consumer_offsets-3 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,196] INFO [Broker id=1] Follower __consumer_offsets-36 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,196] INFO [Broker id=1] Follower docker-connect-status-3 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,196] INFO [Broker id=1] Follower _confluent-metrics-9 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,197] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 starts at leader epoch 1 from offset 105 with partition epoch 1 and high watermark 105. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,197] INFO [Broker id=1] Follower __transaction_state-2 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,197] INFO [Broker id=1] Follower __transaction_state-35 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,198] INFO [Broker id=1] Follower docker-connect-offsets-6 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,198] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,198] INFO [Broker id=1] Follower __consumer_offsets-18 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,199] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,200] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 starts at leader epoch 1 from offset 105 with partition epoch 1 and high watermark 105. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,200] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,200] INFO [Broker id=1] Follower __transaction_state-19 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,200] INFO [Broker id=1] Follower __consumer_offsets-35 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,200] INFO [Broker id=1] Follower docker-connect-offsets-23 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,201] INFO [Broker id=1] Follower docker-connect-status-4 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,201] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,201] INFO [Broker id=1] Follower __consumer_offsets-2 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,201] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 starts at leader epoch 1 from offset 210 with partition epoch 1 and high watermark 210. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,201] INFO [Broker id=1] Follower default_ksql_processing_log-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,201] INFO [Broker id=1] Follower __transaction_state-17 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,202] INFO [Broker id=1] Follower __transaction_state-34 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,202] INFO [Broker id=1] Follower docker-connect-offsets-3 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,202] INFO [Broker id=1] Follower __consumer_offsets-21 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,202] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,202] INFO [Broker id=1] Follower __transaction_state-1 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,203] INFO [Broker id=1] Follower __transaction_state-18 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,203] INFO [Broker id=1] Follower docker-connect-offsets-20 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,204] INFO [Broker id=1] Follower __consumer_offsets-5 starts at leader epoch 1 from offset 284 with partition epoch 1 and high watermark 284. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,205] INFO [Broker id=1] Follower __consumer_offsets-38 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,205] INFO [Broker id=1] Follower _confluent-metrics-2 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,206] INFO [Broker id=1] Follower __transaction_state-16 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,206] INFO [Broker id=1] Follower __transaction_state-49 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,208] INFO [Broker id=1] Follower docker-connect-offsets-4 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,209] INFO [Broker id=1] Follower __consumer_offsets-20 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,209] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,210] INFO [Broker id=1] Follower __transaction_state-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,210] INFO [Broker id=1] Follower __transaction_state-33 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,210] INFO [Broker id=1] Follower _confluent-metrics-3 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,211] INFO [Broker id=1] Follower __consumer_offsets-37 starts at leader epoch 1 from offset 2 with partition epoch 1 and high watermark 2. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,211] INFO [Broker id=1] Follower docker-connect-offsets-21 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,211] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,212] INFO [Broker id=1] Follower __consumer_offsets-4 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,212] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,212] INFO [Broker id=1] Follower __transaction_state-15 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,212] INFO [Broker id=1] Follower __transaction_state-48 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,213] INFO [Broker id=1] Follower docker-connect-offsets-17 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,213] INFO [Broker id=1] Follower __consumer_offsets-23 starts at leader epoch 1 from offset 2 with partition epoch 1 and high watermark 2. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,213] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,214] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,214] INFO [Broker id=1] Follower __transaction_state-32 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,214] INFO [Broker id=1] Follower docker-connect-offsets-1 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,215] INFO [Broker id=1] Follower docker-connect-offsets-18 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,216] INFO [Broker id=1] Follower __consumer_offsets-7 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,216] INFO [Broker id=1] Follower __consumer_offsets-40 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,217] INFO [Broker id=1] Follower _confluent-metrics-4 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,217] INFO [Broker id=1] Follower docker-connect-configs-0 starts at leader epoch 1 from offset 1 with partition epoch 1 and high watermark 1. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,217] INFO [Broker id=1] Follower __transaction_state-14 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,218] INFO [Broker id=1] Follower __transaction_state-47 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,218] INFO [Broker id=1] Follower docker-connect-offsets-2 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,218] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,218] INFO [Controller 1] The request from broker 1 to shut down has been granted since the lowest active offset 9223372036854775807 is now greater than the broker's controlled shutdown offset 4435. (org.apache.kafka.controller.BrokerHeartbeatManager)
broker                             | [2023-08-04 10:39:50,219] INFO [Broker id=1] Follower __consumer_offsets-22 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,219] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,219] INFO [Broker id=1] Follower __transaction_state-31 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,220] INFO [Broker id=1] Follower _confluent-metrics-5 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,220] INFO [Broker id=1] Follower __consumer_offsets-39 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,221] INFO [Broker id=1] Follower docker-connect-offsets-19 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,221] INFO [Broker id=1] Follower __consumer_offsets-6 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,221] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,222] INFO [Broker id=1] Follower __transaction_state-13 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,222] INFO [Broker id=1] Follower __transaction_state-46 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,223] INFO [Broker id=1] Follower docker-connect-offsets-15 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,224] INFO [Broker id=1] Follower __consumer_offsets-9 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,224] INFO [Broker id=1] Follower __consumer_offsets-42 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,225] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,225] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,226] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 starts at leader epoch 1 from offset 105 with partition epoch 1 and high watermark 105. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,226] INFO [Broker id=1] Follower __transaction_state-30 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,227] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,227] INFO [Broker id=1] Follower __consumer_offsets-26 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,228] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,228] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,228] INFO [Broker id=1] Follower _schemas-0 starts at leader epoch 1 from offset 2 with partition epoch 1 and high watermark 2. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,229] INFO [Broker id=1] Follower __transaction_state-12 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,229] INFO [Broker id=1] Follower __transaction_state-45 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,230] INFO [Broker id=1] Follower docker-connect-offsets-16 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,230] INFO [Broker id=1] Follower __consumer_offsets-41 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,230] INFO [Broker id=1] Follower __consumer_offsets-24 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,231] INFO [Broker id=1] Follower __transaction_state-29 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,232] INFO [Broker id=1] Follower docker-connect-offsets-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,232] INFO [Broker id=1] Follower __consumer_offsets-25 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,233] INFO [Broker id=1] Follower __consumer_offsets-8 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,233] INFO [Broker id=1] Follower _confluent-monitoring-0 starts at leader epoch 1 from offset 105 with partition epoch 1 and high watermark 105. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,234] INFO [Broker id=1] Follower __transaction_state-11 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,234] INFO [Broker id=1] Follower __transaction_state-44 starts at leader epoch 1 from offset 4 with partition epoch 1 and high watermark 4. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,235] INFO [Broker id=1] Follower docker-connect-offsets-13 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,235] INFO [Broker id=1] Follower __consumer_offsets-11 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,236] INFO [Broker id=1] Follower __consumer_offsets-44 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,236] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,237] INFO [Broker id=1] Follower __transaction_state-28 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,238] INFO [Broker id=1] Follower __consumer_offsets-28 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,239] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,240] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,240] INFO [Broker id=1] Follower _confluent-metrics-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,241] INFO [Broker id=1] Follower __transaction_state-10 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,241] INFO [Broker id=1] Follower __transaction_state-43 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,242] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,242] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,243] INFO [Broker id=1] Follower docker-connect-offsets-14 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,244] INFO [Broker id=1] Follower __consumer_offsets-43 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,246] INFO [Broker id=1] Follower __consumer_offsets-10 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,247] INFO [Broker id=1] Follower _confluent-command-0 starts at leader epoch 1 from offset 1 with partition epoch 1 and high watermark 1. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,249] INFO [Broker id=1] Follower __transaction_state-27 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,250] INFO [Broker id=1] Follower gate_of_word-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,250] INFO [Broker id=1] Follower __consumer_offsets-27 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,251] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,251] INFO [Broker id=1] Follower _confluent-metrics-1 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader epoch was 0. (state.change.logger)
broker                             | [2023-08-04 10:39:50,252] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, __transaction_state-42, docker-connect-status-0, docker-connect-offsets-11, __consumer_offsets-13, __consumer_offsets-46, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, _confluent-metrics-10, __transaction_state-9, __transaction_state-26, __consumer_offsets-30, docker-connect-status-1, __consumer_offsets-45, _confluent-metrics-11, docker-connect-offsets-12, __consumer_offsets-12, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0, __transaction_state-8, __transaction_state-41, __consumer_offsets-29, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0, __transaction_state-25, __consumer_offsets-15, __consumer_offsets-48, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, __transaction_state-7, __transaction_state-40, docker-connect-offsets-9, __consumer_offsets-32, __transaction_state-24, __consumer_offsets-47, docker-connect-offsets-10, __consumer_offsets-14, _confluent-controlcenter-7-4-1-1-cluster-rekey-0, _confluent-ksql-default__command_topic-0, __transaction_state-6, __transaction_state-39, __consumer_offsets-31, __transaction_state-23, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, __transaction_state-5, __transaction_state-38, docker-connect-offsets-7, __consumer_offsets-17, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, _confluent-metrics-6, __transaction_state-22, __consumer_offsets-1, docker-connect-offsets-24, __consumer_offsets-34, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0, __consumer_offsets-16, _confluent-metrics-7, __transaction_state-4, __transaction_state-37, __consumer_offsets-49, docker-connect-offsets-8, __consumer_offsets-0, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0, __transaction_state-21, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0, __consumer_offsets-33, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0, docker-connect-status-2, _confluent-metrics-8, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, __transaction_state-3, __transaction_state-36, docker-connect-offsets-5, __consumer_offsets-19, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, __transaction_state-20, __consumer_offsets-3, docker-connect-offsets-22, __consumer_offsets-36, docker-connect-status-3, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, _confluent-metrics-9, __transaction_state-2, __transaction_state-35, docker-connect-offsets-6, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0, __consumer_offsets-18, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, __transaction_state-19, __consumer_offsets-35, docker-connect-status-4, docker-connect-offsets-23, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0, __consumer_offsets-2, default_ksql_processing_log-0, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, __transaction_state-17, __transaction_state-34, docker-connect-offsets-3, __consumer_offsets-21, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, __transaction_state-1, __transaction_state-18, __consumer_offsets-5, docker-connect-offsets-20, __consumer_offsets-38, _confluent-metrics-2, __transaction_state-16, __transaction_state-49, docker-connect-offsets-4, __consumer_offsets-20, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, __transaction_state-0, __transaction_state-33, __consumer_offsets-37, _confluent-metrics-3, docker-connect-offsets-21, __consumer_offsets-4, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0, __transaction_state-15, __transaction_state-48, docker-connect-offsets-17, __consumer_offsets-23, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, __transaction_state-32, docker-connect-offsets-1, __consumer_offsets-7, docker-connect-offsets-18, __consumer_offsets-40, _confluent-metrics-4, docker-connect-configs-0, __transaction_state-14, __transaction_state-47, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, docker-connect-offsets-2, __consumer_offsets-22, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0, __transaction_state-31, __consumer_offsets-39, _confluent-metrics-5, docker-connect-offsets-19, __consumer_offsets-6, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0, __transaction_state-13, __transaction_state-46, docker-connect-offsets-15, __consumer_offsets-9, __consumer_offsets-42, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, __transaction_state-30, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, __consumer_offsets-26, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0, _schemas-0, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, __transaction_state-12, __transaction_state-45, __consumer_offsets-41, docker-connect-offsets-16, __consumer_offsets-24, __transaction_state-29, __consumer_offsets-25, docker-connect-offsets-0, __consumer_offsets-8, _confluent-monitoring-0, __transaction_state-11, __transaction_state-44, docker-connect-offsets-13, __consumer_offsets-11, __consumer_offsets-44, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, __transaction_state-28, __consumer_offsets-28, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, _confluent-metrics-0, __transaction_state-10, __transaction_state-43, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, __consumer_offsets-43, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0, docker-connect-offsets-14, __consumer_offsets-10, _confluent-command-0, __transaction_state-27, gate_of_word-0, __consumer_offsets-27, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, _confluent-metrics-1) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:39:50,254] INFO [ReplicaAlterLogDirsManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, __transaction_state-42, docker-connect-status-0, docker-connect-offsets-11, __consumer_offsets-13, __consumer_offsets-46, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, _confluent-metrics-10, __transaction_state-9, __transaction_state-26, __consumer_offsets-30, docker-connect-status-1, __consumer_offsets-45, _confluent-metrics-11, docker-connect-offsets-12, __consumer_offsets-12, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0, __transaction_state-8, __transaction_state-41, __consumer_offsets-29, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0, __transaction_state-25, __consumer_offsets-15, __consumer_offsets-48, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, __transaction_state-7, __transaction_state-40, docker-connect-offsets-9, __consumer_offsets-32, __transaction_state-24, __consumer_offsets-47, docker-connect-offsets-10, __consumer_offsets-14, _confluent-controlcenter-7-4-1-1-cluster-rekey-0, _confluent-ksql-default__command_topic-0, __transaction_state-6, __transaction_state-39, __consumer_offsets-31, __transaction_state-23, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, __transaction_state-5, __transaction_state-38, docker-connect-offsets-7, __consumer_offsets-17, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, _confluent-metrics-6, __transaction_state-22, __consumer_offsets-1, docker-connect-offsets-24, __consumer_offsets-34, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0, __consumer_offsets-16, _confluent-metrics-7, __transaction_state-4, __transaction_state-37, __consumer_offsets-49, docker-connect-offsets-8, __consumer_offsets-0, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0, __transaction_state-21, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0, __consumer_offsets-33, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0, docker-connect-status-2, _confluent-metrics-8, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, __transaction_state-3, __transaction_state-36, docker-connect-offsets-5, __consumer_offsets-19, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, __transaction_state-20, __consumer_offsets-3, docker-connect-offsets-22, __consumer_offsets-36, docker-connect-status-3, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, _confluent-metrics-9, __transaction_state-2, __transaction_state-35, docker-connect-offsets-6, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0, __consumer_offsets-18, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, __transaction_state-19, __consumer_offsets-35, docker-connect-status-4, docker-connect-offsets-23, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0, __consumer_offsets-2, default_ksql_processing_log-0, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, __transaction_state-17, __transaction_state-34, docker-connect-offsets-3, __consumer_offsets-21, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, __transaction_state-1, __transaction_state-18, __consumer_offsets-5, docker-connect-offsets-20, __consumer_offsets-38, _confluent-metrics-2, __transaction_state-16, __transaction_state-49, docker-connect-offsets-4, __consumer_offsets-20, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, __transaction_state-0, __transaction_state-33, __consumer_offsets-37, _confluent-metrics-3, docker-connect-offsets-21, __consumer_offsets-4, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0, __transaction_state-15, __transaction_state-48, docker-connect-offsets-17, __consumer_offsets-23, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, __transaction_state-32, docker-connect-offsets-1, __consumer_offsets-7, docker-connect-offsets-18, __consumer_offsets-40, _confluent-metrics-4, docker-connect-configs-0, __transaction_state-14, __transaction_state-47, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, docker-connect-offsets-2, __consumer_offsets-22, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0, __transaction_state-31, __consumer_offsets-39, _confluent-metrics-5, docker-connect-offsets-19, __consumer_offsets-6, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0, __transaction_state-13, __transaction_state-46, docker-connect-offsets-15, __consumer_offsets-9, __consumer_offsets-42, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, __transaction_state-30, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, __consumer_offsets-26, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0, _schemas-0, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, __transaction_state-12, __transaction_state-45, __consumer_offsets-41, docker-connect-offsets-16, __consumer_offsets-24, __transaction_state-29, __consumer_offsets-25, docker-connect-offsets-0, __consumer_offsets-8, _confluent-monitoring-0, __transaction_state-11, __transaction_state-44, docker-connect-offsets-13, __consumer_offsets-11, __consumer_offsets-44, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, __transaction_state-28, __consumer_offsets-28, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, _confluent-metrics-0, __transaction_state-10, __transaction_state-43, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, __consumer_offsets-43, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0, docker-connect-offsets-14, __consumer_offsets-10, _confluent-command-0, __transaction_state-27, gate_of_word-0, __consumer_offsets-27, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, _confluent-metrics-1) (kafka.server.ReplicaAlterLogDirsManager)
broker                             | [2023-08-04 10:39:50,264] INFO [Broker id=1] Stopped fetchers as part of controlled shutdown for 196 partitions (state.change.logger)
broker                             | [2023-08-04 10:39:50,267] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 13 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,270] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,284] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-13 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,284] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 46 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,285] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,287] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 9 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,289] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,289] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-46 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,291] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 42 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,292] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,292] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-9 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,295] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-42 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,296] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 21 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,296] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,297] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 17 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,299] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,299] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-21 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,301] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 30 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,302] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,302] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-17 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,303] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-30 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,304] INFO [Controller 1] handleBrokerFenced: changing 196 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
broker                             | [2023-08-04 10:39:50,304] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 26 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,305] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,306] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 5 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,307] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,307] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 38 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,308] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,308] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 1 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,309] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,307] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-26 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,315] INFO [GroupCoordinator 1]: Unloading group metadata for _confluent-controlcenter-7-4-1-1 with generation 2 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,315] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 34 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,316] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,317] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 16 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,317] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,318] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 45 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,319] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,319] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 12 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,320] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,324] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 41 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,324] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,324] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 24 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,325] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,325] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 20 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,325] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,326] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 49 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,326] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,326] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 0 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,326] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,327] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 29 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,327] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,327] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 25 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,329] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,330] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 8 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,330] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,331] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 37 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,331] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,332] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 4 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,333] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,334] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 33 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,338] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-5 for coordinator epoch Some(1). Removed 17 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,340] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-38 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,339] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,341] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 15 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,341] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,342] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 48 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,343] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,344] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 11 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,345] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,345] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 44 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,346] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,348] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 23 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,349] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,351] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 19 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,352] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,352] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-1 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,354] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 32 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,354] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,354] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-34 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,355] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 28 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,356] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,356] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 7 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,355] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-16 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,358] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,358] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 40 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,359] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,359] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-45 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,361] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-12 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,362] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-41 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,361] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 3 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,363] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,364] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 36 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,367] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,366] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-24 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,369] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 47 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,371] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,371] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-20 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,374] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-49 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,375] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-0 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,375] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 14 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,378] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,379] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 43 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,381] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,381] INFO [GroupCoordinator 1]: Unloading group metadata for schema-registry with generation 2 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,383] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 10 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,384] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,384] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-29 for coordinator epoch Some(1). Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,385] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 22 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,385] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,386] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 18 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,385] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-25 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,387] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-8 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,388] INFO [GroupCoordinator 1]: Unloading group metadata for compose-connect-group with generation 2 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,388] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,389] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 31 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,390] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,391] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-37 for coordinator epoch Some(1). Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,392] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-4 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,392] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 27 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,393] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,395] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 39 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,395] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,395] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 6 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,396] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,393] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-33 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,398] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-15 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,399] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 35 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,400] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,401] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-48 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,401] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 2 in epoch Some(1) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,402] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,406] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 42 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,407] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-11 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,424] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-44 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,430] INFO [BrokerLifecycleManager id=1] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
broker                             | [2023-08-04 10:39:50,450] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-42 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,464] INFO [BrokerLifecycleManager id=1] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
broker                             | [2023-08-04 10:39:50,465] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 13 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,465] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-13 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,465] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 46 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,465] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-46 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,466] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 17 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,466] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-17 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,466] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 34 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,466] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-34 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,456] INFO [GroupCoordinator 1]: Unloading group metadata for _confluent-controlcenter-7-4-1-1-command with generation 1 (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,466] INFO [BrokerLifecycleManager id=1] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
broker                             | [2023-08-04 10:39:50,466] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-23 for coordinator epoch Some(1). Removed 1 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,467] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-19 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,467] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-32 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,467] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-28 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,468] INFO [BrokerMetadataListener id=1] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
broker                             | [2023-08-04 10:39:50,478] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-7 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,479] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-40 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,479] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-3 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,480] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-36 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,480] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-47 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,481] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-14 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,481] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-43 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,481] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-10 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,482] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-22 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,482] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-18 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,482] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-31 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,483] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-27 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,483] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-39 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,483] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-6 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,484] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-35 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,484] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-2 for coordinator epoch Some(1). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,478] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 5 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,482] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Shutting down (kafka.server.BrokerToControllerRequestThread)
broker                             | [2023-08-04 10:39:50,493] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-5 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,499] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)
broker                             | [2023-08-04 10:39:50,499] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Stopped (kafka.server.BrokerToControllerRequestThread)
broker                             | [2023-08-04 10:39:50,494] INFO [SocketServer listenerType=BROKER, nodeId=1] Stopping socket server request processors (kafka.network.SocketServer)
broker                             | [2023-08-04 10:39:50,499] INFO [BrokerMetadataListener id=1] Not processing HandleCommitsEvent because the event queue is closed. (kafka.server.metadata.BrokerMetadataListener)
broker                             | [2023-08-04 10:39:50,511] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 38 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,511] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-38 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,512] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 9 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,512] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-9 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,512] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 26 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,512] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-26 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,512] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 30 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,512] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-30 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,512] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 1 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,512] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-1 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,513] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 18 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,513] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-18 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,513] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 22 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,513] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-22 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,513] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 12 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,513] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-12 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,513] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 45 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,513] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-45 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,514] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 16 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,514] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-16 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,514] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 49 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,514] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-49 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,514] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 4 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,514] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-4 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,514] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 37 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,514] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-37 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,514] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 8 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,514] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-8 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,515] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 41 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,515] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-41 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,515] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 29 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,515] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-29 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,515] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 0 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,515] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-0 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,515] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 33 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,515] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-33 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,516] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 21 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,516] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-21 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,516] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 25 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,516] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-25 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,516] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 11 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,516] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-11 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,516] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 44 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,517] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=1) for __transaction_state-44 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,547] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 15 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,548] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-15 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,549] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 48 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,551] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-48 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,575] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 3 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,579] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-3 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,580] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 36 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,584] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-36 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,585] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 7 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,586] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-7 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,586] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 40 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,588] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-40 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,588] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 28 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,590] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-28 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,592] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 32 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,593] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-32 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,593] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 20 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,594] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-20 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,595] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 24 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,596] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-24 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,596] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 10 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,596] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-10 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,597] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 43 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,597] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-43 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,598] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 14 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,598] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-14 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,599] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 47 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,599] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-47 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,600] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 2 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,600] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-2 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,601] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 35 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,602] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-35 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,602] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 6 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,603] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-6 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,603] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 39 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,604] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-39 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,604] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 27 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,604] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-27 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,605] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 31 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,605] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-31 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,606] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 19 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,606] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-19 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,607] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 23 at epoch Some(1) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,607] INFO [Transaction State Manager 1]: Unloaded transaction metadata TxnMetadataCacheEntry(coordinatorEpoch=0, numTransactionalEntries=0) for __transaction_state-23 on become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,614] INFO [Broker id=1] Transitioning 196 partition(s) to local followers. (state.change.logger)
broker                             | [2023-08-04 10:39:50,614] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,615] INFO [Broker id=1] Follower __transaction_state-42 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,616] INFO [Broker id=1] Follower docker-connect-offsets-11 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,617] INFO [Broker id=1] Follower docker-connect-status-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,617] INFO [Broker id=1] Follower __consumer_offsets-13 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,618] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,618] INFO [Broker id=1] Follower __consumer_offsets-46 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,619] INFO [Broker id=1] Follower _confluent-metrics-10 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,620] INFO [Broker id=1] Follower __transaction_state-9 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,620] INFO [Broker id=1] Follower __transaction_state-26 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,621] INFO [Broker id=1] Follower __consumer_offsets-30 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,623] INFO [Broker id=1] Follower docker-connect-status-1 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,628] INFO [Broker id=1] Follower docker-connect-offsets-12 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,629] INFO [Broker id=1] Follower _confluent-metrics-11 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,630] INFO [Broker id=1] Follower __consumer_offsets-45 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,635] INFO Broker to controller channel manager for heartbeat shutdown (kafka.server.BrokerToControllerChannelManagerImpl)
broker                             | [2023-08-04 10:39:50,636] INFO [Broker id=1] Follower __consumer_offsets-12 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,637] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,637] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,638] INFO [Broker id=1] Follower __transaction_state-8 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,638] INFO [Broker id=1] Follower __transaction_state-41 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,639] INFO [Broker id=1] Follower __consumer_offsets-29 starts at leader epoch 2 from offset 2 with partition epoch 2 and high watermark 2. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,639] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,640] INFO [Broker id=1] Follower __transaction_state-25 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,643] INFO [Broker id=1] Follower __consumer_offsets-15 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,647] INFO [Broker id=1] Follower __consumer_offsets-48 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,650] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,651] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,660] INFO [Broker id=1] Follower __transaction_state-7 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,663] INFO [Broker id=1] Follower __transaction_state-40 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,667] INFO [Broker id=1] Follower docker-connect-offsets-9 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,666] INFO [SocketServer listenerType=BROKER, nodeId=1] Stopped socket server request processors (kafka.network.SocketServer)
broker                             | [2023-08-04 10:39:50,671] INFO [Broker id=1] Follower __consumer_offsets-32 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,672] INFO [Broker id=1] Follower __transaction_state-24 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,673] INFO [data-plane Kafka Request Handler on Broker 1], shutting down (kafka.server.KafkaRequestHandlerPool)
broker                             | [2023-08-04 10:39:50,674] INFO [Broker id=1] Follower docker-connect-offsets-10 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,675] INFO [Broker id=1] Follower __consumer_offsets-47 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,676] INFO [Broker id=1] Follower _confluent-ksql-default__command_topic-0 starts at leader epoch 2 from offset 2 with partition epoch 2 and high watermark 2. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,681] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-cluster-rekey-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,682] INFO [Broker id=1] Follower __consumer_offsets-14 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,683] INFO [Broker id=1] Follower __transaction_state-6 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,684] INFO [Broker id=1] Follower __transaction_state-39 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,686] INFO [Broker id=1] Follower __consumer_offsets-31 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,687] INFO [Broker id=1] Follower __transaction_state-23 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,688] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,689] INFO [Broker id=1] Follower __transaction_state-5 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,690] INFO [Broker id=1] Follower __transaction_state-38 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,690] INFO [Broker id=1] Follower docker-connect-offsets-7 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,691] INFO [Broker id=1] Follower __consumer_offsets-17 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,691] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,691] INFO [Broker id=1] Follower _confluent-metrics-6 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,691] INFO [Broker id=1] Follower __transaction_state-22 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,692] INFO [Broker id=1] Follower docker-connect-offsets-24 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,692] INFO [Broker id=1] Follower __consumer_offsets-1 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,692] INFO [Broker id=1] Follower __consumer_offsets-34 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,692] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,693] INFO [Broker id=1] Follower __consumer_offsets-16 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,693] INFO [Broker id=1] Follower _confluent-metrics-7 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,694] INFO [Broker id=1] Follower __transaction_state-4 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,694] INFO [Broker id=1] Follower __transaction_state-37 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,694] INFO [Broker id=1] Follower docker-connect-offsets-8 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,696] INFO [Broker id=1] Follower __consumer_offsets-49 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,696] INFO [Broker id=1] Follower __consumer_offsets-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,696] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,697] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,697] INFO [Broker id=1] Follower __transaction_state-21 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,697] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,697] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,697] INFO [Broker id=1] Follower __consumer_offsets-33 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,698] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,698] INFO [Broker id=1] Follower docker-connect-status-2 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,698] INFO [Broker id=1] Follower _confluent-metrics-8 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,698] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,698] INFO [Broker id=1] Follower __transaction_state-3 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,699] INFO [Broker id=1] Follower __transaction_state-36 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,699] INFO [Broker id=1] Follower docker-connect-offsets-5 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,699] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 starts at leader epoch 2 from offset 105 with partition epoch 2 and high watermark 105. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,699] INFO [Broker id=1] Follower __consumer_offsets-19 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,699] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 starts at leader epoch 2 from offset 105 with partition epoch 2 and high watermark 105. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,700] INFO [Broker id=1] Follower __transaction_state-20 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,700] INFO [Broker id=1] Follower docker-connect-offsets-22 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,700] INFO [Broker id=1] Follower __consumer_offsets-3 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,700] INFO [Broker id=1] Follower __consumer_offsets-36 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,700] INFO [Broker id=1] Follower docker-connect-status-3 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,701] INFO [Broker id=1] Follower _confluent-metrics-9 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,701] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 starts at leader epoch 2 from offset 105 with partition epoch 2 and high watermark 105. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,701] INFO [Broker id=1] Follower __transaction_state-2 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,701] INFO [Broker id=1] Follower __transaction_state-35 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,701] INFO [Broker id=1] Follower docker-connect-offsets-6 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,702] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,702] INFO [Broker id=1] Follower __consumer_offsets-18 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,702] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,702] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 starts at leader epoch 2 from offset 105 with partition epoch 2 and high watermark 105. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,702] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,702] INFO [Broker id=1] Follower __transaction_state-19 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,703] INFO [Broker id=1] Follower __consumer_offsets-35 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,703] INFO [Broker id=1] Follower docker-connect-offsets-23 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,703] INFO [Broker id=1] Follower docker-connect-status-4 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,703] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,704] INFO [Broker id=1] Follower __consumer_offsets-2 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,704] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 starts at leader epoch 2 from offset 210 with partition epoch 2 and high watermark 210. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,704] INFO [Broker id=1] Follower default_ksql_processing_log-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,704] INFO [Broker id=1] Follower __transaction_state-17 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,704] INFO [Broker id=1] Follower __transaction_state-34 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,704] INFO [Broker id=1] Follower docker-connect-offsets-3 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,705] INFO [Broker id=1] Follower __consumer_offsets-21 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,705] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,705] INFO [Broker id=1] Follower __transaction_state-1 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,705] INFO [Broker id=1] Follower __transaction_state-18 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,705] INFO [Broker id=1] Follower docker-connect-offsets-20 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,705] INFO [Broker id=1] Follower __consumer_offsets-5 starts at leader epoch 2 from offset 284 with partition epoch 2 and high watermark 284. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,706] INFO [Broker id=1] Follower __consumer_offsets-38 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,706] INFO [Broker id=1] Follower _confluent-metrics-2 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,706] INFO [Broker id=1] Follower __transaction_state-16 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,706] INFO [data-plane Kafka Request Handler on Broker 1], shut down completely (kafka.server.KafkaRequestHandlerPool)
broker                             | [2023-08-04 10:39:50,708] INFO [ExpirationReaper-1-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,706] INFO [Broker id=1] Follower __transaction_state-49 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,713] INFO [Broker id=1] Follower docker-connect-offsets-4 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,713] INFO [ExpirationReaper-1-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,713] INFO [ExpirationReaper-1-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,715] INFO [Broker id=1] Follower __consumer_offsets-20 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,716] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,717] INFO [Broker id=1] Follower __transaction_state-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,717] INFO [Broker id=1] Follower __transaction_state-33 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,718] INFO [Broker id=1] Follower _confluent-metrics-3 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,720] INFO [KafkaApi-1] Shutdown complete. (kafka.server.KafkaApis)
broker                             | [2023-08-04 10:39:50,722] INFO [Broker id=1] Follower __consumer_offsets-37 starts at leader epoch 2 from offset 2 with partition epoch 2 and high watermark 2. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,723] INFO [Broker id=1] Follower docker-connect-offsets-21 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,724] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,733] INFO [Broker id=1] Follower __consumer_offsets-4 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,734] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,734] INFO [Broker id=1] Follower __transaction_state-15 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,734] INFO [Broker id=1] Follower __transaction_state-48 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,734] INFO [Broker id=1] Follower docker-connect-offsets-17 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,734] INFO [Broker id=1] Follower __consumer_offsets-23 starts at leader epoch 2 from offset 2 with partition epoch 2 and high watermark 2. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,735] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,735] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,736] INFO [Broker id=1] Follower __transaction_state-32 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,736] INFO [Broker id=1] Follower docker-connect-offsets-1 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,736] INFO [Broker id=1] Follower docker-connect-offsets-18 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,736] INFO [Broker id=1] Follower __consumer_offsets-7 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,736] INFO [Broker id=1] Follower __consumer_offsets-40 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,737] INFO [Broker id=1] Follower _confluent-metrics-4 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,737] INFO [Broker id=1] Follower docker-connect-configs-0 starts at leader epoch 2 from offset 1 with partition epoch 2 and high watermark 1. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,737] INFO [Broker id=1] Follower __transaction_state-14 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,737] INFO [Broker id=1] Follower __transaction_state-47 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,737] INFO [Broker id=1] Follower docker-connect-offsets-2 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,737] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,737] INFO [Broker id=1] Follower __consumer_offsets-22 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,738] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,738] INFO [Broker id=1] Follower __transaction_state-31 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,738] INFO [Broker id=1] Follower _confluent-metrics-5 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,739] INFO [Broker id=1] Follower __consumer_offsets-39 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,746] INFO [Broker id=1] Follower docker-connect-offsets-19 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,746] INFO [Broker id=1] Follower __consumer_offsets-6 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,747] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,747] INFO [Broker id=1] Follower __transaction_state-13 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,747] INFO [Broker id=1] Follower __transaction_state-46 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,748] INFO [Broker id=1] Follower docker-connect-offsets-15 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,748] INFO [Broker id=1] Follower __consumer_offsets-9 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,748] INFO [Broker id=1] Follower __consumer_offsets-42 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,748] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,748] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,749] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 starts at leader epoch 2 from offset 105 with partition epoch 2 and high watermark 105. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,749] INFO [Broker id=1] Follower __transaction_state-30 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,749] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,749] INFO [Broker id=1] Follower __consumer_offsets-26 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,749] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,750] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,750] INFO [Broker id=1] Follower _schemas-0 starts at leader epoch 2 from offset 2 with partition epoch 2 and high watermark 2. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,750] INFO [Broker id=1] Follower __transaction_state-12 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,750] INFO [Broker id=1] Follower __transaction_state-45 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,755] INFO [Broker id=1] Follower docker-connect-offsets-16 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,757] INFO [Broker id=1] Follower __consumer_offsets-41 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,758] INFO [Broker id=1] Follower __consumer_offsets-24 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,758] INFO [Broker id=1] Follower __transaction_state-29 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,758] INFO [Broker id=1] Follower docker-connect-offsets-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,761] INFO [Broker id=1] Follower __consumer_offsets-25 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,761] INFO [Broker id=1] Follower __consumer_offsets-8 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,762] INFO [Broker id=1] Follower _confluent-monitoring-0 starts at leader epoch 2 from offset 105 with partition epoch 2 and high watermark 105. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,762] INFO [Broker id=1] Follower __transaction_state-11 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,763] INFO [Broker id=1] Follower __transaction_state-44 starts at leader epoch 2 from offset 4 with partition epoch 2 and high watermark 4. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,763] INFO [Broker id=1] Follower docker-connect-offsets-13 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,764] INFO [Broker id=1] Follower __consumer_offsets-11 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,764] INFO [Broker id=1] Follower __consumer_offsets-44 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,765] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,765] INFO [Broker id=1] Follower __transaction_state-28 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,766] INFO [Broker id=1] Follower __consumer_offsets-28 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,766] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,769] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,770] INFO [Broker id=1] Follower _confluent-metrics-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,770] INFO [Broker id=1] Follower __transaction_state-10 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,770] INFO [Broker id=1] Follower __transaction_state-43 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,771] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,772] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,772] INFO [Broker id=1] Follower docker-connect-offsets-14 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,772] INFO [Broker id=1] Follower __consumer_offsets-43 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,773] INFO [Broker id=1] Follower __consumer_offsets-10 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,773] INFO [Broker id=1] Follower _confluent-command-0 starts at leader epoch 2 from offset 1 with partition epoch 2 and high watermark 1. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,774] INFO [Broker id=1] Follower __transaction_state-27 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,774] INFO [Broker id=1] Follower gate_of_word-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,775] INFO [Broker id=1] Follower __consumer_offsets-27 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,777] INFO [Broker id=1] Follower _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,777] INFO [Broker id=1] Follower _confluent-metrics-1 starts at leader epoch 2 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader epoch was 1. (state.change.logger)
broker                             | [2023-08-04 10:39:50,778] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, __transaction_state-42, docker-connect-status-0, docker-connect-offsets-11, __consumer_offsets-13, __consumer_offsets-46, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, _confluent-metrics-10, __transaction_state-9, __transaction_state-26, __consumer_offsets-30, docker-connect-status-1, __consumer_offsets-45, _confluent-metrics-11, docker-connect-offsets-12, __consumer_offsets-12, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0, __transaction_state-8, __transaction_state-41, __consumer_offsets-29, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0, __transaction_state-25, __consumer_offsets-15, __consumer_offsets-48, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, __transaction_state-7, __transaction_state-40, docker-connect-offsets-9, __consumer_offsets-32, __transaction_state-24, __consumer_offsets-47, docker-connect-offsets-10, __consumer_offsets-14, _confluent-controlcenter-7-4-1-1-cluster-rekey-0, _confluent-ksql-default__command_topic-0, __transaction_state-6, __transaction_state-39, __consumer_offsets-31, __transaction_state-23, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, __transaction_state-5, __transaction_state-38, docker-connect-offsets-7, __consumer_offsets-17, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, _confluent-metrics-6, __transaction_state-22, __consumer_offsets-1, docker-connect-offsets-24, __consumer_offsets-34, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0, __consumer_offsets-16, _confluent-metrics-7, __transaction_state-4, __transaction_state-37, __consumer_offsets-49, docker-connect-offsets-8, __consumer_offsets-0, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0, __transaction_state-21, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0, __consumer_offsets-33, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0, docker-connect-status-2, _confluent-metrics-8, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, __transaction_state-3, __transaction_state-36, docker-connect-offsets-5, __consumer_offsets-19, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, __transaction_state-20, __consumer_offsets-3, docker-connect-offsets-22, __consumer_offsets-36, docker-connect-status-3, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, _confluent-metrics-9, __transaction_state-2, __transaction_state-35, docker-connect-offsets-6, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0, __consumer_offsets-18, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, __transaction_state-19, __consumer_offsets-35, docker-connect-status-4, docker-connect-offsets-23, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0, __consumer_offsets-2, default_ksql_processing_log-0, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, __transaction_state-17, __transaction_state-34, docker-connect-offsets-3, __consumer_offsets-21, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, __transaction_state-1, __transaction_state-18, __consumer_offsets-5, docker-connect-offsets-20, __consumer_offsets-38, _confluent-metrics-2, __transaction_state-16, __transaction_state-49, docker-connect-offsets-4, __consumer_offsets-20, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, __transaction_state-0, __transaction_state-33, __consumer_offsets-37, _confluent-metrics-3, docker-connect-offsets-21, __consumer_offsets-4, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0, __transaction_state-15, __transaction_state-48, docker-connect-offsets-17, __consumer_offsets-23, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, __transaction_state-32, docker-connect-offsets-1, __consumer_offsets-7, docker-connect-offsets-18, __consumer_offsets-40, _confluent-metrics-4, docker-connect-configs-0, __transaction_state-14, __transaction_state-47, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, docker-connect-offsets-2, __consumer_offsets-22, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0, __transaction_state-31, __consumer_offsets-39, _confluent-metrics-5, docker-connect-offsets-19, __consumer_offsets-6, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0, __transaction_state-13, __transaction_state-46, docker-connect-offsets-15, __consumer_offsets-9, __consumer_offsets-42, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, __transaction_state-30, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, __consumer_offsets-26, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0, _schemas-0, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, __transaction_state-12, __transaction_state-45, __consumer_offsets-41, docker-connect-offsets-16, __consumer_offsets-24, __transaction_state-29, __consumer_offsets-25, docker-connect-offsets-0, __consumer_offsets-8, _confluent-monitoring-0, __transaction_state-11, __transaction_state-44, docker-connect-offsets-13, __consumer_offsets-11, __consumer_offsets-44, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, __transaction_state-28, __consumer_offsets-28, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, _confluent-metrics-0, __transaction_state-10, __transaction_state-43, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, __consumer_offsets-43, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0, docker-connect-offsets-14, __consumer_offsets-10, _confluent-command-0, __transaction_state-27, gate_of_word-0, __consumer_offsets-27, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, _confluent-metrics-1) (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:39:50,779] INFO [ReplicaAlterLogDirsManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, __transaction_state-42, docker-connect-status-0, docker-connect-offsets-11, __consumer_offsets-13, __consumer_offsets-46, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, _confluent-metrics-10, __transaction_state-9, __transaction_state-26, __consumer_offsets-30, docker-connect-status-1, __consumer_offsets-45, _confluent-metrics-11, docker-connect-offsets-12, __consumer_offsets-12, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0, __transaction_state-8, __transaction_state-41, __consumer_offsets-29, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0, __transaction_state-25, __consumer_offsets-15, __consumer_offsets-48, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, __transaction_state-7, __transaction_state-40, docker-connect-offsets-9, __consumer_offsets-32, __transaction_state-24, __consumer_offsets-47, docker-connect-offsets-10, __consumer_offsets-14, _confluent-controlcenter-7-4-1-1-cluster-rekey-0, _confluent-ksql-default__command_topic-0, __transaction_state-6, __transaction_state-39, __consumer_offsets-31, __transaction_state-23, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, __transaction_state-5, __transaction_state-38, docker-connect-offsets-7, __consumer_offsets-17, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, _confluent-metrics-6, __transaction_state-22, __consumer_offsets-1, docker-connect-offsets-24, __consumer_offsets-34, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0, __consumer_offsets-16, _confluent-metrics-7, __transaction_state-4, __transaction_state-37, __consumer_offsets-49, docker-connect-offsets-8, __consumer_offsets-0, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0, __transaction_state-21, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0, __consumer_offsets-33, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0, docker-connect-status-2, _confluent-metrics-8, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, __transaction_state-3, __transaction_state-36, docker-connect-offsets-5, __consumer_offsets-19, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, __transaction_state-20, __consumer_offsets-3, docker-connect-offsets-22, __consumer_offsets-36, docker-connect-status-3, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, _confluent-metrics-9, __transaction_state-2, __transaction_state-35, docker-connect-offsets-6, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0, __consumer_offsets-18, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, __transaction_state-19, __consumer_offsets-35, docker-connect-status-4, docker-connect-offsets-23, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0, __consumer_offsets-2, default_ksql_processing_log-0, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, __transaction_state-17, __transaction_state-34, docker-connect-offsets-3, __consumer_offsets-21, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, __transaction_state-1, __transaction_state-18, __consumer_offsets-5, docker-connect-offsets-20, __consumer_offsets-38, _confluent-metrics-2, __transaction_state-16, __transaction_state-49, docker-connect-offsets-4, __consumer_offsets-20, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, __transaction_state-0, __transaction_state-33, __consumer_offsets-37, _confluent-metrics-3, docker-connect-offsets-21, __consumer_offsets-4, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0, __transaction_state-15, __transaction_state-48, docker-connect-offsets-17, __consumer_offsets-23, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, __transaction_state-32, docker-connect-offsets-1, __consumer_offsets-7, docker-connect-offsets-18, __consumer_offsets-40, _confluent-metrics-4, docker-connect-configs-0, __transaction_state-14, __transaction_state-47, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, docker-connect-offsets-2, __consumer_offsets-22, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0, __transaction_state-31, __consumer_offsets-39, _confluent-metrics-5, docker-connect-offsets-19, __consumer_offsets-6, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0, __transaction_state-13, __transaction_state-46, docker-connect-offsets-15, __consumer_offsets-9, __consumer_offsets-42, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, __transaction_state-30, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, __consumer_offsets-26, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0, _schemas-0, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, __transaction_state-12, __transaction_state-45, __consumer_offsets-41, docker-connect-offsets-16, __consumer_offsets-24, __transaction_state-29, __consumer_offsets-25, docker-connect-offsets-0, __consumer_offsets-8, _confluent-monitoring-0, __transaction_state-11, __transaction_state-44, docker-connect-offsets-13, __consumer_offsets-11, __consumer_offsets-44, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, __transaction_state-28, __consumer_offsets-28, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, _confluent-metrics-0, __transaction_state-10, __transaction_state-43, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, __consumer_offsets-43, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0, docker-connect-offsets-14, __consumer_offsets-10, _confluent-command-0, __transaction_state-27, gate_of_word-0, __consumer_offsets-27, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, _confluent-metrics-1) (kafka.server.ReplicaAlterLogDirsManager)
broker                             | [2023-08-04 10:39:50,781] INFO [Broker id=1] Stopped fetchers as part of controlled shutdown for 196 partitions (state.change.logger)
broker                             | [2023-08-04 10:39:50,782] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 13 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,782] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,783] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-13 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,783] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 46 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,784] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,784] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-46 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,784] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 9 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,785] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,785] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-9 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,785] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 42 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,786] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,786] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-42 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,786] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 21 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,787] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,787] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-21 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,787] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 17 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,787] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,788] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-17 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,788] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 30 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,788] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,789] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-30 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,789] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 26 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,789] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,790] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-26 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,790] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 5 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,790] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,791] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-5 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,791] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 38 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,792] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,793] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-38 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,793] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 1 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,794] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,795] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-1 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,795] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 34 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,796] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,797] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-34 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,798] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 16 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,798] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,798] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-16 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,799] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 45 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,799] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,799] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-45 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,800] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 12 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,800] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,800] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-12 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,801] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 41 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,801] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,801] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-41 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,802] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 24 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,802] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,802] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-24 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,803] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 20 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,803] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,804] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-20 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,804] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 49 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,805] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,806] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-49 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,806] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 0 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,807] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,807] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-0 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,808] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 29 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,808] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,808] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-29 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,808] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 25 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,809] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,809] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-25 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,809] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 8 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,809] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,810] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-8 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,810] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 37 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,810] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,810] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 4 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,810] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,810] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 33 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,810] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,810] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 15 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,810] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,810] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 48 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,810] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,811] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 11 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,811] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,811] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 44 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,811] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,811] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-37 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,811] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-4 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,812] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-33 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,812] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-15 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,813] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-48 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,814] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-11 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,814] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-44 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,812] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 23 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,815] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,816] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-23 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,816] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 19 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,816] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,817] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 32 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,817] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,817] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 28 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,817] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,818] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 7 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,818] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,818] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 40 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,818] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,818] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 3 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,818] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,818] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 36 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,818] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,818] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 47 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,819] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,819] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 14 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,819] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,819] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 43 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,819] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,820] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 10 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,820] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,820] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 22 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,820] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,820] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 18 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,820] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,820] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 31 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,820] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,820] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 27 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,821] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,821] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 39 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,821] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,821] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 6 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,821] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,821] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 35 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,821] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,821] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 2 in epoch Some(2) (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,821] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,822] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 42 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,823] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-19 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,824] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-42 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,824] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 13 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,825] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-13 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,825] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 46 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,825] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-46 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,826] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 17 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,827] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-32 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,827] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-28 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,827] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-7 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,827] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-40 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,827] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-3 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,827] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-36 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,828] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-47 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,828] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-14 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,828] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-43 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,828] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-10 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,828] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-22 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,828] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-18 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,828] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-31 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,828] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-27 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,828] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-39 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,828] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-6 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,829] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-35 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,829] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-2 for coordinator epoch Some(2). Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
broker                             | [2023-08-04 10:39:50,829] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-17 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,829] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 34 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,829] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-34 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,829] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 5 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,829] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-5 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,829] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 38 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,829] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-38 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,829] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 9 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,830] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-9 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,830] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 26 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,830] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-26 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,830] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 30 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,830] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-30 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,830] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 1 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,830] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-1 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,830] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 18 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,831] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-18 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,831] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 22 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,833] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-22 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,834] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 12 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,834] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-12 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,834] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 45 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,835] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-45 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,835] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 16 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,835] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-16 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,836] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 49 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,836] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-49 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,836] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 4 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,836] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-4 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,837] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 37 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,837] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-37 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,837] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 8 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,838] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-8 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,838] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 41 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,838] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-41 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,840] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 29 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,840] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-29 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,841] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 0 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,841] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-0 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,842] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 33 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,843] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-33 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,844] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 21 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,844] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-21 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,846] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 25 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,846] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-25 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,846] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 11 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,846] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-11 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,846] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 44 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,846] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-44 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,847] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 15 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,847] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-15 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,847] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 48 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,847] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-48 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,847] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 3 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,847] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-3 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,847] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 36 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,847] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-36 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,847] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 7 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,847] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-7 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,847] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 40 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,847] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-40 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,847] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 28 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,848] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-28 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,848] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 32 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,848] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-32 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,848] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 20 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,848] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-20 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,848] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 24 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,848] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-24 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,848] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 10 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,848] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-10 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,849] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 43 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,849] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-43 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,849] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 14 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,849] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-14 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,849] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 47 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,849] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-47 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,849] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 2 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,850] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-2 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,850] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 35 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,850] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-35 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,850] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 6 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,850] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-6 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,850] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 39 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,850] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-39 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,850] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 27 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,850] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-27 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,850] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 31 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,850] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-31 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,850] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 19 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,851] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-19 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,851] INFO [TransactionCoordinator id=1] Resigned as the txn coordinator for partition 23 at epoch Some(2) (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,851] INFO [Transaction State Manager 1]: No cached transaction metadata found for __transaction_state-23 during become-follower transition (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,851] INFO [BrokerMetadataListener id=1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
broker                             | [2023-08-04 10:39:50,870] INFO [TransactionCoordinator id=1] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,873] INFO [Transaction State Manager 1]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
broker                             | [2023-08-04 10:39:50,873] INFO [Transaction Marker Channel Manager 1]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
broker                             | [2023-08-04 10:39:50,874] INFO [Transaction Marker Channel Manager 1]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
broker                             | [2023-08-04 10:39:50,875] INFO [Transaction Marker Channel Manager 1]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
broker                             | [2023-08-04 10:39:50,880] INFO [TransactionCoordinator id=1] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
broker                             | [2023-08-04 10:39:50,883] INFO [GroupCoordinator 1]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,885] INFO [ExpirationReaper-1-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,887] INFO [ExpirationReaper-1-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,887] INFO [ExpirationReaper-1-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,889] INFO [ExpirationReaper-1-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,890] INFO [ExpirationReaper-1-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,890] INFO [ExpirationReaper-1-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,892] INFO [GroupCoordinator 1]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
broker                             | [2023-08-04 10:39:50,896] INFO [ReplicaManager broker=1] Shutting down (kafka.server.ReplicaManager)
broker                             | [2023-08-04 10:39:50,897] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
broker                             | [2023-08-04 10:39:50,897] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
broker                             | [2023-08-04 10:39:50,898] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
broker                             | [2023-08-04 10:39:50,900] INFO [ReplicaFetcherManager on broker 1] shutting down (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:39:50,904] INFO [ReplicaFetcherManager on broker 1] shutdown completed (kafka.server.ReplicaFetcherManager)
broker                             | [2023-08-04 10:39:50,905] INFO [ReplicaAlterLogDirsManager on broker 1] shutting down (kafka.server.ReplicaAlterLogDirsManager)
broker                             | [2023-08-04 10:39:50,907] INFO [ReplicaAlterLogDirsManager on broker 1] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
broker                             | [2023-08-04 10:39:50,908] INFO [ExpirationReaper-1-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,908] INFO [ExpirationReaper-1-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,909] INFO [ExpirationReaper-1-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,910] INFO [ExpirationReaper-1-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,911] INFO [ExpirationReaper-1-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,911] INFO [ExpirationReaper-1-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,912] INFO [ExpirationReaper-1-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,917] INFO [ExpirationReaper-1-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,918] INFO [ExpirationReaper-1-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,918] INFO [ExpirationReaper-1-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,920] INFO [ExpirationReaper-1-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,920] INFO [ExpirationReaper-1-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:50,945] INFO [ReplicaManager broker=1] Shut down completely (kafka.server.ReplicaManager)
broker                             | [2023-08-04 10:39:50,946] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Shutting down (kafka.server.BrokerToControllerRequestThread)
broker                             | [2023-08-04 10:39:50,947] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Stopped (kafka.server.BrokerToControllerRequestThread)
broker                             | [2023-08-04 10:39:50,947] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)
broker                             | [2023-08-04 10:39:50,949] INFO Broker to controller channel manager for alterPartition shutdown (kafka.server.BrokerToControllerChannelManagerImpl)
broker                             | [2023-08-04 10:39:50,950] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread)
broker                             | [2023-08-04 10:39:50,950] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread)
broker                             | [2023-08-04 10:39:50,951] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)
broker                             | [2023-08-04 10:39:50,956] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl)
broker                             | [2023-08-04 10:39:50,959] INFO Shutting down. (kafka.log.LogManager)
broker                             | [2023-08-04 10:39:50,962] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
broker                             | [2023-08-04 10:39:50,963] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner)
broker                             | [2023-08-04 10:39:50,965] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner)
broker                             | [2023-08-04 10:39:50,965] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner)
broker                             | [2023-08-04 10:39:50,974] INFO [BrokerMetadataListener id=1] Not processing HandleCommitsEvent because the event queue is closed. (kafka.server.metadata.BrokerMetadataListener)
broker                             | [2023-08-04 10:39:51,175] INFO [ProducerStateManager partition=__consumer_offsets-23] Wrote producer snapshot at offset 2 with 0 producer ids in 8 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:51,196] INFO [ProducerStateManager partition=__transaction_state-44] Wrote producer snapshot at offset 4 with 0 producer ids in 4 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:51,297] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 2 with 0 producer ids in 4 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:51,337] INFO [ProducerStateManager partition=__consumer_offsets-37] Wrote producer snapshot at offset 2 with 0 producer ids in 5 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:51,427] INFO [ProducerStateManager partition=_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0] Wrote producer snapshot at offset 105 with 1 producer ids in 7 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:51,468] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 2 with 0 producer ids in 4 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:51,474] INFO [BrokerMetadataListener id=1] Not processing HandleCommitsEvent because the event queue is closed. (kafka.server.metadata.BrokerMetadataListener)
broker                             | [2023-08-04 10:39:51,481] INFO [ProducerStateManager partition=_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0] Wrote producer snapshot at offset 105 with 1 producer ids in 7 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:51,491] INFO [ProducerStateManager partition=_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0] Wrote producer snapshot at offset 210 with 1 producer ids in 5 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:51,506] INFO [ProducerStateManager partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0] Wrote producer snapshot at offset 105 with 1 producer ids in 3 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:51,539] INFO [ProducerStateManager partition=__consumer_offsets-5] Wrote producer snapshot at offset 284 with 0 producer ids in 3 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:51,576] INFO [ProducerStateManager partition=docker-connect-configs-0] Wrote producer snapshot at offset 1 with 0 producer ids in 4 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:51,617] INFO [ProducerStateManager partition=_confluent-command-0] Wrote producer snapshot at offset 1 with 1 producer ids in 3 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:51,847] INFO [ProducerStateManager partition=_confluent-ksql-default__command_topic-0] Wrote producer snapshot at offset 2 with 1 producer ids in 3 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:51,915] INFO [ProducerStateManager partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0] Wrote producer snapshot at offset 105 with 1 producer ids in 4 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:51,927] INFO [ProducerStateManager partition=_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0] Wrote producer snapshot at offset 105 with 1 producer ids in 4 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:51,945] INFO [ProducerStateManager partition=_confluent-monitoring-0] Wrote producer snapshot at offset 105 with 1 producer ids in 3 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:51,973] INFO [BrokerMetadataListener id=1] Not processing HandleCommitsEvent because the event queue is closed. (kafka.server.metadata.BrokerMetadataListener)
broker                             | [2023-08-04 10:39:52,010] INFO Shutdown complete. (kafka.log.LogManager)
broker                             | [2023-08-04 10:39:52,012] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,017] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,017] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,018] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,019] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,019] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,019] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,020] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,020] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,021] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,021] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,022] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,025] INFO [SocketServer listenerType=BROKER, nodeId=1] Shutting down socket server (kafka.network.SocketServer)
broker                             | [2023-08-04 10:39:52,087] INFO [SocketServer listenerType=BROKER, nodeId=1] Shutdown completed (kafka.network.SocketServer)
broker                             | [2023-08-04 10:39:52,091] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
broker                             | [2023-08-04 10:39:52,092] INFO [BrokerLifecycleManager id=1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
broker                             | [2023-08-04 10:39:52,096] INFO [BrokerServer id=1] shut down completed (kafka.server.BrokerServer)
broker                             | [2023-08-04 10:39:52,096] INFO [BrokerServer id=1] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
broker                             | [2023-08-04 10:39:52,100] INFO [ControllerServer id=1] shutting down (kafka.server.ControllerServer)
broker                             | [2023-08-04 10:39:52,104] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:52,143] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:52,143] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:52,145] INFO [kafka-raft-io-thread]: Shutting down (kafka.raft.KafkaRaftManager$RaftIoThread)
broker                             | [2023-08-04 10:39:52,145] INFO [RaftManager nodeId=1] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
broker                             | [2023-08-04 10:39:52,148] INFO [RaftManager nodeId=1] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
broker                             | [2023-08-04 10:39:52,148] INFO [kafka-raft-io-thread]: Stopped (kafka.raft.KafkaRaftManager$RaftIoThread)
broker                             | [2023-08-04 10:39:52,150] INFO [kafka-raft-io-thread]: Completed graceful shutdown of RaftClient (kafka.raft.KafkaRaftManager$RaftIoThread)
broker                             | [2023-08-04 10:39:52,150] INFO [kafka-raft-io-thread]: Shutdown completed (kafka.raft.KafkaRaftManager$RaftIoThread)
broker                             | [2023-08-04 10:39:52,156] INFO [kafka-raft-outbound-request-thread]: Shutting down (kafka.raft.RaftSendThread)
broker                             | [2023-08-04 10:39:52,157] INFO [kafka-raft-outbound-request-thread]: Shutdown completed (kafka.raft.RaftSendThread)
broker                             | [2023-08-04 10:39:52,156] INFO [kafka-raft-outbound-request-thread]: Stopped (kafka.raft.RaftSendThread)
broker                             | [2023-08-04 10:39:52,162] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 4637 with 0 producer ids in 3 ms. (kafka.log.ProducerStateManager)
broker                             | [2023-08-04 10:39:52,169] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Stopping socket server request processors (kafka.network.SocketServer)
broker                             | [2023-08-04 10:39:52,179] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Stopped socket server request processors (kafka.network.SocketServer)
broker                             | [2023-08-04 10:39:52,183] INFO [Controller 1] QuorumController#beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
broker                             | [2023-08-04 10:39:52,184] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Shutting down socket server (kafka.network.SocketServer)
broker                             | [2023-08-04 10:39:52,187] ERROR [Controller 1] writeNoOpRecord: unable to start processing because of TimeoutException. (org.apache.kafka.controller.QuorumController)
broker                             | [2023-08-04 10:39:52,188] ERROR [Controller 1] maybeBalancePartitionLeaders: unable to start processing because of TimeoutException. (org.apache.kafka.controller.QuorumController)
broker                             | [2023-08-04 10:39:52,200] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Shutdown completed (kafka.network.SocketServer)
broker                             | [2023-08-04 10:39:52,202] INFO [data-plane Kafka Request Handler on Broker 1], shutting down (kafka.server.KafkaRequestHandlerPool)
broker                             | [2023-08-04 10:39:52,205] INFO [data-plane Kafka Request Handler on Broker 1], shut down completely (kafka.server.KafkaRequestHandlerPool)
broker                             | [2023-08-04 10:39:52,206] INFO [ExpirationReaper-1-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:52,208] INFO [ExpirationReaper-1-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:52,208] INFO [ExpirationReaper-1-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                             | [2023-08-04 10:39:52,210] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,211] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,211] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,211] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,212] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,213] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,213] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,213] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,213] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,214] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,214] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,214] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                             | [2023-08-04 10:39:52,214] INFO [Controller 1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
broker                             | [2023-08-04 10:39:52,221] INFO [SharedServer id=1] Stopping SharedServer (kafka.server.SharedServer)
broker                             | [2023-08-04 10:39:52,223] INFO [MetadataLoader 1] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
broker                             | [2023-08-04 10:39:52,223] INFO [SnapshotGenerator 1] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
broker                             | [2023-08-04 10:39:52,223] INFO [SnapshotGenerator 1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
broker                             | [2023-08-04 10:39:52,225] INFO [MetadataLoader 1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
broker                             | [2023-08-04 10:39:52,226] INFO [SnapshotGenerator 1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
broker                             | [2023-08-04 10:39:52,231] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
broker                             | [2023-08-04 10:39:52,232] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
broker                             | [2023-08-04 10:39:52,232] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
broker                             | [2023-08-04 10:39:52,234] INFO App info kafka.server for 1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
broker                             | [2023-08-04 10:39:52,236] INFO App info kafka.server for 1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
broker exited with code 0
