 Network data-agrigator_default  Creating
 Network data-agrigator_default  Created
 Container broker  Creating
 Container postgres_db  Creating
 Container data-agrigator-jobmanager-1  Creating
 Container data-agrigator-jobmanager-1  Created
 Container data-agrigator-taskmanager-1  Creating
 Container postgres_db  Created
 Container broker  Created
 Container schema-registry  Creating
 Container data-agrigator-taskmanager-1  Created
 Container schema-registry  Created
 Container control-center  Creating
 Container control-center  Created
Attaching to broker, control-center, data-agrigator-jobmanager-1, data-agrigator-taskmanager-1, postgres_db, schema-registry
postgres_db                   | 
postgres_db                   | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres_db                   | 
postgres_db                   | 
broker                        | ===> User
broker                        | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
broker                        | ===> Configuring ...
broker                        | Running in KRaft mode...
data-agrigator-jobmanager-1   | Starting Job Manager
postgres_db                   | 2023-08-04 11:54:43.878 UTC [1] LOG:  starting PostgreSQL 15.3 on x86_64-pc-linux-musl, compiled by gcc (Alpine 12.2.1_git20220924-r10) 12.2.1 20220924, 64-bit
postgres_db                   | 2023-08-04 11:54:43.882 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgres_db                   | 2023-08-04 11:54:43.882 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres_db                   | 2023-08-04 11:54:43.902 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres_db                   | 2023-08-04 11:54:43.993 UTC [24] LOG:  database system was shut down at 2023-08-04 11:09:04 UTC
postgres_db                   | 2023-08-04 11:54:44.132 UTC [1] LOG:  database system is ready to accept connections
schema-registry               | ===> User
schema-registry               | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
schema-registry               | ===> Configuring ...
data-agrigator-taskmanager-1  | Starting Task Manager
control-center                | ===> User
control-center                | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
control-center                | ===> Configuring ...
data-agrigator-jobmanager-1   | Starting standalonejob as a console application on host 1c15045e7060.
data-agrigator-taskmanager-1  | Starting taskexecutor as a console application on host dbe01c266f17.
control-center                | ===> Check if /etc/confluent-control-center is writable ...
control-center                | ===> Check if /var/lib/confluent-control-center is writable ...
broker                        | ===> Running preflight checks ... 
broker                        | ===> Check if /var/lib/kafka/data is writable ...
schema-registry               | ===> Running preflight checks ... 
schema-registry               | ===> Check if Kafka is healthy ...
broker                        | ===> Running in KRaft mode, skipping Zookeeper health check...
broker                        | ===> Using provided cluster id MkU3OEVBNTcwNTJENDM2Qk ...
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,090 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,133 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Preconfiguration: 
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,139 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - 
data-agrigator-jobmanager-1   | 
data-agrigator-jobmanager-1   | 
data-agrigator-jobmanager-1   | RESOURCE_PARAMS extraction logs:
data-agrigator-jobmanager-1   | jvm_params: -Xmx1073741824 -Xms1073741824 -XX:MaxMetaspaceSize=268435456
data-agrigator-jobmanager-1   | dynamic_configs: -D jobmanager.memory.off-heap.size=134217728b -D jobmanager.memory.jvm-overhead.min=201326592b -D jobmanager.memory.jvm-metaspace.size=268435456b -D jobmanager.memory.heap.size=1073741824b -D jobmanager.memory.jvm-overhead.max=201326592b
data-agrigator-jobmanager-1   | logs: WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.
data-agrigator-jobmanager-1   | INFO  [] - Loading configuration property: blob.server.port, 6124
data-agrigator-jobmanager-1   | INFO  [] - Loading configuration property: taskmanager.memory.process.size, 1728m
data-agrigator-jobmanager-1   | INFO  [] - Loading configuration property: taskmanager.bind-host, 0.0.0.0
data-agrigator-jobmanager-1   | INFO  [] - Loading configuration property: jobmanager.execution.failover-strategy, region
data-agrigator-jobmanager-1   | INFO  [] - Loading configuration property: jobmanager.rpc.address, jobmanager
data-agrigator-jobmanager-1   | INFO  [] - Loading configuration property: jobmanager.memory.process.size, 1600m
data-agrigator-jobmanager-1   | INFO  [] - Loading configuration property: jobmanager.rpc.port, 6123
data-agrigator-jobmanager-1   | INFO  [] - Loading configuration property: query.server.port, 6125
data-agrigator-jobmanager-1   | INFO  [] - Loading configuration property: rest.bind-address, 0.0.0.0
data-agrigator-jobmanager-1   | INFO  [] - Loading configuration property: jobmanager.bind-host, 0.0.0.0
data-agrigator-jobmanager-1   | INFO  [] - Loading configuration property: parallelism.default, 2
data-agrigator-jobmanager-1   | INFO  [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
data-agrigator-jobmanager-1   | INFO  [] - Loading configuration property: rest.address, 0.0.0.0
data-agrigator-jobmanager-1   | INFO  [] - The derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
data-agrigator-jobmanager-1   | INFO  [] - Final Master Memory configuration:
data-agrigator-jobmanager-1   | INFO  [] -   Total Process Memory: 1.563gb (1677721600 bytes)
data-agrigator-jobmanager-1   | INFO  [] -     Total Flink Memory: 1.125gb (1207959552 bytes)
data-agrigator-jobmanager-1   | INFO  [] -       JVM Heap:         1024.000mb (1073741824 bytes)
data-agrigator-jobmanager-1   | INFO  [] -       Off-heap:         128.000mb (134217728 bytes)
data-agrigator-jobmanager-1   | INFO  [] -     JVM Metaspace:      256.000mb (268435456 bytes)
data-agrigator-jobmanager-1   | INFO  [] -     JVM Overhead:       192.000mb (201326592 bytes)
data-agrigator-jobmanager-1   | 
data-agrigator-jobmanager-1   | 
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,142 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,153 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Starting StandaloneApplicationClusterEntryPoint (Version: 1.17.1, Scala: 2.12, Rev:2750d5c, Date:2023-05-19T10:45:46+02:00)
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,165 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  OS current user: flink
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,170 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Current Hadoop/Kerberos user: <no hadoop dependency found>
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,180 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM: OpenJDK 64-Bit Server VM - Eclipse Adoptium - 11/11.0.20+8
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,181 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Arch: amd64
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,182 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Maximum heap size: 1024 MiBytes
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,194 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JAVA_HOME: /opt/java/openjdk
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,195 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  No Hadoop Dependency available
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,203 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM Options:
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,207 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xmx1073741824
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,211 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xms1073741824
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,213 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:MaxMetaspaceSize=268435456
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,216 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog.file=/opt/flink/log/flink--standalonejob-0-1c15045e7060.log
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,222 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configuration=file:/opt/flink/conf/log4j-console.properties
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,223 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configurationFile=file:/opt/flink/conf/log4j-console.properties
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,225 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlogback.configurationFile=file:/opt/flink/conf/logback-console.xml
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,232 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Program Arguments:
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,256 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     --configDir
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,257 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     /opt/flink/conf
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,258 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,274 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.off-heap.size=134217728b
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,276 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,281 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.min=201326592b
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,288 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,293 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-metaspace.size=268435456b
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,296 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,298 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.heap.size=1073741824b
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,301 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,306 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.max=201326592b
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,308 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     --job-classname
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,311 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     com.example.sharov.anatoliy.DataStreamJob
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,324 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Classpath: /opt/flink/lib/flink-cep-1.17.1.jar:/opt/flink/lib/flink-connector-files-1.17.1.jar:/opt/flink/lib/flink-csv-1.17.1.jar:/opt/flink/lib/flink-json-1.17.1.jar:/opt/flink/lib/flink-scala_2.12-1.17.1.jar:/opt/flink/lib/flink-table-api-java-uber-1.17.1.jar:/opt/flink/lib/flink-table-planner-loader-1.17.1.jar:/opt/flink/lib/flink-table-runtime-1.17.1.jar:/opt/flink/lib/log4j-1.2-api-2.17.1.jar:/opt/flink/lib/log4j-api-2.17.1.jar:/opt/flink/lib/log4j-core-2.17.1.jar:/opt/flink/lib/log4j-slf4j-impl-2.17.1.jar:/opt/flink/lib/flink-dist-1.17.1.jar::::
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,328 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,364 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Registered UNIX signal handlers for [TERM, HUP, INT]
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,851 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: blob.server.port, 6124
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,860 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,869 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.bind-host, 0.0.0.0
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,871 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,874 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, jobmanager
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,880 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.process.size, 1600m
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,885 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,890 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: query.server.port, 6125
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,901 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.bind-address, 0.0.0.0
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,903 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.bind-host, 0.0.0.0
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,907 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 2
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,916 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,921 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.address, 0.0.0.0
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,932 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: jobmanager.memory.off-heap.size, 134217728b
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,934 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: jobmanager.memory.jvm-overhead.min, 201326592b
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,936 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: jobmanager.memory.jvm-metaspace.size, 268435456b
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,944 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: jobmanager.memory.heap.size, 1073741824b
data-agrigator-jobmanager-1   | 2023-08-04 11:55:31,948 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: jobmanager.memory.jvm-overhead.max, 201326592b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,161 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,249 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Preconfiguration: 
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,251 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - 
data-agrigator-taskmanager-1  | 
data-agrigator-taskmanager-1  | 
data-agrigator-taskmanager-1  | RESOURCE_PARAMS extraction logs:
data-agrigator-taskmanager-1  | jvm_params: -Xmx536870902 -Xms536870902 -XX:MaxDirectMemorySize=268435458 -XX:MaxMetaspaceSize=268435456
data-agrigator-taskmanager-1  | dynamic_configs: -D taskmanager.memory.network.min=134217730b -D taskmanager.cpu.cores=2.0 -D taskmanager.memory.task.off-heap.size=0b -D taskmanager.memory.jvm-metaspace.size=268435456b -D external-resources=none -D taskmanager.memory.jvm-overhead.min=201326592b -D taskmanager.memory.framework.off-heap.size=134217728b -D taskmanager.memory.network.max=134217730b -D taskmanager.memory.framework.heap.size=134217728b -D taskmanager.memory.managed.size=536870920b -D taskmanager.memory.task.heap.size=402653174b -D taskmanager.numberOfTaskSlots=2 -D taskmanager.memory.jvm-overhead.max=201326592b
data-agrigator-taskmanager-1  | logs: WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.
data-agrigator-taskmanager-1  | INFO  [] - Loading configuration property: blob.server.port, 6124
data-agrigator-taskmanager-1  | INFO  [] - Loading configuration property: taskmanager.memory.process.size, 1728m
data-agrigator-taskmanager-1  | INFO  [] - Loading configuration property: taskmanager.bind-host, 0.0.0.0
data-agrigator-taskmanager-1  | INFO  [] - Loading configuration property: jobmanager.execution.failover-strategy, region
data-agrigator-taskmanager-1  | INFO  [] - Loading configuration property: jobmanager.rpc.address, jobmanager
data-agrigator-taskmanager-1  | INFO  [] - Loading configuration property: jobmanager.memory.process.size, 1600m
data-agrigator-taskmanager-1  | INFO  [] - Loading configuration property: jobmanager.rpc.port, 6123
data-agrigator-taskmanager-1  | INFO  [] - Loading configuration property: query.server.port, 6125
data-agrigator-taskmanager-1  | INFO  [] - Loading configuration property: rest.bind-address, 0.0.0.0
data-agrigator-taskmanager-1  | INFO  [] - Loading configuration property: jobmanager.bind-host, 0.0.0.0
data-agrigator-taskmanager-1  | INFO  [] - Loading configuration property: parallelism.default, 2
data-agrigator-taskmanager-1  | INFO  [] - Loading configuration property: taskmanager.numberOfTaskSlots, 2
data-agrigator-taskmanager-1  | INFO  [] - Loading configuration property: rest.address, 0.0.0.0
data-agrigator-taskmanager-1  | INFO  [] - The derived from fraction jvm overhead memory (172.800mb (181193935 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
data-agrigator-taskmanager-1  | INFO  [] - Final TaskExecutor Memory configuration:
data-agrigator-taskmanager-1  | INFO  [] -   Total Process Memory:          1.688gb (1811939328 bytes)
data-agrigator-taskmanager-1  | INFO  [] -     Total Flink Memory:          1.250gb (1342177280 bytes)
data-agrigator-taskmanager-1  | INFO  [] -       Total JVM Heap Memory:     512.000mb (536870902 bytes)
data-agrigator-taskmanager-1  | INFO  [] -         Framework:               128.000mb (134217728 bytes)
data-agrigator-taskmanager-1  | INFO  [] -         Task:                    384.000mb (402653174 bytes)
data-agrigator-taskmanager-1  | INFO  [] -       Total Off-heap Memory:     768.000mb (805306378 bytes)
data-agrigator-taskmanager-1  | INFO  [] -         Managed:                 512.000mb (536870920 bytes)
data-agrigator-taskmanager-1  | INFO  [] -         Total JVM Direct Memory: 256.000mb (268435458 bytes)
data-agrigator-taskmanager-1  | INFO  [] -           Framework:             128.000mb (134217728 bytes)
data-agrigator-taskmanager-1  | INFO  [] -           Task:                  0 bytes
data-agrigator-taskmanager-1  | INFO  [] -           Network:               128.000mb (134217730 bytes)
data-agrigator-taskmanager-1  | INFO  [] -     JVM Metaspace:               256.000mb (268435456 bytes)
data-agrigator-taskmanager-1  | INFO  [] -     JVM Overhead:                192.000mb (201326592 bytes)
data-agrigator-taskmanager-1  | 
data-agrigator-taskmanager-1  | 
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,266 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,277 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Starting TaskManager (Version: 1.17.1, Scala: 2.12, Rev:2750d5c, Date:2023-05-19T10:45:46+02:00)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,282 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  OS current user: flink
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,291 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Current Hadoop/Kerberos user: <no hadoop dependency found>
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,296 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  JVM: OpenJDK 64-Bit Server VM - Eclipse Adoptium - 11/11.0.20+8
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,300 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Arch: amd64
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,304 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Maximum heap size: 512 MiBytes
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,317 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  JAVA_HOME: /opt/java/openjdk
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,320 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  No Hadoop Dependency available
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,326 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  JVM Options:
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,334 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -XX:+UseG1GC
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,336 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Xmx536870902
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,344 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Xms536870902
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,346 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -XX:MaxDirectMemorySize=268435458
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,349 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -XX:MaxMetaspaceSize=268435456
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,364 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlog.file=/opt/flink/log/flink--taskexecutor-0-dbe01c266f17.log
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,366 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlog4j.configuration=file:/opt/flink/conf/log4j-console.properties
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,368 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlog4j.configurationFile=file:/opt/flink/conf/log4j-console.properties
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,370 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlogback.configurationFile=file:/opt/flink/conf/logback-console.xml
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,378 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Program Arguments:
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,416 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --configDir
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,428 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     /opt/flink/conf
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,430 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,436 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.network.min=134217730b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,459 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,472 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.cpu.cores=2.0
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,481 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,493 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.task.off-heap.size=0b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,500 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,532 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.jvm-metaspace.size=268435456b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,543 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,548 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     external-resources=none
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,550 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,555 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.jvm-overhead.min=201326592b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,564 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,578 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.framework.off-heap.size=134217728b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,582 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,588 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.network.max=134217730b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,596 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,601 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.framework.heap.size=134217728b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,610 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,614 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.managed.size=536870920b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,615 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,617 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.task.heap.size=402653174b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,622 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,633 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.numberOfTaskSlots=2
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,643 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,645 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.jvm-overhead.max=201326592b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,646 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Classpath: /opt/flink/lib/flink-cep-1.17.1.jar:/opt/flink/lib/flink-connector-files-1.17.1.jar:/opt/flink/lib/flink-csv-1.17.1.jar:/opt/flink/lib/flink-json-1.17.1.jar:/opt/flink/lib/flink-scala_2.12-1.17.1.jar:/opt/flink/lib/flink-table-api-java-uber-1.17.1.jar:/opt/flink/lib/flink-table-planner-loader-1.17.1.jar:/opt/flink/lib/flink-table-runtime-1.17.1.jar:/opt/flink/lib/log4j-1.2-api-2.17.1.jar:/opt/flink/lib/log4j-api-2.17.1.jar:/opt/flink/lib/log4j-core-2.17.1.jar:/opt/flink/lib/log4j-slf4j-impl-2.17.1.jar:/opt/flink/lib/flink-dist-1.17.1.jar::::
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,654 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,690 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Registered UNIX signal handlers for [TERM, HUP, INT]
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,720 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Maximum number of open file descriptors is 1048576.
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,950 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: blob.server.port, 6124
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,953 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,967 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.bind-host, 0.0.0.0
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,976 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,981 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, jobmanager
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,986 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.process.size, 1600m
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,996 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
data-agrigator-taskmanager-1  | 2023-08-04 11:55:33,998 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: query.server.port, 6125
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,005 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.bind-address, 0.0.0.0
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,014 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.bind-host, 0.0.0.0
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,015 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 2
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,026 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 2
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,036 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.address, 0.0.0.0
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,038 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.network.min, 134217730b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,048 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.jvm-metaspace.size, 268435456b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,049 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.task.off-heap.size, 0b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,069 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.cpu.cores, 2.0
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,071 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: external-resources, none
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,082 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.jvm-overhead.min, 201326592b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,117 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.framework.off-heap.size, 134217728b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,122 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.network.max, 134217730b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,124 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.framework.heap.size, 134217728b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,128 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.managed.size, 536870920b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,136 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.task.heap.size, 402653174b
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,138 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.numberOfTaskSlots, 2
data-agrigator-taskmanager-1  | 2023-08-04 11:55:34,140 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.jvm-overhead.max, 201326592b
data-agrigator-jobmanager-1   | 2023-08-04 11:55:34,176 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Starting StandaloneApplicationClusterEntryPoint.
data-agrigator-jobmanager-1   | 2023-08-04 11:55:34,715 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install default filesystem.
data-agrigator-jobmanager-1   | 2023-08-04 11:55:34,817 INFO  org.apache.flink.core.fs.FileSystem                          [] - Hadoop is not in the classpath/dependencies. The extended set of supported File Systems via Hadoop is not available.
data-agrigator-taskmanager-1  | 2023-08-04 11:55:35,149 INFO  org.apache.flink.core.fs.FileSystem                          [] - Hadoop is not in the classpath/dependencies. The extended set of supported File Systems via Hadoop is not available.
data-agrigator-jobmanager-1   | 2023-08-04 11:55:35,383 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-statsd
data-agrigator-taskmanager-1  | 2023-08-04 11:55:35,439 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-statsd
data-agrigator-jobmanager-1   | 2023-08-04 11:55:35,486 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-datadog
data-agrigator-jobmanager-1   | 2023-08-04 11:55:35,488 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-influx
data-agrigator-jobmanager-1   | 2023-08-04 11:55:35,502 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-prometheus
data-agrigator-jobmanager-1   | 2023-08-04 11:55:35,505 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-jmx
data-agrigator-jobmanager-1   | 2023-08-04 11:55:35,509 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-graphite
data-agrigator-jobmanager-1   | 2023-08-04 11:55:35,511 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: external-resource-gpu
data-agrigator-jobmanager-1   | 2023-08-04 11:55:35,513 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-slf4j
data-agrigator-taskmanager-1  | 2023-08-04 11:55:35,538 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-datadog
data-agrigator-taskmanager-1  | 2023-08-04 11:55:35,556 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-influx
data-agrigator-taskmanager-1  | 2023-08-04 11:55:35,557 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-prometheus
data-agrigator-taskmanager-1  | 2023-08-04 11:55:35,577 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-jmx
data-agrigator-taskmanager-1  | 2023-08-04 11:55:35,584 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-graphite
data-agrigator-taskmanager-1  | 2023-08-04 11:55:35,601 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: external-resource-gpu
data-agrigator-taskmanager-1  | 2023-08-04 11:55:35,609 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-slf4j
data-agrigator-jobmanager-1   | 2023-08-04 11:55:36,002 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install security context.
data-agrigator-taskmanager-1  | 2023-08-04 11:55:36,142 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - StateChangelogStorageLoader initialized with shortcut names {memory,filesystem}.
data-agrigator-taskmanager-1  | 2023-08-04 11:55:36,143 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-statsd
data-agrigator-taskmanager-1  | 2023-08-04 11:55:36,144 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-datadog
data-agrigator-taskmanager-1  | 2023-08-04 11:55:36,145 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-influx
data-agrigator-taskmanager-1  | 2023-08-04 11:55:36,146 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-prometheus
data-agrigator-taskmanager-1  | 2023-08-04 11:55:36,147 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-jmx
data-agrigator-taskmanager-1  | 2023-08-04 11:55:36,148 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-graphite
data-agrigator-taskmanager-1  | 2023-08-04 11:55:36,149 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: external-resource-gpu
data-agrigator-taskmanager-1  | 2023-08-04 11:55:36,149 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-slf4j
data-agrigator-taskmanager-1  | 2023-08-04 11:55:36,169 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - StateChangelogStorageLoader initialized with shortcut names {memory,filesystem}.
data-agrigator-jobmanager-1   | 2023-08-04 11:55:36,566 INFO  org.apache.flink.runtime.security.modules.HadoopModuleFactory [] - Cannot create Hadoop Security Module because Hadoop cannot be found in the Classpath.
data-agrigator-taskmanager-1  | 2023-08-04 11:55:36,599 INFO  org.apache.flink.runtime.security.modules.HadoopModuleFactory [] - Cannot create Hadoop Security Module because Hadoop cannot be found in the Classpath.
data-agrigator-jobmanager-1   | 2023-08-04 11:55:36,731 INFO  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file will be created as /tmp/jaas-3046675101895124000.conf.
data-agrigator-jobmanager-1   | 2023-08-04 11:55:36,939 INFO  org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory [] - Cannot install HadoopSecurityContext because Hadoop cannot be found in the Classpath.
data-agrigator-taskmanager-1  | 2023-08-04 11:55:36,982 INFO  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file will be created as /tmp/jaas-18019855611278805244.conf.
data-agrigator-jobmanager-1   | 2023-08-04 11:55:37,004 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Initializing cluster services.
data-agrigator-taskmanager-1  | 2023-08-04 11:55:37,161 INFO  org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory [] - Cannot install HadoopSecurityContext because Hadoop cannot be found in the Classpath.
data-agrigator-jobmanager-1   | 2023-08-04 11:55:37,222 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Using working directory: WorkingDirectory(/tmp/jm_122a0f5539dfb4680be86c02507cca4f).
schema-registry               | [2023-08-04 11:55:42,332] INFO AdminClientConfig values: 
schema-registry               | 	auto.include.jmx.reporter = true
schema-registry               | 	bootstrap.servers = [broker:29092]
schema-registry               | 	client.dns.lookup = use_all_dns_ips
schema-registry               | 	client.id = 
schema-registry               | 	connections.max.idle.ms = 300000
schema-registry               | 	default.api.timeout.ms = 60000
schema-registry               | 	metadata.max.age.ms = 300000
schema-registry               | 	metric.reporters = []
schema-registry               | 	metrics.num.samples = 2
schema-registry               | 	metrics.recording.level = INFO
schema-registry               | 	metrics.sample.window.ms = 30000
schema-registry               | 	receive.buffer.bytes = 65536
schema-registry               | 	reconnect.backoff.max.ms = 1000
schema-registry               | 	reconnect.backoff.ms = 50
schema-registry               | 	request.timeout.ms = 30000
schema-registry               | 	retries = 2147483647
schema-registry               | 	retry.backoff.ms = 100
schema-registry               | 	sasl.client.callback.handler.class = null
schema-registry               | 	sasl.jaas.config = null
schema-registry               | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
schema-registry               | 	sasl.kerberos.min.time.before.relogin = 60000
schema-registry               | 	sasl.kerberos.service.name = null
schema-registry               | 	sasl.kerberos.ticket.renew.jitter = 0.05
schema-registry               | 	sasl.kerberos.ticket.renew.window.factor = 0.8
schema-registry               | 	sasl.login.callback.handler.class = null
schema-registry               | 	sasl.login.class = null
schema-registry               | 	sasl.login.connect.timeout.ms = null
schema-registry               | 	sasl.login.read.timeout.ms = null
schema-registry               | 	sasl.login.refresh.buffer.seconds = 300
schema-registry               | 	sasl.login.refresh.min.period.seconds = 60
schema-registry               | 	sasl.login.refresh.window.factor = 0.8
schema-registry               | 	sasl.login.refresh.window.jitter = 0.05
schema-registry               | 	sasl.login.retry.backoff.max.ms = 10000
schema-registry               | 	sasl.login.retry.backoff.ms = 100
schema-registry               | 	sasl.mechanism = GSSAPI
schema-registry               | 	sasl.oauthbearer.clock.skew.seconds = 30
schema-registry               | 	sasl.oauthbearer.expected.audience = null
schema-registry               | 	sasl.oauthbearer.expected.issuer = null
schema-registry               | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
schema-registry               | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
schema-registry               | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
schema-registry               | 	sasl.oauthbearer.jwks.endpoint.url = null
schema-registry               | 	sasl.oauthbearer.scope.claim.name = scope
schema-registry               | 	sasl.oauthbearer.sub.claim.name = sub
schema-registry               | 	sasl.oauthbearer.token.endpoint.url = null
schema-registry               | 	security.protocol = PLAINTEXT
schema-registry               | 	security.providers = null
schema-registry               | 	send.buffer.bytes = 131072
schema-registry               | 	socket.connection.setup.timeout.max.ms = 30000
schema-registry               | 	socket.connection.setup.timeout.ms = 10000
schema-registry               | 	ssl.cipher.suites = null
schema-registry               | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
schema-registry               | 	ssl.endpoint.identification.algorithm = https
schema-registry               | 	ssl.engine.factory.class = null
schema-registry               | 	ssl.key.password = null
schema-registry               | 	ssl.keymanager.algorithm = SunX509
schema-registry               | 	ssl.keystore.certificate.chain = null
schema-registry               | 	ssl.keystore.key = null
schema-registry               | 	ssl.keystore.location = null
schema-registry               | 	ssl.keystore.password = null
schema-registry               | 	ssl.keystore.type = JKS
schema-registry               | 	ssl.protocol = TLSv1.3
schema-registry               | 	ssl.provider = null
schema-registry               | 	ssl.secure.random.implementation = null
schema-registry               | 	ssl.trustmanager.algorithm = PKIX
schema-registry               | 	ssl.truststore.certificates = null
schema-registry               | 	ssl.truststore.location = null
schema-registry               | 	ssl.truststore.password = null
schema-registry               | 	ssl.truststore.type = JKS
schema-registry               |  (org.apache.kafka.clients.admin.AdminClientConfig)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:45,545 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils           [] - Trying to select the network interface and address to use by connecting to the leading JobManager.
data-agrigator-taskmanager-1  | 2023-08-04 11:55:45,546 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils           [] - TaskManager will try to connect for PT10S before falling back to heuristics
data-agrigator-jobmanager-1   | 2023-08-04 11:55:46,016 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address jobmanager:6123, bind address 0.0.0.0:6123.
schema-registry               | [2023-08-04 11:55:49,194] INFO Kafka version: 7.4.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
control-center                | ===> Running preflight checks ... 
schema-registry               | [2023-08-04 11:55:49,202] INFO Kafka commitId: fed9c006bfc7ba5b (org.apache.kafka.common.utils.AppInfoParser)
schema-registry               | [2023-08-04 11:55:49,215] INFO Kafka startTimeMs: 1691150149105 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | ===> Check if Kafka is healthy ...
schema-registry               | [2023-08-04 11:55:49,751] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:49,812] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:49,851] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:49,852] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:50,065] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:50,066] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:50,280] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:50,293] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,365 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to address jobmanager/172.21.0.4:6123
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,367 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [dbe01c266f17/172.21.0.5] with timeout [200] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,371 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,378 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,390 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/127.0.0.1] with timeout [50] due to: Invalid argument (connect failed)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,394 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [1000] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,404 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/127.0.0.1] with timeout [1000] due to: Invalid argument (connect failed)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,511 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to address jobmanager/172.21.0.4:6123
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,514 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [dbe01c266f17/172.21.0.5] with timeout [200] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,518 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,529 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,536 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/127.0.0.1] with timeout [50] due to: Invalid argument (connect failed)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,545 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [1000] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,552 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/127.0.0.1] with timeout [1000] due to: Invalid argument (connect failed)
schema-registry               | [2023-08-04 11:55:50,719] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:50,720] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,763 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to address jobmanager/172.21.0.4:6123
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,765 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [dbe01c266f17/172.21.0.5] with timeout [200] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,770 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,778 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,785 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/127.0.0.1] with timeout [50] due to: Invalid argument (connect failed)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,798 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [1000] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:50,814 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/127.0.0.1] with timeout [1000] due to: Invalid argument (connect failed)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:51,235 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to address jobmanager/172.21.0.4:6123
data-agrigator-taskmanager-1  | 2023-08-04 11:55:51,240 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [dbe01c266f17/172.21.0.5] with timeout [200] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:51,246 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:51,255 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:51,270 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/127.0.0.1] with timeout [50] due to: Invalid argument (connect failed)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:51,280 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [1000] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:51,285 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/127.0.0.1] with timeout [1000] due to: Invalid argument (connect failed)
schema-registry               | [2023-08-04 11:55:51,568] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:51,569] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:52,088 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to address jobmanager/172.21.0.4:6123
data-agrigator-taskmanager-1  | 2023-08-04 11:55:52,096 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [dbe01c266f17/172.21.0.5] with timeout [200] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:52,101 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:52,105 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:52,126 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/127.0.0.1] with timeout [50] due to: Invalid argument (connect failed)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:52,138 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [1000] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:52,144 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/127.0.0.1] with timeout [1000] due to: Invalid argument (connect failed)
schema-registry               | [2023-08-04 11:55:52,701] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:52,703] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:53,746 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to address jobmanager/172.21.0.4:6123
data-agrigator-taskmanager-1  | 2023-08-04 11:55:53,749 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [dbe01c266f17/172.21.0.5] with timeout [200] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:53,762 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:53,766 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [50] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:53,773 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/127.0.0.1] with timeout [50] due to: Invalid argument (connect failed)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:53,779 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/172.21.0.5] with timeout [1000] due to: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:53,781 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.21.0.4:6123] from local address [/127.0.0.1] with timeout [1000] due to: Invalid argument (connect failed)
schema-registry               | [2023-08-04 11:55:53,943] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:53,943] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:54,866] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:54,873] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker                        | ===> Launching ... 
broker                        | ===> Launching kafka ... 
data-agrigator-taskmanager-1  | 2023-08-04 11:55:55,552 WARN  org.apache.flink.runtime.net.ConnectionUtils                 [] - Could not connect to jobmanager/172.21.0.4:6123. Selecting a local address using heuristics.
data-agrigator-taskmanager-1  | 2023-08-04 11:55:55,555 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - TaskManager will use hostname/address 'dbe01c266f17' (172.21.0.5) for communication.
schema-registry               | [2023-08-04 11:55:55,919] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:55,920] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1  | 2023-08-04 11:55:55,988 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address 172.21.0.5:0, bind address 0.0.0.0:0.
schema-registry               | [2023-08-04 11:55:56,844] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:56,846] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:57,783] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:57,786] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1   | 2023-08-04 11:55:58,834 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
schema-registry               | [2023-08-04 11:55:59,011] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:55:59,012] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1   | 2023-08-04 11:55:59,260 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
data-agrigator-jobmanager-1   | 2023-08-04 11:55:59,262 INFO  akka.remote.Remoting                                         [] - Starting remoting
schema-registry               | [2023-08-04 11:56:00,230] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:00,241] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:01,162] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:01,171] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:01,252 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink@jobmanager:6123]
schema-registry               | [2023-08-04 11:56:02,214] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:02,214] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,630 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink@jobmanager:6123
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,747 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Loading delegation token providers
data-agrigator-taskmanager-1  | 2023-08-04 11:56:02,746 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,775 INFO  org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider [] - Hadoop FS is not available (not packaged with this application): NoClassDefFoundError : "org/apache/hadoop/conf/Configuration".
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,776 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Delegation token provider hadoopfs loaded and initialized
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,786 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Delegation token provider hbase loaded and initialized
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,787 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-statsd
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,789 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-datadog
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,791 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-influx
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,795 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-prometheus
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,796 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-jmx
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,810 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-graphite
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,811 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: external-resource-gpu
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,821 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-slf4j
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,831 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Delegation token providers loaded successfully
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,838 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Loading delegation token receivers
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,853 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receiver hadoopfs loaded and initialized
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,867 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receiver hbase loaded and initialized
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,871 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-statsd
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,883 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-datadog
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,887 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-influx
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,888 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-prometheus
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,888 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-jmx
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,890 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-graphite
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,890 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: external-resource-gpu
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,891 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-slf4j
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,893 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receivers loaded successfully
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,894 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Checking provider and receiver instances consistency
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,894 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Provider and receiver instances are consistent
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,895 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Obtaining delegation tokens
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,910 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Delegation tokens obtained successfully
data-agrigator-jobmanager-1   | 2023-08-04 11:56:02,913 WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - No tokens obtained so skipping notifications
data-agrigator-taskmanager-1  | 2023-08-04 11:56:02,996 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:03,026 INFO  akka.remote.Remoting                                         [] - Starting remoting
data-agrigator-jobmanager-1   | 2023-08-04 11:56:03,063 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Created BLOB server storage directory /tmp/jm_122a0f5539dfb4680be86c02507cca4f/blobStorage
data-agrigator-jobmanager-1   | 2023-08-04 11:56:03,086 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Started BLOB server at 0.0.0.0:6124 - max concurrent requests: 50 - max backlog: 1000
data-agrigator-jobmanager-1   | 2023-08-04 11:56:03,194 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:03,216 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address jobmanager:0, bind address 0.0.0.0:0.
schema-registry               | [2023-08-04 11:56:03,229] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:03,231] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:03,362 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
data-agrigator-jobmanager-1   | 2023-08-04 11:56:03,426 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:03,429 INFO  akka.remote.Remoting                                         [] - Starting remoting
data-agrigator-jobmanager-1   | 2023-08-04 11:56:03,612 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink-metrics@jobmanager:33559]
data-agrigator-jobmanager-1   | 2023-08-04 11:56:03,781 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink-metrics@jobmanager:33559
data-agrigator-jobmanager-1   | 2023-08-04 11:56:03,944 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
schema-registry               | [2023-08-04 11:56:04,245] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:04,246] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:04,555 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink@172.21.0.5:43917]
data-agrigator-jobmanager-1   | 2023-08-04 11:56:04,970 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Upload directory /tmp/flink-web-ba93d87e-c3f8-43ab-a1ae-6e5ab6848cc7/flink-web-upload does not exist. 
data-agrigator-jobmanager-1   | 2023-08-04 11:56:04,987 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Created directory /tmp/flink-web-ba93d87e-c3f8-43ab-a1ae-6e5ab6848cc7/flink-web-upload for file uploads.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:05,019 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Starting rest endpoint.
broker                        | [2023-08-04 11:56:05,031] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
schema-registry               | [2023-08-04 11:56:05,158] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:05,159] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:05,771 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink@172.21.0.5:43917
data-agrigator-taskmanager-1  | 2023-08-04 11:56:05,977 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Using working directory: WorkingDirectory(/tmp/tm_172.21.0.5:43917-648a7f)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:06,058 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
schema-registry               | [2023-08-04 11:56:06,074] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:06,074] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:06,094 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address 172.21.0.5:0, bind address 0.0.0.0:0.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:06,371 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
data-agrigator-taskmanager-1  | 2023-08-04 11:56:06,470 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:06,485 INFO  akka.remote.Remoting                                         [] - Starting remoting
data-agrigator-taskmanager-1  | 2023-08-04 11:56:06,617 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink-metrics@172.21.0.5:36931]
data-agrigator-taskmanager-1  | 2023-08-04 11:56:06,736 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink-metrics@172.21.0.5:36931
data-agrigator-taskmanager-1  | 2023-08-04 11:56:06,959 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService_172.21.0.5:43917-648a7f .
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,137 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Created BLOB cache storage directory /tmp/tm_172.21.0.5:43917-648a7f/blobStorage
schema-registry               | [2023-08-04 11:56:07,200] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:07,202] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,199 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Created BLOB cache storage directory /tmp/tm_172.21.0.5:43917-648a7f/blobStorage
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,240 INFO  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Enabled external resources: []
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,242 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Loading delegation token receivers
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,293 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receiver hadoopfs loaded and initialized
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,296 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receiver hbase loaded and initialized
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,297 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-statsd
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,300 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-datadog
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,301 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-influx
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,301 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-prometheus
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,302 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-jmx
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,325 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-graphite
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,326 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: external-resource-gpu
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,327 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-slf4j
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,328 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receivers loaded successfully
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,329 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Starting TaskManager with ResourceID: 172.21.0.5:43917-648a7f
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,607 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices    [] - Temporary file directory '/tmp': total 56 GB, usable 4 GB (7.14% usable)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,659 INFO  org.apache.flink.runtime.io.disk.iomanager.IOManager         [] - Created a new FileChannelManager for spilling of task related data to disk (joins, sorting, ...). Used directories:
data-agrigator-taskmanager-1  | 	/tmp/flink-io-5a012b4d-d5a1-407a-b8a0-bf9e4e3b3557
data-agrigator-taskmanager-1  | 2023-08-04 11:56:07,796 INFO  org.apache.flink.runtime.io.network.netty.NettyConfig        [] - NettyConfig [server address: /0.0.0.0, server port: 0, ssl enabled: false, memory segment size (bytes): 32768, transport type: AUTO, number of server threads: 2 (manual), number of client threads: 2 (manual), server connect backlog: 0 (use Netty's default), client connect timeout (sec): 120, send/receive buffer size (bytes): 0 (use Netty's default)]
data-agrigator-jobmanager-1   | 2023-08-04 11:56:07,872 INFO  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Determined location of main cluster component log file: /opt/flink/log/flink--standalonejob-0-1c15045e7060.log
data-agrigator-jobmanager-1   | 2023-08-04 11:56:07,873 INFO  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Determined location of main cluster component stdout file: /opt/flink/log/flink--standalonejob-0-1c15045e7060.out
schema-registry               | [2023-08-04 11:56:08,024] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:08,024] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker                        | [2023-08-04 11:56:08,543] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:09,009 INFO  org.apache.flink.runtime.io.network.NettyShuffleServiceFactory [] - Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
data-agrigator-taskmanager-1  | 	/tmp/flink-netty-shuffle-3c448a91-bda5-4602-8a6e-3ab77ef54fb4
data-agrigator-jobmanager-1   | 2023-08-04 11:56:09,108 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Rest endpoint listening at 0.0.0.0:8081
data-agrigator-jobmanager-1   | 2023-08-04 11:56:09,119 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - http://0.0.0.0:8081 was granted leadership with leaderSessionID=00000000-0000-0000-0000-000000000000
data-agrigator-jobmanager-1   | 2023-08-04 11:56:09,126 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Web frontend listening at http://0.0.0.0:8081.
schema-registry               | [2023-08-04 11:56:09,138] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:09,139] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:09,259 INFO  org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner [] - DefaultDispatcherRunner was granted leadership with leader id 00000000-0000-0000-0000-000000000000. Creating new DispatcherLeaderProcess.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:09,337 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:09,346 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Starting resource manager service.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:09,376 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is granted leadership with session id 00000000-0000-0000-0000-000000000000.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:09,447 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs that are not finished, yet.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:09,468 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:09,760 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_0 .
data-agrigator-jobmanager-1   | 2023-08-04 11:56:09,827 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .
data-agrigator-jobmanager-1   | 2023-08-04 11:56:10,132 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Starting the resource manager.
schema-registry               | [2023-08-04 11:56:10,250] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:10,253] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:10,337 INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool [] - Allocated 128 MB for network buffer pool (number of memory segments: 4096, bytes per segment: 32768).
data-agrigator-jobmanager-1   | 2023-08-04 11:56:10,394 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Starting tokens update task
data-agrigator-jobmanager-1   | 2023-08-04 11:56:10,403 WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - No tokens obtained so skipping notifications
data-agrigator-jobmanager-1   | 2023-08-04 11:56:10,405 WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Tokens update task not started because either no tokens obtained or none of the tokens specified its renewal date
data-agrigator-taskmanager-1  | 2023-08-04 11:56:10,505 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Starting the network environment and its components.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:10,511 INFO  org.apache.flink.client.ClientUtils                          [] - Starting program (detached: true)
broker                        | [2023-08-04 11:56:10,601] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
broker                        | [2023-08-04 11:56:10,622] INFO Starting controller (kafka.server.ControllerServer)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:11,211 WARN  org.apache.flink.connector.kafka.source.KafkaSourceBuilder   [] - Offset commit on checkpoint is disabled because group.id is not specified
schema-registry               | [2023-08-04 11:56:11,268] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:11,270] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:11,314 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Transport type 'auto': using EPOLL.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:11,337 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful initialization (took 821 ms).
data-agrigator-taskmanager-1  | 2023-08-04 11:56:11,416 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Transport type 'auto': using EPOLL.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:12,005 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful initialization (took 645 ms). Listening on SocketAddress /0.0.0.0:44103.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:12,013 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Starting the kvState service and its components.
schema-registry               | [2023-08-04 11:56:12,285] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:12,286] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1   | WARNING: An illegal reflective access operation has occurred
data-agrigator-jobmanager-1   | WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/opt/flink/lib/flink-dist-1.17.1.jar) to field java.lang.String.value
data-agrigator-jobmanager-1   | WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner
data-agrigator-jobmanager-1   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
data-agrigator-jobmanager-1   | WARNING: All illegal access operations will be denied in a future release
data-agrigator-jobmanager-1   | 
data-agrigator-jobmanager-1   | 
data-agrigator-taskmanager-1  | 2023-08-04 11:56:12,524 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
data-agrigator-taskmanager-1  | 2023-08-04 11:56:12,891 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:12,952 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - User file cache uses directory /tmp/flink-dist-cache-f90515ff-91c3-4b82-b5a6-7251f1fd0af9
data-agrigator-taskmanager-1  | 2023-08-04 11:56:13,061 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000).
schema-registry               | [2023-08-04 11:56:13,319] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:13,320] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:14,545] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:14,546] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:15,456] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:15,457] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:15,604 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Job fd25526942cf157358239c96d744dcfc is submitted.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:15,614 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Submitting Job with JobId=fd25526942cf157358239c96d744dcfc.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:15,808 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'Flink Java API Skeleton' (fd25526942cf157358239c96d744dcfc).
data-agrigator-jobmanager-1   | 2023-08-04 11:56:15,815 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'Flink Java API Skeleton' (fd25526942cf157358239c96d744dcfc).
data-agrigator-jobmanager-1   | 2023-08-04 11:56:16,043 INFO  org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner [] - JobMasterServiceLeadershipRunner for job fd25526942cf157358239c96d744dcfc was granted leadership with leader id 00000000-0000-0000-0000-000000000000. Creating new JobMasterServiceProcess.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:16,255 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_2 .
data-agrigator-jobmanager-1   | 2023-08-04 11:56:16,428 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'Flink Java API Skeleton' (fd25526942cf157358239c96d744dcfc).
schema-registry               | [2023-08-04 11:56:16,571] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:16,573] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker                        | [2023-08-04 11:56:16,585] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
broker                        | [2023-08-04 11:56:16,716] INFO Awaiting socket connections on broker:29093. (kafka.network.DataPlaneAcceptor)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:17,050 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy NoRestartBackoffTimeStrategy for Flink Java API Skeleton (fd25526942cf157358239c96d744dcfc).
data-agrigator-taskmanager-1  | 2023-08-04 11:56:17,303 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Resolved ResourceManager address, beginning registration
broker                        | [2023-08-04 11:56:17,379] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
broker                        | [2023-08-04 11:56:17,412] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)
schema-registry               | [2023-08-04 11:56:17,505] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:17,506] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:17,631 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Created execution graph 197f406d52d4ec7491425bba174cc621 for job fd25526942cf157358239c96d744dcfc.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:18,258 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job Flink Java API Skeleton (fd25526942cf157358239c96d744dcfc).
data-agrigator-jobmanager-1   | 2023-08-04 11:56:18,264 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 5 ms.
broker                        | [2023-08-04 11:56:18,375] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:56:18,381] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:56:18,386] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Producer state recovery took 5ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
schema-registry               | [2023-08-04 11:56:18,534] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:18,534] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker                        | [2023-08-04 11:56:18,771] INFO Initialized snapshots with IDs SortedSet() from /tmp/kraft-combined-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
broker                        | [2023-08-04 11:56:18,919] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
data-agrigator-taskmanager-1  | WARNING: An illegal reflective access operation has occurred
data-agrigator-taskmanager-1  | WARNING: Illegal reflective access by org.jboss.netty.util.internal.ByteBufferUtil (file:/tmp/flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar) to method java.nio.DirectByteBuffer.cleaner()
data-agrigator-taskmanager-1  | WARNING: Please consider reporting this to the maintainers of org.jboss.netty.util.internal.ByteBufferUtil
data-agrigator-taskmanager-1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
data-agrigator-taskmanager-1  | WARNING: All illegal access operations will be denied in a future release
data-agrigator-taskmanager-1  | 
data-agrigator-taskmanager-1  | 
schema-registry               | [2023-08-04 11:56:19,303] INFO [AdminClient clientId=adminclient-1] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
schema-registry               | org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: fetchMetadata
data-agrigator-jobmanager-1   | 2023-08-04 11:56:19,333 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 172.21.0.5:43917-648a7f (akka.tcp://flink@172.21.0.5:43917/user/rpc/taskmanager_0) at ResourceManager
data-agrigator-jobmanager-1   | 2023-08-04 11:56:19,420 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 2 new pipelined regions in 103 ms, total 2 pipelined regions currently.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:19,526 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 172.21.0.5:43917-648a7f (akka.tcp://flink@172.21.0.5:43917/user/rpc/taskmanager_0) at ResourceManager
data-agrigator-jobmanager-1   | 2023-08-04 11:56:19,537 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3e192edd
data-agrigator-jobmanager-1   | 2023-08-04 11:56:19,539 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
data-agrigator-jobmanager-1   | 2023-08-04 11:56:19,547 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Checkpoint storage is set to 'jobmanager'
data-agrigator-taskmanager-1  | 2023-08-04 11:56:19,604 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_* under registration id e1a23b47cc783010576c0a2d2c5062b3.
schema-registry               | [2023-08-04 11:56:19,624] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:19,625] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:19,952 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:20,072 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@578b6c74 for Flink Java API Skeleton (fd25526942cf157358239c96d744dcfc).
data-agrigator-jobmanager-1   | 2023-08-04 11:56:20,240 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'Flink Java API Skeleton' (fd25526942cf157358239c96d744dcfc) under job master id 00000000000000000000000000000000.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:20,351 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Starting split enumerator for source Source: Kafka Source.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:20,406 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
data-agrigator-jobmanager-1   | 2023-08-04 11:56:20,408 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Java API Skeleton (fd25526942cf157358239c96d744dcfc) switched from state CREATED to RUNNING.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:20,469 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2) (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from CREATED to SCHEDULED.
schema-registry               | [2023-08-04 11:56:20,751] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:20,752] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:20,770 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from CREATED to SCHEDULED.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:20,792 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:20,804 INFO  org.apache.kafka.clients.admin.AdminClientConfig             [] - AdminClientConfig values: 
data-agrigator-jobmanager-1   | 	bootstrap.servers = [localhost:9092]
data-agrigator-jobmanager-1   | 	client.dns.lookup = use_all_dns_ips
data-agrigator-jobmanager-1   | 	client.id = KafkaSource-525723299120177754-enumerator-admin-client
data-agrigator-jobmanager-1   | 	connections.max.idle.ms = 300000
data-agrigator-jobmanager-1   | 	default.api.timeout.ms = 60000
data-agrigator-jobmanager-1   | 	metadata.max.age.ms = 300000
data-agrigator-jobmanager-1   | 	metric.reporters = []
data-agrigator-jobmanager-1   | 	metrics.num.samples = 2
data-agrigator-jobmanager-1   | 	metrics.recording.level = INFO
data-agrigator-jobmanager-1   | 	metrics.sample.window.ms = 30000
data-agrigator-jobmanager-1   | 	receive.buffer.bytes = 65536
data-agrigator-jobmanager-1   | 	reconnect.backoff.max.ms = 1000
data-agrigator-jobmanager-1   | 	reconnect.backoff.ms = 50
data-agrigator-jobmanager-1   | 	request.timeout.ms = 30000
data-agrigator-jobmanager-1   | 	retries = 2147483647
data-agrigator-jobmanager-1   | 	retry.backoff.ms = 100
data-agrigator-jobmanager-1   | 	sasl.client.callback.handler.class = null
data-agrigator-jobmanager-1   | 	sasl.jaas.config = null
data-agrigator-jobmanager-1   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
data-agrigator-jobmanager-1   | 	sasl.kerberos.min.time.before.relogin = 60000
data-agrigator-jobmanager-1   | 	sasl.kerberos.service.name = null
data-agrigator-jobmanager-1   | 	sasl.kerberos.ticket.renew.jitter = 0.05
data-agrigator-jobmanager-1   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
data-agrigator-jobmanager-1   | 	sasl.login.callback.handler.class = null
data-agrigator-jobmanager-1   | 	sasl.login.class = null
data-agrigator-jobmanager-1   | 	sasl.login.connect.timeout.ms = null
data-agrigator-jobmanager-1   | 	sasl.login.read.timeout.ms = null
data-agrigator-jobmanager-1   | 	sasl.login.refresh.buffer.seconds = 300
data-agrigator-jobmanager-1   | 	sasl.login.refresh.min.period.seconds = 60
data-agrigator-jobmanager-1   | 	sasl.login.refresh.window.factor = 0.8
data-agrigator-jobmanager-1   | 	sasl.login.refresh.window.jitter = 0.05
data-agrigator-jobmanager-1   | 	sasl.login.retry.backoff.max.ms = 10000
data-agrigator-jobmanager-1   | 	sasl.login.retry.backoff.ms = 100
data-agrigator-jobmanager-1   | 	sasl.mechanism = GSSAPI
data-agrigator-jobmanager-1   | 	sasl.oauthbearer.clock.skew.seconds = 30
data-agrigator-jobmanager-1   | 	sasl.oauthbearer.expected.audience = null
data-agrigator-jobmanager-1   | 	sasl.oauthbearer.expected.issuer = null
data-agrigator-jobmanager-1   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
data-agrigator-jobmanager-1   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
data-agrigator-jobmanager-1   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
data-agrigator-jobmanager-1   | 	sasl.oauthbearer.jwks.endpoint.url = null
data-agrigator-jobmanager-1   | 	sasl.oauthbearer.scope.claim.name = scope
data-agrigator-jobmanager-1   | 	sasl.oauthbearer.sub.claim.name = sub
data-agrigator-jobmanager-1   | 	sasl.oauthbearer.token.endpoint.url = null
data-agrigator-jobmanager-1   | 	security.protocol = PLAINTEXT
data-agrigator-jobmanager-1   | 	security.providers = null
data-agrigator-jobmanager-1   | 	send.buffer.bytes = 131072
data-agrigator-jobmanager-1   | 	socket.connection.setup.timeout.max.ms = 30000
data-agrigator-jobmanager-1   | 	socket.connection.setup.timeout.ms = 10000
data-agrigator-jobmanager-1   | 	ssl.cipher.suites = null
data-agrigator-jobmanager-1   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
data-agrigator-jobmanager-1   | 	ssl.endpoint.identification.algorithm = https
data-agrigator-jobmanager-1   | 	ssl.engine.factory.class = null
data-agrigator-jobmanager-1   | 	ssl.key.password = null
data-agrigator-jobmanager-1   | 	ssl.keymanager.algorithm = SunX509
data-agrigator-jobmanager-1   | 	ssl.keystore.certificate.chain = null
data-agrigator-jobmanager-1   | 	ssl.keystore.key = null
data-agrigator-jobmanager-1   | 	ssl.keystore.location = null
data-agrigator-jobmanager-1   | 	ssl.keystore.password = null
data-agrigator-jobmanager-1   | 	ssl.keystore.type = JKS
data-agrigator-jobmanager-1   | 	ssl.protocol = TLSv1.3
data-agrigator-jobmanager-1   | 	ssl.provider = null
data-agrigator-jobmanager-1   | 	ssl.secure.random.implementation = null
data-agrigator-jobmanager-1   | 	ssl.trustmanager.algorithm = PKIX
data-agrigator-jobmanager-1   | 	ssl.truststore.certificates = null
data-agrigator-jobmanager-1   | 	ssl.truststore.location = null
data-agrigator-jobmanager-1   | 	ssl.truststore.password = null
data-agrigator-jobmanager-1   | 	ssl.truststore.type = JKS
data-agrigator-jobmanager-1   | 
data-agrigator-jobmanager-1   | 
data-agrigator-jobmanager-1   | 2023-08-04 11:56:20,888 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration
data-agrigator-jobmanager-1   | 2023-08-04 11:56:20,907 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager 00000000000000000000000000000000@akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job fd25526942cf157358239c96d744dcfc.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:20,971 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 00000000000000000000000000000000@akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job fd25526942cf157358239c96d744dcfc.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:21,002 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: 00000000000000000000000000000000.
broker                        | [2023-08-04 11:56:21,002] INFO [RaftManager nodeId=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1711) (org.apache.kafka.raft.QuorumState)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:21,021 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job fd25526942cf157358239c96d744dcfc: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
broker                        | [2023-08-04 11:56:21,035] INFO [RaftManager nodeId=1] Completed transition to CandidateState(localId=1, epoch=1, retries=1, electionTimeoutMs=1617) (org.apache.kafka.raft.QuorumState)
broker                        | [2023-08-04 11:56:21,085] INFO [RaftManager nodeId=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) (org.apache.kafka.raft.QuorumState)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:21,165 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 4f88966124ec24b5b23c12ceca13c33d for job fd25526942cf157358239c96d744dcfc from resource manager with leader id 00000000000000000000000000000000.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:21,197 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 4f88966124ec24b5b23c12ceca13c33d.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:21,202 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job fd25526942cf157358239c96d744dcfc for job leader monitoring.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:21,209 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 with leader id 00000000-0000-0000-0000-000000000000.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:21,241 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 467d7476e2537b93e00452b71c0c326a for job fd25526942cf157358239c96d744dcfc from resource manager with leader id 00000000000000000000000000000000.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:21,242 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 467d7476e2537b93e00452b71c0c326a.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:21,400 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
broker                        | [2023-08-04 11:56:21,418] INFO [kafka-raft-outbound-request-thread]: Starting (kafka.raft.RaftSendThread)
broker                        | [2023-08-04 11:56:21,424] INFO [kafka-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:21,715 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'key.deserializer' was supplied but isn't a known config.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:21,723 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'commit.offsets.on.checkpoint' was supplied but isn't a known config.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:21,724 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'value.deserializer' was supplied but isn't a known config.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:21,732 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'enable.auto.commit' was supplied but isn't a known config.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:21,735 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:21,736 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:21,737 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'auto.offset.reset' was supplied but isn't a known config.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:21,769 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.2.3
schema-registry               | [2023-08-04 11:56:21,772] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:21,776 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 50029d3ed8ba576f
data-agrigator-jobmanager-1   | 2023-08-04 11:56:21,778 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1691150181737
schema-registry               | [2023-08-04 11:56:21,773] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker                        | [2023-08-04 11:56:21,818] INFO [MetadataLoader 1] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:21,835 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Starting the KafkaSourceEnumerator for consumer group null without periodic partition discovery.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:21,851 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job fd25526942cf157358239c96d744dcfc.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:21,869 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job fd25526942cf157358239c96d744dcfc.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:21,894 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job fd25526942cf157358239c96d744dcfc.
broker                        | [2023-08-04 11:56:21,919] INFO [RaftManager nodeId=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)
broker                        | [2023-08-04 11:56:21,926] INFO [MetadataLoader 1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:21,998 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:22,019 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
broker                        | [2023-08-04 11:56:22,027] INFO [MetadataLoader 1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
broker                        | [2023-08-04 11:56:22,034] INFO [RaftManager nodeId=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1319043836 (org.apache.kafka.raft.KafkaRaftClient)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:22,038 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:22,038 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:22,069 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2) (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from SCHEDULED to DEPLOYING.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:22,078 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2) (attempt #0) with attempt id 197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_0_0 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_0 to 172.21.0.5:43917-648a7f @ data-agrigator-taskmanager-1.data-agrigator_default (dataPort=44103) with allocation id 4f88966124ec24b5b23c12ceca13c33d
data-agrigator-jobmanager-1   | 2023-08-04 11:56:22,122 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from SCHEDULED to DEPLOYING.
broker                        | [2023-08-04 11:56:22,131] INFO [MetadataLoader 1] handleCommit: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:22,137 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (attempt #0) with attempt id 197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_1_0 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_1 to 172.21.0.5:43917-648a7f @ data-agrigator-taskmanager-1.data-agrigator_default (dataPort=44103) with allocation id 467d7476e2537b93e00452b71c0c326a
broker                        | [2023-08-04 11:56:22,145] INFO [MetadataLoader 1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:22,156 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:22,170 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
broker                        | [2023-08-04 11:56:22,218] INFO [Controller 1] Creating new QuorumController with clusterId MkU3OEVBNTcwNTJENDM2Qg, authorizer Optional.empty. (org.apache.kafka.controller.QuorumController)
broker                        | [2023-08-04 11:56:22,219] INFO [RaftManager nodeId=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@451809690 (org.apache.kafka.raft.KafkaRaftClient)
broker                        | [2023-08-04 11:56:22,236] INFO [Controller 1] Becoming the active controller at epoch 1, committed offset -1, committed epoch -1 (org.apache.kafka.controller.QuorumController)
broker                        | [2023-08-04 11:56:22,251] INFO [MetadataLoader 1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:22,254 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 4f88966124ec24b5b23c12ceca13c33d.
broker                        | [2023-08-04 11:56:22,284] INFO [Controller 1] The metadata log appears to be empty. Appending 1 bootstrap record(s) at metadata.version 3.4-IV0 from the binary bootstrap metadata file: /tmp/kraft-combined-logs/bootstrap.checkpoint. (org.apache.kafka.controller.QuorumController)
broker                        | [2023-08-04 11:56:22,289] INFO [Controller 1] Setting metadata.version to 8 (org.apache.kafka.controller.FeatureControlManager)
broker                        | [2023-08-04 11:56:22,331] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                        | [2023-08-04 11:56:22,333] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                        | [2023-08-04 11:56:22,357] INFO [MetadataLoader 1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 2 (org.apache.kafka.image.loader.MetadataLoader)
broker                        | [2023-08-04 11:56:22,360] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                        | [2023-08-04 11:56:22,362] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:22,381 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:22,382 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:22,413 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
broker                        | [2023-08-04 11:56:22,447] INFO [MetadataLoader 1] handleCommit: The loader finished catching up to the current high water mark of 2 (org.apache.kafka.image.loader.MetadataLoader)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:22,470 INFO  org.apache.flink.runtime.state.TaskExecutorChannelStateExecutorFactoryManager [] - Creating the channel state executor factory for job id fd25526942cf157358239c96d744dcfc
broker                        | [2023-08-04 11:56:22,487] INFO [MetadataLoader 1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 1 (org.apache.kafka.image.loader.MetadataLoader)
broker                        | [2023-08-04 11:56:22,497] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                        | [2023-08-04 11:56:22,516] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:22,522 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_0_0), deploy into slot with allocation id 4f88966124ec24b5b23c12ceca13c33d.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:22,537 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from CREATED to DEPLOYING.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:22,537 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 467d7476e2537b93e00452b71c0c326a.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:22,551 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_1_0), deploy into slot with allocation id 467d7476e2537b93e00452b71c0c326a.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:22,557 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 4f88966124ec24b5b23c12ceca13c33d.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:22,559 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 467d7476e2537b93e00452b71c0c326a.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:22,572 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from CREATED to DEPLOYING.
broker                        | [2023-08-04 11:56:22,586] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
broker                        | [2023-08-04 11:56:22,589] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:22,593 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_1_0) [DEPLOYING].
data-agrigator-taskmanager-1  | 2023-08-04 11:56:22,605 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_0_0) [DEPLOYING].
schema-registry               | [2023-08-04 11:56:22,705] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry               | [2023-08-04 11:56:22,706] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/172.21.0.3:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker                        | [2023-08-04 11:56:22,709] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                        | [2023-08-04 11:56:22,733] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                        | [2023-08-04 11:56:22,733] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker                        | [2023-08-04 11:56:22,738] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:22,788 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:22,788 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
broker                        | [2023-08-04 11:56:23,009] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:23,012 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2e0097af
data-agrigator-taskmanager-1  | 2023-08-04 11:56:23,015 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
data-agrigator-taskmanager-1  | 2023-08-04 11:56:23,021 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
broker                        | [2023-08-04 11:56:23,023] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Recorded new controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:23,024 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5ac27df7
data-agrigator-taskmanager-1  | 2023-08-04 11:56:23,024 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
data-agrigator-taskmanager-1  | 2023-08-04 11:56:23,025 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
data-agrigator-taskmanager-1  | 2023-08-04 11:56:23,088 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from DEPLOYING to INITIALIZING.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:23,088 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from DEPLOYING to INITIALIZING.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:23,142 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2) (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from DEPLOYING to INITIALIZING.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:23,174 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from DEPLOYING to INITIALIZING.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:23,524 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:23,528 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
broker                        | [2023-08-04 11:56:23,760] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
broker                        | [2023-08-04 11:56:23,773] INFO Awaiting socket connections on broker:29092. (kafka.network.DataPlaneAcceptor)
broker                        | [2023-08-04 11:56:23,849] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
broker                        | [2023-08-04 11:56:23,852] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
broker                        | [2023-08-04 11:56:23,861] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
broker                        | [2023-08-04 11:56:23,903] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
broker                        | [2023-08-04 11:56:24,047] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Starting (kafka.server.BrokerToControllerRequestThread)
broker                        | [2023-08-04 11:56:24,073] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Recorded new controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
broker                        | [2023-08-04 11:56:24,239] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                        | [2023-08-04 11:56:24,257] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                        | [2023-08-04 11:56:24,288] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                        | [2023-08-04 11:56:24,297] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                        | [2023-08-04 11:56:24,509] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker                        | [2023-08-04 11:56:24,519] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:24,538 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:24,540 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:24,994 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:25,003 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
broker                        | [2023-08-04 11:56:25,192] INFO [RaftManager nodeId=1] Registered the listener kafka.server.metadata.BrokerMetadataListener@2058788810 (org.apache.kafka.raft.KafkaRaftClient)
broker                        | [2023-08-04 11:56:25,228] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Starting (kafka.server.BrokerToControllerRequestThread)
broker                        | [2023-08-04 11:56:25,228] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Recorded new controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
broker                        | [2023-08-04 11:56:25,248] INFO [BrokerLifecycleManager id=1] Incarnation CR9Gn_hjRFiYk8EneLkPYA of broker 1 in cluster MkU3OEVBNTcwNTJENDM2Qg is now STARTING. (kafka.server.BrokerLifecycleManager)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:25,653 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:25,654 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:25,689 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:25,694 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
broker                        | [2023-08-04 11:56:25,753] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:25,714 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from INITIALIZING to FAILED with failure cause:
data-agrigator-taskmanager-1  | java.io.IOException: unable to open JDBC writer
data-agrigator-taskmanager-1  | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at java.lang.Thread.run(Unknown Source) [?:?]
data-agrigator-taskmanager-1  | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-taskmanager-1  | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.Driver.connect(Driver.java:297) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	... 14 more
data-agrigator-taskmanager-1  | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-taskmanager-1  | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1  | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1  | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1  | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1  | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.Driver.connect(Driver.java:297) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	... 14 more
data-agrigator-taskmanager-1  | 2023-08-04 11:56:25,809 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_1_0).
data-agrigator-taskmanager-1  | 2023-08-04 11:56:25,718 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from INITIALIZING to FAILED with failure cause:
data-agrigator-taskmanager-1  | java.io.IOException: unable to open JDBC writer
data-agrigator-taskmanager-1  | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at java.lang.Thread.run(Unknown Source) [?:?]
data-agrigator-taskmanager-1  | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-taskmanager-1  | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.Driver.connect(Driver.java:297) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	... 14 more
data-agrigator-taskmanager-1  | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-taskmanager-1  | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-taskmanager-1  | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1  | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1  | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1  | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1  | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.postgresql.Driver.connect(Driver.java:297) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[my_api.jar:?]
data-agrigator-taskmanager-1  | 	... 14 more
data-agrigator-taskmanager-1  | 2023-08-04 11:56:25,818 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_0_0).
data-agrigator-taskmanager-1  | 2023-08-04 11:56:26,002 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2)#0 197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_0_0.
broker                        | [2023-08-04 11:56:26,069] INFO [BrokerServer id=1] Waiting for broker metadata to catch up. (kafka.server.BrokerServer)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:26,253 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2)#0 197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_1_0.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:26,565 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:26,567 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:26,671 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (1/2) (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from INITIALIZING to FAILED on 172.21.0.5:43917-648a7f @ data-agrigator-taskmanager-1.data-agrigator_default (dataPort=44103).
data-agrigator-jobmanager-1   | java.io.IOException: unable to open JDBC writer
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-jobmanager-1   | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1   | 	... 14 more
data-agrigator-jobmanager-1   | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-jobmanager-1   | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1   | 	... 14 more
data-agrigator-jobmanager-1   | 2023-08-04 11:56:26,770 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job fd25526942cf157358239c96d744dcfc: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
data-agrigator-jobmanager-1   | 2023-08-04 11:56:26,872 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Removing registered reader after failure for subtask 0 (#0) of source Source: Kafka Source.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:26,973 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Java API Skeleton (fd25526942cf157358239c96d744dcfc) switched from state RUNNING to FAILING.
data-agrigator-jobmanager-1   | org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:258) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:242) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:748) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
data-agrigator-jobmanager-1   | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
data-agrigator-jobmanager-1   | Caused by: java.io.IOException: unable to open JDBC writer
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-jobmanager-1   | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.op
data-agrigator-jobmanager-1   | en(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-jobmanager-1   | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 2023-08-04 11:56:27,170 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from INITIALIZING to CANCELING.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:27,399 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source -> Map -> (Filter -> Sink: Unnamed, Filter -> Sink: Unnamed) (2/2) (197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from CANCELING to CANCELED.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:27,404 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job fd25526942cf157358239c96d744dcfc
data-agrigator-jobmanager-1   | 2023-08-04 11:56:27,429 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Java API Skeleton (fd25526942cf157358239c96d744dcfc) switched from state FAILING to FAILED.
data-agrigator-jobmanager-1   | org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:258) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:242) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:748) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
data-agrigator-jobmanager-1   | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
data-agrigator-jobmanager-1   | Caused by: java.io.IOException: unable to open JDBC writer
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-jobmanager-1   | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-jobmanager-1   | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 2023-08-04 11:56:27,528 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job fd25526942cf157358239c96d744dcfc.
broker                        | [2023-08-04 11:56:27,617] INFO [Controller 1] Registered new broker: RegisterBrokerRecord(brokerId=1, isMigratingZkBroker=false, incarnationId=CR9Gn_hjRFiYk8EneLkPYA, brokerEpoch=11, endPoints=[BrokerEndpoint(name='PLAINTEXT', host='broker', port=29092, securityProtocol=0), BrokerEndpoint(name='PLAINTEXT_HOST', host='localhost', port=9092, securityProtocol=0)], features=[BrokerFeature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=8)], rack=null, fenced=true, inControlledShutdown=false) (org.apache.kafka.controller.ClusterControlManager)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:27,669 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 197f406d52d4ec7491425bba174cc621_cbc357ccb763df2852fee8c4fc7d55f2_1_0.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:27,713 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:27,715 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
broker                        | [2023-08-04 11:56:27,721] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 11 (kafka.server.BrokerLifecycleManager)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:27,838 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job fd25526942cf157358239c96d744dcfc reached terminal state FAILED.
data-agrigator-jobmanager-1   | org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:258)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:242)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:748)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
data-agrigator-jobmanager-1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
data-agrigator-jobmanager-1   | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
data-agrigator-jobmanager-1   | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
data-agrigator-jobmanager-1   | 	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
data-agrigator-jobmanager-1   | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
data-agrigator-jobmanager-1   | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
data-agrigator-jobmanager-1   | 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
data-agrigator-jobmanager-1   | 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
data-agrigator-jobmanager-1   | 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
data-agrigator-jobmanager-1   | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
data-agrigator-jobmanager-1   | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
data-agrigator-jobmanager-1   | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
data-agrigator-jobmanager-1   | 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
data-agrigator-jobmanager-1   | 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
data-agrigator-jobmanager-1   | 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
data-agrigator-jobmanager-1   | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579)
data-agrigator-jobmanager-1   | 	at akka.actor.ActorCell.invoke(ActorCell.scala:547)
data-agrigator-jobmanager-1   | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
data-agrigator-jobmanager-1   | 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
data-agrigator-jobmanager-1   | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
data-agrigator-jobmanager-1   | 	at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
data-agrigator-jobmanager-1   | 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
data-agrigator-jobmanager-1   | 	at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
data-agrigator-jobmanager-1   | 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
data-agrigator-jobmanager-1   | 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
data-agrigator-jobmanager-1   | Caused by: java.io.IOException: unable to open JDBC writer
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142)
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52)
data-agrigator-jobmanager-1   | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101)
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46)
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734)
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709)
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
data-agrigator-jobmanager-1   | 	at java.base/java.lang.Thread.run(Unknown Source)
data-agrigator-jobmanager-1   | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-jobmanager-1   | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342)
data-agrigator-jobmanager-1   | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
data-agrigator-jobmanager-1   | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.makeConnection(Driver.java:443)
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.connect(Driver.java:297)
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121)
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140)
data-agrigator-jobmanager-1   | 	... 14 more
data-agrigator-jobmanager-1   | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-jobmanager-1   | 	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
data-agrigator-jobmanager-1   | 	at java.base/java.net.AbstractPlainSocketImpl.doConnect(Unknown Source)
data-agrigator-jobmanager-1   | 	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source)
data-agrigator-jobmanager-1   | 	at java.base/java.net.AbstractPlainSocketImpl.connect(Unknown Source)
data-agrigator-jobmanager-1   | 	at java.base/java.net.SocksSocketImpl.connect(Unknown Source)
data-agrigator-jobmanager-1   | 	at java.base/java.net.Socket.connect(Unknown Source)
data-agrigator-jobmanager-1   | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243)
data-agrigator-jobmanager-1   | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98)
data-agrigator-jobmanager-1   | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132)
data-agrigator-jobmanager-1   | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)
data-agrigator-jobmanager-1   | 	... 20 more
broker                        | [2023-08-04 11:56:27,870] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
broker                        | [2023-08-04 11:56:27,954] INFO [BrokerLifecycleManager id=1] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
broker                        | [2023-08-04 11:56:27,967] INFO [BrokerMetadataListener id=1] Starting to publish metadata events at offset 12. (kafka.server.metadata.BrokerMetadataListener)
broker                        | [2023-08-04 11:56:28,034] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=12, epoch=1) with metadata.version 3.4-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
broker                        | [2023-08-04 11:56:28,046] INFO Loading logs from log dirs ArraySeq(/tmp/kraft-combined-logs) (kafka.log.LogManager)
broker                        | [2023-08-04 11:56:28,058] INFO Attempting recovery for all logs in /tmp/kraft-combined-logs since no clean shutdown file was found (kafka.log.LogManager)
broker                        | [2023-08-04 11:56:28,150] INFO Loaded 0 logs in 102ms. (kafka.log.LogManager)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,158 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job fd25526942cf157358239c96d744dcfc has been registered for cleanup in the JobResultStore after reaching a terminal state.
broker                        | [2023-08-04 11:56:28,173] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
broker                        | [2023-08-04 11:56:28,186] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,252 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job 'Flink Java API Skeleton' (fd25526942cf157358239c96d744dcfc).
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,093 INFO  org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap [] - Application FAILED: 
data-agrigator-jobmanager-1   | java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: FAILED
data-agrigator-jobmanager-1   | 	at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$unwrapJobResultException$7(ApplicationDispatcherBootstrap.java:403) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.CompletableFuture.postComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.CompletableFuture.complete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.client.deployment.application.JobStatusPollingUtils.lambda$null$2(JobStatusPollingUtils.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.CompletableFuture.postComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.CompletableFuture.complete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:267) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.CompletableFuture.postComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.CompletableFuture.complete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1300) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.CompletableFuture.postComplete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.CompletableFuture.complete(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.dispatch.OnComplete.internal(Future.scala:300) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.dispatch.OnComplete.internal(Future.scala:297) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:622) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) [flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
data-agrigator-jobmanager-1   | 	at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
data-agrigator-jobmanager-1   | Caused by: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: FAILED
data-agrigator-jobmanager-1   | 	at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:71) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	... 52 more
data-agrigator-jobmanager-1   | Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:60) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	... 52 more
data-agrigator-jobmanager-1   | Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:258) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:242) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTask
data-agrigator-jobmanager-1   | ExecutionStateUpdate(SchedulerBase.java:748) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
data-agrigator-jobmanager-1   | 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_999d9168-9363-411f-9cff-448196d8a379.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[?:?]
data-agrigator-jobmanager-1   | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
data-agrigator-jobmanager-1   | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
data-agrigator-jobmanager-1   | 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
data-agrigator-jobmanager-1   | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
data-agrigator-jobmanager-1   | 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
data-agrigator-jobmanager-1   | 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
data-agrigator-jobmanager-1   | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) ~[?:?]
data-agrigator-jobmanager-1   | 	at akka.actor.ActorCell.invoke(ActorCell.scala:547) ~[?:?]
data-agrigator-jobmanager-1   | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
data-agrigator-jobmanager-1   | 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
data-agrigator-jobmanager-1   | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
data-agrigator-jobmanager-1   | 	... 5 more
data-agrigator-jobmanager-1   | Caused by: java.io.IOException: unable to open JDBC writer
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:142) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
data-agrigator-jobmanager-1   | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:342) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | Caused by: java.net.ConnectException: Connection refused (Connection refused)
data-agrigator-jobmanager-1   | 	at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.SocksSocketImpl.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at java.net.Socket.connect(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.PGStream.createSocket(PGStream.java:243) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.makeConnection(Driver.java:443) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.postgresql.Driver.connect(Driver.java:297) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getOrEstablishConnection(SimpleJdbcConnectionProvider.java:121) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.open(JdbcOutputFormat.java:140) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:52) ~[?:?]
data-agrigator-jobmanager-1   | 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:101) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-jobmanager-1   | 	at java.lang.Thread.run(Unknown Source) ~[?:?]
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,338 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting StandaloneApplicationClusterEntryPoint down with application status FAILED. Diagnostics null.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,368 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Closing SourceCoordinator for source Source: Kafka Source.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,391 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,416 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Disconnect TaskExecutor 172.21.0.5:43917-648a7f because: Stopping JobMaster for job 'Flink Java API Skeleton' (fd25526942cf157358239c96d744dcfc).
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,430 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [4f88966124ec24b5b23c12ceca13c33d].
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,454 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shutting down rest endpoint.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:28,473 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1, taskHeapMemory=192.000mb (201326587 bytes), taskOffHeapMemory=0 bytes, managedMemory=256.000mb (268435460 bytes), networkMemory=64.000mb (67108865 bytes)}, allocationId: 4f88966124ec24b5b23c12ceca13c33d, jobId: fd25526942cf157358239c96d744dcfc).
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,469 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [467d7476e2537b93e00452b71c0c326a].
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,478 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 122a0f5539dfb4680be86c02507cca4f: Stopping JobMaster for job 'Flink Java API Skeleton' (fd25526942cf157358239c96d744dcfc).
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,489 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job fd25526942cf157358239c96d744dcfc from the resource manager.
broker                        | [2023-08-04 11:56:28,515] INFO Starting the log cleaner (kafka.log.LogCleaner)
data-agrigator-taskmanager-1  | 2023-08-04 11:56:28,521 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:1, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1, taskHeapMemory=192.000mb (201326587 bytes), taskOffHeapMemory=0 bytes, managedMemory=256.000mb (268435460 bytes), networkMemory=64.000mb (67108865 bytes)}, allocationId: 467d7476e2537b93e00452b71c0c326a, jobId: fd25526942cf157358239c96d744dcfc).
data-agrigator-taskmanager-1  | 2023-08-04 11:56:28,526 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job fd25526942cf157358239c96d744dcfc from job leader monitoring.
data-agrigator-taskmanager-1  | 2023-08-04 11:56:28,530 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job fd25526942cf157358239c96d744dcfc.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,854 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,858 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,884 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Removing cache directory /tmp/flink-web-ba93d87e-c3f8-43ab-a1ae-6e5ab6848cc7/flink-web-ui
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,888 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - http://0.0.0.0:8081 lost leadership
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,890 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shut down complete.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,928 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Shut down cluster because application is in FAILED, diagnostics null.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,931 INFO  org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent [] - Closing components.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,939 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Stopping SessionDispatcherLeaderProcess.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,944 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping dispatcher akka.tcp://flink@jobmanager:6123/user/rpc/dispatcher_1.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,951 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping all currently running jobs of dispatcher akka.tcp://flink@jobmanager:6123/user/rpc/dispatcher_1.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,970 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka.tcp://flink@jobmanager:6123/user/rpc/dispatcher_1.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,981 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Stopping resource manager service.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,987 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Stopping credential renewal
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,991 INFO  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Stopped credential renewal
data-agrigator-jobmanager-1   | 2023-08-04 11:56:28,998 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Closing the slot manager.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:29,000 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Suspending the slot manager.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:29,024 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:6124
broker                        | [2023-08-04 11:56:29,091] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:29,108 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:29,150 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
broker                        | [2023-08-04 11:56:29,167] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:56:29,187] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:56:29,194] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
broker                        | [2023-08-04 11:56:29,236] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
broker                        | [2023-08-04 11:56:29,286] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
broker                        | [2023-08-04 11:56:29,291] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 8 at offset OffsetAndEpoch(offset=12, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)
broker                        | [2023-08-04 11:56:29,286] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:29,377 INFO  akka.actor.CoordinatedShutdown                               [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
data-agrigator-jobmanager-1   | 2023-08-04 11:56:29,397 INFO  akka.actor.CoordinatedShutdown                               [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
broker                        | [2023-08-04 11:56:29,398] INFO KafkaConfig values: 
broker                        | 	advertised.listeners = PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
broker                        | 	alter.config.policy.class.name = null
broker                        | 	alter.log.dirs.replication.quota.window.num = 11
broker                        | 	alter.log.dirs.replication.quota.window.size.seconds = 1
broker                        | 	authorizer.class.name = 
broker                        | 	auto.create.topics.enable = true
broker                        | 	auto.include.jmx.reporter = true
broker                        | 	auto.leader.rebalance.enable = true
broker                        | 	background.threads = 10
broker                        | 	broker.heartbeat.interval.ms = 2000
broker                        | 	broker.id = 1
broker                        | 	broker.id.generation.enable = true
broker                        | 	broker.rack = null
broker                        | 	broker.session.timeout.ms = 9000
broker                        | 	client.quota.callback.class = null
broker                        | 	compression.type = producer
broker                        | 	connection.failed.authentication.delay.ms = 100
broker                        | 	connections.max.idle.ms = 600000
broker                        | 	connections.max.reauth.ms = 0
broker                        | 	control.plane.listener.name = null
broker                        | 	controlled.shutdown.enable = true
broker                        | 	controlled.shutdown.max.retries = 3
broker                        | 	controlled.shutdown.retry.backoff.ms = 5000
broker                        | 	controller.listener.names = CONTROLLER
broker                        | 	controller.quorum.append.linger.ms = 25
broker                        | 	controller.quorum.election.backoff.max.ms = 1000
broker                        | 	controller.quorum.election.timeout.ms = 1000
broker                        | 	controller.quorum.fetch.timeout.ms = 2000
broker                        | 	controller.quorum.request.timeout.ms = 2000
broker                        | 	controller.quorum.retry.backoff.ms = 20
broker                        | 	controller.quorum.voters = [1@broker:29093]
broker                        | 	controller.quota.window.num = 11
broker                        | 	controller.quota.window.size.seconds = 1
broker                        | 	controller.socket.timeout.ms = 30000
broker                        | 	create.topic.policy.class.name = null
broker                        | 	default.replication.factor = 1
broker                        | 	delegation.token.expiry.check.interval.ms = 3600000
broker                        | 	delegation.token.expiry.time.ms = 86400000
broker                        | 	delegation.token.master.key = null
broker                        | 	delegation.token.max.lifetime.ms = 604800000
broker                        | 	delegation.token.secret.key = null
broker                        | 	delete.records.purgatory.purge.interval.requests = 1
broker                        | 	delete.topic.enable = true
broker                        | 	early.start.listeners = null
broker                        | 	fetch.max.bytes = 57671680
broker                        | 	fetch.purgatory.purge.interval.requests = 1000
broker                        | 	group.initial.rebalance.delay.ms = 0
broker                        | 	group.max.session.timeout.ms = 1800000
broker                        | 	group.max.size = 2147483647
broker                        | 	group.min.session.timeout.ms = 6000
broker                        | 	initial.broker.registration.timeout.ms = 60000
broker                        | 	inter.broker.listener.name = PLAINTEXT
broker                        | 	inter.broker.protocol.version = 3.4-IV0
broker                        | 	kafka.metrics.polling.interval.secs = 10
broker                        | 	kafka.metrics.reporters = []
broker                        | 	leader.imbalance.check.interval.seconds = 300
broker                        | 	leader.imbalance.per.broker.percentage = 10
broker                        | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
broker                        | 	listeners = PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092
broker                        | 	log.cleaner.backoff.ms = 15000
broker                        | 	log.cleaner.dedupe.buffer.size = 134217728
broker                        | 	log.cleaner.delete.retention.ms = 86400000
broker                        | 	log.cleaner.enable = true
broker                        | 	log.cleaner.io.buffer.load.factor = 0.9
broker                        | 	log.cleaner.io.buffer.size = 524288
broker                        | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
broker                        | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
broker                        | 	log.cleaner.min.cleanable.ratio = 0.5
broker                        | 	log.cleaner.min.compaction.lag.ms = 0
broker                        | 	log.cleaner.threads = 1
broker                        | 	log.cleanup.policy = [delete]
broker                        | 	log.dir = /tmp/kafka-logs
broker                        | 	log.dirs = /tmp/kraft-combined-logs
broker                        | 	log.flush.interval.messages = 9223372036854775807
broker                        | 	log.flush.interval.ms = null
broker                        | 	log.flush.offset.checkpoint.interval.ms = 60000
broker                        | 	log.flush.scheduler.interval.ms = 9223372036854775807
broker                        | 	log.flush.start.offset.checkpoint.interval.ms = 60000
broker                        | 	log.index.interval.bytes = 4096
broker                        | 	log.index.size.max.bytes = 10485760
broker                        | 	log.message.downconversion.enable = true
broker                        | 	log.message.format.version = 3.0-IV1
broker                        | 	log.message.timestamp.difference.max.ms = 9223372036854775807
broker                        | 	log.message.timestamp.type = CreateTime
broker                        | 	log.preallocate = false
broker                        | 	log.retention.bytes = -1
broker                        | 	log.retention.check.interval.ms = 300000
broker                        | 	log.retention.hours = 168
broker                        | 	log.retention.minutes = null
broker                        | 	log.retention.ms = null
broker                        | 	log.roll.hours = 168
broker                        | 	log.roll.jitter.hours = 0
broker                        | 	log.roll.jitter.ms = null
broker                        | 	log.roll.ms = null
broker                        | 	log.segment.bytes = 1073741824
broker                        | 	log.segment.delete.delay.ms = 60000
broker                        | 	max.connection.creation.rate = 2147483647
broker                        | 	max.connections = 2147483647
broker                        | 	max.connections.per.ip = 2147483647
broker                        | 	max.connections.per.ip.overrides = 
broker                        | 	max.incremental.fetch.session.cache.slots = 1000
broker                        | 	message.max.bytes = 1048588
broker                        | 	metadata.log.dir = null
broker                        | 	metadata.log.max.record.bytes.between.snapshots = 20971520
broker                        | 	metadata.log.max.snapshot.interval.ms = 3600000
broker                        | 	metadata.log.segment.bytes = 1073741824
broker                        | 	metadata.log.segment.min.bytes = 8388608
broker                        | 	metadata.log.segment.ms = 604800000
broker                        | 	metadata.max.idle.interval.ms = 500
broker                        | 	metadata.max.retention.bytes = 104857600
broker                        | 	metadata.max.retention.ms = 604800000
broker                        | 	metric.reporters = []
broker                        | 	metrics.num.samples = 2
broker                        | 	metrics.recording.level = INFO
broker                        | 	metrics.sample.window.ms = 30000
broker                        | 	min.insync.replicas = 1
broker                        | 	node.id = 1
broker                        | 	num.io.threads = 8
broker                        | 	num.network.threads = 3
broker                        | 	num.partitions = 1
broker                        | 	num.recovery.threads.per.data.dir = 1
broker                        | 	num.replica.alter.log.dirs.threads = null
broker                        | 	num.replica.fetchers = 1
broker                        | 	offset.metadata.max.bytes = 4096
broker                        | 	offsets.commit.required.acks = -1
broker                        | 	offsets.commit.timeout.ms = 5000
broker                        | 	offsets.load.buffer.size = 5242880
broker                        | 	offsets.retention.check.interval.ms = 600000
broker                        | 	offsets.retention.minutes = 10080
broker                        | 	offsets.topic.compression.codec = 0
broker                        | 	offsets.topic.num.partitions = 50
broker                        | 	offsets.topic.replication.factor = 1
broker                        | 	offsets.topic.segment.bytes = 104857600
broker                        | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
broker                        | 	password.encoder.iterations = 4096
broker                        | 	password.encoder.key.length = 128
broker                        | 	password.encoder.keyfactory.algorithm = null
broker                        | 	password.encoder.old.secret = null
broker                        | 	password.encoder.secret = null
broker                        | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
broker                        | 	process.roles = [broker, controller]
broker                        | 	producer.id.expiration.check.interval.ms = 600000
broker                        | 	producer.id.expiration.ms = 86400000
broker                        | 	producer.purgatory.purge.interval.requests = 1000
broker                        | 	queued.max.request.bytes = -1
broker                        | 	queued.max.requests = 500
broker                        | 	quota.window.num = 11
broker                        | 	quota.window.size.seconds = 1
broker                        | 	remote.log.index.file.cache.total.size.bytes = 1073741824
broker                        | 	remote.log.manager.task.interval.ms = 30000
broker                        | 	remote.log.manager.task.retry.backoff.max.ms = 30000
broker                        | 	remote.log.manager.task.retry.backoff.ms = 500
broker                        | 	remote.log.manager.task.retry.jitter = 0.2
broker                        | 	remote.log.manager.thread.pool.size = 10
broker                        | 	remote.log.metadata.manager.class.name = null
broker                        | 	remote.log.metadata.manager.class.path = null
broker                        | 	remote.log.metadata.manager.impl.prefix = null
broker                        | 	remote.log.metadata.manager.listener.name = null
broker                        | 	remote.log.reader.max.pending.tasks = 100
broker                        | 	remote.log.reader.threads = 10
broker                        | 	remote.log.storage.manager.class.name = null
broker                        | 	remote.log.storage.manager.class.path = null
broker                        | 	remote.log.storage.manager.impl.prefix = null
broker                        | 	remote.log.storage.system.enable = false
broker                        | 	replica.fetch.backoff.ms = 1000
broker                        | 	replica.fetch.max.bytes = 1048576
broker                        | 	replica.fetch.min.bytes = 1
broker                        | 	replica.fetch.response.max.bytes = 10485760
broker                        | 	replica.fetch.wait.max.ms = 500
broker                        | 	replica.high.watermark.checkpoint.interval.ms = 5000
broker                        | 	replica.lag.time.max.ms = 30000
broker                        | 	replica.selector.class = null
broker                        | 	replica.socket.receive.buffer.bytes = 65536
broker                        | 	replica.socket.timeout.ms = 30000
broker                        | 	replication.quota.window.num = 11
broker                        | 	replication.quota.window.size.seconds = 1
broker                        | 	request.timeout.ms = 30000
broker                        | 	reserved.broker.max.id = 1000
broker                        | 	sasl.client.callback.handler.class = null
broker                        | 	sasl.enabled.mechanisms = [GSSAPI]
broker                        | 	sasl.jaas.config = null
broker                        | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
broker                        | 	sasl.kerberos.min.time.before.relogin = 60000
broker                        | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
broker                        | 	sasl.kerberos.service.name = null
broker                        | 	sasl.kerberos.ticket.renew.jitter = 0.05
broker                        | 	sasl.kerberos.ticket.renew.window.factor = 0.8
broker                        | 	sasl.login.callback.handler.class = null
broker                        | 	sasl.login.class = null
broker                        | 	sasl.login.connect.timeout.ms = null
broker                        | 	sasl.login.read.timeout.ms = null
broker                        | 	sasl.login.refresh.buffer.seconds = 300
broker                        | 	sasl.login.refresh.min.period.seconds = 60
broker                        | 	sasl.login.refresh.window.factor = 0.8
broker                        | 	sasl.login.refresh.window.jitter = 0.05
broker                        | 	sasl.login.retry.backoff.max.ms = 10000
broker                        | 	sasl.login.retry.backoff.ms = 100
broker                        | 	sasl.mechanism.controller.protocol = GSSAPI
broker                        | 	sasl.mechanism.inter.broker.protocol = GSSAPI
broker                        | 	sasl.oauthbearer.clock.skew.seconds = 30
broker                        | 	sasl.oauthbearer.expected.audience = null
broker                        | 	sasl.oauthbearer.expected.issuer = null
broker                        | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
broker                        | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
broker                        | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
broker                        | 	sasl.oauthbearer.jwks.endpoint.url = null
broker                        | 	sasl.oauthbearer.scope.claim.name = scope
broker                        | 	sasl.oauthbearer.sub.claim.name = sub
broker                        | 	sasl.oauthbearer.token.endpoint.url = null
broker                        | 	sasl.server.callback.handler.class = null
broker                        | 	sasl.server.max.receive.size = 524288
broker                        | 	security.inter.broker.protocol = PLAINTEXT
broker                        | 	security.providers = null
broker                        | 	socket.connection.setup.timeout.max.ms = 30000
broker                        | 	socket.connection.setup.timeout.ms = 10000
broker                        | 	socket.listen.backlog.size = 50
broker                        | 	socket.receive.buffer.bytes = 102400
broker                        | 	socket.request.max.bytes = 104857600
broker                        | 	socket.send.buffer.bytes = 102400
broker                        | 	ssl.cipher.suites = []
broker                        | 	ssl.client.auth = none
broker                        | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
broker                        | 	ssl.endpoint.identification.algorithm = https
broker                        | 	ssl.engine.factory.class = null
broker                        | 	ssl.key.password = null
broker                        | 	ssl.keymanager.algorithm = SunX509
broker                        | 	ssl.keystore.certificate.chain = null
broker                        | 	ssl.keystore.key = null
broker                        | 	ssl.keystore.location = null
broker                        | 	ssl.keystore.password = null
broker                        | 	ssl.keystore.type = JKS
broker                        | 	ssl.principal.mapping.rules = DEFAULT
broker                        | 	ssl.protocol = TLSv1.3
broker                        | 	ssl.provider = null
broker                        | 	ssl.secure.random.implementation = null
broker                        | 	ssl.trustmanager.algorithm = PKIX
broker                        | 	ssl.truststore.certificates = null
broker                        | 	ssl.truststore.location = null
broker                        | 	ssl.truststore.password = null
broker                        | 	ssl.truststore.type = JKS
broker                        | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
broker                        | 	transaction.max.timeout.ms = 900000
broker                        | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
broker                        | 	transaction.state.log.load.buffer.size = 5242880
broker                        | 	transaction.state.log.min.isr = 1
broker                        | 	transaction.state.log.num.partitions = 50
broker                        | 	transaction.state.log.replication.factor = 1
broker                        | 	transaction.state.log.segment.bytes = 104857600
broker                        | 	transactional.id.expiration.ms = 604800000
broker                        | 	unclean.leader.election.enable = false
broker                        | 	zookeeper.clientCnxnSocket = null
broker                        | 	zookeeper.connect = 
broker                        | 	zookeeper.connection.timeout.ms = null
broker                        | 	zookeeper.max.in.flight.requests = 10
broker                        | 	zookeeper.metadata.migration.enable = false
broker                        | 	zookeeper.session.timeout.ms = 18000
broker                        | 	zookeeper.set.acl = false
broker                        | 	zookeeper.ssl.cipher.suites = null
broker                        | 	zookeeper.ssl.client.enable = false
broker                        | 	zookeeper.ssl.crl.enable = false
broker                        | 	zookeeper.ssl.enabled.protocols = null
broker                        | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
broker                        | 	zookeeper.ssl.keystore.location = null
broker                        | 	zookeeper.ssl.keystore.password = null
broker                        | 	zookeeper.ssl.keystore.type = null
broker                        | 	zookeeper.ssl.ocsp.enable = false
broker                        | 	zookeeper.ssl.protocol = TLSv1.2
broker                        | 	zookeeper.ssl.truststore.location = null
broker                        | 	zookeeper.ssl.truststore.password = null
broker                        | 	zookeeper.ssl.truststore.type = null
broker                        |  (kafka.server.KafkaConfig)
schema-registry               | [2023-08-04 11:56:29,427] ERROR Error while getting broker list. (io.confluent.admin.utils.ClusterStatus)
schema-registry               | java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: listNodes
schema-registry               | 	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
schema-registry               | 	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
schema-registry               | 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
schema-registry               | 	at io.confluent.admin.utils.ClusterStatus.isKafkaReady(ClusterStatus.java:147)
schema-registry               | 	at io.confluent.admin.utils.cli.KafkaReadyCommand.main(KafkaReadyCommand.java:149)
schema-registry               | Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: listNodes
broker                        | [2023-08-04 11:56:29,508] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:29,563 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:29,565 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:29,585 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:29,597 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
broker                        | [2023-08-04 11:56:29,617] INFO [Controller 1] The request from broker 1 to unfence has been granted because it has caught up with the offset of it's register broker record 11. (org.apache.kafka.controller.BrokerHeartbeatManager)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:29,740 INFO  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Node -1 disconnected.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:29,745 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=KafkaSource-525723299120177754-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
broker                        | [2023-08-04 11:56:29,937] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
broker                        | [2023-08-04 11:56:29,939] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)
broker                        | [2023-08-04 11:56:29,949] INFO Kafka version: 7.4.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
broker                        | [2023-08-04 11:56:29,949] INFO Kafka commitId: fed9c006bfc7ba5bf7d2dee840e041d1a851d903 (org.apache.kafka.common.utils.AppInfoParser)
broker                        | [2023-08-04 11:56:29,950] INFO Kafka startTimeMs: 1691150189939 (org.apache.kafka.common.utils.AppInfoParser)
broker                        | [2023-08-04 11:56:30,002] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:30,050 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:30,052 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:30,380 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
schema-registry               | [2023-08-04 11:56:30,430] INFO Expected 1 brokers but found only 0. Trying to query Kafka for metadata again ... (io.confluent.admin.utils.ClusterStatus)
schema-registry               | [2023-08-04 11:56:30,430] ERROR Expected 1 brokers but found only 0. Brokers found []. (io.confluent.admin.utils.ClusterStatus)
data-agrigator-jobmanager-1   | 2023-08-04 11:56:30,497 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
data-agrigator-jobmanager-1   | 2023-08-04 11:56:30,499 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Terminating cluster entrypoint process StandaloneApplicationClusterEntryPoint with exit code 1443.
schema-registry               | Using log4j config /etc/schema-registry/log4j.properties
control-center                | Using log4j config /etc/cp-base-new/log4j.properties
schema-registry exited with code 1
control-center                | ===> Launching ... 
control-center                | ===> Launching control-center ... 
data-agrigator-jobmanager-1 exited with code 163
control-center                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
control-center                | SLF4J: Class path contains multiple SLF4J bindings.
control-center                | SLF4J: Found binding in [jar:file:/usr/share/java/acl/acl-7.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
control-center                | SLF4J: Found binding in [jar:file:/usr/share/java/confluent-control-center/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
control-center                | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
control-center                | SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]
control-center                | [2023-08-04 11:56:39,316] WARN Invalid value 1 for configuration confluent.controlcenter.internal.topics.replication: Value must be at least 3 (io.confluent.controlcenter.ControlCenterConfig)
control-center                | [2023-08-04 11:56:39,321] WARN Invalid value 1 for configuration confluent.controlcenter.internal.topics.replication: Value must be at least 3 (io.confluent.controlcenter.ControlCenterConfig)
control-center                | [2023-08-04 11:56:39,326] INFO ControlCenterConfig values: 
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	confluent.controlcenter.alert.cluster.down.autocreate = false
control-center                | 	confluent.controlcenter.alert.cluster.down.send.rate = 12
control-center                | 	confluent.controlcenter.alert.cluster.down.to.email = 
control-center                | 	confluent.controlcenter.alert.cluster.down.to.pagerduty.integrationkey = 
control-center                | 	confluent.controlcenter.alert.cluster.down.to.webhookurl.slack = 
control-center                | 	confluent.controlcenter.alert.max.trigger.events = 1000
control-center                | 	confluent.controlcenter.armeria.healthcheck.force.http1 = false
control-center                | 	confluent.controlcenter.auth.bearer.issuer = Confluent
control-center                | 	confluent.controlcenter.auth.bearer.roles.claim = 
control-center                | 	confluent.controlcenter.auth.restricted.roles = []
control-center                | 	confluent.controlcenter.auth.session.expiration.ms = 0
control-center                | 	confluent.controlcenter.broker.config.edit.enable = true
control-center                | 	confluent.controlcenter.command.streams.start.timeout = 300000
control-center                | 	confluent.controlcenter.command.topic = _confluent-command
control-center                | 	confluent.controlcenter.command.topic.replication = 1
control-center                | 	confluent.controlcenter.command.topic.retention.ms = 86400000
control-center                | 	confluent.controlcenter.command.topic.segment.bytes = 1073741824
control-center                | 	confluent.controlcenter.connect.connect.alias.name = 
control-center                | 	confluent.controlcenter.connect.healthcheck.endpoint = /v1/metadata/id
control-center                | 	confluent.controlcenter.consumer.metadata.timeout.ms = 15000
control-center                | 	confluent.controlcenter.consumers.view.enable = true
control-center                | 	confluent.controlcenter.data.dir = /var/lib/confluent-control-center
control-center                | 	confluent.controlcenter.deprecated.views.enable = false
control-center                | 	confluent.controlcenter.disk.skew.warning.min.bytes = 1073741824
control-center                | 	confluent.controlcenter.embedded.kafkarest.enable = true
control-center                | 	confluent.controlcenter.hostedmonitoring.enable = false
control-center                | 	confluent.controlcenter.id = 1
control-center                | 	confluent.controlcenter.internal.streams.start.timeout = 21600000
control-center                | 	confluent.controlcenter.internal.topics.changelog.segment.bytes = 134217728
control-center                | 	confluent.controlcenter.internal.topics.partitions = 1
control-center                | 	confluent.controlcenter.internal.topics.replication = 1
control-center                | 	confluent.controlcenter.internal.topics.retention.bytes = -1
control-center                | 	confluent.controlcenter.internal.topics.retention.ms = 604800000
control-center                | 	confluent.controlcenter.ksql.enable = true
control-center                | 	confluent.controlcenter.ksql.ksql.alias.name = 
control-center                | 	confluent.controlcenter.license.manager = _confluent-controlcenter-license-manager-7-4-1
control-center                | 	confluent.controlcenter.license.manager.enable = true
control-center                | 	confluent.controlcenter.mail.bounce.address = 
control-center                | 	confluent.controlcenter.mail.enabled = false
control-center                | 	confluent.controlcenter.mail.from = c3@confluent.io
control-center                | 	confluent.controlcenter.mail.host.name = localhost
control-center                | 	confluent.controlcenter.mail.password = [hidden]
control-center                | 	confluent.controlcenter.mail.port = 587
control-center                | 	confluent.controlcenter.mail.ssl.checkserveridentity = false
control-center                | 	confluent.controlcenter.mail.ssl.port = 465
control-center                | 	confluent.controlcenter.mail.starttls.required = false
control-center                | 	confluent.controlcenter.mail.username = 
control-center                | 	confluent.controlcenter.mds.client.idle.timeout = null
control-center                | 	confluent.controlcenter.mds.client.max.requests.queued.per.destination = null
control-center                | 	confluent.controlcenter.mode.enable = all
control-center                | 	confluent.controlcenter.name = _confluent-controlcenter
control-center                | 	confluent.controlcenter.private.installer.id = 
control-center                | 	confluent.controlcenter.proactive.support.ui.cta.enable = true
control-center                | 	confluent.controlcenter.purge.stale.cluster.enable = false
control-center                | 	confluent.controlcenter.request.buffer.size.bytes = 10000
control-center                | 	confluent.controlcenter.rest.advertised.url = 
control-center                | 	confluent.controlcenter.rest.compression.enable = true
control-center                | 	confluent.controlcenter.rest.csrf.prevention.enable = false
control-center                | 	confluent.controlcenter.rest.csrf.prevention.token.endpoint = /csrf
control-center                | 	confluent.controlcenter.rest.csrf.prevention.token.expiration.minutes = 30
control-center                | 	confluent.controlcenter.rest.hsts.enable = true
control-center                | 	confluent.controlcenter.rest.nosniff.prevention.enable = true
control-center                | 	confluent.controlcenter.rest.port = 9021
control-center                | 	confluent.controlcenter.rest.proxy.alias.name = 
control-center                | 	confluent.controlcenter.sbk.ui.enable = true
control-center                | 	confluent.controlcenter.schema.registry.enable = true
control-center                | 	confluent.controlcenter.schema.registry.schema.registry.alias.name = 
control-center                | 	confluent.controlcenter.schema.registry.url = [http://schema-registry:8081]
control-center                | 	confluent.controlcenter.service.healthcheck.interval.sec = 20
control-center                | 	confluent.controlcenter.streams.cache.max.bytes.buffering = 1073741824
control-center                | 	confluent.controlcenter.streams.consumer.session.timeout.ms = 60000
control-center                | 	confluent.controlcenter.streams.num.stream.threads = 12
control-center                | 	confluent.controlcenter.streams.producer.compression.type = lz4
control-center                | 	confluent.controlcenter.streams.producer.delivery.timeout.ms = 2147483647
control-center                | 	confluent.controlcenter.streams.producer.linger.ms = 500
control-center                | 	confluent.controlcenter.streams.producer.max.block.ms = 9223372036854775807
control-center                | 	confluent.controlcenter.streams.producer.retries = 2147483647
control-center                | 	confluent.controlcenter.streams.producer.retry.backoff.ms = 100
control-center                | 	confluent.controlcenter.streams.task.timeout.ms = 0
control-center                | 	confluent.controlcenter.streams.upgrade.from = 2.3
control-center                | 	confluent.controlcenter.topic.inspection.enable = true
control-center                | 	confluent.controlcenter.topic.inspection.message.max.bytes = 1048576
control-center                | 	confluent.controlcenter.trigger.active-controller-count.enable = false
control-center                | 	confluent.controlcenter.ui.acl.kafkarest.enable = false
control-center                | 	confluent.controlcenter.ui.autoupdate.enable = true
control-center                | 	confluent.controlcenter.ui.basepath = /
control-center                | 	confluent.controlcenter.ui.broker.kafkarest.enable = false
control-center                | 	confluent.controlcenter.ui.brokersettings.kafkarest.enable = true
control-center                | 	confluent.controlcenter.ui.consumer.group.kafkarest.enable = false
control-center                | 	confluent.controlcenter.ui.controller.chart.enable = false
control-center                | 	confluent.controlcenter.ui.data.expired.threshold = 120
control-center                | 	confluent.controlcenter.ui.external.css.files = null
control-center                | 	confluent.controlcenter.ui.external.js.files = null
control-center                | 	confluent.controlcenter.ui.replicator.monitoring.enable = true
control-center                | 	confluent.controlcenter.ui.topic.kafkarest.enable = false
control-center                | 	confluent.controlcenter.usage.data.collection.enable = true
control-center                | 	confluent.controlcenter.use.default.jvm.truststore = false
control-center                | 	confluent.controlcenter.use.default.os.truststore = false
control-center                | 	confluent.controlcenter.webhook.enabled = true
control-center                | 	confluent.license = 
control-center                | 	confluent.metadata.basic.auth.user.info = [hidden]
control-center                | 	confluent.metadata.bootstrap.server.urls = []
control-center                | 	confluent.metadata.cluster.registry.enable = false
control-center                | 	confluent.metadata.cluster.registry.merge.configuration.enable = true
control-center                | 	confluent.metrics.broker.count.staleness.threshold.ms = 120000
control-center                | 	confluent.metrics.topic = _confluent-metrics
control-center                | 	confluent.metrics.topic.config.validate = false
control-center                | 	confluent.metrics.topic.max.message.bytes = 10485760
control-center                | 	confluent.metrics.topic.partitions = 12
control-center                | 	confluent.metrics.topic.replication = 1
control-center                | 	confluent.metrics.topic.retention.bytes = -1
control-center                | 	confluent.metrics.topic.retention.ms = 259200000
control-center                | 	confluent.metrics.topic.skip.backlog.minutes = 15
control-center                | 	confluent.monitoring.interceptor.topic = _confluent-monitoring
control-center                | 	confluent.monitoring.interceptor.topic.config.validate = false
control-center                | 	confluent.monitoring.interceptor.topic.partitions = 1
control-center                | 	confluent.monitoring.interceptor.topic.replication = 1
control-center                | 	confluent.monitoring.interceptor.topic.retention.bytes = -1
control-center                | 	confluent.monitoring.interceptor.topic.retention.ms = 259200000
control-center                | 	confluent.monitoring.interceptor.topic.skip.backlog.minutes = 15
control-center                | 	confluent.support.metrics.enable = true
control-center                | 	confluent.support.metrics.segment.endpoint = https://analytics-api.confluent.io
control-center                | 	confluent.support.metrics.segment.id = MORqDG61F2eE5mfxAXVqpEblmFG18nbv
control-center                | 	public.key.path = 
control-center                | 	zookeeper.connect = 
control-center                |  (io.confluent.controlcenter.ControlCenterConfig)
control-center                | [2023-08-04 11:56:42,684] INFO Capturing metrics for topic names _confluent-monitoring _confluent-metrics (io.confluent.controlcenter.streams.WindowExtractor)
control-center                | [2023-08-04 11:56:42,700] INFO transformerStore=MonitoringVerifierStore (io.confluent.controlcenter.streams.StreamsModule)
control-center                | [2023-08-04 11:56:42,711] INFO transformerStore=MonitoringTriggerStore (io.confluent.controlcenter.streams.StreamsModule)
control-center                | [2023-08-04 11:56:42,711] INFO transformerStore=TriggerActionsStore (io.confluent.controlcenter.streams.StreamsModule)
control-center                | [2023-08-04 11:56:42,711] INFO transformerStore=TriggerEventsStore (io.confluent.controlcenter.streams.StreamsModule)
control-center                | [2023-08-04 11:56:42,711] INFO transformerStore=AlertHistoryStore (io.confluent.controlcenter.streams.StreamsModule)
control-center                | [2023-08-04 11:56:42,733] INFO StreamsConfig values: 
control-center                | 	acceptable.recovery.lag = 10000
control-center                | 	application.id = _confluent-controlcenter-7-4-1-1
control-center                | 	application.server = 
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffered.records.per.partition = 100
control-center                | 	built.in.metrics.version = latest
control-center                | 	cache.max.bytes.buffering = 1073741824
control-center                | 	client.id = 
control-center                | 	commit.interval.ms = 30000
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndContinueExceptionHandler
control-center                | 	default.dsl.store = rocksDB
control-center                | 	default.key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
control-center                | 	default.list.key.serde.inner = null
control-center                | 	default.list.key.serde.type = null
control-center                | 	default.list.value.serde.inner = null
control-center                | 	default.list.value.serde.type = null
control-center                | 	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
control-center                | 	default.timestamp.extractor = class io.confluent.controlcenter.streams.WindowExtractor
control-center                | 	default.value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
control-center                | 	max.task.idle.ms = 0
control-center                | 	max.warmup.replicas = 2
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	num.standby.replicas = 0
control-center                | 	num.stream.threads = 12
control-center                | 	poll.ms = 100
control-center                | 	probing.rebalance.interval.ms = 600000
control-center                | 	processing.guarantee = at_least_once
control-center                | 	rack.aware.assignment.tags = []
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	repartition.purge.interval.ms = 30000
control-center                | 	replication.factor = 1
control-center                | 	request.timeout.ms = 40000
control-center                | 	retries = 0
control-center                | 	retry.backoff.ms = 100
control-center                | 	rocksdb.config.setter = class io.confluent.controlcenter.streams.RocksDBConfigurator
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	send.buffer.bytes = 131072
control-center                | 	state.cleanup.delay.ms = 600000
control-center                | 	state.dir = /var/lib/confluent-control-center/1/kafka-streams
control-center                | 	statestore.cache.max.bytes = 10485760
control-center                | 	task.timeout.ms = 0
control-center                | 	topology.optimization = all
control-center                | 	upgrade.from = 2.3
control-center                | 	window.size.ms = null
control-center                | 	windowed.inner.class.serde = null
control-center                | 	windowstore.changelog.additional.retention.ms = 86400000
control-center                |  (org.apache.kafka.streams.StreamsConfig)
control-center                | [2023-08-04 11:56:42,832] INFO ProducerConfig values: 
control-center                | 	acks = -1
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	batch.size = 16384
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffer.memory = 33554432
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = confluent-control-center-heartbeat-sender-1
control-center                | 	compression.type = lz4
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	delivery.timeout.ms = 2147483647
control-center                | 	enable.idempotence = true
control-center                | 	interceptor.classes = []
control-center                | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                | 	linger.ms = 500
control-center                | 	max.block.ms = 9223372036854775807
control-center                | 	max.in.flight.requests.per.connection = 1
control-center                | 	max.request.size = 10485760
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metadata.max.idle.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partitioner.adaptive.partitioning.enable = true
control-center                | 	partitioner.availability.timeout.ms = 0
control-center                | 	partitioner.class = null
control-center                | 	partitioner.ignore.keys = false
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 500
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	transaction.timeout.ms = 60000
control-center                | 	transactional.id = null
control-center                | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                | [2023-08-04 11:56:42,943] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                | [2023-08-04 11:56:43,183] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:43,185] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:43,185] INFO Kafka startTimeMs: 1691150203176 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:43,599] INFO Fetching bootstrap cluster id (io.confluent.controlcenter.BootstrapClusterIdSupplier)
control-center                | [2023-08-04 11:56:43,608] INFO AdminClientConfig values: 
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = 
control-center                | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	confluent.use.controller.listener = false
control-center                | 	connections.max.idle.ms = 300000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                | [2023-08-04 11:56:43,850] WARN These configurations '[consumer.session.timeout.ms, producer.max.block.ms, producer.retries, upgrade.from, producer.retry.backoff.ms, producer.linger.ms, producer.delivery.timeout.ms, task.timeout.ms, cache.max.bytes.buffering, producer.compression.type, num.stream.threads]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                | [2023-08-04 11:56:43,851] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:43,851] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:43,851] INFO Kafka startTimeMs: 1691150203850 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:45,795] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:56:45,832] INFO Bootstrap cluster id MkU3OEVBNTcwNTJENDM2Qg (io.confluent.controlcenter.BootstrapClusterIdSupplier)
control-center                | [2023-08-04 11:56:45,912] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] ProducerId set to 0 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                | [2023-08-04 11:56:47,566] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                | 	metrics.jmx.prefix = confluent.controlcenter
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = true
control-center                | 	port = 9021
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:56:47,573] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                | 	metrics.jmx.prefix = confluent.controlcenter
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = true
control-center                | 	port = 9021
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:56:47,577] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = []
control-center                | 	metrics.jmx.prefix = rest-utils
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = false
control-center                | 	port = 8080
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:56:47,589] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = []
control-center                | 	metrics.jmx.prefix = rest-utils
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = false
control-center                | 	port = 8080
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:56:47,592] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = []
control-center                | 	metrics.jmx.prefix = rest-utils
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = false
control-center                | 	port = 8080
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:56:47,595] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = []
control-center                | 	metrics.jmx.prefix = rest-utils
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = false
control-center                | 	port = 8080
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:56:47,597] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = []
control-center                | 	metrics.jmx.prefix = rest-utils
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = false
control-center                | 	port = 8080
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:56:47,660] INFO com.linecorp.armeria.verboseExceptions: rate-limit=10 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:47,678] INFO com.linecorp.armeria.preferredIpV4Addresses:  (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:47,726] INFO com.linecorp.armeria.verboseSocketExceptions: false (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:47,726] INFO com.linecorp.armeria.verboseResponses: false (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:47,726] INFO com.linecorp.armeria.warnNettyVersions: true (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:47,805] WARN Inconsistent Netty versions detected: {netty-buffer=netty-buffer-4.1.86.Final.cde0e2d050, netty-codec=netty-codec-4.1.86.Final.cde0e2d050, netty-codec-dns=netty-codec-dns-4.1.86.Final.cde0e2d050, netty-codec-http=netty-codec-http-4.1.86.Final.cde0e2d050, netty-codec-http2=netty-codec-http2-4.1.86.Final.cde0e2d (repository: dirty), netty-codec-socks=netty-codec-socks-4.1.86.Final.cde0e2d (repository: dirty), netty-common=netty-common-4.1.86.Final.cde0e2d050, netty-handler=netty-handler-4.1.86.Final.cde0e2d050, netty-handler-proxy=netty-handler-proxy-4.1.92.Final.acc3525 (repository: dirty), netty-resolver=netty-resolver-4.1.86.Final.cde0e2d050, netty-resolver-dns=netty-resolver-dns-4.1.86.Final.cde0e2d050, netty-resolver-dns-classes-macos=netty-resolver-dns-classes-macos-4.1.86.Final.cde0e2d050, netty-resolver-dns-native-macos=netty-resolver-dns-native-macos-4.1.86.Final.cde0e2d050, netty-transport=netty-transport-4.1.86.Final.cde0e2d050, netty-transport-classes-epoll=netty-transport-classes-epoll-4.1.86.Final.cde0e2d (repository: dirty), netty-transport-classes-kqueue=netty-transport-classes-kqueue-4.1.86.Final.cde0e2d050, netty-transport-native-epoll=netty-transport-native-epoll-4.1.86.Final.cde0e2d (repository: dirty), netty-transport-native-kqueue=netty-transport-native-kqueue-4.1.86.Final.cde0e2d050, netty-transport-native-unix-common=netty-transport-native-unix-common-4.1.86.Final.cde0e2d050} This means 1) you specified Netty versions inconsistently in your build or 2) the Netty JARs in the classpath were repackaged or shaded incorrectly. Specify the '-Dcom.linecorp.armeria.warnNettyVersions=false' JVM option to disable this warning at the risk of unexpected Netty behavior, if you think it is a false positive. (com.linecorp.armeria.internal.common.util.TransportTypeProvider)
control-center                | [2023-08-04 11:56:48,030] INFO com.linecorp.armeria.useEpoll: false (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,031] INFO com.linecorp.armeria.transportType: nio (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,064] INFO com.linecorp.armeria.maxNumConnections: 2147483647 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,065] INFO com.linecorp.armeria.numCommonWorkers: 8 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,066] INFO com.linecorp.armeria.numCommonBlockingTaskThreads: 200 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,073] INFO com.linecorp.armeria.defaultMaxRequestLength: 10485760 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,075] INFO com.linecorp.armeria.defaultMaxResponseLength: 10485760 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,076] INFO com.linecorp.armeria.defaultRequestTimeoutMillis: 10000 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,077] INFO com.linecorp.armeria.defaultResponseTimeoutMillis: 15000 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,078] INFO com.linecorp.armeria.defaultConnectTimeoutMillis: 3200 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,080] INFO com.linecorp.armeria.defaultWriteTimeoutMillis: 1000 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,081] INFO com.linecorp.armeria.defaultServerIdleTimeoutMillis: 15000 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,082] INFO com.linecorp.armeria.defaultClientIdleTimeoutMillis: 10000 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,085] INFO com.linecorp.armeria.defaultPingIntervalMillis: 0 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,087] INFO com.linecorp.armeria.defaultMaxServerNumRequestsPerConnection: 0 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,089] INFO com.linecorp.armeria.defaultMaxClientNumRequestsPerConnection: 0 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,090] INFO com.linecorp.armeria.defaultMaxServerConnectionAgeMillis: 0 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,091] INFO com.linecorp.armeria.defaultMaxClientConnectionAgeMillis: 0 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,092] INFO com.linecorp.armeria.defaultServerConnectionDrainDurationMicros: 1000000 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,094] INFO com.linecorp.armeria.defaultHttp2InitialConnectionWindowSize: 1048576 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,095] INFO com.linecorp.armeria.defaultHttp2InitialStreamWindowSize: 1048576 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,097] INFO com.linecorp.armeria.defaultHttp2MaxFrameSize: 16384 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,098] INFO com.linecorp.armeria.defaultHttp2MaxStreamsPerConnection: 2147483647 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,100] INFO com.linecorp.armeria.defaultHttp2MaxHeaderListSize: 8192 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,103] INFO com.linecorp.armeria.defaultHttp1MaxInitialLineLength: 4096 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,105] INFO com.linecorp.armeria.defaultHttp1MaxHeaderSize: 8192 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,106] INFO com.linecorp.armeria.defaultHttp1MaxChunkSize: 8192 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,107] INFO com.linecorp.armeria.defaultUseHttp2Preface: true (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,107] INFO com.linecorp.armeria.defaultUseHttp1Pipelining: false (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,109] INFO com.linecorp.armeria.defaultBackoffSpec: exponential=200:10000,jitter=0.2 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,111] INFO com.linecorp.armeria.defaultMaxTotalAttempts: 10 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,113] INFO com.linecorp.armeria.routeCache: maximumSize=4096 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,114] INFO com.linecorp.armeria.routeDecoratorCache: maximumSize=4096 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,114] INFO com.linecorp.armeria.parsedPathCache: maximumSize=4096 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,115] INFO com.linecorp.armeria.headerValueCache: maximumSize=4096 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,118] INFO com.linecorp.armeria.cachedHeaders: :authority,:scheme,:method,accept-encoding,content-type (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,120] INFO com.linecorp.armeria.fileServiceCache: maximumSize=1024 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,121] INFO com.linecorp.armeria.dnsCacheSpec: maximumSize=4096 (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,122] INFO com.linecorp.armeria.annotatedServiceExceptionVerbosity: unhandled (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,124] INFO com.linecorp.armeria.useJdkDnsResolver: false (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,124] INFO com.linecorp.armeria.reportBlockedEventLoop: true (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,125] INFO com.linecorp.armeria.validateHeaders: true (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,125] INFO com.linecorp.armeria.tlsAllowUnsafeCiphers: false (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,126] INFO com.linecorp.armeria.transientServiceOptions:  (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,139] INFO com.linecorp.armeria.useLegacyRouteDecoratorOrdering: false (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,139] INFO com.linecorp.armeria.useDefaultSocketOptions: true (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,140] INFO Using nio (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:48,673] INFO IPv6: disabled (from /proc/sys/net/ipv6/conf/all/disable_ipv6) (com.linecorp.armeria.common.util.SystemInfo)
control-center                | [2023-08-04 11:56:48,971] INFO com.linecorp.armeria.useOpenSsl: true (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:49,901] INFO Using OpenSSL: BoringSSL, 0x1010107f (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:49,902] INFO com.linecorp.armeria.dumpOpenSslInfo: false (default) (com.linecorp.armeria.common.Flags)
control-center                | [2023-08-04 11:56:50,811] INFO HV000001: Hibernate Validator null (org.hibernate.validator.internal.util.Version)
control-center                | [2023-08-04 11:56:51,440] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                | 	metrics.jmx.prefix = confluent.controlcenter
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = true
control-center                | 	port = 9021
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:56:51,933] INFO getPersistentStoreTopicNames=[_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center                | [2023-08-04 11:56:51,953] INFO getLruStoreTopicNames=[_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center                | [2023-08-04 11:56:51,955] INFO getWindowedStoreTopicNames=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center                | [2023-08-04 11:56:51,958] INFO getLogAppendTimeIntermediateTopicNames=[_confluent-controlcenter-7-4-1-1-cluster-rekey, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store] (io.confluent.controlcenter.ControlCenterModule)
control-center                | [2023-08-04 11:56:51,961] INFO intermediateTopics=[_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center                | [2023-08-04 11:56:51,965] INFO StreamsConfig values: 
control-center                | 	acceptable.recovery.lag = 10000
control-center                | 	application.id = _confluent-controlcenter-7-4-1-1-command
control-center                | 	application.server = 
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffered.records.per.partition = 1000
control-center                | 	built.in.metrics.version = latest
control-center                | 	cache.max.bytes.buffering = 0
control-center                | 	client.id = 
control-center                | 	commit.interval.ms = 30000
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
control-center                | 	default.dsl.store = rocksDB
control-center                | 	default.key.serde = null
control-center                | 	default.list.key.serde.inner = null
control-center                | 	default.list.key.serde.type = null
control-center                | 	default.list.value.serde.inner = null
control-center                | 	default.list.value.serde.type = null
control-center                | 	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
control-center                | 	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
control-center                | 	default.value.serde = null
control-center                | 	max.task.idle.ms = 0
control-center                | 	max.warmup.replicas = 2
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	num.standby.replicas = 0
control-center                | 	num.stream.threads = 1
control-center                | 	poll.ms = 100
control-center                | 	probing.rebalance.interval.ms = 600000
control-center                | 	processing.guarantee = at_least_once
control-center                | 	rack.aware.assignment.tags = []
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	repartition.purge.interval.ms = 30000
control-center                | 	replication.factor = -1
control-center                | 	request.timeout.ms = 40000
control-center                | 	retries = 0
control-center                | 	retry.backoff.ms = 100
control-center                | 	rocksdb.config.setter = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	send.buffer.bytes = 131072
control-center                | 	state.cleanup.delay.ms = 600000
control-center                | 	state.dir = /var/lib/confluent-control-center/1/cp-command
control-center                | 	statestore.cache.max.bytes = 10485760
control-center                | 	task.timeout.ms = 0
control-center                | 	topology.optimization = all
control-center                | 	upgrade.from = 2.3
control-center                | 	window.size.ms = null
control-center                | 	windowed.inner.class.serde = null
control-center                | 	windowstore.changelog.additional.retention.ms = 86400000
control-center                |  (org.apache.kafka.streams.StreamsConfig)
control-center                | [2023-08-04 11:56:52,042] WARN Deprecated config cache.max.bytes.buffering is set, and will be used; we suggest setting the new config statestore.cache.max.bytes instead as deprecated cache.max.bytes.buffering would be removed in the future. (org.apache.kafka.streams.internals.StreamsConfigUtils)
control-center                | [2023-08-04 11:56:52,138] INFO No process id found on disk, got fresh process id fc02bb9d-1669-4403-9b48-1381c4309ebc (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                | [2023-08-04 11:56:52,341] INFO AdminClientConfig values: 
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-admin
control-center                | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	confluent.use.controller.listener = false
control-center                | 	connections.max.idle.ms = 300000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                | [2023-08-04 11:56:52,388] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:52,388] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:52,388] INFO Kafka startTimeMs: 1691150212388 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:52,393] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc] Kafka Streams version: 7.4.1-ce (org.apache.kafka.streams.KafkaStreams)
control-center                | [2023-08-04 11:56:52,393] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc] Kafka Streams commit ID: 96cc303d3f85bf31 (org.apache.kafka.streams.KafkaStreams)
control-center                | [2023-08-04 11:56:52,441] WARN Deprecated config cache.max.bytes.buffering is set, and will be used; we suggest setting the new config statestore.cache.max.bytes instead as deprecated cache.max.bytes.buffering would be removed in the future. (org.apache.kafka.streams.internals.StreamsConfigUtils)
control-center                | [2023-08-04 11:56:52,518] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:56:52,529] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = true
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-restore-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = null
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 300000
control-center                | 	max.poll.records = 1000
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:56:52,647] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:52,647] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:52,647] INFO Kafka startTimeMs: 1691150212647 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:52,678] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:56:52,687] INFO ProducerConfig values: 
control-center                | 	acks = -1
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	batch.size = 16384
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffer.memory = 33554432
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-producer
control-center                | 	compression.type = lz4
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	delivery.timeout.ms = 2147483647
control-center                | 	enable.idempotence = true
control-center                | 	interceptor.classes = []
control-center                | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                | 	linger.ms = 500
control-center                | 	max.block.ms = 9223372036854775807
control-center                | 	max.in.flight.requests.per.connection = 5
control-center                | 	max.request.size = 10485760
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metadata.max.idle.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partitioner.adaptive.partitioning.enable = true
control-center                | 	partitioner.availability.timeout.ms = 0
control-center                | 	partitioner.class = null
control-center                | 	partitioner.ignore.keys = false
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	transaction.timeout.ms = 60000
control-center                | 	transactional.id = null
control-center                | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                | [2023-08-04 11:56:52,691] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                | [2023-08-04 11:56:52,720] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:52,720] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:52,720] INFO Kafka startTimeMs: 1691150212720 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:52,780] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:56:52,792] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:56:52,794] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-producer] ProducerId set to 1 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                | [2023-08-04 11:56:52,805] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = false
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = _confluent-controlcenter-7-4-1-1-command
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 300000
control-center                | 	max.poll.records = 1000
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:56:52,880] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:56:52,883] WARN stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:56:52,961] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:52,962] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:52,963] INFO Kafka startTimeMs: 1691150212961 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:53,009] INFO ProducerConfig values: 
control-center                | 	acks = -1
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	batch.size = 16384
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffer.memory = 33554432
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = c3-command
control-center                | 	compression.type = lz4
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	delivery.timeout.ms = 2147483647
control-center                | 	enable.idempotence = true
control-center                | 	interceptor.classes = []
control-center                | 	key.serializer = class io.confluent.serializers.ProtoSerde
control-center                | 	linger.ms = 500
control-center                | 	max.block.ms = 9223372036854775807
control-center                | 	max.in.flight.requests.per.connection = 5
control-center                | 	max.request.size = 10485760
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metadata.max.idle.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partitioner.adaptive.partitioning.enable = true
control-center                | 	partitioner.availability.timeout.ms = 0
control-center                | 	partitioner.class = null
control-center                | 	partitioner.ignore.keys = false
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	transaction.timeout.ms = 60000
control-center                | 	transactional.id = null
control-center                | 	value.serializer = class io.confluent.serializers.ProtoSerde
control-center                |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                | [2023-08-04 11:56:53,011] INFO [Producer clientId=c3-command] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                | [2023-08-04 11:56:53,031] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:53,031] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:53,032] INFO Kafka startTimeMs: 1691150213031 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:53,075] INFO [Producer clientId=c3-command] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:56:53,082] INFO [Producer clientId=c3-command] ProducerId set to 2 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                | [2023-08-04 11:56:53,109] INFO getPersistentStoreTopicNames=[_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center                | [2023-08-04 11:56:53,110] INFO getLruStoreTopicNames=[_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center                | [2023-08-04 11:56:53,112] INFO getWindowedStoreTopicNames=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center                | [2023-08-04 11:56:53,115] INFO getLogAppendTimeIntermediateTopicNames=[_confluent-controlcenter-7-4-1-1-cluster-rekey, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store] (io.confluent.controlcenter.ControlCenterModule)
control-center                | [2023-08-04 11:56:53,116] INFO intermediateTopics=[_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center                | WARNING: An illegal reflective access operation has occurred
control-center                | WARNING: Illegal reflective access by retrofit2.Platform (file:/usr/share/java/acl/acl-7.4.1.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class,int)
control-center                | WARNING: Please consider reporting this to the maintainers of retrofit2.Platform
control-center                | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
control-center                | WARNING: All illegal access operations will be denied in a future release
control-center                | [2023-08-04 11:56:53,742] INFO StreamsConfig values: 
control-center                | 	acceptable.recovery.lag = 10000
control-center                | 	application.id = _confluent-controlcenter-7-4-1-1-command
control-center                | 	application.server = 
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffered.records.per.partition = 1000
control-center                | 	built.in.metrics.version = latest
control-center                | 	cache.max.bytes.buffering = 0
control-center                | 	client.id = 
control-center                | 	commit.interval.ms = 30000
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
control-center                | 	default.dsl.store = rocksDB
control-center                | 	default.key.serde = null
control-center                | 	default.list.key.serde.inner = null
control-center                | 	default.list.key.serde.type = null
control-center                | 	default.list.value.serde.inner = null
control-center                | 	default.list.value.serde.type = null
control-center                | 	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
control-center                | 	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
control-center                | 	default.value.serde = null
control-center                | 	max.task.idle.ms = 0
control-center                | 	max.warmup.replicas = 2
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	num.standby.replicas = 0
control-center                | 	num.stream.threads = 1
control-center                | 	poll.ms = 100
control-center                | 	probing.rebalance.interval.ms = 600000
control-center                | 	processing.guarantee = at_least_once
control-center                | 	rack.aware.assignment.tags = []
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	repartition.purge.interval.ms = 30000
control-center                | 	replication.factor = -1
control-center                | 	request.timeout.ms = 40000
control-center                | 	retries = 0
control-center                | 	retry.backoff.ms = 100
control-center                | 	rocksdb.config.setter = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	send.buffer.bytes = 131072
control-center                | 	state.cleanup.delay.ms = 600000
control-center                | 	state.dir = /var/lib/confluent-control-center/1/cp-command
control-center                | 	statestore.cache.max.bytes = 10485760
control-center                | 	task.timeout.ms = 0
control-center                | 	topology.optimization = all
control-center                | 	upgrade.from = 2.3
control-center                | 	window.size.ms = null
control-center                | 	windowed.inner.class.serde = null
control-center                | 	windowstore.changelog.additional.retention.ms = 86400000
control-center                |  (org.apache.kafka.streams.StreamsConfig)
control-center                | [2023-08-04 11:56:53,773] INFO AdminClientConfig values: 
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1
control-center                | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	confluent.use.controller.listener = false
control-center                | 	connections.max.idle.ms = 300000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                | [2023-08-04 11:56:53,789] WARN These configurations '[replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                | [2023-08-04 11:56:53,789] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:53,789] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:53,790] INFO Kafka startTimeMs: 1691150213789 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:53,904] INFO App info kafka.admin.client for _confluent-controlcenter-license-manager-7-4-1-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:53,921] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:56:53,922] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:56:53,922] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:56:53,940] INFO Starting License Store (io.confluent.license.LicenseStore)
control-center                | [2023-08-04 11:56:53,941] INFO Starting KafkaBasedLog with topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
control-center                | [2023-08-04 11:56:53,942] INFO AdminClientConfig values: 
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1
control-center                | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	confluent.use.controller.listener = false
control-center                | 	connections.max.idle.ms = 300000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                | [2023-08-04 11:56:53,958] WARN These configurations '[replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                | [2023-08-04 11:56:53,959] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:53,960] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:53,961] INFO Kafka startTimeMs: 1691150213959 (org.apache.kafka.common.utils.AppInfoParser)
broker                        | [2023-08-04 11:56:54,152] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:56:54,155] INFO [Controller 1] Created topic _confluent-command with topic ID kN5UClpoRbyl7_kzDFgGsA. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:56:54,156] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-command'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:56:54,157] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-command'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:56:54,158] INFO [Controller 1] Created partition _confluent-command-0 with topic ID kN5UClpoRbyl7_kzDFgGsA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:56:54,212] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:56:54,228] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-command-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:56:54,242] INFO [Broker id=1] Creating new partition _confluent-command-0 with topic id kN5UClpoRbyl7_kzDFgGsA. (state.change.logger)
control-center                | [2023-08-04 11:56:54,246] INFO App info kafka.admin.client for _confluent-controlcenter-license-manager-7-4-1-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:54,265] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:56:54,266] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:56:54,266] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:56:54,269] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = true
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1-global-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = null
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 300000
control-center                | 	max.poll.records = 1000
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 120000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:56:54,302] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:54,304] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:54,305] INFO Kafka startTimeMs: 1691150214302 (org.apache.kafka.common.utils.AppInfoParser)
broker                        | [2023-08-04 11:56:54,343] INFO [LogLoader partition=_confluent-command-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:56:54,380] INFO Created log for partition _confluent-command-0 in /tmp/kraft-combined-logs/_confluent-command-0 with properties {cleanup.policy=compact, min.insync.replicas=1} (kafka.log.LogManager)
control-center                | [2023-08-04 11:56:54,410] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
broker                        | [2023-08-04 11:56:54,427] INFO [Partition _confluent-command-0 broker=1] No checkpointed highwatermark is found for partition _confluent-command-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:56:54,429] INFO [Partition _confluent-command-0 broker=1] Log loaded for partition _confluent-command-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:56:54,442] INFO [Broker id=1] Leader _confluent-command-0 with topic id Some(kN5UClpoRbyl7_kzDFgGsA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center                | [2023-08-04 11:56:54,455] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:56:54,456] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:56:54,457] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:56:54,474] INFO App info kafka.consumer for _confluent-controlcenter-license-manager-7-4-1-1-global-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:54,477] INFO ProducerConfig values: 
control-center                | 	acks = -1
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	batch.size = 16384
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffer.memory = 33554432
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1
control-center                | 	compression.type = lz4
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	delivery.timeout.ms = 2147483647
control-center                | 	enable.idempotence = false
control-center                | 	interceptor.classes = []
control-center                | 	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
control-center                | 	linger.ms = 500
control-center                | 	max.block.ms = 9223372036854775807
control-center                | 	max.in.flight.requests.per.connection = 1
control-center                | 	max.request.size = 10485760
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metadata.max.idle.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partitioner.adaptive.partitioning.enable = true
control-center                | 	partitioner.availability.timeout.ms = 0
control-center                | 	partitioner.class = null
control-center                | 	partitioner.ignore.keys = false
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	transaction.timeout.ms = 60000
control-center                | 	transactional.id = null
control-center                | 	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
control-center                |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                | [2023-08-04 11:56:54,525] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:54,526] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:54,527] INFO Kafka startTimeMs: 1691150214525 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:54,531] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = true
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = earliest
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1-global-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = null
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 300000
control-center                | 	max.poll.records = 1000
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 120000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
broker                        | [2023-08-04 11:56:54,544] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-command with new configuration : cleanup.policy -> compact,min.insync.replicas -> 1 (kafka.server.metadata.DynamicConfigPublisher)
control-center                | [2023-08-04 11:56:54,594] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:54,607] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:54,607] INFO Kafka startTimeMs: 1691150214594 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:54,609] INFO [Producer clientId=_confluent-controlcenter-license-manager-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:56:54,644] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:56:54,654] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Assigned to partition(s): _confluent-command-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:56:54,671] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Seeking to earliest offset of partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:56:54,737] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-command-0 to 0 since the associated topicId changed from null to kN5UClpoRbyl7_kzDFgGsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:56:54,838] INFO Finished reading KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
control-center                | [2023-08-04 11:56:54,838] INFO Started KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
control-center                | [2023-08-04 11:56:54,838] INFO Started License Store (io.confluent.license.LicenseStore)
control-center                | [2023-08-04 11:56:55,756] INFO AdminClientConfig values: 
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1
control-center                | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	confluent.use.controller.listener = false
control-center                | 	connections.max.idle.ms = 300000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                | [2023-08-04 11:56:55,766] WARN These configurations '[replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                | [2023-08-04 11:56:55,767] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:55,768] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:55,769] INFO Kafka startTimeMs: 1691150215766 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:55,868] INFO App info kafka.admin.client for _confluent-controlcenter-license-manager-7-4-1-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:55,879] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:56:55,879] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:56:55,888] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:56:56,257] INFO AdminClientConfig values: 
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1
control-center                | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	confluent.use.controller.listener = false
control-center                | 	connections.max.idle.ms = 300000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                | [2023-08-04 11:56:56,268] WARN These configurations '[replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                | [2023-08-04 11:56:56,268] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:56,268] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:56,269] INFO Kafka startTimeMs: 1691150216268 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:56,304] INFO App info kafka.admin.client for _confluent-controlcenter-license-manager-7-4-1-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:56:56,313] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:56:56,313] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:56:56,314] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:56:56,314] INFO License for single cluster, single node (io.confluent.license.LicenseManager)
control-center                | [2023-08-04 11:56:56,319] INFO License: Free Tier license for Confluent Enterprise. (io.confluent.controlcenter.license.LicenseModule)
control-center                | [2023-08-04 11:56:56,320] INFO License: Free Tier license for Confluent Enterprise. (io.confluent.controlcenter.license.LicenseModule)
control-center                | [2023-08-04 11:56:56,337] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                | 	metrics.jmx.prefix = confluent.controlcenter
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = true
control-center                | 	port = 9021
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:56:57,812] INFO Reflections took 1443 ms to scan 5 urls, producing 215 keys and 847 values  (org.reflections.Reflections)
control-center                | [2023-08-04 11:56:57,909] INFO EventEmitterConfig values: 
control-center                |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
control-center                | [2023-08-04 11:56:57,912] INFO EventEmitterConfig values: 
control-center                |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
control-center                | [2023-08-04 11:56:57,984] INFO Linux CPU collector enabled: true (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                | [2023-08-04 11:56:57,984] INFO Using cpu metric: io\.confluent\.kafka\.server/server/linux_system_cpu_utilization (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                | [2023-08-04 11:56:58,061] INFO ConfluentTelemetryConfig values: 
control-center                | 	confluent.telemetry.api.key = null
control-center                | 	confluent.telemetry.api.secret = null
control-center                | 	confluent.telemetry.debug.enabled = false
control-center                | 	confluent.telemetry.enabled = false
control-center                | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
control-center                | 	confluent.telemetry.events.enable = true
control-center                | 	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*
control-center                | 	confluent.telemetry.metrics.collector.interval.ms = 60000
control-center                | 	confluent.telemetry.metrics.collector.slo.enabled = false
control-center                | 	confluent.telemetry.proxy.password = null
control-center                | 	confluent.telemetry.proxy.url = null
control-center                | 	confluent.telemetry.proxy.username = null
control-center                |  (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                | [2023-08-04 11:56:58,077] INFO VolumeMetricsCollectorConfig values: 
control-center                | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
control-center                |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
control-center                | [2023-08-04 11:56:58,121] INFO HttpExporterConfig values: 
control-center                | 	api.key = null
control-center                | 	api.secret = null
control-center                | 	buffer.batch.duration.max.ms = null
control-center                | 	buffer.batch.items.max = null
control-center                | 	buffer.inflight.submissions.max = null
control-center                | 	buffer.pending.batches.max = null
control-center                | 	client.attempts.max = null
control-center                | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center                | 	client.compression = null
control-center                | 	client.connect.timeout.ms = null
control-center                | 	client.contentType = null
control-center                | 	client.request.timeout.ms = null
control-center                | 	client.retry.delay.seconds = null
control-center                | 	enabled = false
control-center                | 	events.enabled = true
control-center                | 	metrics.enabled = true
control-center                | 	metrics.include = null
control-center                | 	proxy.password = null
control-center                | 	proxy.url = null
control-center                | 	proxy.username = null
control-center                | 	type = http
control-center                |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
control-center                | [2023-08-04 11:56:58,126] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                | [2023-08-04 11:56:58,128] INFO RemoteConfigConfiguration values: 
control-center                | 	enabled = true
control-center                | 	polling.interval.ms = 60000
control-center                |  (io.confluent.shaded.io.confluent.telemetry.config.remote.RemoteConfigConfiguration)
control-center                | [2023-08-04 11:56:58,241] WARN Ignoring redefinition of existing telemetry label controlcenter.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
control-center                | [2023-08-04 11:56:58,263] INFO ConfluentTelemetryConfig values: 
control-center                | 	confluent.telemetry.api.key = null
control-center                | 	confluent.telemetry.api.secret = null
control-center                | 	confluent.telemetry.debug.enabled = false
control-center                | 	confluent.telemetry.enabled = false
control-center                | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
control-center                | 	confluent.telemetry.events.enable = true
control-center                | 	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.controlcenter/.*(metrics_input_topic_progress|monitoring_input_topic_progress|misconfigured_topics|missing_topic_configurations|broker_log_persistent_dir|cluster_offline|streams_status|total_lag|request_latency|response_size|response_rate).*
control-center                | 	confluent.telemetry.metrics.collector.interval.ms = 60000
control-center                | 	confluent.telemetry.metrics.collector.slo.enabled = false
control-center                | 	confluent.telemetry.proxy.password = null
control-center                | 	confluent.telemetry.proxy.url = null
control-center                | 	confluent.telemetry.proxy.username = null
control-center                |  (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                | [2023-08-04 11:56:58,265] INFO VolumeMetricsCollectorConfig values: 
control-center                | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
control-center                |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
control-center                | [2023-08-04 11:56:58,266] INFO HttpExporterConfig values: 
control-center                | 	api.key = null
control-center                | 	api.secret = null
control-center                | 	buffer.batch.duration.max.ms = null
control-center                | 	buffer.batch.items.max = null
control-center                | 	buffer.inflight.submissions.max = null
control-center                | 	buffer.pending.batches.max = null
control-center                | 	client.attempts.max = null
control-center                | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center                | 	client.compression = null
control-center                | 	client.connect.timeout.ms = null
control-center                | 	client.contentType = null
control-center                | 	client.request.timeout.ms = null
control-center                | 	client.retry.delay.seconds = null
control-center                | 	enabled = false
control-center                | 	events.enabled = true
control-center                | 	metrics.enabled = true
control-center                | 	metrics.include = null
control-center                | 	proxy.password = null
control-center                | 	proxy.url = null
control-center                | 	proxy.username = null
control-center                | 	type = http
control-center                |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
control-center                | [2023-08-04 11:56:58,267] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                | [2023-08-04 11:56:58,267] INFO RemoteConfigConfiguration values: 
control-center                | 	enabled = true
control-center                | 	polling.interval.ms = 60000
control-center                |  (io.confluent.shaded.io.confluent.telemetry.config.remote.RemoteConfigConfiguration)
control-center                | [2023-08-04 11:56:58,268] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
control-center                | [2023-08-04 11:56:58,277] INFO EventLoggerConfig values: 
control-center                | 	event.logger.cloudevent.codec = structured
control-center                | 	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter
control-center                |  (io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig)
control-center                | [2023-08-04 11:56:58,302] INFO HttpExporterConfig values: 
control-center                | 	api.key = null
control-center                | 	api.secret = null
control-center                | 	buffer.batch.duration.max.ms = null
control-center                | 	buffer.batch.items.max = null
control-center                | 	buffer.inflight.submissions.max = null
control-center                | 	buffer.pending.batches.max = null
control-center                | 	client.attempts.max = null
control-center                | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center                | 	client.compression = null
control-center                | 	client.connect.timeout.ms = null
control-center                | 	client.request.timeout.ms = null
control-center                | 	client.retry.delay.seconds = null
control-center                | 	enabled = false
control-center                | 	events.enabled = true
control-center                | 	metrics.enabled = true
control-center                | 	proxy.password = null
control-center                | 	proxy.url = null
control-center                | 	proxy.username = null
control-center                | 	type = http
control-center                |  (io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig)
control-center                | [2023-08-04 11:57:00,554] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter)
control-center                | [2023-08-04 11:57:00,707] INFO Logging initialized @24717ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
control-center                | [2023-08-04 11:57:00,843] INFO CONTROL CENTER UI
control-center                | 
control-center                | By using Control Center, subject to any license you may have with Confluent, you agree to the Confluent Data Protection Agreement.  In particular, please note that the version check feature of Control Center is enabled.
control-center                | 
control-center                | With this enabled, this instance is configured to collect and report certain data (version information, time stamped session IDs, instance ID, instance uptime, license key for subscription customers, IP address, and other product data)  to Confluent, Inc. ("Confluent") or its parent, subsidiaries, affiliates or service providers every hour.  By proceeding with `confluent.support.metrics.enable=true`, you agree to all such collection, transfer and use of Version information by Confluent. You can turn the version check feature off by setting `confluent.support.metrics.enable=false` in the Control Center configuration and restarting Control Center.  See the Confluent Enterprise documentation for further information.
control-center                |  (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | 
control-center                | [2023-08-04 11:57:00,894] INFO Starting Control Center version=7.4.1 (io.confluent.controlcenter.application.AllControlCenter)
control-center                | [2023-08-04 11:57:00,921] INFO topicListings=[(name=_confluent-command, topicId=kN5UClpoRbyl7_kzDFgGsA, internal=false)] (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:00,926] INFO missingTopics=[_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, _confluent-metrics, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-cluster-rekey, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, _confluent-monitoring, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition] (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:00,929] INFO extantTopics=[_confluent-command] (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:00,947] INFO checking topicDescription=(name=_confluent-command, internal=false, partitions=(partition=0, leader=broker:29092 (id: 1 rack: null), replicas=broker:29092 (id: 1 rack: null), isr=broker:29092 (id: 1 rack: null)), authorizedOperations=null) (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:00,948] INFO found topic=_confluent-command with partitions=1 (io.confluent.controlcenter.KafkaHelper)
broker                        | [2023-08-04 11:57:00,977] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:00,990] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition with topic ID WxsE3khbTSeLjB1T1J8emQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:00,991] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:00,992] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:00,993] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:00,997] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,006] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,008] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,009] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,009] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,009] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with topic ID WxsE3khbTSeLjB1T1J8emQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,039] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:01,040] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:01,041] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with topic id WxsE3khbTSeLjB1T1J8emQ. (state.change.logger)
broker                        | [2023-08-04 11:57:01,080] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:01,093] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,094] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog with topic ID CSKlFJarTLeavMmlMoLkNg. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,094] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,096] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,097] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,104] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,105] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,105] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,106] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,107] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,107] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with topic ID CSKlFJarTLeavMmlMoLkNg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,103] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:01,111] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,111] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,113] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with topic id Some(WxsE3khbTSeLjB1T1J8emQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:01,122] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:01,142] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:01,143] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:01,146] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with topic id CSKlFJarTLeavMmlMoLkNg. (state.change.logger)
broker                        | [2023-08-04 11:57:01,168] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,170] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog with topic ID udOxxlJuRZOwWTijIN5vaw. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,170] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,171] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,171] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,172] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,173] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,174] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,174] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,175] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,175] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with topic ID udOxxlJuRZOwWTijIN5vaw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,189] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:01,195] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:01,198] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,198] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,199] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with topic id Some(CSKlFJarTLeavMmlMoLkNg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:01,217] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:01,227] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:01,228] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:01,229] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with topic id udOxxlJuRZOwWTijIN5vaw. (state.change.logger)
broker                        | [2023-08-04 11:57:01,247] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,251] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog with topic ID IW2eSmqhSXaPTTq83LBS5g. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,252] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,253] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,254] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,255] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,249] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:01,257] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,259] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,264] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,266] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,268] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with topic ID IW2eSmqhSXaPTTq83LBS5g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,268] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:01,270] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,271] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,272] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with topic id Some(udOxxlJuRZOwWTijIN5vaw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:01,284] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:01,309] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:01,309] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:01,309] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with topic id IW2eSmqhSXaPTTq83LBS5g. (state.change.logger)
broker                        | [2023-08-04 11:57:01,338] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,339] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition with topic ID oX7QDCssTMyPiUUXzSsOBw. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,340] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,340] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,341] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,341] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,341] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,342] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,342] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,342] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,343] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with topic ID oX7QDCssTMyPiUUXzSsOBw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,349] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:01,355] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:01,357] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,357] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,359] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with topic id Some(IW2eSmqhSXaPTTq83LBS5g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:01,380] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:01,408] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:01,409] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:01,410] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with topic id oX7QDCssTMyPiUUXzSsOBw. (state.change.logger)
broker                        | [2023-08-04 11:57:01,419] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,420] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog with topic ID DE82LZAvR3qeRkgbe_n0JA. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,427] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,431] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,433] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,434] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,435] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,436] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,437] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,438] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,439] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with topic ID DE82LZAvR3qeRkgbe_n0JA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,432] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:01,448] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:01,451] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,452] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,453] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with topic id Some(oX7QDCssTMyPiUUXzSsOBw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:01,473] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:01,485] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:01,490] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:01,494] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,498] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog with topic ID 3Mo-8h7IQJejhwp_sOpLmA. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,500] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,501] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,502] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,499] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with topic id DE82LZAvR3qeRkgbe_n0JA. (state.change.logger)
broker                        | [2023-08-04 11:57:01,507] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,512] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,513] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,515] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,517] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,519] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with topic ID 3Mo-8h7IQJejhwp_sOpLmA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,543] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:01,549] INFO Created log for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:01,551] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,551] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,552] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with topic id Some(DE82LZAvR3qeRkgbe_n0JA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:01,570] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:01,576] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:01,576] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:01,577] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with topic id 3Mo-8h7IQJejhwp_sOpLmA. (state.change.logger)
broker                        | [2023-08-04 11:57:01,580] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,583] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition with topic ID OXQfEqBORpejqIqTxEl2xw. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,584] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,585] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,586] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,587] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,588] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,589] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,590] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,591] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,592] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with topic ID OXQfEqBORpejqIqTxEl2xw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,608] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:01,623] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:01,634] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,635] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,635] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with topic id Some(3Mo-8h7IQJejhwp_sOpLmA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:01,648] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:01,652] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,655] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store with topic ID C_nrxqIHSiOvyQnBjl1AUg. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,656] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,657] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,658] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,659] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,660] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:01,661] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:01,662] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with topic id OXQfEqBORpejqIqTxEl2xw. (state.change.logger)
broker                        | [2023-08-04 11:57:01,665] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,666] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with topic ID C_nrxqIHSiOvyQnBjl1AUg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,680] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:01,689] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:01,690] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,691] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,691] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with topic id Some(OXQfEqBORpejqIqTxEl2xw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:01,708] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:01,719] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:01,720] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:01,720] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with topic id C_nrxqIHSiOvyQnBjl1AUg. (state.change.logger)
broker                        | [2023-08-04 11:57:01,729] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,733] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog with topic ID H1w9PWSHRsOWqiZQQDBPqQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,735] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,735] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,736] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,737] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,738] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,740] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,742] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:01,755] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,755] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,756] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:01,757] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,758] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,758] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with topic id Some(C_nrxqIHSiOvyQnBjl1AUg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:01,756] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with topic ID H1w9PWSHRsOWqiZQQDBPqQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,773] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:01,790] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:01,793] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:01,798] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with topic id H1w9PWSHRsOWqiZQQDBPqQ. (state.change.logger)
broker                        | [2023-08-04 11:57:01,821] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,822] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog with topic ID m7OhDxavRJ-M5HmBzaV5yg. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,823] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,824] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,825] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,826] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,826] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,827] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,828] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,833] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,834] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with topic ID m7OhDxavRJ-M5HmBzaV5yg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,839] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:01,851] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:01,853] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,854] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,856] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with topic id Some(H1w9PWSHRsOWqiZQQDBPqQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:01,872] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:01,889] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:01,890] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:01,890] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with topic id m7OhDxavRJ-M5HmBzaV5yg. (state.change.logger)
broker                        | [2023-08-04 11:57:01,896] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,900] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition with topic ID XVsNE4hQQ_GMmfYnXHoYvQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,901] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,901] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,901] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,902] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,902] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,902] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,903] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,903] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,903] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with topic ID XVsNE4hQQ_GMmfYnXHoYvQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,917] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:01,936] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:01,938] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,939] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:01,940] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with topic id Some(m7OhDxavRJ-M5HmBzaV5yg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:01,944] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,949] INFO [Controller 1] Created topic _confluent-metrics with topic ID wK8jWwjgTTqetlRuGAkiWQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,950] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,951] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration max.message.bytes to 10485760 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,952] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,953] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,954] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration retention.ms to 259200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,951] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:01,956] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,958] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:01,959] INFO [Controller 1] Created partition _confluent-metrics-0 with topic ID wK8jWwjgTTqetlRuGAkiWQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,960] INFO [Controller 1] Created partition _confluent-metrics-1 with topic ID wK8jWwjgTTqetlRuGAkiWQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,960] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:01,961] INFO [Controller 1] Created partition _confluent-metrics-2 with topic ID wK8jWwjgTTqetlRuGAkiWQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,965] INFO [Controller 1] Created partition _confluent-metrics-3 with topic ID wK8jWwjgTTqetlRuGAkiWQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,967] INFO [Controller 1] Created partition _confluent-metrics-4 with topic ID wK8jWwjgTTqetlRuGAkiWQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,962] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:01,969] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with topic id XVsNE4hQQ_GMmfYnXHoYvQ. (state.change.logger)
broker                        | [2023-08-04 11:57:01,971] INFO [Controller 1] Created partition _confluent-metrics-5 with topic ID wK8jWwjgTTqetlRuGAkiWQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,975] INFO [Controller 1] Created partition _confluent-metrics-6 with topic ID wK8jWwjgTTqetlRuGAkiWQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,977] INFO [Controller 1] Created partition _confluent-metrics-7 with topic ID wK8jWwjgTTqetlRuGAkiWQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,978] INFO [Controller 1] Created partition _confluent-metrics-8 with topic ID wK8jWwjgTTqetlRuGAkiWQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,980] INFO [Controller 1] Created partition _confluent-metrics-9 with topic ID wK8jWwjgTTqetlRuGAkiWQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,985] INFO [Controller 1] Created partition _confluent-metrics-10 with topic ID wK8jWwjgTTqetlRuGAkiWQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,987] INFO [Controller 1] Created partition _confluent-metrics-11 with topic ID wK8jWwjgTTqetlRuGAkiWQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:01,999] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,003] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,005] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,005] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,006] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with topic id Some(XVsNE4hQQ_GMmfYnXHoYvQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:02,015] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:02,023] INFO [Broker id=1] Transitioning 12 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:02,024] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-metrics-11, _confluent-metrics-9, _confluent-metrics-10, _confluent-metrics-7, _confluent-metrics-8, _confluent-metrics-5, _confluent-metrics-6, _confluent-metrics-3, _confluent-metrics-4, _confluent-metrics-1, _confluent-metrics-2, _confluent-metrics-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:02,024] INFO [Broker id=1] Creating new partition _confluent-metrics-11 with topic id wK8jWwjgTTqetlRuGAkiWQ. (state.change.logger)
broker                        | [2023-08-04 11:57:02,040] INFO [LogLoader partition=_confluent-metrics-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,044] INFO Created log for partition _confluent-metrics-11 in /tmp/kraft-combined-logs/_confluent-metrics-11 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,047] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,049] INFO [Partition _confluent-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-11 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,053] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog with topic ID 1DqRjjOIRXmdRKnCn_UUXw. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,057] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,057] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,057] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,058] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,058] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,058] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,058] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,059] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,059] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with topic ID 1DqRjjOIRXmdRKnCn_UUXw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,056] INFO [Partition _confluent-metrics-11 broker=1] Log loaded for partition _confluent-metrics-11 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,060] INFO [Broker id=1] Leader _confluent-metrics-11 with topic id Some(wK8jWwjgTTqetlRuGAkiWQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:02,070] INFO [Broker id=1] Creating new partition _confluent-metrics-9 with topic id wK8jWwjgTTqetlRuGAkiWQ. (state.change.logger)
broker                        | [2023-08-04 11:57:02,081] INFO [LogLoader partition=_confluent-metrics-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,086] INFO Created log for partition _confluent-metrics-9 in /tmp/kraft-combined-logs/_confluent-metrics-9 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,087] INFO [Partition _confluent-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-9 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,088] INFO [Partition _confluent-metrics-9 broker=1] Log loaded for partition _confluent-metrics-9 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,088] INFO [Broker id=1] Leader _confluent-metrics-9 with topic id Some(wK8jWwjgTTqetlRuGAkiWQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:02,119] INFO [Broker id=1] Creating new partition _confluent-metrics-10 with topic id wK8jWwjgTTqetlRuGAkiWQ. (state.change.logger)
broker                        | [2023-08-04 11:57:02,127] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-cluster-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,129] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-cluster-rekey with topic ID Z2pHmm4pTUKxYyUe94ozjg. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,132] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-cluster-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,133] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-cluster-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,134] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-cluster-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,135] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-cluster-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,136] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-cluster-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,136] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 with topic ID Z2pHmm4pTUKxYyUe94ozjg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,136] INFO [LogLoader partition=_confluent-metrics-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,142] INFO Created log for partition _confluent-metrics-10 in /tmp/kraft-combined-logs/_confluent-metrics-10 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,143] INFO [Partition _confluent-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-10 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,144] INFO [Partition _confluent-metrics-10 broker=1] Log loaded for partition _confluent-metrics-10 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,145] INFO [Broker id=1] Leader _confluent-metrics-10 with topic id Some(wK8jWwjgTTqetlRuGAkiWQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:02,157] INFO [Broker id=1] Creating new partition _confluent-metrics-7 with topic id wK8jWwjgTTqetlRuGAkiWQ. (state.change.logger)
broker                        | [2023-08-04 11:57:02,188] INFO [LogLoader partition=_confluent-metrics-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,192] INFO Created log for partition _confluent-metrics-7 in /tmp/kraft-combined-logs/_confluent-metrics-7 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,195] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,198] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition with topic ID lq4rDYV1QDqKOwkzyAoo9A. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,198] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,199] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,199] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,200] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,200] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,200] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,201] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,201] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,201] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with topic ID lq4rDYV1QDqKOwkzyAoo9A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,203] INFO [Partition _confluent-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-7 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,204] INFO [Partition _confluent-metrics-7 broker=1] Log loaded for partition _confluent-metrics-7 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,205] INFO [Broker id=1] Leader _confluent-metrics-7 with topic id Some(wK8jWwjgTTqetlRuGAkiWQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:02,220] INFO [Broker id=1] Creating new partition _confluent-metrics-8 with topic id wK8jWwjgTTqetlRuGAkiWQ. (state.change.logger)
broker                        | [2023-08-04 11:57:02,234] INFO [LogLoader partition=_confluent-metrics-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,238] INFO Created log for partition _confluent-metrics-8 in /tmp/kraft-combined-logs/_confluent-metrics-8 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,239] INFO [Partition _confluent-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-8 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,240] INFO [Partition _confluent-metrics-8 broker=1] Log loaded for partition _confluent-metrics-8 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,241] INFO [Broker id=1] Leader _confluent-metrics-8 with topic id Some(wK8jWwjgTTqetlRuGAkiWQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:02,253] INFO [Broker id=1] Creating new partition _confluent-metrics-5 with topic id wK8jWwjgTTqetlRuGAkiWQ. (state.change.logger)
broker                        | [2023-08-04 11:57:02,270] INFO [LogLoader partition=_confluent-metrics-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,284] INFO Created log for partition _confluent-metrics-5 in /tmp/kraft-combined-logs/_confluent-metrics-5 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,285] INFO [Partition _confluent-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-5 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,286] INFO [Partition _confluent-metrics-5 broker=1] Log loaded for partition _confluent-metrics-5 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,287] INFO [Broker id=1] Leader _confluent-metrics-5 with topic id Some(wK8jWwjgTTqetlRuGAkiWQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:02,294] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,296] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog with topic ID RFgEQoDlQFa80gJgHe9JRA. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,298] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,300] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,301] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,303] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,304] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,305] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,306] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,305] INFO [Broker id=1] Creating new partition _confluent-metrics-6 with topic id wK8jWwjgTTqetlRuGAkiWQ. (state.change.logger)
broker                        | [2023-08-04 11:57:02,319] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,320] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with topic ID RFgEQoDlQFa80gJgHe9JRA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,328] INFO [LogLoader partition=_confluent-metrics-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,333] INFO Created log for partition _confluent-metrics-6 in /tmp/kraft-combined-logs/_confluent-metrics-6 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,335] INFO [Partition _confluent-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-6 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,338] INFO [Partition _confluent-metrics-6 broker=1] Log loaded for partition _confluent-metrics-6 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,339] INFO [Broker id=1] Leader _confluent-metrics-6 with topic id Some(wK8jWwjgTTqetlRuGAkiWQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:02,365] INFO [Broker id=1] Creating new partition _confluent-metrics-3 with topic id wK8jWwjgTTqetlRuGAkiWQ. (state.change.logger)
broker                        | [2023-08-04 11:57:02,377] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,378] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition with topic ID RR06KGwRRYOaeNU4Gqm-Zg. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,385] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,392] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,394] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,394] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,395] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,396] INFO [LogLoader partition=_confluent-metrics-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,396] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,399] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with topic ID RR06KGwRRYOaeNU4Gqm-Zg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,401] INFO Created log for partition _confluent-metrics-3 in /tmp/kraft-combined-logs/_confluent-metrics-3 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,401] INFO [Partition _confluent-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-3 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,402] INFO [Partition _confluent-metrics-3 broker=1] Log loaded for partition _confluent-metrics-3 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,403] INFO [Broker id=1] Leader _confluent-metrics-3 with topic id Some(wK8jWwjgTTqetlRuGAkiWQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:02,410] INFO [Broker id=1] Creating new partition _confluent-metrics-4 with topic id wK8jWwjgTTqetlRuGAkiWQ. (state.change.logger)
broker                        | [2023-08-04 11:57:02,433] INFO [LogLoader partition=_confluent-metrics-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,437] INFO Created log for partition _confluent-metrics-4 in /tmp/kraft-combined-logs/_confluent-metrics-4 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,437] INFO [Partition _confluent-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-4 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,437] INFO [Partition _confluent-metrics-4 broker=1] Log loaded for partition _confluent-metrics-4 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,438] INFO [Broker id=1] Leader _confluent-metrics-4 with topic id Some(wK8jWwjgTTqetlRuGAkiWQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:02,454] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,455] INFO [Broker id=1] Creating new partition _confluent-metrics-1 with topic id wK8jWwjgTTqetlRuGAkiWQ. (state.change.logger)
broker                        | [2023-08-04 11:57:02,467] INFO [LogLoader partition=_confluent-metrics-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,456] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog with topic ID G13gO5m1Tu65SpppF8ORAQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,485] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,486] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,486] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,487] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,488] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,489] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,490] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,491] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,492] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with topic ID G13gO5m1Tu65SpppF8ORAQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,497] INFO Created log for partition _confluent-metrics-1 in /tmp/kraft-combined-logs/_confluent-metrics-1 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,498] INFO [Partition _confluent-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-1 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,500] INFO [Partition _confluent-metrics-1 broker=1] Log loaded for partition _confluent-metrics-1 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,502] INFO [Broker id=1] Leader _confluent-metrics-1 with topic id Some(wK8jWwjgTTqetlRuGAkiWQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:02,510] INFO [Broker id=1] Creating new partition _confluent-metrics-2 with topic id wK8jWwjgTTqetlRuGAkiWQ. (state.change.logger)
broker                        | [2023-08-04 11:57:02,541] INFO [LogLoader partition=_confluent-metrics-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,551] INFO Created log for partition _confluent-metrics-2 in /tmp/kraft-combined-logs/_confluent-metrics-2 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,553] INFO [Partition _confluent-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-2 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,557] INFO [Partition _confluent-metrics-2 broker=1] Log loaded for partition _confluent-metrics-2 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,562] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,565] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog with topic ID Z14vI2zRTVykA1OcTvSTYA. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,566] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,567] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,568] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,569] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,570] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,571] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,571] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,572] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,572] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with topic ID Z14vI2zRTVykA1OcTvSTYA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,573] INFO [Broker id=1] Leader _confluent-metrics-2 with topic id Some(wK8jWwjgTTqetlRuGAkiWQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:02,600] INFO [Broker id=1] Creating new partition _confluent-metrics-0 with topic id wK8jWwjgTTqetlRuGAkiWQ. (state.change.logger)
broker                        | [2023-08-04 11:57:02,640] INFO [LogLoader partition=_confluent-metrics-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,648] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,651] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition with topic ID JAXqb7GGTdqtek9MkwaOpA. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,652] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,652] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,653] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,654] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,656] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,657] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,658] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,659] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,659] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with topic ID JAXqb7GGTdqtek9MkwaOpA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,654] INFO Created log for partition _confluent-metrics-0 in /tmp/kraft-combined-logs/_confluent-metrics-0 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,665] INFO [Partition _confluent-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,667] INFO [Partition _confluent-metrics-0 broker=1] Log loaded for partition _confluent-metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,668] INFO [Broker id=1] Leader _confluent-metrics-0 with topic id Some(wK8jWwjgTTqetlRuGAkiWQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:02,678] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-metrics with new configuration : cleanup.policy -> delete,max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:02,685] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:02,686] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:02,687] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with topic id 1DqRjjOIRXmdRKnCn_UUXw. (state.change.logger)
broker                        | [2023-08-04 11:57:02,703] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,711] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,712] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,713] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,714] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with topic id Some(1DqRjjOIRXmdRKnCn_UUXw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:02,718] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,725] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition with topic ID 57o9ucS7SheIO2bNS2WFSg. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,726] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,728] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,729] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,734] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,735] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,736] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,731] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:02,755] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,757] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,760] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:02,760] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with topic ID 57o9ucS7SheIO2bNS2WFSg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,767] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-cluster-rekey-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:02,770] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 with topic id Z2pHmm4pTUKxYyUe94ozjg. (state.change.logger)
broker                        | [2023-08-04 11:57:02,789] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-cluster-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,796] INFO Created log for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-cluster-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,805] INFO [Partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,805] INFO [Partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,806] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-cluster-rekey-0 with topic id Some(Z2pHmm4pTUKxYyUe94ozjg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:02,820] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-cluster-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:02,821] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,823] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey with topic ID L-ObqkHFS6CjenaFpJHZaA. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,824] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,825] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,826] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,826] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,827] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,827] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with topic ID L-ObqkHFS6CjenaFpJHZaA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,834] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:02,836] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:02,837] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with topic id lq4rDYV1QDqKOwkzyAoo9A. (state.change.logger)
broker                        | [2023-08-04 11:57:02,885] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,886] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey with topic ID yaGoMLo8Td-SorQ7IkopwQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,887] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,888] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,889] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,890] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,891] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,891] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with topic ID yaGoMLo8Td-SorQ7IkopwQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,894] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,901] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,904] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,905] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,906] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with topic id Some(lq4rDYV1QDqKOwkzyAoo9A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:02,918] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:02,928] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:02,929] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:02,930] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with topic id RFgEQoDlQFa80gJgHe9JRA. (state.change.logger)
broker                        | [2023-08-04 11:57:02,939] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,941] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey with topic ID 3Ikd_jUKStGHd1em7l_jkA. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,941] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,942] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,943] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,944] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,945] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:02,947] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with topic ID 3Ikd_jUKStGHd1em7l_jkA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:02,974] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:02,978] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:02,980] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,980] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:02,981] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with topic id Some(RFgEQoDlQFa80gJgHe9JRA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:03,007] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,009] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog with topic ID tErqfg2lSt2ont74tat6Mg. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,010] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,010] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,010] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,010] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,010] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,011] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,011] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,011] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,012] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with topic ID tErqfg2lSt2ont74tat6Mg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,009] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:03,021] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:03,022] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:03,023] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with topic id RR06KGwRRYOaeNU4Gqm-Zg. (state.change.logger)
broker                        | [2023-08-04 11:57:03,038] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:03,042] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:03,054] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,056] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,060] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,064] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition with topic ID U8HfQfJVRdyml8fRNiDflA. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,066] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,067] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,068] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,069] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,070] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,071] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,072] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,073] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,074] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with topic ID U8HfQfJVRdyml8fRNiDflA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,076] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with topic id Some(RR06KGwRRYOaeNU4Gqm-Zg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:03,092] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition with new configuration : cleanup.policy -> delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 604800000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:03,098] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:03,100] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:03,100] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with topic id G13gO5m1Tu65SpppF8ORAQ. (state.change.logger)
broker                        | [2023-08-04 11:57:03,128] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,131] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition with topic ID ghHTzj_ySuGkMDlILihrKw. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,133] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,134] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,135] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,136] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,137] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,138] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,140] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,141] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,142] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with topic ID ghHTzj_ySuGkMDlILihrKw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,147] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:03,158] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:03,159] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,160] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,161] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with topic id Some(G13gO5m1Tu65SpppF8ORAQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:03,187] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:03,191] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:03,191] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,192] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog with topic ID XLmE0NflT4CcMuJMDPnJnA. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,192] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,193] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,193] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,193] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,195] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,197] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,199] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,200] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,201] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with topic ID XLmE0NflT4CcMuJMDPnJnA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,203] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:03,205] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with topic id Z14vI2zRTVykA1OcTvSTYA. (state.change.logger)
broker                        | [2023-08-04 11:57:03,219] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:03,224] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:03,225] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,226] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,231] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with topic id Some(Z14vI2zRTVykA1OcTvSTYA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:03,254] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:03,259] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,260] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition with topic ID Z1JInkhAQeOqQi_VEyyZIA. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,260] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,260] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,260] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,260] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,261] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,261] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,261] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,261] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,261] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with topic ID Z1JInkhAQeOqQi_VEyyZIA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,266] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:03,270] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:03,272] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with topic id JAXqb7GGTdqtek9MkwaOpA. (state.change.logger)
broker                        | [2023-08-04 11:57:03,293] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:03,304] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:03,306] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,307] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,308] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with topic id Some(JAXqb7GGTdqtek9MkwaOpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:03,322] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:03,326] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:03,329] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:03,332] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with topic id 57o9ucS7SheIO2bNS2WFSg. (state.change.logger)
broker                        | [2023-08-04 11:57:03,353] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,355] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition with topic ID wj0jvN-hRme9pGvic2I8ZQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,356] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,356] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,357] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,358] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,359] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,359] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,360] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,361] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,362] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with topic ID wj0jvN-hRme9pGvic2I8ZQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,368] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:03,424] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:03,425] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,425] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,427] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with topic id Some(57o9ucS7SheIO2bNS2WFSg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:03,440] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:03,445] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,445] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:03,447] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:03,447] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog with topic ID pff6SMW7TImfl9gdGHtPbA. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,448] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,448] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,449] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,449] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,455] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,455] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,456] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,456] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,457] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with topic ID pff6SMW7TImfl9gdGHtPbA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,449] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with topic id L-ObqkHFS6CjenaFpJHZaA. (state.change.logger)
broker                        | [2023-08-04 11:57:03,472] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:03,475] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:03,477] INFO [Partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,478] INFO [Partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,480] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with topic id Some(L-ObqkHFS6CjenaFpJHZaA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:03,495] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:03,501] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:03,502] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:03,502] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with topic id yaGoMLo8Td-SorQ7IkopwQ. (state.change.logger)
broker                        | [2023-08-04 11:57:03,518] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,518] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog with topic ID 0CHwPhqaS4OeJ6WSlifxKA. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,518] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,519] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,519] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,519] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,519] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,519] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,520] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,520] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,520] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with topic ID 0CHwPhqaS4OeJ6WSlifxKA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,518] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:03,525] INFO Created log for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:03,527] INFO [Partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,529] INFO [Partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,531] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with topic id Some(yaGoMLo8Td-SorQ7IkopwQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:03,543] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:03,548] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:03,549] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:03,550] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with topic id 3Ikd_jUKStGHd1em7l_jkA. (state.change.logger)
broker                        | [2023-08-04 11:57:03,561] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:03,569] INFO Created log for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:03,571] INFO [Partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,574] INFO [Partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,574] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with topic id Some(3Ikd_jUKStGHd1em7l_jkA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:03,585] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,586] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog with topic ID XiYOYBFMSJmNBxVgCK1fWg. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,587] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,588] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,589] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,590] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,590] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,591] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,592] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,593] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,594] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with topic ID XiYOYBFMSJmNBxVgCK1fWg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,602] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:03,605] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:03,606] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:03,607] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with topic id tErqfg2lSt2ont74tat6Mg. (state.change.logger)
broker                        | [2023-08-04 11:57:03,622] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:03,643] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:03,644] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,645] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,645] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with topic id Some(tErqfg2lSt2ont74tat6Mg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:03,651] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,652] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey with topic ID YZgYTx8aSQGlP9yT8BRY6A. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,653] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,653] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,653] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,654] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,654] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,654] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with topic ID YZgYTx8aSQGlP9yT8BRY6A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,664] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:03,669] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:03,669] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:03,670] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with topic id U8HfQfJVRdyml8fRNiDflA. (state.change.logger)
broker                        | [2023-08-04 11:57:03,689] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:03,695] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:03,697] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,698] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,700] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with topic id Some(U8HfQfJVRdyml8fRNiDflA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:03,702] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,702] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition with topic ID S1F7vK69RgOOQfMEbFc03w. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,703] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,703] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,703] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,703] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,704] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,704] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,704] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,705] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,705] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with topic ID S1F7vK69RgOOQfMEbFc03w and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,716] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:03,719] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:03,720] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:03,720] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with topic id ghHTzj_ySuGkMDlILihrKw. (state.change.logger)
broker                        | [2023-08-04 11:57:03,733] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:03,736] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:03,737] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,738] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,739] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with topic id Some(ghHTzj_ySuGkMDlILihrKw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:03,754] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:03,754] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,755] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey with topic ID QpQV-WRTTiy9FejLVOtjsA. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,756] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,756] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,756] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,756] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,757] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,757] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with topic ID QpQV-WRTTiy9FejLVOtjsA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,757] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:03,757] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:03,757] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with topic id XLmE0NflT4CcMuJMDPnJnA. (state.change.logger)
broker                        | [2023-08-04 11:57:03,774] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:03,777] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:03,780] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,780] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,781] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with topic id Some(XLmE0NflT4CcMuJMDPnJnA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:03,791] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:03,794] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:03,794] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:03,795] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with topic id Z1JInkhAQeOqQi_VEyyZIA. (state.change.logger)
broker                        | [2023-08-04 11:57:03,807] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,809] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition with topic ID B0VF6WymRKWUb1_mSPHrNQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,810] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,812] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,813] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,818] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,818] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,818] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,818] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,818] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,819] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with topic ID B0VF6WymRKWUb1_mSPHrNQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,820] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:03,824] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:03,825] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,826] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,826] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with topic id Some(Z1JInkhAQeOqQi_VEyyZIA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:03,837] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:03,841] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:03,841] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:03,842] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with topic id wj0jvN-hRme9pGvic2I8ZQ. (state.change.logger)
broker                        | [2023-08-04 11:57:03,879] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,879] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition with topic ID EFEDEKlhQneKmkKv6vq2ug. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,880] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,880] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,880] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,881] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,881] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,881] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,882] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,883] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,883] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with topic ID EFEDEKlhQneKmkKv6vq2ug and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,890] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:03,896] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:03,903] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,903] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,904] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with topic id Some(wj0jvN-hRme9pGvic2I8ZQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:03,920] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:03,923] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:03,924] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:03,925] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with topic id pff6SMW7TImfl9gdGHtPbA. (state.change.logger)
broker                        | [2023-08-04 11:57:03,939] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,939] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog with topic ID jTIWXGalTvWLtZI3sjE5sw. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,940] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,940] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,940] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,941] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,941] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,941] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,941] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,941] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:03,942] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with topic ID jTIWXGalTvWLtZI3sjE5sw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:03,944] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:03,949] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:03,950] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,951] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:03,951] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with topic id Some(pff6SMW7TImfl9gdGHtPbA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:03,975] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:04,003] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,004] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition with topic ID r0uQNoysQn2xMJcEpcXoNg. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,004] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,004] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,005] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,005] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,005] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,005] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,005] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,005] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,010] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with topic ID r0uQNoysQn2xMJcEpcXoNg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,012] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:04,012] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:04,012] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with topic id 0CHwPhqaS4OeJ6WSlifxKA. (state.change.logger)
broker                        | [2023-08-04 11:57:04,077] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:04,102] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:04,104] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,104] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,105] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with topic id Some(0CHwPhqaS4OeJ6WSlifxKA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:04,108] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,109] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog with topic ID W5DcavKWTu200d1tpaXEpg. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,110] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,111] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,111] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,112] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,112] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,112] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,112] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,112] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,112] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with topic ID W5DcavKWTu200d1tpaXEpg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,133] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:04,145] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:04,146] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:04,160] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with topic id XiYOYBFMSJmNBxVgCK1fWg. (state.change.logger)
broker                        | [2023-08-04 11:57:04,187] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:04,187] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,188] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog with topic ID nOK0g_4LSpKR7it_Lpsx_Q. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,188] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,189] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,190] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,190] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,190] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,191] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,191] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,192] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,192] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with topic ID nOK0g_4LSpKR7it_Lpsx_Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,195] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:04,197] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,198] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,199] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with topic id Some(XiYOYBFMSJmNBxVgCK1fWg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:04,244] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:04,279] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,281] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog with topic ID T5FHHsk0SCS2RlMMCe7jWA. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,282] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,283] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,284] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,284] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,284] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,285] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,286] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,287] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,287] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with topic ID T5FHHsk0SCS2RlMMCe7jWA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,290] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:04,293] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:04,295] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with topic id YZgYTx8aSQGlP9yT8BRY6A. (state.change.logger)
broker                        | [2023-08-04 11:57:04,339] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:04,346] INFO Created log for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:04,349] INFO [Partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,351] INFO [Partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,352] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with topic id Some(YZgYTx8aSQGlP9yT8BRY6A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:04,389] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-monitoring', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,390] INFO [Controller 1] Created topic _confluent-monitoring with topic ID 5-cwUQMrRLuRuq67G0mKBQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,390] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,390] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,392] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,392] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration retention.ms to 259200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,392] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,393] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,394] INFO [Controller 1] Created partition _confluent-monitoring-0 with topic ID 5-cwUQMrRLuRuq67G0mKBQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,399] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:04,405] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:04,406] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:04,407] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with topic id S1F7vK69RgOOQfMEbFc03w. (state.change.logger)
broker                        | [2023-08-04 11:57:04,420] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:04,454] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:04,465] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,467] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition with topic ID qG5cOYixScid67xECGeg3A. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,467] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,470] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,473] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,474] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,475] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,476] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,476] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,476] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,476] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with topic ID qG5cOYixScid67xECGeg3A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,483] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,485] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,486] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with topic id Some(S1F7vK69RgOOQfMEbFc03w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:04,501] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:04,504] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:04,505] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:04,508] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with topic id QpQV-WRTTiy9FejLVOtjsA. (state.change.logger)
broker                        | [2023-08-04 11:57:04,536] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,540] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition with topic ID tQqpYlAhTY6G0QpY8FyKQQ. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,542] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,542] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,543] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,544] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,544] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,545] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,546] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,548] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,549] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with topic ID tQqpYlAhTY6G0QpY8FyKQQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,576] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:04,587] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:04,598] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,601] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,601] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with topic id Some(QpQV-WRTTiy9FejLVOtjsA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:04,613] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:04,616] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,617] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition with topic ID RULHYa_ARZGUuAfobThlxg. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,618] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,618] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,619] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,619] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,620] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,620] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,621] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,624] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,626] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with topic ID RULHYa_ARZGUuAfobThlxg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,616] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:04,630] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:04,637] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with topic id B0VF6WymRKWUb1_mSPHrNQ. (state.change.logger)
broker                        | [2023-08-04 11:57:04,677] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:04,683] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:04,688] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,689] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,690] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with topic id Some(B0VF6WymRKWUb1_mSPHrNQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:04,695] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,696] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition with topic ID jJOhY1W9QsObrKoApgpW4w. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,697] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,698] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,699] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,699] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,700] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,700] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,701] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,701] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:04,701] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with topic ID jJOhY1W9QsObrKoApgpW4w and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:04,714] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:04,725] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:04,726] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:04,732] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with topic id EFEDEKlhQneKmkKv6vq2ug. (state.change.logger)
control-center                | [2023-08-04 11:57:04,751] INFO describing topics=[_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, _confluent-metrics, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-cluster-rekey, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, _confluent-monitoring, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition] (io.confluent.controlcenter.KafkaHelper)
broker                        | [2023-08-04 11:57:04,758] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:04,769] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:04,774] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,775] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,777] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with topic id Some(EFEDEKlhQneKmkKv6vq2ug) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:04,794] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:04,800] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:04,803] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:04,804] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with topic id jTIWXGalTvWLtZI3sjE5sw. (state.change.logger)
broker                        | [2023-08-04 11:57:04,830] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:04,842] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:04,865] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,868] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,871] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with topic id Some(jTIWXGalTvWLtZI3sjE5sw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:04,886] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:04,889] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:04,891] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:04,893] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with topic id r0uQNoysQn2xMJcEpcXoNg. (state.change.logger)
broker                        | [2023-08-04 11:57:04,912] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:04,920] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:04,924] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,931] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,933] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with topic id Some(r0uQNoysQn2xMJcEpcXoNg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:04,943] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:04,950] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:04,951] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:04,953] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with topic id W5DcavKWTu200d1tpaXEpg. (state.change.logger)
broker                        | [2023-08-04 11:57:04,976] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:04,986] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:04,990] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,990] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:04,991] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with topic id Some(W5DcavKWTu200d1tpaXEpg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:05,003] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:05,008] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:05,009] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:05,009] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with topic id nOK0g_4LSpKR7it_Lpsx_Q. (state.change.logger)
broker                        | [2023-08-04 11:57:05,027] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:05,034] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:05,039] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:05,040] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:05,040] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with topic id Some(nOK0g_4LSpKR7it_Lpsx_Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:05,050] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:05,055] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:05,056] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:05,056] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with topic id T5FHHsk0SCS2RlMMCe7jWA. (state.change.logger)
broker                        | [2023-08-04 11:57:05,074] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:05,077] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:05,083] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:05,084] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:05,085] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with topic id Some(T5FHHsk0SCS2RlMMCe7jWA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:05,100] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:05,105] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:05,106] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-monitoring-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:05,107] INFO [Broker id=1] Creating new partition _confluent-monitoring-0 with topic id 5-cwUQMrRLuRuq67G0mKBQ. (state.change.logger)
broker                        | [2023-08-04 11:57:05,123] INFO [LogLoader partition=_confluent-monitoring-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:05,126] INFO Created log for partition _confluent-monitoring-0 in /tmp/kraft-combined-logs/_confluent-monitoring-0 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:05,129] INFO [Partition _confluent-monitoring-0 broker=1] No checkpointed highwatermark is found for partition _confluent-monitoring-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:05,130] INFO [Partition _confluent-monitoring-0 broker=1] Log loaded for partition _confluent-monitoring-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:05,130] INFO [Broker id=1] Leader _confluent-monitoring-0 with topic id Some(5-cwUQMrRLuRuq67G0mKBQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:05,141] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-monitoring with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 259200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:05,147] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:05,149] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:05,150] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with topic id qG5cOYixScid67xECGeg3A. (state.change.logger)
broker                        | [2023-08-04 11:57:05,167] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:05,175] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:05,186] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:05,187] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:05,188] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with topic id Some(qG5cOYixScid67xECGeg3A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:05,207] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:05,219] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:05,220] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:05,221] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with topic id tQqpYlAhTY6G0QpY8FyKQQ. (state.change.logger)
broker                        | [2023-08-04 11:57:05,253] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:05,260] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:05,263] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:05,263] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:05,264] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with topic id Some(tQqpYlAhTY6G0QpY8FyKQQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:05,286] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:05,302] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:05,302] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:05,303] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with topic id RULHYa_ARZGUuAfobThlxg. (state.change.logger)
broker                        | [2023-08-04 11:57:05,340] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:05,356] INFO Created log for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:05,358] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:05,359] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:05,363] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with topic id Some(RULHYa_ARZGUuAfobThlxg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:05,371] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:05,374] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:05,375] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:05,375] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with topic id jJOhY1W9QsObrKoApgpW4w. (state.change.logger)
broker                        | [2023-08-04 11:57:05,399] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:05,402] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:05,410] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:05,411] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:05,414] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with topic id Some(jJOhY1W9QsObrKoApgpW4w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:05,426] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
data-agrigator-taskmanager-1  | 2023-08-04 11:57:10,464 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - The heartbeat of ResourceManager with id 122a0f5539dfb4680be86c02507cca4f timed out.
data-agrigator-taskmanager-1  | 2023-08-04 11:57:10,465 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close ResourceManager connection 122a0f5539dfb4680be86c02507cca4f.
data-agrigator-taskmanager-1  | 2023-08-04 11:57:10,469 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000).
data-agrigator-taskmanager-1  | 2023-08-04 11:57:11,874 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 11:57:11,881 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 11:57:19,970] INFO describing topics=[_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, _confluent-metrics, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-cluster-rekey, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, _confluent-monitoring, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition] (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,016] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,018] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,019] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,019] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,019] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,019] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,019] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,020] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,020] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,020] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,020] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,020] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,020] INFO create=success topic=TopicInfo{name=_confluent-metrics, partitions=12, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,021] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,021] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-cluster-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,021] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,021] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,021] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,021] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,022] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,022] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,022] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,022] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,022] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,023] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,024] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,024] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,025] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,028] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,029] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,030] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,031] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,032] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,033] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,034] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,034] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,034] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,035] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,035] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,035] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,036] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,036] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,036] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,037] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,037] INFO create=success topic=TopicInfo{name=_confluent-monitoring, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,037] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,037] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,038] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,038] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,042] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = true
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = latest
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = will-delete-this
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:20,063] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:20,064] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:20,064] INFO Kafka startTimeMs: 1691150240063 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:20,066] INFO Setting offsets for topic=_confluent-monitoring (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,083] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:20,138] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Assigned to partition(s): _confluent-monitoring-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:20,138] INFO found 1 topicPartitions for topic=_confluent-monitoring (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:20,156] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 5-cwUQMrRLuRuq67G0mKBQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:20,171] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-monitoring-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
broker                        | [2023-08-04 11:57:20,217] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
broker                        | [2023-08-04 11:57:20,227] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='__consumer_offsets', numPartitions=50, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='compression.type', value='producer'), CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='segment.bytes', value='104857600')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,229] INFO [Controller 1] Created topic __consumer_offsets with topic ID hDmuUPPaRx62VZA_gShfTw. (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,229] INFO [Controller 1] ConfigResource(type=TOPIC, name='__consumer_offsets'): set configuration compression.type to producer (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:20,230] INFO [Controller 1] ConfigResource(type=TOPIC, name='__consumer_offsets'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:20,230] INFO [Controller 1] ConfigResource(type=TOPIC, name='__consumer_offsets'): set configuration segment.bytes to 104857600 (org.apache.kafka.controller.ConfigurationControlManager)
broker                        | [2023-08-04 11:57:20,230] INFO [Controller 1] Created partition __consumer_offsets-0 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,231] INFO [Controller 1] Created partition __consumer_offsets-1 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,231] INFO [Controller 1] Created partition __consumer_offsets-2 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,233] INFO [Controller 1] Created partition __consumer_offsets-3 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,234] INFO [Controller 1] Created partition __consumer_offsets-4 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,235] INFO [Controller 1] Created partition __consumer_offsets-5 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,236] INFO [Controller 1] Created partition __consumer_offsets-6 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,238] INFO [Controller 1] Created partition __consumer_offsets-7 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,239] INFO [Controller 1] Created partition __consumer_offsets-8 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,241] INFO [Controller 1] Created partition __consumer_offsets-9 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,242] INFO [Controller 1] Created partition __consumer_offsets-10 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,243] INFO [Controller 1] Created partition __consumer_offsets-11 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,244] INFO [Controller 1] Created partition __consumer_offsets-12 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,245] INFO [Controller 1] Created partition __consumer_offsets-13 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,246] INFO [Controller 1] Created partition __consumer_offsets-14 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,248] INFO [Controller 1] Created partition __consumer_offsets-15 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,249] INFO [Controller 1] Created partition __consumer_offsets-16 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,250] INFO [Controller 1] Created partition __consumer_offsets-17 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,250] INFO [Controller 1] Created partition __consumer_offsets-18 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,250] INFO [Controller 1] Created partition __consumer_offsets-19 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,251] INFO [Controller 1] Created partition __consumer_offsets-20 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,251] INFO [Controller 1] Created partition __consumer_offsets-21 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,251] INFO [Controller 1] Created partition __consumer_offsets-22 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,252] INFO [Controller 1] Created partition __consumer_offsets-23 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,252] INFO [Controller 1] Created partition __consumer_offsets-24 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,252] INFO [Controller 1] Created partition __consumer_offsets-25 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,253] INFO [Controller 1] Created partition __consumer_offsets-26 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,253] INFO [Controller 1] Created partition __consumer_offsets-27 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,254] INFO [Controller 1] Created partition __consumer_offsets-28 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,254] INFO [Controller 1] Created partition __consumer_offsets-29 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,254] INFO [Controller 1] Created partition __consumer_offsets-30 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,254] INFO [Controller 1] Created partition __consumer_offsets-31 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,254] INFO [Controller 1] Created partition __consumer_offsets-32 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,255] INFO [Controller 1] Created partition __consumer_offsets-33 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,258] INFO [Controller 1] Created partition __consumer_offsets-34 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,263] INFO [Controller 1] Created partition __consumer_offsets-35 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,264] INFO [Controller 1] Created partition __consumer_offsets-36 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,264] INFO [Controller 1] Created partition __consumer_offsets-37 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,264] INFO [Controller 1] Created partition __consumer_offsets-38 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,265] INFO [Controller 1] Created partition __consumer_offsets-39 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,265] INFO [Controller 1] Created partition __consumer_offsets-40 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,265] INFO [Controller 1] Created partition __consumer_offsets-41 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,265] INFO [Controller 1] Created partition __consumer_offsets-42 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,266] INFO [Controller 1] Created partition __consumer_offsets-43 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,266] INFO [Controller 1] Created partition __consumer_offsets-44 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,266] INFO [Controller 1] Created partition __consumer_offsets-45 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,267] INFO [Controller 1] Created partition __consumer_offsets-46 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,267] INFO [Controller 1] Created partition __consumer_offsets-47 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,268] INFO [Controller 1] Created partition __consumer_offsets-48 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,269] INFO [Controller 1] Created partition __consumer_offsets-49 with topic ID hDmuUPPaRx62VZA_gShfTw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker                        | [2023-08-04 11:57:20,304] INFO [Broker id=1] Transitioning 50 partition(s) to local leaders. (state.change.logger)
broker                        | [2023-08-04 11:57:20,305] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
broker                        | [2023-08-04 11:57:20,307] INFO [Broker id=1] Creating new partition __consumer_offsets-13 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:20,319] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,322] INFO Created log for partition __consumer_offsets-13 in /tmp/kraft-combined-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,328] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,328] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,329] INFO [Broker id=1] Leader __consumer_offsets-13 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,336] INFO [Broker id=1] Creating new partition __consumer_offsets-46 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:20,349] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,352] INFO Created log for partition __consumer_offsets-46 in /tmp/kraft-combined-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,352] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,353] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,354] INFO [Broker id=1] Leader __consumer_offsets-46 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,364] INFO [Broker id=1] Creating new partition __consumer_offsets-9 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:20,376] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,380] INFO Created log for partition __consumer_offsets-9 in /tmp/kraft-combined-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,381] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,383] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,384] INFO [Broker id=1] Leader __consumer_offsets-9 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,395] INFO [Broker id=1] Creating new partition __consumer_offsets-42 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
control-center                | [2023-08-04 11:57:20,402] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:20,410] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,416] INFO Created log for partition __consumer_offsets-42 in /tmp/kraft-combined-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,416] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,418] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,419] INFO [Broker id=1] Leader __consumer_offsets-42 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,436] INFO [Broker id=1] Creating new partition __consumer_offsets-21 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:20,458] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,466] INFO Created log for partition __consumer_offsets-21 in /tmp/kraft-combined-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,466] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,467] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,468] INFO [Broker id=1] Leader __consumer_offsets-21 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center                | [2023-08-04 11:57:20,481] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:20,482] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:20,482] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:20,487] INFO [Broker id=1] Creating new partition __consumer_offsets-17 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:20,499] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,503] INFO Created log for partition __consumer_offsets-17 in /tmp/kraft-combined-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,503] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,504] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,504] INFO [Broker id=1] Leader __consumer_offsets-17 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,515] INFO [Broker id=1] Creating new partition __consumer_offsets-30 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:20,524] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,529] INFO Created log for partition __consumer_offsets-30 in /tmp/kraft-combined-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,529] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,529] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,530] INFO [Broker id=1] Leader __consumer_offsets-30 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,540] INFO [Broker id=1] Creating new partition __consumer_offsets-26 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:20,550] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,552] INFO Created log for partition __consumer_offsets-26 in /tmp/kraft-combined-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,553] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,553] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,553] INFO [Broker id=1] Leader __consumer_offsets-26 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,572] INFO [Broker id=1] Creating new partition __consumer_offsets-5 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
control-center                | [2023-08-04 11:57:20,584] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker                        | [2023-08-04 11:57:20,587] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,590] INFO Created log for partition __consumer_offsets-5 in /tmp/kraft-combined-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,592] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,592] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,594] INFO [Broker id=1] Leader __consumer_offsets-5 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,612] INFO [Broker id=1] Creating new partition __consumer_offsets-38 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
control-center                | [2023-08-04 11:57:20,613] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:20,613] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:20,614] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:20,632] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,636] INFO Created log for partition __consumer_offsets-38 in /tmp/kraft-combined-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,636] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,637] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,637] INFO [Broker id=1] Leader __consumer_offsets-38 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,644] INFO [Broker id=1] Creating new partition __consumer_offsets-1 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:20,652] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,654] INFO Created log for partition __consumer_offsets-1 in /tmp/kraft-combined-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,654] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,655] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,655] INFO [Broker id=1] Leader __consumer_offsets-1 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,664] INFO [Broker id=1] Creating new partition __consumer_offsets-34 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:20,673] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,677] INFO Created log for partition __consumer_offsets-34 in /tmp/kraft-combined-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,678] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,678] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,678] INFO [Broker id=1] Leader __consumer_offsets-34 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,686] INFO [Broker id=1] Creating new partition __consumer_offsets-16 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:20,697] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,701] INFO Created log for partition __consumer_offsets-16 in /tmp/kraft-combined-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,701] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,701] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,702] INFO [Broker id=1] Leader __consumer_offsets-16 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,714] INFO [Broker id=1] Creating new partition __consumer_offsets-45 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
control-center                | [2023-08-04 11:57:20,726] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:20,738] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,750] INFO Created log for partition __consumer_offsets-45 in /tmp/kraft-combined-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,751] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,751] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,751] INFO [Broker id=1] Leader __consumer_offsets-45 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center                | [2023-08-04 11:57:20,755] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:20,764] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:20,764] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:20,768] INFO [Broker id=1] Creating new partition __consumer_offsets-12 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:20,784] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,788] INFO Created log for partition __consumer_offsets-12 in /tmp/kraft-combined-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,790] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,792] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,794] INFO [Broker id=1] Leader __consumer_offsets-12 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,807] INFO [Broker id=1] Creating new partition __consumer_offsets-41 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:20,820] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,824] INFO Created log for partition __consumer_offsets-41 in /tmp/kraft-combined-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,827] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,828] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,829] INFO [Broker id=1] Leader __consumer_offsets-41 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,837] INFO [Broker id=1] Creating new partition __consumer_offsets-24 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:20,850] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,854] INFO Created log for partition __consumer_offsets-24 in /tmp/kraft-combined-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,854] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,854] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,854] INFO [Broker id=1] Leader __consumer_offsets-24 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,863] INFO [Broker id=1] Creating new partition __consumer_offsets-20 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
control-center                | [2023-08-04 11:57:20,865] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
control-center                | [2023-08-04 11:57:20,880] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:20,880] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:20,880] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:20,885] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,888] INFO Created log for partition __consumer_offsets-20 in /tmp/kraft-combined-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,889] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,890] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,890] INFO [Broker id=1] Leader __consumer_offsets-20 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,906] INFO [Broker id=1] Creating new partition __consumer_offsets-49 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:20,917] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,920] INFO Created log for partition __consumer_offsets-49 in /tmp/kraft-combined-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,920] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,920] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,921] INFO [Broker id=1] Leader __consumer_offsets-49 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,932] INFO [Broker id=1] Creating new partition __consumer_offsets-0 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:20,942] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,946] INFO Created log for partition __consumer_offsets-0 in /tmp/kraft-combined-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,946] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,947] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,947] INFO [Broker id=1] Leader __consumer_offsets-0 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,957] INFO [Broker id=1] Creating new partition __consumer_offsets-29 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:20,970] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:20,973] INFO Created log for partition __consumer_offsets-29 in /tmp/kraft-combined-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:20,974] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,975] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:20,975] INFO [Broker id=1] Leader __consumer_offsets-29 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:20,986] INFO [Broker id=1] Creating new partition __consumer_offsets-25 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,001] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
control-center                | [2023-08-04 11:57:21,003] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:21,004] INFO Created log for partition __consumer_offsets-25 in /tmp/kraft-combined-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,005] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,005] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,006] INFO [Broker id=1] Leader __consumer_offsets-25 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,024] INFO [Broker id=1] Creating new partition __consumer_offsets-8 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
control-center                | [2023-08-04 11:57:21,026] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:21,028] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:21,030] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:21,036] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,038] INFO Created log for partition __consumer_offsets-8 in /tmp/kraft-combined-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,038] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,039] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,039] INFO [Broker id=1] Leader __consumer_offsets-8 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,050] INFO [Broker id=1] Creating new partition __consumer_offsets-37 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,059] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,063] INFO Created log for partition __consumer_offsets-37 in /tmp/kraft-combined-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,063] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,064] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,064] INFO [Broker id=1] Leader __consumer_offsets-37 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,076] INFO [Broker id=1] Creating new partition __consumer_offsets-4 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,085] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,087] INFO Created log for partition __consumer_offsets-4 in /tmp/kraft-combined-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,087] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,087] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,088] INFO [Broker id=1] Leader __consumer_offsets-4 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,098] INFO [Broker id=1] Creating new partition __consumer_offsets-33 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,106] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,112] INFO Created log for partition __consumer_offsets-33 in /tmp/kraft-combined-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,112] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,112] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,113] INFO [Broker id=1] Leader __consumer_offsets-33 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,121] INFO [Broker id=1] Creating new partition __consumer_offsets-15 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
control-center                | [2023-08-04 11:57:21,131] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker                        | [2023-08-04 11:57:21,131] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,134] INFO Created log for partition __consumer_offsets-15 in /tmp/kraft-combined-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,135] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,135] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,135] INFO [Broker id=1] Leader __consumer_offsets-15 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center                | [2023-08-04 11:57:21,141] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:21,141] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:21,142] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:21,151] INFO [Broker id=1] Creating new partition __consumer_offsets-48 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,171] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,176] INFO Created log for partition __consumer_offsets-48 in /tmp/kraft-combined-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,177] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,177] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,178] INFO [Broker id=1] Leader __consumer_offsets-48 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,185] INFO [Broker id=1] Creating new partition __consumer_offsets-11 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,196] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,200] INFO Created log for partition __consumer_offsets-11 in /tmp/kraft-combined-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,201] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,202] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,203] INFO [Broker id=1] Leader __consumer_offsets-11 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,213] INFO [Broker id=1] Creating new partition __consumer_offsets-44 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,225] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,229] INFO Created log for partition __consumer_offsets-44 in /tmp/kraft-combined-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,230] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,230] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,231] INFO [Broker id=1] Leader __consumer_offsets-44 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,241] INFO [Broker id=1] Creating new partition __consumer_offsets-23 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
control-center                | [2023-08-04 11:57:21,254] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:21,265] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,278] INFO Created log for partition __consumer_offsets-23 in /tmp/kraft-combined-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,279] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
control-center                | [2023-08-04 11:57:21,282] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:21,283] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,283] INFO [Broker id=1] Leader __consumer_offsets-23 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center                | [2023-08-04 11:57:21,283] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:21,283] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:21,294] INFO [Broker id=1] Creating new partition __consumer_offsets-19 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,306] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,314] INFO Created log for partition __consumer_offsets-19 in /tmp/kraft-combined-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,315] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,316] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,317] INFO [Broker id=1] Leader __consumer_offsets-19 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,329] INFO [Broker id=1] Creating new partition __consumer_offsets-32 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,354] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,363] INFO Created log for partition __consumer_offsets-32 in /tmp/kraft-combined-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,364] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,365] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,367] INFO [Broker id=1] Leader __consumer_offsets-32 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,377] INFO [Broker id=1] Creating new partition __consumer_offsets-28 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
control-center                | [2023-08-04 11:57:21,384] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
control-center                | [2023-08-04 11:57:21,400] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:21,401] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:21,401] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:21,404] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,418] INFO Created log for partition __consumer_offsets-28 in /tmp/kraft-combined-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,419] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,420] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,421] INFO [Broker id=1] Leader __consumer_offsets-28 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,433] INFO [Broker id=1] Creating new partition __consumer_offsets-7 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,445] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,449] INFO Created log for partition __consumer_offsets-7 in /tmp/kraft-combined-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,450] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,451] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,451] INFO [Broker id=1] Leader __consumer_offsets-7 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,459] INFO [Broker id=1] Creating new partition __consumer_offsets-40 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,470] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,473] INFO Created log for partition __consumer_offsets-40 in /tmp/kraft-combined-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,474] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,475] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,476] INFO [Broker id=1] Leader __consumer_offsets-40 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,489] INFO [Broker id=1] Creating new partition __consumer_offsets-3 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
control-center                | [2023-08-04 11:57:21,507] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:21,513] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,527] INFO Created log for partition __consumer_offsets-3 in /tmp/kraft-combined-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,528] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,528] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,528] INFO [Broker id=1] Leader __consumer_offsets-3 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,540] INFO [Broker id=1] Creating new partition __consumer_offsets-36 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
control-center                | [2023-08-04 11:57:21,541] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:21,543] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:21,546] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:21,558] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,567] INFO Created log for partition __consumer_offsets-36 in /tmp/kraft-combined-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,568] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,568] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,569] INFO [Broker id=1] Leader __consumer_offsets-36 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,579] INFO [Broker id=1] Creating new partition __consumer_offsets-47 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,594] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,598] INFO Created log for partition __consumer_offsets-47 in /tmp/kraft-combined-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,600] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,600] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,600] INFO [Broker id=1] Leader __consumer_offsets-47 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,610] INFO [Broker id=1] Creating new partition __consumer_offsets-14 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,619] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,622] INFO Created log for partition __consumer_offsets-14 in /tmp/kraft-combined-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,622] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,623] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,623] INFO [Broker id=1] Leader __consumer_offsets-14 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,637] INFO [Broker id=1] Creating new partition __consumer_offsets-43 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,647] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,650] INFO Created log for partition __consumer_offsets-43 in /tmp/kraft-combined-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,650] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,650] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
control-center                | [2023-08-04 11:57:21,650] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker                        | [2023-08-04 11:57:21,651] INFO [Broker id=1] Leader __consumer_offsets-43 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center                | [2023-08-04 11:57:21,667] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:21,668] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:21,668] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:21,670] INFO [Broker id=1] Creating new partition __consumer_offsets-10 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,683] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,686] INFO Created log for partition __consumer_offsets-10 in /tmp/kraft-combined-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,686] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,686] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,687] INFO [Broker id=1] Leader __consumer_offsets-10 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,697] INFO [Broker id=1] Creating new partition __consumer_offsets-22 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,710] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,712] INFO Created log for partition __consumer_offsets-22 in /tmp/kraft-combined-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,713] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,713] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,713] INFO [Broker id=1] Leader __consumer_offsets-22 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,723] INFO [Broker id=1] Creating new partition __consumer_offsets-18 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,735] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,737] INFO Created log for partition __consumer_offsets-18 in /tmp/kraft-combined-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,739] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,740] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,740] INFO [Broker id=1] Leader __consumer_offsets-18 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,759] INFO [Broker id=1] Creating new partition __consumer_offsets-31 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,780] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,784] INFO Created log for partition __consumer_offsets-31 in /tmp/kraft-combined-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,785] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,787] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,788] INFO [Broker id=1] Leader __consumer_offsets-31 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center                | [2023-08-04 11:57:21,785] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:21,805] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:21,806] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:21,806] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:21,816] INFO [Broker id=1] Creating new partition __consumer_offsets-27 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,834] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,837] INFO Created log for partition __consumer_offsets-27 in /tmp/kraft-combined-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,837] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,837] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,840] INFO [Broker id=1] Leader __consumer_offsets-27 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,850] INFO [Broker id=1] Creating new partition __consumer_offsets-39 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,864] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,869] INFO Created log for partition __consumer_offsets-39 in /tmp/kraft-combined-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,869] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,869] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,870] INFO [Broker id=1] Leader __consumer_offsets-39 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,881] INFO [Broker id=1] Creating new partition __consumer_offsets-6 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,896] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,900] INFO Created log for partition __consumer_offsets-6 in /tmp/kraft-combined-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,902] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,903] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,904] INFO [Broker id=1] Leader __consumer_offsets-6 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center                | [2023-08-04 11:57:21,908] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
control-center                | [2023-08-04 11:57:21,917] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:21,918] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:21,918] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:21,936] INFO [Broker id=1] Creating new partition __consumer_offsets-35 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
data-agrigator-taskmanager-1  | 2023-08-04 11:57:21,948 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
broker                        | [2023-08-04 11:57:21,954] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
data-agrigator-taskmanager-1  | 2023-08-04 11:57:21,957 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
broker                        | [2023-08-04 11:57:21,957] INFO Created log for partition __consumer_offsets-35 in /tmp/kraft-combined-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,958] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,958] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,958] INFO [Broker id=1] Leader __consumer_offsets-35 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:21,974] INFO [Broker id=1] Creating new partition __consumer_offsets-2 with topic id hDmuUPPaRx62VZA_gShfTw. (state.change.logger)
broker                        | [2023-08-04 11:57:21,986] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker                        | [2023-08-04 11:57:21,994] INFO Created log for partition __consumer_offsets-2 in /tmp/kraft-combined-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker                        | [2023-08-04 11:57:21,995] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,996] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
broker                        | [2023-08-04 11:57:21,996] INFO [Broker id=1] Leader __consumer_offsets-2 with topic id Some(hDmuUPPaRx62VZA_gShfTw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker                        | [2023-08-04 11:57:22,022] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,027] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,036] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:22,030] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:22,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,038] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,039] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,039] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,040] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,041] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,041] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,042] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,042] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,043] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,044] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,044] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,047] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,048] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,048] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,048] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,048] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,048] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,048] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,049] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,049] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,049] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,049] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,049] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,049] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,049] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,050] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,050] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,050] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,050] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,050] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,050] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,051] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,051] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,051] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,051] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,051] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,051] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,052] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,052] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,052] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,052] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,054] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
control-center                | [2023-08-04 11:57:22,056] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:22,057] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:22,057] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:22,072] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,081] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,082] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,088] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,090] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,092] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,094] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,097] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,098] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,099] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,101] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,103] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,107] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,113] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,114] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,115] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,116] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,119] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,120] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,121] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,123] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,126] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,129] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,131] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,135] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,137] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,138] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,142] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,145] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,146] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,147] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,148] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,151] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,152] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,154] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,153] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 112 milliseconds for epoch 0, of which 38 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,161] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,161] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,161] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,161] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,161] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:22,158] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker                        | [2023-08-04 11:57:22,166] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,166] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,167] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,167] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,168] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,168] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,168] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
control-center                | [2023-08-04 11:57:22,168] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:22,169] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:22,169] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:22,171] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
broker                        | [2023-08-04 11:57:22,178] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 141 milliseconds for epoch 0, of which 118 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,180] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 143 milliseconds for epoch 0, of which 143 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,184] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 145 milliseconds for epoch 0, of which 144 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,184] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 144 milliseconds for epoch 0, of which 144 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,185] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 144 milliseconds for epoch 0, of which 144 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,186] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 143 milliseconds for epoch 0, of which 142 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,187] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 142 milliseconds for epoch 0, of which 142 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,187] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 139 milliseconds for epoch 0, of which 139 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,188] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 140 milliseconds for epoch 0, of which 140 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,190] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 142 milliseconds for epoch 0, of which 141 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,191] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 142 milliseconds for epoch 0, of which 141 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,192] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 143 milliseconds for epoch 0, of which 143 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,193] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 144 milliseconds for epoch 0, of which 144 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,194] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 145 milliseconds for epoch 0, of which 145 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,195] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 145 milliseconds for epoch 0, of which 144 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,195] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 145 milliseconds for epoch 0, of which 145 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,196] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 146 milliseconds for epoch 0, of which 145 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,196] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 145 milliseconds for epoch 0, of which 145 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,197] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 145 milliseconds for epoch 0, of which 145 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,197] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 146 milliseconds for epoch 0, of which 146 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,198] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 145 milliseconds for epoch 0, of which 145 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,198] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 146 milliseconds for epoch 0, of which 146 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,199] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 129 milliseconds for epoch 0, of which 128 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,200] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 118 milliseconds for epoch 0, of which 117 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,200] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 116 milliseconds for epoch 0, of which 116 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,201] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 115 milliseconds for epoch 0, of which 115 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,202] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 114 milliseconds for epoch 0, of which 113 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,202] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 111 milliseconds for epoch 0, of which 111 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,203] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 106 milliseconds for epoch 0, of which 106 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,204] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 105 milliseconds for epoch 0, of which 105 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 102 milliseconds for epoch 0, of which 101 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,207] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 97 milliseconds for epoch 0, of which 96 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,209] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 94 milliseconds for epoch 0, of which 93 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,210] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 92 milliseconds for epoch 0, of which 91 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,211] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 91 milliseconds for epoch 0, of which 91 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,212] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 86 milliseconds for epoch 0, of which 86 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,213] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 82 milliseconds for epoch 0, of which 81 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,213] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 77 milliseconds for epoch 0, of which 77 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,214] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 75 milliseconds for epoch 0, of which 75 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,215] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 69 milliseconds for epoch 0, of which 68 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,216] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 68 milliseconds for epoch 0, of which 67 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,217] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 65 milliseconds for epoch 0, of which 65 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,218] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 64 milliseconds for epoch 0, of which 64 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,219] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 58 milliseconds for epoch 0, of which 58 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,220] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 59 milliseconds for epoch 0, of which 59 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,222] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 56 milliseconds for epoch 0, of which 55 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,225] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 58 milliseconds for epoch 0, of which 57 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,226] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 58 milliseconds for epoch 0, of which 58 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker                        | [2023-08-04 11:57:22,227] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 59 milliseconds for epoch 0, of which 59 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
control-center                | [2023-08-04 11:57:22,275] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:22,405] INFO Setting offsets for topic=_confluent-metrics (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:22,465] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Assigned to partition(s): _confluent-metrics-11, _confluent-metrics-9, _confluent-metrics-10, _confluent-metrics-7, _confluent-metrics-8, _confluent-metrics-5, _confluent-metrics-6, _confluent-metrics-3, _confluent-metrics-4, _confluent-metrics-1, _confluent-metrics-2, _confluent-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:22,465] INFO found 12 topicPartitions for topic=_confluent-metrics (io.confluent.controlcenter.KafkaHelper)
control-center                | [2023-08-04 11:57:22,471] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:22,473] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:22,473] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:22,473] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:22,474] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:22,474] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:22,474] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:22,474] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:22,476] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:22,476] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:22,476] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:22,477] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:22,486] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:22,486] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:22,486] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:22,487] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:22,487] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:22,487] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:22,487] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:22,487] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:22,487] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:22,487] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:22,487] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:22,487] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:22,518] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:57:22,518] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:57:22,519] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:57:22,527] INFO App info kafka.consumer for will-delete-this unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:22,527] INFO action=starting topology=command (io.confluent.controlcenter.application.AllControlCenter)
control-center                | [2023-08-04 11:57:22,530] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc] State transition from CREATED to REBALANCING (org.apache.kafka.streams.KafkaStreams)
control-center                | [2023-08-04 11:57:22,532] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc] Started 1 stream threads (org.apache.kafka.streams.KafkaStreams)
control-center                | [2023-08-04 11:57:22,533] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:22,535] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:22,535] INFO waiting for streams to be in running state. Current state is REBALANCING (io.confluent.command.kafka.CommandStore)
control-center                | [2023-08-04 11:57:22,548] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Subscribed to topic(s): _confluent-command (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:22,571] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Resetting the last seen epoch of partition _confluent-command-0 to 0 since the associated topicId changed from null to kN5UClpoRbyl7_kzDFgGsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:22,573] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:22,578] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:22,584] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:22,667] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
broker                        | [2023-08-04 11:57:22,699] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1-command in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer-4044ed9c-2990-45ff-b8c4-95ab126cf08a and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:22,728] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer-4044ed9c-2990-45ff-b8c4-95ab126cf08a (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:22,730] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:22,730] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:22,779] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1-command in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member _confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer-4044ed9c-2990-45ff-b8c4-95ab126cf08a with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:22,832] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1-command generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:22,841] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Successfully joined group with generation Generation{generationId=1, memberId='_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer-4044ed9c-2990-45ff-b8c4-95ab126cf08a', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:22,868] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer] Skipping the repartition topic validation since there are no repartition topics. (org.apache.kafka.streams.processor.internals.RepartitionTopics)
control-center                | [2023-08-04 11:57:22,945] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Found no committed offset for partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:22,950] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer] All members participating in this rebalance: 
control-center                | fc02bb9d-1669-4403-9b48-1381c4309ebc: [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer-4044ed9c-2990-45ff-b8c4-95ab126cf08a]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:23,000] INFO Decided on assignment: {fc02bb9d-1669-4403-9b48-1381c4309ebc=[activeTasks: ([0_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([0_0=0]) clientTags: ([]) capacity: 1 assigned: 1]} with no followup probing rebalance. (org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor)
control-center                | [2023-08-04 11:57:23,002] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer] Assigned tasks [0_0] including stateful [0_0] to clients as: 
control-center                | fc02bb9d-1669-4403-9b48-1381c4309ebc=[activeTasks: ([0_0]) standbyTasks: ([])]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:23,019] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer] Client fc02bb9d-1669-4403-9b48-1381c4309ebc per-consumer assignment:
control-center                | 	prev owned active {}
control-center                | 	prev owned standby {_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer-4044ed9c-2990-45ff-b8c4-95ab126cf08a=[]}
control-center                | 	assigned active {_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer-4044ed9c-2990-45ff-b8c4-95ab126cf08a=[0_0]}
control-center                | 	revoking active {}
control-center                | 	assigned standby {}
control-center                |  (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:23,019] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:23,020] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Finished assignment for group at generation 1: {_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer-4044ed9c-2990-45ff-b8c4-95ab126cf08a=Assignment(partitions=[_confluent-command-0], userDataSize=52)} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:23,050] INFO [GroupCoordinator 1]: Assignment received from leader _confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer-4044ed9c-2990-45ff-b8c4-95ab126cf08a for group _confluent-controlcenter-7-4-1-1-command for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:23,095] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Successfully synced group in generation Generation{generationId=1, memberId='_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer-4044ed9c-2990-45ff-b8c4-95ab126cf08a', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:23,096] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Notifying assignor about the new Assignment(partitions=[_confluent-command-0], userDataSize=52) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:23,098] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:23,102] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] Handle new assignment with:
control-center                | 	New active tasks: [0_0]
control-center                | 	New standby tasks: []
control-center                | 	Existing active tasks: []
control-center                | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                | [2023-08-04 11:57:23,184] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Adding newly assigned partitions: _confluent-command-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:23,184] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:23,234] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Found no committed offset for partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:23,537] INFO waiting for streams to be in running state. Current state is REBALANCING (io.confluent.command.kafka.CommandStore)
control-center                | [2023-08-04 11:57:23,790] INFO Opening store commander in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore)
control-center                | [2023-08-04 11:57:23,813] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] stream-task [0_0] State store commander did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-command-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:23,813] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] task [0_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:23,894] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] End offset for changelog _confluent-command-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:23,895] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-restore-consumer, groupId=null] Assigned to partition(s): _confluent-command-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:23,899] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:23,915] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-command-0 to 0 since the associated topicId changed from null to kN5UClpoRbyl7_kzDFgGsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:23,916] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:24,036] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] Finished restoring changelog _confluent-command-0 to store commander with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:24,039] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] Setting topic '_confluent-command' to consume from earliest offset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:24,039] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Seeking to earliest offset of partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:24,047] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Found no committed offset for partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:24,056] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] task [0_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:24,057] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] Restoration took 872 ms for all tasks [0_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:24,058] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:24,064] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc] State transition from REBALANCING to RUNNING (org.apache.kafka.streams.KafkaStreams)
control-center                | [2023-08-04 11:57:24,065] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Requesting the log end offset for _confluent-command-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:24,537] INFO Streams state is RUNNING (io.confluent.command.kafka.CommandStore)
control-center                | [2023-08-04 11:57:25,546] INFO action=started topology=command (io.confluent.controlcenter.application.AllControlCenter)
control-center                | [2023-08-04 11:57:25,547] INFO action=starting operation=command-migration  (io.confluent.controlcenter.application.AllControlCenter)
control-center                | [2023-08-04 11:57:25,556] INFO action=completed operation=command-migration (io.confluent.controlcenter.application.AllControlCenter)
control-center                | [2023-08-04 11:57:25,556] INFO action=starting topology=monitoring (io.confluent.controlcenter.application.AllControlCenter)
control-center                | [2023-08-04 11:57:25,602] WARN Deprecated config cache.max.bytes.buffering is set, and will be used; we suggest setting the new config statestore.cache.max.bytes instead as deprecated cache.max.bytes.buffering would be removed in the future. (org.apache.kafka.streams.internals.StreamsConfigUtils)
control-center                | [2023-08-04 11:57:25,676] INFO No process id found on disk, got fresh process id 294242ab-6891-4246-9408-d412e91c3d10 (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center                | [2023-08-04 11:57:25,682] INFO AdminClientConfig values: 
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-admin
control-center                | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	confluent.use.controller.listener = false
control-center                | 	connections.max.idle.ms = 300000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                | [2023-08-04 11:57:25,693] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,693] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,693] INFO Kafka startTimeMs: 1691150245693 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,695] INFO stream-client [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10] Kafka Streams version: 7.4.1-ce (org.apache.kafka.streams.KafkaStreams)
control-center                | [2023-08-04 11:57:25,695] INFO stream-client [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10] Kafka Streams commit ID: 96cc303d3f85bf31 (org.apache.kafka.streams.KafkaStreams)
control-center                | [2023-08-04 11:57:25,700] WARN Deprecated config cache.max.bytes.buffering is set, and will be used; we suggest setting the new config statestore.cache.max.bytes instead as deprecated cache.max.bytes.buffering would be removed in the future. (org.apache.kafka.streams.internals.StreamsConfigUtils)
control-center                | [2023-08-04 11:57:25,701] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:25,714] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = true
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-restore-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = null
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:25,733] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,733] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,733] INFO Kafka startTimeMs: 1691150245733 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,734] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:25,736] INFO ProducerConfig values: 
control-center                | 	acks = -1
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	batch.size = 16384
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffer.memory = 33554432
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-producer
control-center                | 	compression.type = lz4
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	delivery.timeout.ms = 2147483647
control-center                | 	enable.idempotence = true
control-center                | 	interceptor.classes = []
control-center                | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                | 	linger.ms = 500
control-center                | 	max.block.ms = 9223372036854775807
control-center                | 	max.in.flight.requests.per.connection = 5
control-center                | 	max.request.size = 10485760
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metadata.max.idle.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partitioner.adaptive.partitioning.enable = true
control-center                | 	partitioner.availability.timeout.ms = 0
control-center                | 	partitioner.class = null
control-center                | 	partitioner.ignore.keys = false
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	transaction.timeout.ms = 60000
control-center                | 	transactional.id = null
control-center                | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                | [2023-08-04 11:57:25,740] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                | [2023-08-04 11:57:25,755] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,757] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,761] INFO Kafka startTimeMs: 1691150245754 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,763] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:25,766] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = false
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:25,784] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:25,790] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-producer] ProducerId set to 3 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                | [2023-08-04 11:57:25,795] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:25,797] WARN stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:25,803] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,803] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,804] INFO Kafka startTimeMs: 1691150245803 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,811] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:25,813] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = true
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-restore-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = null
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:25,826] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,826] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,827] INFO Kafka startTimeMs: 1691150245826 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,828] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:25,831] INFO ProducerConfig values: 
control-center                | 	acks = -1
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	batch.size = 16384
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffer.memory = 33554432
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-producer
control-center                | 	compression.type = lz4
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	delivery.timeout.ms = 2147483647
control-center                | 	enable.idempotence = true
control-center                | 	interceptor.classes = []
control-center                | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                | 	linger.ms = 500
control-center                | 	max.block.ms = 9223372036854775807
control-center                | 	max.in.flight.requests.per.connection = 5
control-center                | 	max.request.size = 10485760
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metadata.max.idle.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partitioner.adaptive.partitioning.enable = true
control-center                | 	partitioner.availability.timeout.ms = 0
control-center                | 	partitioner.class = null
control-center                | 	partitioner.ignore.keys = false
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	transaction.timeout.ms = 60000
control-center                | 	transactional.id = null
control-center                | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                | [2023-08-04 11:57:25,833] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                | [2023-08-04 11:57:25,845] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,845] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,845] INFO Kafka startTimeMs: 1691150245845 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,846] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:25,848] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = false
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:25,895] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:25,896] WARN stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:25,904] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:25,911] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-producer] ProducerId set to 4 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                | [2023-08-04 11:57:25,917] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,917] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,917] INFO Kafka startTimeMs: 1691150245917 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,923] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:25,927] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = true
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-restore-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = null
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:25,946] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,947] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,948] INFO Kafka startTimeMs: 1691150245946 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,950] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:25,952] INFO ProducerConfig values: 
control-center                | 	acks = -1
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	batch.size = 16384
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffer.memory = 33554432
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-producer
control-center                | 	compression.type = lz4
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	delivery.timeout.ms = 2147483647
control-center                | 	enable.idempotence = true
control-center                | 	interceptor.classes = []
control-center                | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                | 	linger.ms = 500
control-center                | 	max.block.ms = 9223372036854775807
control-center                | 	max.in.flight.requests.per.connection = 5
control-center                | 	max.request.size = 10485760
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metadata.max.idle.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partitioner.adaptive.partitioning.enable = true
control-center                | 	partitioner.availability.timeout.ms = 0
control-center                | 	partitioner.class = null
control-center                | 	partitioner.ignore.keys = false
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	transaction.timeout.ms = 60000
control-center                | 	transactional.id = null
control-center                | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                | [2023-08-04 11:57:25,956] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                | [2023-08-04 11:57:25,972] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,972] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,973] INFO Kafka startTimeMs: 1691150245972 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:25,978] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:25,982] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = false
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,014] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,014] WARN stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,024] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,025] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,026] INFO Kafka startTimeMs: 1691150246024 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,027] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:26,030] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-producer] ProducerId set to 5 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                | [2023-08-04 11:57:26,035] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,045] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = true
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-restore-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = null
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,062] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,062] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,062] INFO Kafka startTimeMs: 1691150246062 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,063] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,065] INFO ProducerConfig values: 
control-center                | 	acks = -1
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	batch.size = 16384
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffer.memory = 33554432
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-producer
control-center                | 	compression.type = lz4
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	delivery.timeout.ms = 2147483647
control-center                | 	enable.idempotence = true
control-center                | 	interceptor.classes = []
control-center                | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                | 	linger.ms = 500
control-center                | 	max.block.ms = 9223372036854775807
control-center                | 	max.in.flight.requests.per.connection = 5
control-center                | 	max.request.size = 10485760
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metadata.max.idle.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partitioner.adaptive.partitioning.enable = true
control-center                | 	partitioner.availability.timeout.ms = 0
control-center                | 	partitioner.class = null
control-center                | 	partitioner.ignore.keys = false
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	transaction.timeout.ms = 60000
control-center                | 	transactional.id = null
control-center                | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                | [2023-08-04 11:57:26,069] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                | [2023-08-04 11:57:26,098] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,099] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,099] INFO Kafka startTimeMs: 1691150246098 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,100] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,102] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = false
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,132] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:26,134] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-producer] ProducerId set to 6 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                | [2023-08-04 11:57:26,136] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,143] WARN stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,148] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,148] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,149] INFO Kafka startTimeMs: 1691150246148 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,157] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,160] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = true
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-restore-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = null
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,200] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,200] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,200] INFO Kafka startTimeMs: 1691150246200 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,201] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,214] INFO ProducerConfig values: 
control-center                | 	acks = -1
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	batch.size = 16384
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffer.memory = 33554432
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-producer
control-center                | 	compression.type = lz4
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	delivery.timeout.ms = 2147483647
control-center                | 	enable.idempotence = true
control-center                | 	interceptor.classes = []
control-center                | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                | 	linger.ms = 500
control-center                | 	max.block.ms = 9223372036854775807
control-center                | 	max.in.flight.requests.per.connection = 5
control-center                | 	max.request.size = 10485760
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metadata.max.idle.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partitioner.adaptive.partitioning.enable = true
control-center                | 	partitioner.availability.timeout.ms = 0
control-center                | 	partitioner.class = null
control-center                | 	partitioner.ignore.keys = false
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	transaction.timeout.ms = 60000
control-center                | 	transactional.id = null
control-center                | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                | [2023-08-04 11:57:26,218] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                | [2023-08-04 11:57:26,265] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,265] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,265] INFO Kafka startTimeMs: 1691150246265 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,266] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,269] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = false
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,298] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,299] WARN stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,308] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,310] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,310] INFO Kafka startTimeMs: 1691150246308 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,314] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,316] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = true
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-restore-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = null
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,329] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,330] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,330] INFO Kafka startTimeMs: 1691150246329 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,332] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,334] INFO ProducerConfig values: 
control-center                | 	acks = -1
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	batch.size = 16384
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffer.memory = 33554432
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-producer
control-center                | 	compression.type = lz4
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	delivery.timeout.ms = 2147483647
control-center                | 	enable.idempotence = true
control-center                | 	interceptor.classes = []
control-center                | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                | 	linger.ms = 500
control-center                | 	max.block.ms = 9223372036854775807
control-center                | 	max.in.flight.requests.per.connection = 5
control-center                | 	max.request.size = 10485760
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metadata.max.idle.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partitioner.adaptive.partitioning.enable = true
control-center                | 	partitioner.availability.timeout.ms = 0
control-center                | 	partitioner.class = null
control-center                | 	partitioner.ignore.keys = false
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	transaction.timeout.ms = 60000
control-center                | 	transactional.id = null
control-center                | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                | [2023-08-04 11:57:26,336] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:26,344] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                | [2023-08-04 11:57:26,346] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-producer] ProducerId set to 7 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                | [2023-08-04 11:57:26,358] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,358] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,358] INFO Kafka startTimeMs: 1691150246358 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,359] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,361] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = false
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,386] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,387] WARN stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,394] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:26,399] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-producer] ProducerId set to 8 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                | [2023-08-04 11:57:26,404] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,406] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,408] INFO Kafka startTimeMs: 1691150246403 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,413] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,415] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = true
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-restore-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = null
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,427] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,427] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,427] INFO Kafka startTimeMs: 1691150246427 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,428] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,430] INFO ProducerConfig values: 
control-center                | 	acks = -1
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	batch.size = 16384
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffer.memory = 33554432
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-producer
control-center                | 	compression.type = lz4
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	delivery.timeout.ms = 2147483647
control-center                | 	enable.idempotence = true
control-center                | 	interceptor.classes = []
control-center                | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                | 	linger.ms = 500
control-center                | 	max.block.ms = 9223372036854775807
control-center                | 	max.in.flight.requests.per.connection = 5
control-center                | 	max.request.size = 10485760
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metadata.max.idle.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partitioner.adaptive.partitioning.enable = true
control-center                | 	partitioner.availability.timeout.ms = 0
control-center                | 	partitioner.class = null
control-center                | 	partitioner.ignore.keys = false
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	transaction.timeout.ms = 60000
control-center                | 	transactional.id = null
control-center                | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                | [2023-08-04 11:57:26,432] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                | [2023-08-04 11:57:26,444] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,445] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,446] INFO Kafka startTimeMs: 1691150246443 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,450] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,452] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = false
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,466] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:26,478] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-producer] ProducerId set to 9 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                | [2023-08-04 11:57:26,482] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,483] WARN stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,490] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,493] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,493] INFO Kafka startTimeMs: 1691150246490 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,497] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,500] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = true
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-restore-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = null
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,519] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,520] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,521] INFO Kafka startTimeMs: 1691150246519 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,523] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,526] INFO ProducerConfig values: 
control-center                | 	acks = -1
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	batch.size = 16384
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffer.memory = 33554432
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-producer
control-center                | 	compression.type = lz4
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	delivery.timeout.ms = 2147483647
control-center                | 	enable.idempotence = true
control-center                | 	interceptor.classes = []
control-center                | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                | 	linger.ms = 500
control-center                | 	max.block.ms = 9223372036854775807
control-center                | 	max.in.flight.requests.per.connection = 5
control-center                | 	max.request.size = 10485760
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metadata.max.idle.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partitioner.adaptive.partitioning.enable = true
control-center                | 	partitioner.availability.timeout.ms = 0
control-center                | 	partitioner.class = null
control-center                | 	partitioner.ignore.keys = false
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	transaction.timeout.ms = 60000
control-center                | 	transactional.id = null
control-center                | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                | [2023-08-04 11:57:26,536] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                | [2023-08-04 11:57:26,550] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,552] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,553] INFO Kafka startTimeMs: 1691150246550 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,556] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,560] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = false
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,578] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,581] WARN stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,596] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,596] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,596] INFO Kafka startTimeMs: 1691150246596 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,610] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:26,616] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-producer] ProducerId set to 10 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                | [2023-08-04 11:57:26,617] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,620] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = true
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-restore-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = null
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,636] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,636] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,636] INFO Kafka startTimeMs: 1691150246636 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,639] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,643] INFO ProducerConfig values: 
control-center                | 	acks = -1
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	batch.size = 16384
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffer.memory = 33554432
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-producer
control-center                | 	compression.type = lz4
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	delivery.timeout.ms = 2147483647
control-center                | 	enable.idempotence = true
control-center                | 	interceptor.classes = []
control-center                | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                | 	linger.ms = 500
control-center                | 	max.block.ms = 9223372036854775807
control-center                | 	max.in.flight.requests.per.connection = 5
control-center                | 	max.request.size = 10485760
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metadata.max.idle.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partitioner.adaptive.partitioning.enable = true
control-center                | 	partitioner.availability.timeout.ms = 0
control-center                | 	partitioner.class = null
control-center                | 	partitioner.ignore.keys = false
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	transaction.timeout.ms = 60000
control-center                | 	transactional.id = null
control-center                | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                | [2023-08-04 11:57:26,656] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                | [2023-08-04 11:57:26,673] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,675] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,676] INFO Kafka startTimeMs: 1691150246672 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,678] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,680] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = false
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,708] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,710] WARN stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,715] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,716] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,716] INFO Kafka startTimeMs: 1691150246715 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,721] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,727] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:26,733] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = true
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-restore-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = null
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,733] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-producer] ProducerId set to 11 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                | [2023-08-04 11:57:26,750] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,751] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,751] INFO Kafka startTimeMs: 1691150246750 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,752] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,754] INFO ProducerConfig values: 
control-center                | 	acks = -1
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	batch.size = 16384
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffer.memory = 33554432
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-producer
control-center                | 	compression.type = lz4
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	delivery.timeout.ms = 2147483647
control-center                | 	enable.idempotence = true
control-center                | 	interceptor.classes = []
control-center                | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                | 	linger.ms = 500
control-center                | 	max.block.ms = 9223372036854775807
control-center                | 	max.in.flight.requests.per.connection = 5
control-center                | 	max.request.size = 10485760
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metadata.max.idle.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partitioner.adaptive.partitioning.enable = true
control-center                | 	partitioner.availability.timeout.ms = 0
control-center                | 	partitioner.class = null
control-center                | 	partitioner.ignore.keys = false
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	transaction.timeout.ms = 60000
control-center                | 	transactional.id = null
control-center                | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                | [2023-08-04 11:57:26,761] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                | [2023-08-04 11:57:26,773] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,774] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,775] INFO Kafka startTimeMs: 1691150246773 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,776] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,778] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = false
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,790] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,791] WARN stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,802] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,806] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,806] INFO Kafka startTimeMs: 1691150246802 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,812] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,815] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = true
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-restore-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = null
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,827] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,827] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,828] INFO Kafka startTimeMs: 1691150246827 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,828] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,834] INFO ProducerConfig values: 
control-center                | 	acks = -1
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	batch.size = 16384
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffer.memory = 33554432
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-producer
control-center                | 	compression.type = lz4
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	delivery.timeout.ms = 2147483647
control-center                | 	enable.idempotence = true
control-center                | 	interceptor.classes = []
control-center                | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                | 	linger.ms = 500
control-center                | 	max.block.ms = 9223372036854775807
control-center                | 	max.in.flight.requests.per.connection = 5
control-center                | 	max.request.size = 10485760
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metadata.max.idle.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partitioner.adaptive.partitioning.enable = true
control-center                | 	partitioner.availability.timeout.ms = 0
control-center                | 	partitioner.class = null
control-center                | 	partitioner.ignore.keys = false
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	transaction.timeout.ms = 60000
control-center                | 	transactional.id = null
control-center                | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                | [2023-08-04 11:57:26,836] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                | [2023-08-04 11:57:26,843] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:26,851] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,852] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,852] INFO Kafka startTimeMs: 1691150246851 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,853] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,861] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = false
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:26,931] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-producer] ProducerId set to 12 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                | [2023-08-04 11:57:26,948] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,950] WARN stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:26,954] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:26,959] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-producer] ProducerId set to 13 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                | [2023-08-04 11:57:26,961] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,961] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,962] INFO Kafka startTimeMs: 1691150246961 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:26,971] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:26,976] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = true
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-restore-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = null
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:27,000] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:27,002] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:27,002] INFO Kafka startTimeMs: 1691150247000 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:27,005] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,008] INFO ProducerConfig values: 
control-center                | 	acks = -1
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	batch.size = 16384
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	buffer.memory = 33554432
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-producer
control-center                | 	compression.type = lz4
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	delivery.timeout.ms = 2147483647
control-center                | 	enable.idempotence = true
control-center                | 	interceptor.classes = []
control-center                | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                | 	linger.ms = 500
control-center                | 	max.block.ms = 9223372036854775807
control-center                | 	max.in.flight.requests.per.connection = 5
control-center                | 	max.request.size = 10485760
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metadata.max.idle.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partitioner.adaptive.partitioning.enable = true
control-center                | 	partitioner.availability.timeout.ms = 0
control-center                | 	partitioner.class = null
control-center                | 	partitioner.ignore.keys = false
control-center                | 	receive.buffer.bytes = 32768
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	transaction.timeout.ms = 60000
control-center                | 	transactional.id = null
control-center                | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center                |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center                | [2023-08-04 11:57:27,011] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center                | [2023-08-04 11:57:27,024] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:27,025] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:27,025] INFO Kafka startTimeMs: 1691150247024 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:27,027] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,029] INFO ConsumerConfig values: 
control-center                | 	allow.auto.create.topics = false
control-center                | 	auto.commit.interval.ms = 5000
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	auto.offset.reset = none
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	check.crcs = true
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer
control-center                | 	client.rack = 
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	connections.max.idle.ms = 540000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	enable.auto.commit = false
control-center                | 	exclude.internal.topics = true
control-center                | 	fetch.max.bytes = 52428800
control-center                | 	fetch.max.wait.ms = 500
control-center                | 	fetch.min.bytes = 1
control-center                | 	group.id = _confluent-controlcenter-7-4-1-1
control-center                | 	group.instance.id = null
control-center                | 	heartbeat.interval.ms = 3000
control-center                | 	interceptor.classes = []
control-center                | 	internal.leave.group.on.close = false
control-center                | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center                | 	isolation.level = read_uncommitted
control-center                | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                | 	max.partition.fetch.bytes = 1048576
control-center                | 	max.poll.interval.ms = 21600000
control-center                | 	max.poll.records = 100
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	session.timeout.ms = 60000
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center                |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center                | [2023-08-04 11:57:27,045] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:27,047] WARN stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center                | [2023-08-04 11:57:27,053] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:27,055] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:27,055] INFO Kafka startTimeMs: 1691150247053 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:27,068] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,072] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-producer] ProducerId set to 14 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center                | [2023-08-04 11:57:27,076] INFO stream-client [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10] State transition from CREATED to REBALANCING (org.apache.kafka.streams.KafkaStreams)
control-center                | [2023-08-04 11:57:27,079] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,080] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,082] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,082] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,084] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,085] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,084] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,086] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,085] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,086] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,086] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,086] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,087] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,088] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,090] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,091] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,092] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,093] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,094] INFO stream-client [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10] Started 12 stream threads (org.apache.kafka.streams.KafkaStreams)
control-center                | [2023-08-04 11:57:27,096] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,097] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,097] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,099] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,101] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:27,101] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:27,101] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:27,101] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:27,101] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,126] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,101] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:27,101] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:27,100] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:27,100] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:27,100] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:27,100] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:27,099] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                | [2023-08-04 11:57:27,244] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to EFEDEKlhQneKmkKv6vq2ug (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,283] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to yaGoMLo8Td-SorQ7IkopwQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,284] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 5-cwUQMrRLuRuq67G0mKBQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,285] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 57o9ucS7SheIO2bNS2WFSg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,220] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to EFEDEKlhQneKmkKv6vq2ug (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,286] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to yaGoMLo8Td-SorQ7IkopwQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,287] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 5-cwUQMrRLuRuq67G0mKBQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,287] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 57o9ucS7SheIO2bNS2WFSg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,287] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to C_nrxqIHSiOvyQnBjl1AUg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,287] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,288] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,288] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,288] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,288] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,289] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,289] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,289] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,289] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,289] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,290] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,290] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,290] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to 3Ikd_jUKStGHd1em7l_jkA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,290] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to YZgYTx8aSQGlP9yT8BRY6A (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,290] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to L-ObqkHFS6CjenaFpJHZaA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,291] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to wj0jvN-hRme9pGvic2I8ZQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,200] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to EFEDEKlhQneKmkKv6vq2ug (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,178] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to EFEDEKlhQneKmkKv6vq2ug (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,292] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to yaGoMLo8Td-SorQ7IkopwQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,292] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 5-cwUQMrRLuRuq67G0mKBQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,292] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 57o9ucS7SheIO2bNS2WFSg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,292] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to yaGoMLo8Td-SorQ7IkopwQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,292] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to C_nrxqIHSiOvyQnBjl1AUg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,292] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,292] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 5-cwUQMrRLuRuq67G0mKBQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,156] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to EFEDEKlhQneKmkKv6vq2ug (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,293] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to yaGoMLo8Td-SorQ7IkopwQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,293] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 5-cwUQMrRLuRuq67G0mKBQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,293] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 57o9ucS7SheIO2bNS2WFSg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,293] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to C_nrxqIHSiOvyQnBjl1AUg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,293] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,293] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,293] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,293] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,293] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,293] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,294] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,294] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,294] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,294] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,294] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,294] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,294] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to 3Ikd_jUKStGHd1em7l_jkA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,294] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to YZgYTx8aSQGlP9yT8BRY6A (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,294] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to L-ObqkHFS6CjenaFpJHZaA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,295] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to wj0jvN-hRme9pGvic2I8ZQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,151] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:27,295] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to ghHTzj_ySuGkMDlILihrKw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,295] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to RR06KGwRRYOaeNU4Gqm-Zg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,295] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to oX7QDCssTMyPiUUXzSsOBw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,295] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to QpQV-WRTTiy9FejLVOtjsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,296] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to tQqpYlAhTY6G0QpY8FyKQQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,306] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to EFEDEKlhQneKmkKv6vq2ug (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,320] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to yaGoMLo8Td-SorQ7IkopwQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,328] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 5-cwUQMrRLuRuq67G0mKBQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,328] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 57o9ucS7SheIO2bNS2WFSg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,329] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to C_nrxqIHSiOvyQnBjl1AUg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,330] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,330] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,105] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:27,306] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to C_nrxqIHSiOvyQnBjl1AUg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,332] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,333] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,333] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,293] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 57o9ucS7SheIO2bNS2WFSg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,342] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to C_nrxqIHSiOvyQnBjl1AUg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,342] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,342] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,343] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,343] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,345] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,349] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,292] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,351] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,352] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,354] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,363] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,368] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,369] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,371] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,374] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,376] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,377] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,378] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to 3Ikd_jUKStGHd1em7l_jkA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,378] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to YZgYTx8aSQGlP9yT8BRY6A (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,380] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to L-ObqkHFS6CjenaFpJHZaA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,381] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to wj0jvN-hRme9pGvic2I8ZQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,272] INFO tocheck=[Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=AlertHistoryStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=Group, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=TriggerEventsStore, rollup=false}, Store{name=group-aggregate-store, rollup=true}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                | [2023-08-04 11:57:27,383] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                | [2023-08-04 11:57:27,380] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to EFEDEKlhQneKmkKv6vq2ug (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,378] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to EFEDEKlhQneKmkKv6vq2ug (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,400] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to yaGoMLo8Td-SorQ7IkopwQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,400] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 5-cwUQMrRLuRuq67G0mKBQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,401] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 57o9ucS7SheIO2bNS2WFSg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,402] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to C_nrxqIHSiOvyQnBjl1AUg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,403] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,405] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,407] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,409] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,410] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,410] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,372] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to EFEDEKlhQneKmkKv6vq2ug (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,412] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to yaGoMLo8Td-SorQ7IkopwQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,413] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 5-cwUQMrRLuRuq67G0mKBQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,414] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 57o9ucS7SheIO2bNS2WFSg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,415] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to C_nrxqIHSiOvyQnBjl1AUg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,416] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to ghHTzj_ySuGkMDlILihrKw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,416] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to RR06KGwRRYOaeNU4Gqm-Zg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,416] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to oX7QDCssTMyPiUUXzSsOBw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,369] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,417] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,417] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,417] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,417] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,418] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,418] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,418] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,361] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to EFEDEKlhQneKmkKv6vq2ug (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,420] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to yaGoMLo8Td-SorQ7IkopwQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,422] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 5-cwUQMrRLuRuq67G0mKBQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,424] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 57o9ucS7SheIO2bNS2WFSg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,425] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to C_nrxqIHSiOvyQnBjl1AUg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,426] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,427] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to yaGoMLo8Td-SorQ7IkopwQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,427] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 5-cwUQMrRLuRuq67G0mKBQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,428] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 57o9ucS7SheIO2bNS2WFSg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,428] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to C_nrxqIHSiOvyQnBjl1AUg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,428] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,428] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,429] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,429] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,429] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,429] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,430] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,430] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,431] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,431] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,432] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,432] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,433] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to 3Ikd_jUKStGHd1em7l_jkA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,433] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to YZgYTx8aSQGlP9yT8BRY6A (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,350] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,349] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to ghHTzj_ySuGkMDlILihrKw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,435] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to RR06KGwRRYOaeNU4Gqm-Zg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,436] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to oX7QDCssTMyPiUUXzSsOBw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,437] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to QpQV-WRTTiy9FejLVOtjsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,439] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to tQqpYlAhTY6G0QpY8FyKQQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,349] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to EFEDEKlhQneKmkKv6vq2ug (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,443] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to yaGoMLo8Td-SorQ7IkopwQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,443] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 5-cwUQMrRLuRuq67G0mKBQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,444] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 57o9ucS7SheIO2bNS2WFSg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,445] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to C_nrxqIHSiOvyQnBjl1AUg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,446] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,347] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to EFEDEKlhQneKmkKv6vq2ug (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,334] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,447] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,448] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,448] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,448] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,448] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,448] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,449] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,449] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,449] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,449] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to 3Ikd_jUKStGHd1em7l_jkA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,449] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to YZgYTx8aSQGlP9yT8BRY6A (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,450] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to L-ObqkHFS6CjenaFpJHZaA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,450] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to wj0jvN-hRme9pGvic2I8ZQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,450] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to ghHTzj_ySuGkMDlILihrKw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,450] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to RR06KGwRRYOaeNU4Gqm-Zg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,450] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to oX7QDCssTMyPiUUXzSsOBw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,451] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to QpQV-WRTTiy9FejLVOtjsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,451] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to tQqpYlAhTY6G0QpY8FyKQQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,334] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,453] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,454] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,447] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to yaGoMLo8Td-SorQ7IkopwQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,457] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,459] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 5-cwUQMrRLuRuq67G0mKBQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,460] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 57o9ucS7SheIO2bNS2WFSg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,461] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to C_nrxqIHSiOvyQnBjl1AUg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,461] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,462] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,463] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,446] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,464] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,464] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,465] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,446] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,441] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,465] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,434] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,466] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,466] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,471] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,471] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,471] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,472] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,472] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,472] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to 3Ikd_jUKStGHd1em7l_jkA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,473] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to YZgYTx8aSQGlP9yT8BRY6A (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,473] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to L-ObqkHFS6CjenaFpJHZaA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,473] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to wj0jvN-hRme9pGvic2I8ZQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,473] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to ghHTzj_ySuGkMDlILihrKw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,473] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to RR06KGwRRYOaeNU4Gqm-Zg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,474] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to oX7QDCssTMyPiUUXzSsOBw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,474] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to QpQV-WRTTiy9FejLVOtjsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,434] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to L-ObqkHFS6CjenaFpJHZaA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,475] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to wj0jvN-hRme9pGvic2I8ZQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,475] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to ghHTzj_ySuGkMDlILihrKw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,475] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to RR06KGwRRYOaeNU4Gqm-Zg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,475] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to oX7QDCssTMyPiUUXzSsOBw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,475] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to QpQV-WRTTiy9FejLVOtjsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,475] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to tQqpYlAhTY6G0QpY8FyKQQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,427] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,478] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,479] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,479] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,480] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,418] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to QpQV-WRTTiy9FejLVOtjsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,482] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to tQqpYlAhTY6G0QpY8FyKQQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,484] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,486] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,490] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,418] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to 3Ikd_jUKStGHd1em7l_jkA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,493] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to YZgYTx8aSQGlP9yT8BRY6A (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,497] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to L-ObqkHFS6CjenaFpJHZaA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,498] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to wj0jvN-hRme9pGvic2I8ZQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,499] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to ghHTzj_ySuGkMDlILihrKw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,500] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to RR06KGwRRYOaeNU4Gqm-Zg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,501] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to oX7QDCssTMyPiUUXzSsOBw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,502] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to QpQV-WRTTiy9FejLVOtjsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,503] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to tQqpYlAhTY6G0QpY8FyKQQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,508] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,416] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,511] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,512] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,512] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,513] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,492] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,479] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,517] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,476] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to tQqpYlAhTY6G0QpY8FyKQQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,522] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,526] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,529] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,469] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,467] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,466] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,548] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,551] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,552] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,553] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to 3Ikd_jUKStGHd1em7l_jkA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,555] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to YZgYTx8aSQGlP9yT8BRY6A (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,557] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to L-ObqkHFS6CjenaFpJHZaA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,560] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to wj0jvN-hRme9pGvic2I8ZQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,561] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to ghHTzj_ySuGkMDlILihrKw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,563] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to RR06KGwRRYOaeNU4Gqm-Zg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,466] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,464] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,566] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,567] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,568] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,569] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,570] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
broker                        | [2023-08-04 11:57:27,578] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer-3a35bb81-a309-43a0-8dc7-151b9c4457a7 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:27,579] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,454] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,581] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,582] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,582] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,585] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,587] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,589] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,590] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to 3Ikd_jUKStGHd1em7l_jkA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,592] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to YZgYTx8aSQGlP9yT8BRY6A (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,593] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to L-ObqkHFS6CjenaFpJHZaA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,594] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to wj0jvN-hRme9pGvic2I8ZQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,595] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to ghHTzj_ySuGkMDlILihrKw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,596] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to RR06KGwRRYOaeNU4Gqm-Zg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,597] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to oX7QDCssTMyPiUUXzSsOBw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,579] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,560] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,539] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:27,603] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer-a16deb71-b2a7-48e9-afe3-0d7506767675 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:27,534] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,527] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,525] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,515] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,610] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,611] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,612] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,613] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,614] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,614] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,615] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to 3Ikd_jUKStGHd1em7l_jkA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,616] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to YZgYTx8aSQGlP9yT8BRY6A (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,617] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to L-ObqkHFS6CjenaFpJHZaA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,617] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to wj0jvN-hRme9pGvic2I8ZQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,618] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to ghHTzj_ySuGkMDlILihrKw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,619] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to QpQV-WRTTiy9FejLVOtjsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,621] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to tQqpYlAhTY6G0QpY8FyKQQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,599] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer-3a35bb81-a309-43a0-8dc7-151b9c4457a7 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,585] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to oX7QDCssTMyPiUUXzSsOBw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,585] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,624] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,619] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to RR06KGwRRYOaeNU4Gqm-Zg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,615] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer-a16deb71-b2a7-48e9-afe3-0d7506767675 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,626] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,627] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,611] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,611] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,631] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,632] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,632] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to wK8jWwjgTTqetlRuGAkiWQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,632] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to 3Ikd_jUKStGHd1em7l_jkA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,628] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,633] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to YZgYTx8aSQGlP9yT8BRY6A (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,635] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to L-ObqkHFS6CjenaFpJHZaA (org.apache.kafka.clients.Metadata)
broker                        | [2023-08-04 11:57:27,636] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 0 (__consumer_offsets-5) (reason: Adding new member _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer-a16deb71-b2a7-48e9-afe3-0d7506767675 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:27,635] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to wj0jvN-hRme9pGvic2I8ZQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,638] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to ghHTzj_ySuGkMDlILihrKw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,639] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to 3Ikd_jUKStGHd1em7l_jkA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,640] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to RR06KGwRRYOaeNU4Gqm-Zg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,640] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to oX7QDCssTMyPiUUXzSsOBw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,643] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to YZgYTx8aSQGlP9yT8BRY6A (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,644] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to L-ObqkHFS6CjenaFpJHZaA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,644] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to QpQV-WRTTiy9FejLVOtjsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,644] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to oX7QDCssTMyPiUUXzSsOBw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,645] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to QpQV-WRTTiy9FejLVOtjsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,645] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to tQqpYlAhTY6G0QpY8FyKQQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,645] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to wj0jvN-hRme9pGvic2I8ZQ (org.apache.kafka.clients.Metadata)
broker                        | [2023-08-04 11:57:27,647] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1 generation 1 (__consumer_offsets-5) with 1 members (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:27,646] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to tQqpYlAhTY6G0QpY8FyKQQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,649] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,646] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to ghHTzj_ySuGkMDlILihrKw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,657] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,657] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,658] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to RR06KGwRRYOaeNU4Gqm-Zg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,659] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to oX7QDCssTMyPiUUXzSsOBw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,660] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to QpQV-WRTTiy9FejLVOtjsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,660] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to tQqpYlAhTY6G0QpY8FyKQQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,655] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,651] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=1, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer-a16deb71-b2a7-48e9-afe3-0d7506767675', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,650] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to QpQV-WRTTiy9FejLVOtjsA (org.apache.kafka.clients.Metadata)
broker                        | [2023-08-04 11:57:27,664] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in CompletingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer-58bf397b-0feb-4aff-8c99-1cdb97c3a0f0 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:27,664] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to tQqpYlAhTY6G0QpY8FyKQQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,668] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,671] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,668] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer-58bf397b-0feb-4aff-8c99-1cdb97c3a0f0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,676] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,676] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:27,677] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in CompletingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer-2d9367a4-3814-40d2-9da5-05901298f54d and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:27,682] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,683] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,683] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,683] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,693] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,695] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,694] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,699] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:27,702] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,708] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,710] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer-2d9367a4-3814-40d2-9da5-05901298f54d (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,712] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,714] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,715] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:27,727] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:27,736] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 1 (__consumer_offsets-5) (reason: Adding new member _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer-3a35bb81-a309-43a0-8dc7-151b9c4457a7 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:27,750] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer-7444164e-2fdb-461a-bf55-2e6b83998bee and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:27,749] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:27,754] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer-81fdbc59-52dc-4b25-b376-f1d72b8c9b37 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:27,758] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,759] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer-7444164e-2fdb-461a-bf55-2e6b83998bee (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,759] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,759] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,760] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,764] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:27,783] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer-d95894c0-fcd7-4f28-ad59-15639a045e0b and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:27,783] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,772] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,781] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer-81fdbc59-52dc-4b25-b376-f1d72b8c9b37 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,798] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer-d95894c0-fcd7-4f28-ad59-15639a045e0b (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:27,802] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer-2bd6706d-2191-4cef-8825-bf3a42d3b4f8 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:27,810] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer-2bd6706d-2191-4cef-8825-bf3a42d3b4f8 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,818] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,799] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,826] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,832] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,832] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:27,833] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer-e9e15eaa-3d6f-45df-8d83-fadd1c70859d and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:27,841] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer-e9e15eaa-3d6f-45df-8d83-fadd1c70859d (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,843] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,843] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,851] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:27,859] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer-a351f576-9d85-40c8-bddb-72c24c447fe8 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
broker                        | [2023-08-04 11:57:27,872] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer-cf6a565a-5d3c-414c-a0ee-a390d9f78522 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:27,883] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer-a351f576-9d85-40c8-bddb-72c24c447fe8 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,884] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,885] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,891] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer-cf6a565a-5d3c-414c-a0ee-a390d9f78522 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,896] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:27,898] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer-f354568c-16b0-4c69-bbe1-68f21803e840 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:27,900] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,902] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer-f354568c-16b0-4c69-bbe1-68f21803e840 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,906] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,906] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,949] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer] All members participating in this rebalance: 
control-center                | 294242ab-6891-4246-9408-d412e91c3d10: [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer-a16deb71-b2a7-48e9-afe3-0d7506767675]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:27,957] INFO Decided on assignment: {294242ab-6891-4246-9408-d412e91c3d10=[activeTasks: ([0_0, 1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 10_0, 10_1, 10_2, 10_3, 10_4, 10_5, 10_6, 10_7, 10_8, 10_9, 10_10, 10_11, 11_0, 12_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([1_0=0, 2_0=0, 3_0=0, 4_0=0, 5_0=0, 6_0=0, 7_0=0, 8_0=0, 9_0=0, 11_0=0, 12_0=0]) clientTags: ([]) capacity: 1 assigned: 24]} with no followup probing rebalance. (org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor)
control-center                | [2023-08-04 11:57:27,959] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer] Assigned tasks [1_0, 10_9, 10_8, 2_0, 10_7, 3_0, 10_6, 4_0, 10_5, 5_0, 10_4, 6_0, 10_3, 7_0, 10_2, 8_0, 10_1, 9_0, 10_0, 11_0, 12_0, 10_11, 0_0, 10_10] including stateful [1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 11_0, 12_0] to clients as: 
control-center                | 294242ab-6891-4246-9408-d412e91c3d10=[activeTasks: ([0_0, 1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 10_0, 10_1, 10_2, 10_3, 10_4, 10_5, 10_6, 10_7, 10_8, 10_9, 10_10, 10_11, 11_0, 12_0]) standbyTasks: ([])]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:27,964] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer] Client 294242ab-6891-4246-9408-d412e91c3d10 per-consumer assignment:
control-center                | 	prev owned active {}
control-center                | 	prev owned standby {_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer-a16deb71-b2a7-48e9-afe3-0d7506767675=[]}
control-center                | 	assigned active {_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer-a16deb71-b2a7-48e9-afe3-0d7506767675=[1_0, 10_9, 2_0, 10_8, 3_0, 10_7, 4_0, 10_6, 5_0, 10_5, 6_0, 10_4, 7_0, 10_3, 8_0, 10_2, 9_0, 10_1, 10_0, 11_0, 12_0, 10_11, 0_0, 10_10]}
control-center                | 	revoking active {}
control-center                | 	assigned standby {}
control-center                |  (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:27,964] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer] Finished stable assignment of tasks, no followup rebalances required. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:27,965] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Finished assignment for group at generation 1: {_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer-a16deb71-b2a7-48e9-afe3-0d7506767675=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, _confluent-metrics-0, _confluent-metrics-1, _confluent-metrics-2, _confluent-metrics-3, _confluent-metrics-4, _confluent-metrics-5, _confluent-metrics-6, _confluent-metrics-7, _confluent-metrics-8, _confluent-metrics-9, _confluent-metrics-10, _confluent-metrics-11, _confluent-monitoring-0], userDataSize=352)} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,982] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=1, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer-a16deb71-b2a7-48e9-afe3-0d7506767675', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,984] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group is rebalancing, so a rejoin is needed.' (RebalanceInProgressException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:27,984] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:28,008] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1 generation 2 (__consumer_offsets-5) with 12 members (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:28,014] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer-3a35bb81-a309-43a0-8dc7-151b9c4457a7', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,026] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer-2d9367a4-3814-40d2-9da5-05901298f54d', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,026] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer-81fdbc59-52dc-4b25-b376-f1d72b8c9b37', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,033] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer-d95894c0-fcd7-4f28-ad59-15639a045e0b', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,053] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer-2bd6706d-2191-4cef-8825-bf3a42d3b4f8', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,056] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer-58bf397b-0feb-4aff-8c99-1cdb97c3a0f0', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,068] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer-cf6a565a-5d3c-414c-a0ee-a390d9f78522', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,067] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer-e9e15eaa-3d6f-45df-8d83-fadd1c70859d', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,065] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer-f354568c-16b0-4c69-bbe1-68f21803e840', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,063] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer-7444164e-2fdb-461a-bf55-2e6b83998bee', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,056] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer-a351f576-9d85-40c8-bddb-72c24c447fe8', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,081] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer-a16deb71-b2a7-48e9-afe3-0d7506767675', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,195] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer] All members participating in this rebalance: 
control-center                | 294242ab-6891-4246-9408-d412e91c3d10: [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer-d95894c0-fcd7-4f28-ad59-15639a045e0b, _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer-f354568c-16b0-4c69-bbe1-68f21803e840, _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer-2d9367a4-3814-40d2-9da5-05901298f54d, _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer-cf6a565a-5d3c-414c-a0ee-a390d9f78522, _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer-7444164e-2fdb-461a-bf55-2e6b83998bee, _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer-81fdbc59-52dc-4b25-b376-f1d72b8c9b37, _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer-e9e15eaa-3d6f-45df-8d83-fadd1c70859d, _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer-2bd6706d-2191-4cef-8825-bf3a42d3b4f8, _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer-a16deb71-b2a7-48e9-afe3-0d7506767675, _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer-a351f576-9d85-40c8-bddb-72c24c447fe8, _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer-58bf397b-0feb-4aff-8c99-1cdb97c3a0f0, _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer-3a35bb81-a309-43a0-8dc7-151b9c4457a7]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:28,203] INFO Decided on assignment: {294242ab-6891-4246-9408-d412e91c3d10=[activeTasks: ([0_0, 1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 10_0, 10_1, 10_2, 10_3, 10_4, 10_5, 10_6, 10_7, 10_8, 10_9, 10_10, 10_11, 11_0, 12_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([1_0=0, 2_0=0, 3_0=0, 4_0=0, 5_0=0, 6_0=0, 7_0=0, 8_0=0, 9_0=0, 11_0=0, 12_0=0]) clientTags: ([]) capacity: 12 assigned: 24]} with no followup probing rebalance. (org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor)
control-center                | [2023-08-04 11:57:28,210] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer] Assigned tasks [1_0, 10_9, 10_8, 2_0, 10_7, 3_0, 10_6, 4_0, 10_5, 5_0, 10_4, 6_0, 10_3, 7_0, 10_2, 8_0, 10_1, 9_0, 10_0, 11_0, 12_0, 10_11, 0_0, 10_10] including stateful [1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 11_0, 12_0] to clients as: 
control-center                | 294242ab-6891-4246-9408-d412e91c3d10=[activeTasks: ([0_0, 1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 10_0, 10_1, 10_2, 10_3, 10_4, 10_5, 10_6, 10_7, 10_8, 10_9, 10_10, 10_11, 11_0, 12_0]) standbyTasks: ([])]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:28,225] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer] Client 294242ab-6891-4246-9408-d412e91c3d10 per-consumer assignment:
control-center                | 	prev owned active {}
control-center                | 	prev owned standby {_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer-d95894c0-fcd7-4f28-ad59-15639a045e0b=[], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer-f354568c-16b0-4c69-bbe1-68f21803e840=[], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer-2d9367a4-3814-40d2-9da5-05901298f54d=[], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer-cf6a565a-5d3c-414c-a0ee-a390d9f78522=[], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer-7444164e-2fdb-461a-bf55-2e6b83998bee=[], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer-81fdbc59-52dc-4b25-b376-f1d72b8c9b37=[], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer-e9e15eaa-3d6f-45df-8d83-fadd1c70859d=[], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer-2bd6706d-2191-4cef-8825-bf3a42d3b4f8=[], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer-a16deb71-b2a7-48e9-afe3-0d7506767675=[], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer-a351f576-9d85-40c8-bddb-72c24c447fe8=[], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer-58bf397b-0feb-4aff-8c99-1cdb97c3a0f0=[], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer-3a35bb81-a309-43a0-8dc7-151b9c4457a7=[]}
control-center                | 	assigned active {_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer-d95894c0-fcd7-4f28-ad59-15639a045e0b=[1_0, 0_0], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer-f354568c-16b0-4c69-bbe1-68f21803e840=[2_0, 10_0], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer-2d9367a4-3814-40d2-9da5-05901298f54d=[3_0, 10_1], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer-cf6a565a-5d3c-414c-a0ee-a390d9f78522=[4_0, 10_2], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer-7444164e-2fdb-461a-bf55-2e6b83998bee=[5_0, 10_3], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer-81fdbc59-52dc-4b25-b376-f1d72b8c9b37=[6_0, 10_4], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer-e9e15eaa-3d6f-45df-8d83-fadd1c70859d=[10_5, 7_0], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer-2bd6706d-2191-4cef-8825-bf3a42d3b4f8=[10_6, 8_0], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer-a16deb71-b2a7-48e9-afe3-0d7506767675=[10_7, 9_0], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer-a351f576-9d85-40c8-bddb-72c24c447fe8=[10_8, 11_0], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer-58bf397b-0feb-4aff-8c99-1cdb97c3a0f0=[10_9, 12_0], _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer-3a35bb81-a309-43a0-8dc7-151b9c4457a7=[10_11, 10_10]}
control-center                | 	revoking active {}
control-center                | 	assigned standby {}
control-center                |  (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:28,238] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer] Finished stable assignment of tasks, no followup rebalances required. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:28,240] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Finished assignment for group at generation 2: {_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer-f354568c-16b0-4c69-bbe1-68f21803e840=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-metrics-0], userDataSize=64), _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer-a351f576-9d85-40c8-bddb-72c24c447fe8=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, _confluent-metrics-8], userDataSize=64), _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer-3a35bb81-a309-43a0-8dc7-151b9c4457a7=Assignment(partitions=[_confluent-metrics-10, _confluent-metrics-11], userDataSize=64), _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer-7444164e-2fdb-461a-bf55-2e6b83998bee=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, _confluent-metrics-3], userDataSize=64), _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer-e9e15eaa-3d6f-45df-8d83-fadd1c70859d=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, _confluent-metrics-5], userDataSize=64), _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer-d95894c0-fcd7-4f28-ad59-15639a045e0b=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, _confluent-monitoring-0], userDataSize=64), _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer-cf6a565a-5d3c-414c-a0ee-a390d9f78522=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-metrics-2], userDataSize=64), _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer-a16deb71-b2a7-48e9-afe3-0d7506767675=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, _confluent-metrics-7], userDataSize=88), _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer-2bd6706d-2191-4cef-8825-bf3a42d3b4f8=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, _confluent-metrics-6], userDataSize=64), _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer-2d9367a4-3814-40d2-9da5-05901298f54d=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, _confluent-metrics-1], userDataSize=64), _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer-81fdbc59-52dc-4b25-b376-f1d72b8c9b37=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, _confluent-metrics-4], userDataSize=64), _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer-58bf397b-0feb-4aff-8c99-1cdb97c3a0f0=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, _confluent-metrics-9], userDataSize=64)} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker                        | [2023-08-04 11:57:28,249] INFO [GroupCoordinator 1]: Assignment received from leader _confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer-a16deb71-b2a7-48e9-afe3-0d7506767675 for group _confluent-controlcenter-7-4-1-1 for generation 2. The group has 12 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
control-center                | [2023-08-04 11:57:28,271] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer-2d9367a4-3814-40d2-9da5-05901298f54d', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,274] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, _confluent-metrics-1], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,275] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:28,276] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer-d95894c0-fcd7-4f28-ad59-15639a045e0b', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,290] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, _confluent-monitoring-0], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,294] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:28,288] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer-f354568c-16b0-4c69-bbe1-68f21803e840', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,295] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-metrics-0], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,285] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer-81fdbc59-52dc-4b25-b376-f1d72b8c9b37', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,284] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer-3a35bb81-a309-43a0-8dc7-151b9c4457a7', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,296] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-metrics-10, _confluent-metrics-11], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,293] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer-a351f576-9d85-40c8-bddb-72c24c447fe8', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,297] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:28,298] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, _confluent-metrics-8], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,299] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:28,300] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] Handle new assignment with:
control-center                | 	New active tasks: [10_8, 11_0]
control-center                | 	New standby tasks: []
control-center                | 	Existing active tasks: []
control-center                | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                | [2023-08-04 11:57:28,301] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer-7444164e-2fdb-461a-bf55-2e6b83998bee', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,302] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, _confluent-metrics-3], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,303] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:28,303] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] Handle new assignment with:
control-center                | 	New active tasks: [5_0, 10_3]
control-center                | 	New standby tasks: []
control-center                | 	Existing active tasks: []
control-center                | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                | [2023-08-04 11:57:28,297] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, _confluent-metrics-4], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,305] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:28,305] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] Handle new assignment with:
control-center                | 	New active tasks: [10_4, 6_0]
control-center                | 	New standby tasks: []
control-center                | 	Existing active tasks: []
control-center                | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                | [2023-08-04 11:57:28,299] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:28,310] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] Handle new assignment with:
control-center                | 	New active tasks: [10_11, 10_10]
control-center                | 	New standby tasks: []
control-center                | 	Existing active tasks: []
control-center                | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                | [2023-08-04 11:57:28,298] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] Handle new assignment with:
control-center                | 	New active tasks: [3_0, 10_1]
control-center                | 	New standby tasks: []
control-center                | 	Existing active tasks: []
control-center                | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                | [2023-08-04 11:57:28,298] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] Handle new assignment with:
control-center                | 	New active tasks: [2_0, 10_0]
control-center                | 	New standby tasks: []
control-center                | 	Existing active tasks: []
control-center                | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                | [2023-08-04 11:57:28,295] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] Handle new assignment with:
control-center                | 	New active tasks: [1_0, 0_0]
control-center                | 	New standby tasks: []
control-center                | 	Existing active tasks: []
control-center                | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                | [2023-08-04 11:57:28,322] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer-e9e15eaa-3d6f-45df-8d83-fadd1c70859d', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,327] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, _confluent-metrics-5], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,327] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:28,328] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] Handle new assignment with:
control-center                | 	New active tasks: [10_5, 7_0]
control-center                | 	New standby tasks: []
control-center                | 	Existing active tasks: []
control-center                | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                | [2023-08-04 11:57:28,332] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer-cf6a565a-5d3c-414c-a0ee-a390d9f78522', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,333] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-metrics-2], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,343] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer-2bd6706d-2191-4cef-8825-bf3a42d3b4f8', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,344] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, _confluent-metrics-6], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,357] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:28,350] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer-58bf397b-0feb-4aff-8c99-1cdb97c3a0f0', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,360] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] Handle new assignment with:
control-center                | 	New active tasks: [10_6, 8_0]
control-center                | 	New standby tasks: []
control-center                | 	Existing active tasks: []
control-center                | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                | [2023-08-04 11:57:28,360] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer-a16deb71-b2a7-48e9-afe3-0d7506767675', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,363] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:28,364] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] Handle new assignment with:
control-center                | 	New active tasks: [4_0, 10_2]
control-center                | 	New standby tasks: []
control-center                | 	Existing active tasks: []
control-center                | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                | [2023-08-04 11:57:28,367] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, _confluent-metrics-7], userDataSize=88) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,367] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:28,376] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] Handle new assignment with:
control-center                | 	New active tasks: [10_7, 9_0]
control-center                | 	New standby tasks: []
control-center                | 	Existing active tasks: []
control-center                | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                | [2023-08-04 11:57:28,363] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, _confluent-metrics-9], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,377] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center                | [2023-08-04 11:57:28,378] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] Handle new assignment with:
control-center                | 	New active tasks: [10_9, 12_0]
control-center                | 	New standby tasks: []
control-center                | 	Existing active tasks: []
control-center                | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center                | [2023-08-04 11:57:28,385] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                | [2023-08-04 11:57:28,387] INFO tocheck=[Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=AlertHistoryStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=Group, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=TriggerEventsStore, rollup=false}, Store{name=group-aggregate-store, rollup=true}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                | [2023-08-04 11:57:28,390] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                | [2023-08-04 11:57:28,410] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, _confluent-metrics-8 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,413] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, _confluent-metrics-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,422] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:28,426] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:28,434] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-metrics-10, _confluent-metrics-11 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,434] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:28,445] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] task [10_11] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,449] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] task [10_10] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,452] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,452] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] task [10_1] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,457] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,551] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, _confluent-metrics-5 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,552] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:28,562] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] task [10_11] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,567] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] task [10_5] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,585] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,593] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, _confluent-metrics-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,599] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:28,598] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-metrics-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,607] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:28,598] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, _confluent-monitoring-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,609] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:28,596] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-metrics-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,626] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:28,631] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,628] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,634] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,631] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] stream-task [11_0] State store MetricsAggregateStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:28,643] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] task [11_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,644] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] task [10_8] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,648] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] stream-task [3_0] State store MonitoringMessageAggregatorWindows-ONE_MINUTE did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:28,651] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] task [3_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,652] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] task [10_10] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,660] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] Restoration took 225 ms for all tasks [10_11, 10_10] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:28,660] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:28,660] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-10 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:28,651] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,661] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] task [10_2] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,664] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-11 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:28,666] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] stream-task [6_0] State store MonitoringStream-ONE_MINUTE did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:28,667] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] task [6_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,668] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] task [10_4] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,692] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] stream-task [7_0] State store MonitoringStream-THREE_HOURS did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:28,692] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] task [7_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,708] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, _confluent-metrics-6 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,731] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:28,739] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] task [10_1] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,746] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, _confluent-metrics-9 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,746] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:28,762] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,699] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] task [10_4] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,697] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, _confluent-metrics-7 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,693] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, _confluent-metrics-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,766] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:28,766] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:28,723] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] task [10_8] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,778] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,795] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,802] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,802] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,803] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] task [10_7] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,808] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] task [10_3] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,810] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,810] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] task [10_6] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,767] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:28,812] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:28,800] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:28,815] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:28,799] INFO ControlCenterBoundedMemoryConfig values: 
control-center                | 	rocksdb.cache.limit.strict = false
control-center                | 	rocksdb.cache.size = 16106127360
control-center                | 	rocksdb.cache.size.limit.enabled = false
control-center                | 	rocksdb.index.filter.block.ratio = 0.0
control-center                | 	rocksdb.write.buffer.cache.use = false
control-center                | 	rocksdb.write.buffer.size = 5368709120
control-center                |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center                | [2023-08-04 11:57:28,831] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] End offset for changelog _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:28,832] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:28,841] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-10 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,843] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-11 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:28,862] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] stream-task [2_0] State store aggregatedTopicPartitionTableWindows-ONE_MINUTE did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:28,868] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] stream-task [2_0] Initializing to the starting offset for changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 of in-memory state store group-aggregate-store-ONE_MINUTE (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:28,868] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] task [2_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,869] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] task [10_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,867] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] stream-task [4_0] State store aggregatedTopicPartitionTableWindows-THREE_HOURS did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:28,867] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] task [10_5] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,867] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:28,872] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] stream-task [4_0] Initializing to the starting offset for changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 of in-memory state store group-aggregate-store-THREE_HOURS (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:28,867] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:28,876] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] task [4_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,883] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] stream-task [5_0] State store MonitoringMessageAggregatorWindows-THREE_HOURS did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:28,884] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] task [5_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,867] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:28,835] INFO ControlCenterBoundedMemoryConfig values: 
control-center                | 	rocksdb.cache.limit.strict = false
control-center                | 	rocksdb.cache.size = 16106127360
control-center                | 	rocksdb.cache.size.limit.enabled = false
control-center                | 	rocksdb.index.filter.block.ratio = 0.0
control-center                | 	rocksdb.write.buffer.cache.use = false
control-center                | 	rocksdb.write.buffer.size = 5368709120
control-center                |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center                | [2023-08-04 11:57:28,906] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:28,908] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:28,913] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] task [10_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,944] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] task [10_3] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,948] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] task [10_2] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,972] INFO ControlCenterBoundedMemoryConfig values: 
control-center                | 	rocksdb.cache.limit.strict = false
control-center                | 	rocksdb.cache.size = 16106127360
control-center                | 	rocksdb.cache.size.limit.enabled = false
control-center                | 	rocksdb.index.filter.block.ratio = 0.0
control-center                | 	rocksdb.write.buffer.cache.use = false
control-center                | 	rocksdb.write.buffer.size = 5368709120
control-center                |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center                | [2023-08-04 11:57:28,978] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] stream-task [8_0] State store Group-ONE_MINUTE did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:28,982] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] stream-task [8_0] State store Group-THREE_HOURS did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:28,982] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] task [8_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:28,982] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,022] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] task [10_6] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:29,030] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] stream-task [1_0] State store MonitoringVerifierStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:29,030] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] stream-task [1_0] Initializing to the starting offset for changelog _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 of in-memory state store aggregate-topic-partition-store (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:29,030] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] stream-task [1_0] Initializing to the starting offset for changelog _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 of in-memory state store monitoring-aggregate-rekey-store (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:29,030] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] task [1_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:29,030] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] task [0_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:29,043] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to nOK0g_4LSpKR7it_Lpsx_Q (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,044] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,065] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 to 0 since the associated topicId changed from null to XiYOYBFMSJmNBxVgCK1fWg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,065] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,082] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to Z14vI2zRTVykA1OcTvSTYA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,084] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,100] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to CSKlFJarTLeavMmlMoLkNg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,095] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] task [0_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:29,109] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,113] INFO Restore started for store [MetricsAggregateStore] with topic-partition [_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,115] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] End offset for changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,115] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] End offset for changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,115] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,134] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,138] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,145] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,146] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,148] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,181] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] End offset for changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,183] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] End offset for changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,185] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,199] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,227] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,216] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 to store MetricsAggregateStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,253] INFO Restore completed for store [MetricsAggregateStore] and topic-partition [_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,215] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,257] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] End offset for changelog _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,258] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] End offset for changelog _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,259] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,212] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] stream-task [9_0] State store KSTREAM-OUTERTHIS-0000000105-store did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:29,261] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] stream-task [9_0] State store KSTREAM-OUTEROTHER-0000000106-store did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:29,264] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] stream-task [9_0] State store MonitoringTriggerStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:29,264] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] task [9_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:29,268] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,269] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,271] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,300] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to 1DqRjjOIRXmdRKnCn_UUXw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,302] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to jTIWXGalTvWLtZI3sjE5sw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,304] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,212] INFO Restore started for store [MonitoringStream-THREE_HOURS] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,193] INFO Restore started for store [MonitoringStream-ONE_MINUTE] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,192] INFO Restore started for store [MonitoringMessageAggregatorWindows-ONE_MINUTE] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,205] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to 0CHwPhqaS4OeJ6WSlifxKA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,204] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] End offset for changelog _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,201] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to 3Mo-8h7IQJejhwp_sOpLmA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,251] INFO ControlCenterBoundedMemoryConfig values: 
control-center                | 	rocksdb.cache.limit.strict = false
control-center                | 	rocksdb.cache.size = 16106127360
control-center                | 	rocksdb.cache.size.limit.enabled = false
control-center                | 	rocksdb.index.filter.block.ratio = 0.0
control-center                | 	rocksdb.write.buffer.cache.use = false
control-center                | 	rocksdb.write.buffer.size = 5368709120
control-center                |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center                | [2023-08-04 11:57:29,313] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] End offset for changelog _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,313] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,343] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,344] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,361] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 to 0 since the associated topicId changed from null to pff6SMW7TImfl9gdGHtPbA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,361] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 to 0 since the associated topicId changed from null to H1w9PWSHRsOWqiZQQDBPqQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,361] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 to 0 since the associated topicId changed from null to T5FHHsk0SCS2RlMMCe7jWA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,335] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,367] INFO Restore started for store [group-aggregate-store-THREE_HOURS] with topic-partition [_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,367] INFO Restore started for store [aggregatedTopicPartitionTableWindows-THREE_HOURS] with topic-partition [_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,371] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,323] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to tErqfg2lSt2ont74tat6Mg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,382] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,318] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] task [10_7] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:29,316] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,393] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                | [2023-08-04 11:57:29,396] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,397] INFO tocheck=[Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=AlertHistoryStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=Group, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=TriggerEventsStore, rollup=false}, Store{name=group-aggregate-store, rollup=true}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                | [2023-08-04 11:57:29,404] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                | [2023-08-04 11:57:29,406] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to W5DcavKWTu200d1tpaXEpg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,406] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to IW2eSmqhSXaPTTq83LBS5g (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,412] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 to store MonitoringStream-ONE_MINUTE with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,412] INFO Restore completed for store [MonitoringStream-ONE_MINUTE] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,418] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] End offset for changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,432] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] End offset for changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,433] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,422] INFO Restore started for store [MonitoringMessageAggregatorWindows-THREE_HOURS] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,440] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 to store MonitoringMessageAggregatorWindows-ONE_MINUTE with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,442] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 to store MonitoringStream-THREE_HOURS with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,444] INFO Restore completed for store [MonitoringStream-THREE_HOURS] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,451] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] task [11_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:29,451] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] Restoration took 1025 ms for all tasks [10_8, 11_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,451] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,462] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,464] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-8 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,465] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,466] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,423] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,468] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] Finished restoring changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 to store aggregatedTopicPartitionTableWindows-THREE_HOURS with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,467] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,482] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,442] INFO Restore completed for store [MonitoringMessageAggregatorWindows-ONE_MINUTE] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,434] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,483] INFO Restore completed for store [aggregatedTopicPartitionTableWindows-THREE_HOURS] and topic-partition [_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,486] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] Finished restoring changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 to store group-aggregate-store-THREE_HOURS with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,486] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,487] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,487] INFO Restore completed for store [group-aggregate-store-THREE_HOURS] and topic-partition [_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,489] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,492] INFO Restore started for store [group-aggregate-store-ONE_MINUTE] with topic-partition [_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,493] INFO Restore started for store [aggregatedTopicPartitionTableWindows-ONE_MINUTE] with topic-partition [_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,507] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,509] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,510] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,514] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,524] INFO Restore started for store [aggregate-topic-partition-store] with topic-partition [_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,541] INFO Restore started for store [MonitoringVerifierStore] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,542] INFO Restore started for store [monitoring-aggregate-rekey-store] with topic-partition [_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,542] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 to store MonitoringMessageAggregatorWindows-THREE_HOURS with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,536] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] task [7_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:29,535] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] task [6_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:29,542] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] Restoration took 942 ms for all tasks [6_0, 10_4] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,543] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,544] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,544] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-4 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,535] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] task [4_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:29,552] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] Restoration took 923 ms for all tasks [4_0, 10_2] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,558] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,560] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,566] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-2 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,534] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] task [3_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:29,528] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,558] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 to 0 since the associated topicId changed from null to m7OhDxavRJ-M5HmBzaV5yg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,574] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] Restoration took 1150 ms for all tasks [10_1, 3_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,575] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,574] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 to 0 since the associated topicId changed from null to RFgEQoDlQFa80gJgHe9JRA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,575] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 to 0 since the associated topicId changed from null to G13gO5m1Tu65SpppF8ORAQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,576] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-8 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,577] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] Restoration took 1023 ms for all tasks [10_5, 7_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,577] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,578] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,542] INFO Restore completed for store [MonitoringMessageAggregatorWindows-THREE_HOURS] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,576] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,576] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:29,582] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-1 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,603] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] Finished restoring changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 to store aggregatedTopicPartitionTableWindows-ONE_MINUTE with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,616] INFO Restore completed for store [aggregatedTopicPartitionTableWindows-ONE_MINUTE] and topic-partition [_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,617] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] Finished restoring changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 to store group-aggregate-store-ONE_MINUTE with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,618] INFO Restore completed for store [group-aggregate-store-ONE_MINUTE] and topic-partition [_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,599] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,591] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-5 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,590] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,631] INFO Restore started for store [MonitoringTriggerStore] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,590] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,585] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,633] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,632] INFO Restore started for store [KSTREAM-OUTERTHIS-0000000105-store] with topic-partition [_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,629] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-4 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,649] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,649] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,650] INFO Restore started for store [KSTREAM-OUTEROTHER-0000000106-store] with topic-partition [_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,657] INFO Restore started for store [Group-ONE_MINUTE] with topic-partition [_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,638] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] task [5_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:29,670] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] Restoration took 903 ms for all tasks [5_0, 10_3] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,671] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,672] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,676] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-3 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,678] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,693] INFO Restore started for store [Group-THREE_HOURS] with topic-partition [_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,710] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 to store MonitoringVerifierStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,711] INFO Restore completed for store [MonitoringVerifierStore] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,716] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] Finished restoring changelog _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 to store monitoring-aggregate-rekey-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,717] INFO Restore completed for store [monitoring-aggregate-rekey-store] and topic-partition [_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,717] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] Finished restoring changelog _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 to store aggregate-topic-partition-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,717] INFO Restore completed for store [aggregate-topic-partition-store] and topic-partition [_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,716] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,723] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-5 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,725] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,726] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,727] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,728] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-1 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,714] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,731] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,732] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,732] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,739] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,744] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] task [2_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:29,746] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] Restoration took 1137 ms for all tasks [2_0, 10_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,746] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,749] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,763] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] Finished restoring changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 to store KSTREAM-OUTEROTHER-0000000106-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,771] INFO Restore completed for store [KSTREAM-OUTEROTHER-0000000106-store] and topic-partition [_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,772] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] Finished restoring changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 to store KSTREAM-OUTERTHIS-0000000105-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,774] INFO Restore completed for store [KSTREAM-OUTERTHIS-0000000105-store] and topic-partition [_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,777] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 to store MonitoringTriggerStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,778] INFO Restore completed for store [MonitoringTriggerStore] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,790] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,807] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] Finished restoring changelog _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 to store Group-ONE_MINUTE with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,812] INFO Restore completed for store [Group-ONE_MINUTE] and topic-partition [_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,802] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,795] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,794] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,814] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] Finished restoring changelog _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 to store Group-THREE_HOURS with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:29,818] INFO Restore completed for store [Group-THREE_HOURS] and topic-partition [_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:29,825] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,825] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,825] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,826] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-2 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,826] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,826] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,826] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,827] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,832] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-3 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:29,836] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:29,839] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:29,994] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] task [9_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:30,001] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] Restoration took 1225 ms for all tasks [9_0, 10_7] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:30,002] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:30,004] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:29,999] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:30,013] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-7 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:30,013] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:30,023] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:30,025] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:30,030] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:30,029] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 5-cwUQMrRLuRuq67G0mKBQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:30,038] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:30,047] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] task [8_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:30,042] INFO ControlCenterBoundedMemoryConfig values: 
control-center                | 	rocksdb.cache.limit.strict = false
control-center                | 	rocksdb.cache.size = 16106127360
control-center                | 	rocksdb.cache.size.limit.enabled = false
control-center                | 	rocksdb.index.filter.block.ratio = 0.0
control-center                | 	rocksdb.write.buffer.cache.use = false
control-center                | 	rocksdb.write.buffer.size = 5368709120
control-center                |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center                | [2023-08-04 11:57:30,059] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] Restoration took 1328 ms for all tasks [8_0, 10_6] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:30,063] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:30,064] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:30,077] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-6 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:30,096] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:30,097] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:30,098] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:30,101] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] task [1_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:30,103] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] Restoration took 1492 ms for all tasks [1_0, 0_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:30,104] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:30,106] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-monitoring-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:30,108] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:30,123] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-7 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:30,126] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:30,126] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:30,126] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:30,126] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:30,127] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:30,127] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:30,126] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:30,128] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-monitoring-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:30,129] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:30,129] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:30,125] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:30,130] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-6 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:30,131] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:30,131] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:30,223] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] stream-task [12_0] State store TriggerActionsStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:30,224] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] stream-task [12_0] State store TriggerEventsStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:30,224] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] stream-task [12_0] State store AlertHistoryStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center                | [2023-08-04 11:57:30,224] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] task [12_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:30,224] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] task [10_9] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:30,229] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] task [10_9] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:30,234] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:30,248] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] End offset for changelog _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:30,248] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] End offset for changelog _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:30,248] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] End offset for changelog _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:30,249] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:30,249] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:30,249] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:30,249] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:30,291] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 to 0 since the associated topicId changed from null to XLmE0NflT4CcMuJMDPnJnA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:30,293] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 to 0 since the associated topicId changed from null to udOxxlJuRZOwWTijIN5vaw (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:30,296] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 to 0 since the associated topicId changed from null to DE82LZAvR3qeRkgbe_n0JA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:30,296] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:30,314] INFO Restore started for store [AlertHistoryStore] with topic-partition [_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:30,314] INFO Restore started for store [TriggerEventsStore] with topic-partition [_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:30,314] INFO Restore started for store [TriggerActionsStore] with topic-partition [_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:30,404] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                | [2023-08-04 11:57:30,413] INFO tocheck=[Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=AlertHistoryStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=Group, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=TriggerEventsStore, rollup=false}, Store{name=group-aggregate-store, rollup=true}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                | [2023-08-04 11:57:30,413] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                | [2023-08-04 11:57:30,415] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] Finished restoring changelog _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 to store TriggerActionsStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:30,416] INFO Restore completed for store [TriggerActionsStore] and topic-partition [_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:30,417] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] Finished restoring changelog _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 to store TriggerEventsStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:30,418] INFO Restore completed for store [TriggerEventsStore] and topic-partition [_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:30,418] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] Finished restoring changelog _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 to store AlertHistoryStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center                | [2023-08-04 11:57:30,419] INFO Restore completed for store [AlertHistoryStore] and topic-partition [_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center                | [2023-08-04 11:57:30,427] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:30,431] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:30,433] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] task [12_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center                | [2023-08-04 11:57:30,434] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] Restoration took 1688 ms for all tasks [10_9, 12_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:30,435] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:30,437] INFO stream-client [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10] State transition from REBALANCING to RUNNING (org.apache.kafka.streams.KafkaStreams)
control-center                | [2023-08-04 11:57:30,439] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-9 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:30,442] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:30,465] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:30,466] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-9 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center                | [2023-08-04 11:57:30,467] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:57:30,468] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center                | [2023-08-04 11:57:30,750] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center                | [2023-08-04 11:57:31,413] INFO streams in state=RUNNING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center                | [2023-08-04 11:57:31,414] INFO tocheck=[Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=AlertHistoryStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=Group, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=TriggerEventsStore, rollup=false}, Store{name=group-aggregate-store, rollup=true}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
data-agrigator-taskmanager-1  | 2023-08-04 11:57:31,998 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1  | 2023-08-04 11:57:31,997 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
control-center                | [2023-08-04 11:57:32,417] INFO action=started topology=monitoring (io.confluent.controlcenter.application.AllControlCenter)
control-center                | [2023-08-04 11:57:32,444] INFO AdminClientConfig values: 
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = 
control-center                | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	confluent.use.controller.listener = false
control-center                | 	connections.max.idle.ms = 300000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                | [2023-08-04 11:57:32,450] WARN These configurations '[consumer.session.timeout.ms, producer.max.block.ms, producer.retries, upgrade.from, producer.retry.backoff.ms, producer.linger.ms, producer.delivery.timeout.ms, task.timeout.ms, cache.max.bytes.buffering, producer.compression.type, num.stream.threads]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                | [2023-08-04 11:57:32,450] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:32,450] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:32,450] INFO Kafka startTimeMs: 1691150252450 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:32,484] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:32,494] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:57:32,494] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:57:32,494] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center                | [2023-08-04 11:57:32,499] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                | 	metrics.jmx.prefix = confluent.controlcenter
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = true
control-center                | 	port = 9021
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:57:32,511] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                | 	metrics.jmx.prefix = confluent.controlcenter
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = true
control-center                | 	port = 9021
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:57:32,512] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                | 	metrics.jmx.prefix = confluent.controlcenter
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = true
control-center                | 	port = 9021
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:57:32,515] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                | 	metrics.jmx.prefix = confluent.controlcenter
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = true
control-center                | 	port = 9021
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:57:32,524] INFO EventEmitterConfig values: 
control-center                |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
control-center                | [2023-08-04 11:57:32,526] INFO EventEmitterConfig values: 
control-center                |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
control-center                | [2023-08-04 11:57:32,530] INFO ConfluentTelemetryConfig values: 
control-center                | 	confluent.telemetry.api.key = null
control-center                | 	confluent.telemetry.api.secret = null
control-center                | 	confluent.telemetry.debug.enabled = false
control-center                | 	confluent.telemetry.enabled = false
control-center                | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
control-center                | 	confluent.telemetry.events.enable = true
control-center                | 	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*
control-center                | 	confluent.telemetry.metrics.collector.interval.ms = 60000
control-center                | 	confluent.telemetry.metrics.collector.slo.enabled = false
control-center                | 	confluent.telemetry.proxy.password = null
control-center                | 	confluent.telemetry.proxy.url = null
control-center                | 	confluent.telemetry.proxy.username = null
control-center                |  (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                | [2023-08-04 11:57:32,531] INFO VolumeMetricsCollectorConfig values: 
control-center                | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
control-center                |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
control-center                | [2023-08-04 11:57:32,532] INFO HttpExporterConfig values: 
control-center                | 	api.key = null
control-center                | 	api.secret = null
control-center                | 	buffer.batch.duration.max.ms = null
control-center                | 	buffer.batch.items.max = null
control-center                | 	buffer.inflight.submissions.max = null
control-center                | 	buffer.pending.batches.max = null
control-center                | 	client.attempts.max = null
control-center                | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center                | 	client.compression = null
control-center                | 	client.connect.timeout.ms = null
control-center                | 	client.contentType = null
control-center                | 	client.request.timeout.ms = null
control-center                | 	client.retry.delay.seconds = null
control-center                | 	enabled = false
control-center                | 	events.enabled = true
control-center                | 	metrics.enabled = true
control-center                | 	metrics.include = null
control-center                | 	proxy.password = null
control-center                | 	proxy.url = null
control-center                | 	proxy.username = null
control-center                | 	type = http
control-center                |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
control-center                | [2023-08-04 11:57:32,532] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                | [2023-08-04 11:57:32,532] INFO RemoteConfigConfiguration values: 
control-center                | 	enabled = true
control-center                | 	polling.interval.ms = 60000
control-center                |  (io.confluent.shaded.io.confluent.telemetry.config.remote.RemoteConfigConfiguration)
control-center                | [2023-08-04 11:57:32,533] WARN Ignoring redefinition of existing telemetry label controlcenter.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
control-center                | [2023-08-04 11:57:32,538] INFO ConfluentTelemetryConfig values: 
control-center                | 	confluent.telemetry.api.key = null
control-center                | 	confluent.telemetry.api.secret = null
control-center                | 	confluent.telemetry.debug.enabled = false
control-center                | 	confluent.telemetry.enabled = false
control-center                | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
control-center                | 	confluent.telemetry.events.enable = true
control-center                | 	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.controlcenter/.*(metrics_input_topic_progress|monitoring_input_topic_progress|misconfigured_topics|missing_topic_configurations|broker_log_persistent_dir|cluster_offline|streams_status|total_lag|request_latency|response_size|response_rate).*
control-center                | 	confluent.telemetry.metrics.collector.interval.ms = 60000
control-center                | 	confluent.telemetry.metrics.collector.slo.enabled = false
control-center                | 	confluent.telemetry.proxy.password = null
control-center                | 	confluent.telemetry.proxy.url = null
control-center                | 	confluent.telemetry.proxy.username = null
control-center                |  (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                | [2023-08-04 11:57:32,540] INFO VolumeMetricsCollectorConfig values: 
control-center                | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
control-center                |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
control-center                | [2023-08-04 11:57:32,543] INFO HttpExporterConfig values: 
control-center                | 	api.key = null
control-center                | 	api.secret = null
control-center                | 	buffer.batch.duration.max.ms = null
control-center                | 	buffer.batch.items.max = null
control-center                | 	buffer.inflight.submissions.max = null
control-center                | 	buffer.pending.batches.max = null
control-center                | 	client.attempts.max = null
control-center                | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center                | 	client.compression = null
control-center                | 	client.connect.timeout.ms = null
control-center                | 	client.contentType = null
control-center                | 	client.request.timeout.ms = null
control-center                | 	client.retry.delay.seconds = null
control-center                | 	enabled = false
control-center                | 	events.enabled = true
control-center                | 	metrics.enabled = true
control-center                | 	metrics.include = null
control-center                | 	proxy.password = null
control-center                | 	proxy.url = null
control-center                | 	proxy.username = null
control-center                | 	type = http
control-center                |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
control-center                | [2023-08-04 11:57:32,545] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center                | [2023-08-04 11:57:32,546] INFO RemoteConfigConfiguration values: 
control-center                | 	enabled = true
control-center                | 	polling.interval.ms = 60000
control-center                |  (io.confluent.shaded.io.confluent.telemetry.config.remote.RemoteConfigConfiguration)
control-center                | [2023-08-04 11:57:32,546] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
control-center                | [2023-08-04 11:57:32,546] INFO EventLoggerConfig values: 
control-center                | 	event.logger.cloudevent.codec = structured
control-center                | 	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter
control-center                |  (io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig)
control-center                | [2023-08-04 11:57:32,548] INFO HttpExporterConfig values: 
control-center                | 	api.key = null
control-center                | 	api.secret = null
control-center                | 	buffer.batch.duration.max.ms = null
control-center                | 	buffer.batch.items.max = null
control-center                | 	buffer.inflight.submissions.max = null
control-center                | 	buffer.pending.batches.max = null
control-center                | 	client.attempts.max = null
control-center                | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center                | 	client.compression = null
control-center                | 	client.connect.timeout.ms = null
control-center                | 	client.request.timeout.ms = null
control-center                | 	client.retry.delay.seconds = null
control-center                | 	enabled = false
control-center                | 	events.enabled = true
control-center                | 	metrics.enabled = true
control-center                | 	proxy.password = null
control-center                | 	proxy.url = null
control-center                | 	proxy.username = null
control-center                | 	type = http
control-center                |  (io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig)
control-center                | [2023-08-04 11:57:32,581] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter)
control-center                | [2023-08-04 11:57:32,746] INFO [Producer clientId=c3-command] Resetting the last seen epoch of partition _confluent-command-0 to 0 since the associated topicId changed from null to kN5UClpoRbyl7_kzDFgGsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:32,879] INFO KafkaRestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	advertised.listeners = []
control-center                | 	api.endpoints.allowlist = []
control-center                | 	api.endpoints.blocklist = []
control-center                | 	api.v2.enable = true
control-center                | 	api.v3.enable = true
control-center                | 	api.v3.produce.rate.limit.cache.expiry.ms = 3600000
control-center                | 	api.v3.produce.rate.limit.enabled = false
control-center                | 	api.v3.produce.rate.limit.max.bytes.global.per.sec = 10000000
control-center                | 	api.v3.produce.rate.limit.max.bytes.per.sec = 10000000
control-center                | 	api.v3.produce.rate.limit.max.requests.global.per.sec = 10000
control-center                | 	api.v3.produce.rate.limit.max.requests.per.sec = 10000
control-center                | 	api.v3.produce.response.thread.pool.size = 4
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	bootstrap.servers = broker:29092
control-center                | 	client.init.timeout.ms = 60000
control-center                | 	client.sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	client.sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	client.sasl.kerberos.service.name = 
control-center                | 	client.sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	client.sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	client.sasl.mechanism = GSSAPI
control-center                | 	client.security.protocol = PLAINTEXT
control-center                | 	client.ssl.cipher.suites = 
control-center                | 	client.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
control-center                | 	client.ssl.endpoint.identification.algorithm = 
control-center                | 	client.ssl.key.password = [hidden]
control-center                | 	client.ssl.keymanager.algorithm = SunX509
control-center                | 	client.ssl.keystore.location = 
control-center                | 	client.ssl.keystore.password = [hidden]
control-center                | 	client.ssl.keystore.type = JKS
control-center                | 	client.ssl.protocol = TLS
control-center                | 	client.ssl.provider = 
control-center                | 	client.ssl.trustmanager.algorithm = PKIX
control-center                | 	client.ssl.truststore.location = 
control-center                | 	client.ssl.truststore.password = [hidden]
control-center                | 	client.ssl.truststore.type = JKS
control-center                | 	client.timeout.ms = 500
control-center                | 	client.zk.session.timeout.ms = 30000
control-center                | 	compression.enable = true
control-center                | 	confluent.resource.name.authority = 
control-center                | 	connector.connection.limit = 0
control-center                | 	consumer.instance.timeout.ms = 300000
control-center                | 	consumer.iterator.backoff.ms = 50
control-center                | 	consumer.iterator.timeout.ms = 1
control-center                | 	consumer.request.max.bytes = 67108864
control-center                | 	consumer.request.timeout.ms = 1000
control-center                | 	consumer.threads = 50
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	fetch.min.bytes = -1
control-center                | 	host.name = 
control-center                | 	http2.enabled = true
control-center                | 	id = 
control-center                | 	idle.timeout.ms = 30000
control-center                | 	kafka.rest.resource.extension.class = [io.confluent.kafkarest.KafkaRestResourceExtension]
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = []
control-center                | 	metrics.jmx.prefix = kafka.rest
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = false
control-center                | 	port = 8082
control-center                | 	producer.threads = 5
control-center                | 	proxy.protocol.enabled = false
control-center                | 	rate.limit.backend = guava
control-center                | 	rate.limit.costs = 
control-center                | 	rate.limit.default.cost = 1
control-center                | 	rate.limit.enable = false
control-center                | 	rate.limit.per.cluster.cache.expiry.ms = 3600000
control-center                | 	rate.limit.per.cluster.permits.per.sec = 50
control-center                | 	rate.limit.permits.per.sec = 50
control-center                | 	rate.limit.timeout.ms = 0
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json, application/vnd.kafka.v2+json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	schema.registry.url = http://localhost:8081
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	simpleconsumer.pool.size.max = 25
control-center                | 	simpleconsumer.pool.timeout.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	streaming.connection.max.duration.grace.period.ms = 500
control-center                | 	streaming.connection.max.duration.ms = 86400000
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /api/kafka-rest-ws/MkU3OEVBNTcwNTJENDM2Qg
control-center                | 	websocket.servlet.initializor.classes = []
control-center                | 	zookeeper.connect = 
control-center                |  (io.confluent.kafkarest.KafkaRestConfig)
control-center                | [2023-08-04 11:57:33,267] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
control-center                | [2023-08-04 11:57:33,268] INFO SchemaRegistryConfig values: 
control-center                | 	auto.register.schemas = false
control-center                | 	basic.auth.credentials.source = URL
control-center                | 	basic.auth.user.info = [hidden]
control-center                | 	bearer.auth.cache.expiry.buffer.seconds = 300
control-center                | 	bearer.auth.client.id = null
control-center                | 	bearer.auth.client.secret = null
control-center                | 	bearer.auth.credentials.source = STATIC_TOKEN
control-center                | 	bearer.auth.custom.provider.class = null
control-center                | 	bearer.auth.identity.pool.id = null
control-center                | 	bearer.auth.issuer.endpoint.url = null
control-center                | 	bearer.auth.logical.cluster = null
control-center                | 	bearer.auth.scope = null
control-center                | 	bearer.auth.scope.claim.name = scope
control-center                | 	bearer.auth.sub.claim.name = sub
control-center                | 	bearer.auth.token = [hidden]
control-center                | 	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
control-center                | 	http.connect.timeout.ms = 60000
control-center                | 	http.read.timeout.ms = 60000
control-center                | 	id.compatibility.strict = true
control-center                | 	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
control-center                | 	latest.cache.size = 1000
control-center                | 	latest.cache.ttl.sec = -1
control-center                | 	latest.compatibility.strict = true
control-center                | 	max.schemas.per.subject = 1000
control-center                | 	normalize.schemas = false
control-center                | 	proxy.host = 
control-center                | 	proxy.port = -1
control-center                | 	rule.actions = []
control-center                | 	rule.executors = []
control-center                | 	rule.service.loader.enable = true
control-center                | 	schema.format = null
control-center                | 	schema.reflection = false
control-center                | 	schema.registry.basic.auth.user.info = [hidden]
control-center                | 	schema.registry.ssl.cipher.suites = null
control-center                | 	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	schema.registry.ssl.endpoint.identification.algorithm = https
control-center                | 	schema.registry.ssl.engine.factory.class = null
control-center                | 	schema.registry.ssl.key.password = null
control-center                | 	schema.registry.ssl.keymanager.algorithm = SunX509
control-center                | 	schema.registry.ssl.keystore.certificate.chain = null
control-center                | 	schema.registry.ssl.keystore.key = null
control-center                | 	schema.registry.ssl.keystore.location = null
control-center                | 	schema.registry.ssl.keystore.password = null
control-center                | 	schema.registry.ssl.keystore.type = JKS
control-center                | 	schema.registry.ssl.protocol = TLSv1.3
control-center                | 	schema.registry.ssl.provider = null
control-center                | 	schema.registry.ssl.secure.random.implementation = null
control-center                | 	schema.registry.ssl.trustmanager.algorithm = PKIX
control-center                | 	schema.registry.ssl.truststore.certificates = null
control-center                | 	schema.registry.ssl.truststore.location = null
control-center                | 	schema.registry.ssl.truststore.password = null
control-center                | 	schema.registry.ssl.truststore.type = JKS
control-center                | 	schema.registry.url = [http://localhost:8081]
control-center                | 	use.latest.version = false
control-center                | 	use.latest.with.metadata = null
control-center                | 	use.schema.id = -1
control-center                | 	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
control-center                |  (io.confluent.kafkarest.config.SchemaRegistryConfig)
control-center                | [2023-08-04 11:57:33,480] INFO Binding EmbeddedKafkaRestApplication to all listeners. (io.confluent.rest.Application)
control-center                | [2023-08-04 11:57:33,939] INFO Starting Health Check (io.confluent.controlcenter.application.AllControlCenter)
control-center                | [2023-08-04 11:57:33,940] INFO Starting Alert Manager (io.confluent.controlcenter.application.AllControlCenter)
control-center                | [2023-08-04 11:57:33,941] INFO Starting Consumer Offsets Fetch (io.confluent.controlcenter.application.AllControlCenter)
control-center                | [2023-08-04 11:57:33,943] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                | 	metrics.jmx.prefix = confluent.controlcenter
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = true
control-center                | 	port = 9021
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:57:33,952] INFO current clusterId=MkU3OEVBNTcwNTJENDM2Qg (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | [2023-08-04 11:57:33,957] INFO Initial capacity 128, increased by 64, maximum capacity 2147483647. (io.confluent.rest.ApplicationServer)
control-center                | [2023-08-04 11:57:33,983] INFO AdminClientConfig values: 
control-center                | 	auto.include.jmx.reporter = true
control-center                | 	bootstrap.servers = [broker:29092]
control-center                | 	client.dns.lookup = use_all_dns_ips
control-center                | 	client.id = 
control-center                | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center                | 	confluent.proxy.protocol.client.address = null
control-center                | 	confluent.proxy.protocol.client.port = null
control-center                | 	confluent.proxy.protocol.client.version = NONE
control-center                | 	confluent.use.controller.listener = false
control-center                | 	connections.max.idle.ms = 300000
control-center                | 	default.api.timeout.ms = 60000
control-center                | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center                | 	metadata.max.age.ms = 300000
control-center                | 	metric.reporters = []
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.recording.level = INFO
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	receive.buffer.bytes = 65536
control-center                | 	reconnect.backoff.max.ms = 1000
control-center                | 	reconnect.backoff.ms = 50
control-center                | 	request.timeout.ms = 30000
control-center                | 	retries = 2147483647
control-center                | 	retry.backoff.ms = 100
control-center                | 	sasl.client.callback.handler.class = null
control-center                | 	sasl.jaas.config = null
control-center                | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center                | 	sasl.kerberos.min.time.before.relogin = 60000
control-center                | 	sasl.kerberos.service.name = null
control-center                | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center                | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center                | 	sasl.login.callback.handler.class = null
control-center                | 	sasl.login.class = null
control-center                | 	sasl.login.connect.timeout.ms = null
control-center                | 	sasl.login.read.timeout.ms = null
control-center                | 	sasl.login.refresh.buffer.seconds = 300
control-center                | 	sasl.login.refresh.min.period.seconds = 60
control-center                | 	sasl.login.refresh.window.factor = 0.8
control-center                | 	sasl.login.refresh.window.jitter = 0.05
control-center                | 	sasl.login.retry.backoff.max.ms = 10000
control-center                | 	sasl.login.retry.backoff.ms = 100
control-center                | 	sasl.mechanism = GSSAPI
control-center                | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center                | 	sasl.oauthbearer.expected.audience = null
control-center                | 	sasl.oauthbearer.expected.issuer = null
control-center                | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center                | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center                | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center                | 	sasl.oauthbearer.scope.claim.name = scope
control-center                | 	sasl.oauthbearer.sub.claim.name = sub
control-center                | 	sasl.oauthbearer.token.endpoint.url = null
control-center                | 	security.protocol = PLAINTEXT
control-center                | 	security.providers = null
control-center                | 	send.buffer.bytes = 131072
control-center                | 	socket.connection.setup.timeout.max.ms = 30000
control-center                | 	socket.connection.setup.timeout.ms = 10000
control-center                | 	ssl.cipher.suites = null
control-center                | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center                | 	ssl.endpoint.identification.algorithm = https
control-center                | 	ssl.engine.factory.class = null
control-center                | 	ssl.key.password = null
control-center                | 	ssl.keymanager.algorithm = SunX509
control-center                | 	ssl.keystore.certificate.chain = null
control-center                | 	ssl.keystore.key = null
control-center                | 	ssl.keystore.location = null
control-center                | 	ssl.keystore.password = null
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.protocol = TLSv1.3
control-center                | 	ssl.provider = null
control-center                | 	ssl.secure.random.implementation = null
control-center                | 	ssl.trustmanager.algorithm = PKIX
control-center                | 	ssl.truststore.certificates = null
control-center                | 	ssl.truststore.location = null
control-center                | 	ssl.truststore.password = null
control-center                | 	ssl.truststore.type = JKS
control-center                |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                | [2023-08-04 11:57:33,996] INFO broker id set has changed new={1=[broker:29092 (id: 1 rack: null tags: [])]} removed={} (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | [2023-08-04 11:57:34,000] INFO new controller=broker:29092 (id: 1 rack: null tags: []) (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | [2023-08-04 11:57:34,012] WARN These configurations '[consumer.session.timeout.ms, producer.max.block.ms, producer.retries, upgrade.from, producer.retry.backoff.ms, producer.linger.ms, producer.delivery.timeout.ms, task.timeout.ms, cache.max.bytes.buffering, producer.compression.type, num.stream.threads]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center                | [2023-08-04 11:57:34,018] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:34,018] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:34,018] INFO Kafka startTimeMs: 1691150254018 (org.apache.kafka.common.utils.AppInfoParser)
control-center                | [2023-08-04 11:57:34,344] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | [2023-08-04 11:57:34,347] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | [2023-08-04 11:57:34,478] INFO Adding listener with HTTP/2: NamedURI{uri=http://0.0.0.0:9021, name='null'} (io.confluent.rest.ApplicationServer)
control-center                | [2023-08-04 11:57:34,727] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to YZgYTx8aSQGlP9yT8BRY6A (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:34,757] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to yaGoMLo8Td-SorQ7IkopwQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:35,372] INFO name=monitoring-input-topic-progress-.count type=monitoring cluster= value=0.0 (io.confluent.controlcenter.util.StreamProgressReporter)
control-center                | [2023-08-04 11:57:35,375] INFO name=monitoring-input-topic-progress-.rate type=monitoring cluster= value=0.0 (io.confluent.controlcenter.util.StreamProgressReporter)
control-center                | [2023-08-04 11:57:35,377] INFO name=monitoring-input-topic-progress-.timestamp type=monitoring cluster= value=NaN (io.confluent.controlcenter.util.StreamProgressReporter)
control-center                | [2023-08-04 11:57:35,379] INFO name=monitoring-input-topic-progress-.min type=monitoring cluster= value=1.7976931348623157E308 (io.confluent.controlcenter.util.StreamProgressReporter)
control-center                | [2023-08-04 11:57:35,444] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to C_nrxqIHSiOvyQnBjl1AUg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:35,516] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to QpQV-WRTTiy9FejLVOtjsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:35,552] INFO ControlCenterBoundedMemoryConfig values: 
control-center                | 	rocksdb.cache.limit.strict = false
control-center                | 	rocksdb.cache.size = 16106127360
control-center                | 	rocksdb.cache.size.limit.enabled = false
control-center                | 	rocksdb.index.filter.block.ratio = 0.0
control-center                | 	rocksdb.write.buffer.cache.use = false
control-center                | 	rocksdb.write.buffer.size = 5368709120
control-center                |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center                | [2023-08-04 11:57:35,576] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                | 	metrics.jmx.prefix = confluent.controlcenter
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = true
control-center                | 	port = 9021
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:57:35,588] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                | 	metrics.jmx.prefix = confluent.controlcenter
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = true
control-center                | 	port = 9021
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:57:35,590] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                | 	metrics.jmx.prefix = confluent.controlcenter
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = true
control-center                | 	port = 9021
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:57:35,640] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 to 0 since the associated topicId changed from null to RFgEQoDlQFa80gJgHe9JRA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:35,664] INFO ControlCenterBoundedMemoryConfig values: 
control-center                | 	rocksdb.cache.limit.strict = false
control-center                | 	rocksdb.cache.size = 16106127360
control-center                | 	rocksdb.cache.size.limit.enabled = false
control-center                | 	rocksdb.index.filter.block.ratio = 0.0
control-center                | 	rocksdb.write.buffer.cache.use = false
control-center                | 	rocksdb.write.buffer.size = 5368709120
control-center                |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center                | [2023-08-04 11:57:35,744] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 to 0 since the associated topicId changed from null to m7OhDxavRJ-M5HmBzaV5yg (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 11:57:36,941] INFO Binding ControlCenterApplication to all listeners. (io.confluent.rest.Application)
control-center                | [2023-08-04 11:57:36,996] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                | 	metrics.jmx.prefix = confluent.controlcenter
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = true
control-center                | 	port = 9021
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:57:37,022] WARN [creqId=b628995c][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:57:36.232Z(1691150256232000), length=0B, duration=765ms(765186633ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 11:57:37,024] WARN [creqId=b628995c][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:57:37.004Z(1691150257004000), length=0B, duration=0ns, totalDuration=772ms(772883395ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
control-center                | [2023-08-04 11:57:37,032] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                | 	metrics.jmx.prefix = confluent.controlcenter
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = true
control-center                | 	port = 9021
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:57:37,459] INFO RestConfig values: 
control-center                | 	access.control.allow.headers = 
control-center                | 	access.control.allow.methods = 
control-center                | 	access.control.allow.origin = 
control-center                | 	access.control.skip.options = true
control-center                | 	authentication.method = NONE
control-center                | 	authentication.realm = 
control-center                | 	authentication.roles = [*]
control-center                | 	authentication.skip.paths = []
control-center                | 	compression.enable = true
control-center                | 	connector.connection.limit = 0
control-center                | 	csrf.prevention.enable = false
control-center                | 	csrf.prevention.token.endpoint = /csrf
control-center                | 	csrf.prevention.token.expiration.minutes = 30
control-center                | 	csrf.prevention.token.max.entries = 10000
control-center                | 	debug = false
control-center                | 	dos.filter.delay.ms = 100
control-center                | 	dos.filter.enabled = false
control-center                | 	dos.filter.insert.headers = true
control-center                | 	dos.filter.ip.whitelist = []
control-center                | 	dos.filter.managed.attr = false
control-center                | 	dos.filter.max.idle.tracker.ms = 30000
control-center                | 	dos.filter.max.requests.ms = 30000
control-center                | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center                | 	dos.filter.max.requests.per.sec = 25
control-center                | 	dos.filter.max.wait.ms = 50
control-center                | 	dos.filter.throttle.ms = 30000
control-center                | 	dos.filter.throttled.requests = 5
control-center                | 	http2.enabled = true
control-center                | 	idle.timeout.ms = 30000
control-center                | 	listener.protocol.map = []
control-center                | 	listeners = []
control-center                | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center                | 	metrics.jmx.prefix = confluent.controlcenter
control-center                | 	metrics.num.samples = 2
control-center                | 	metrics.sample.window.ms = 30000
control-center                | 	metrics.tag.map = []
control-center                | 	nosniff.prevention.enable = true
control-center                | 	port = 9021
control-center                | 	proxy.protocol.enabled = false
control-center                | 	reject.options.request = false
control-center                | 	request.logger.name = io.confluent.rest-utils.requests
control-center                | 	request.queue.capacity = 2147483647
control-center                | 	request.queue.capacity.growby = 64
control-center                | 	request.queue.capacity.init = 128
control-center                | 	resource.extension.classes = []
control-center                | 	response.http.headers.config = 
control-center                | 	response.mediatype.default = application/json
control-center                | 	response.mediatype.preferred = [application/json]
control-center                | 	rest.servlet.initializor.classes = []
control-center                | 	server.connection.limit = 0
control-center                | 	shutdown.graceful.ms = 1000
control-center                | 	ssl.cipher.suites = []
control-center                | 	ssl.client.auth = false
control-center                | 	ssl.client.authentication = NONE
control-center                | 	ssl.enabled.protocols = []
control-center                | 	ssl.endpoint.identification.algorithm = null
control-center                | 	ssl.key.password = [hidden]
control-center                | 	ssl.keymanager.algorithm = 
control-center                | 	ssl.keystore.location = 
control-center                | 	ssl.keystore.password = [hidden]
control-center                | 	ssl.keystore.reload = false
control-center                | 	ssl.keystore.type = JKS
control-center                | 	ssl.keystore.watch.location = 
control-center                | 	ssl.protocol = TLS
control-center                | 	ssl.provider = 
control-center                | 	ssl.trustmanager.algorithm = 
control-center                | 	ssl.truststore.location = 
control-center                | 	ssl.truststore.password = [hidden]
control-center                | 	ssl.truststore.type = JKS
control-center                | 	suppress.stack.trace.response = true
control-center                | 	thread.pool.max = 200
control-center                | 	thread.pool.min = 8
control-center                | 	websocket.path.prefix = /ws
control-center                | 	websocket.servlet.initializor.classes = []
control-center                |  (io.confluent.rest.RestConfig)
control-center                | [2023-08-04 11:57:37,524] INFO adding websocket endpoint ProducerResource (io.confluent.controlcenter.rest.ControlCenterApplication)
control-center                | [2023-08-04 11:57:37,540] INFO adding websocket endpoint ConsumerResource (io.confluent.controlcenter.rest.ControlCenterApplication)
control-center                | [2023-08-04 11:57:37,765] INFO jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.18+10-LTS (org.eclipse.jetty.server.Server)
control-center                | [2023-08-04 11:57:38,191] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
control-center                | [2023-08-04 11:57:38,192] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
control-center                | [2023-08-04 11:57:38,200] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session)
control-center                | [2023-08-04 11:57:39,481] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
control-center                | [2023-08-04 11:57:39,499] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
control-center                | [2023-08-04 11:57:39,530] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
control-center                | [2023-08-04 11:57:39,561] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
data-agrigator-taskmanager-1  | 2023-08-04 11:57:42,080 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 11:57:42,096 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 11:57:46,532] INFO Started o.e.j.s.ServletContextHandler@77a35b2f{/api/kafka-rest/MkU3OEVBNTcwNTJENDM2Qg/kafka,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
control-center                | Aug 04, 2023 11:57:47 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                | WARNING: A provider io.confluent.controlcenter.rest.CachedConsumerOffsetsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.CachedConsumerOffsetsResource will be ignored. 
control-center                | Aug 04, 2023 11:57:47 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                | WARNING: A provider io.confluent.controlcenter.rest.CommandResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.CommandResource will be ignored. 
control-center                | Aug 04, 2023 11:57:47 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                | WARNING: A provider io.confluent.controlcenter.rest.MessageDeliveryResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.MessageDeliveryResource will be ignored. 
control-center                | Aug 04, 2023 11:57:47 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                | WARNING: A provider io.confluent.controlcenter.rest.PermissionsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.PermissionsResource will be ignored. 
control-center                | Aug 04, 2023 11:57:47 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                | WARNING: A provider io.confluent.controlcenter.rest.AuthResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.AuthResource will be ignored. 
control-center                | Aug 04, 2023 11:57:47 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                | WARNING: A provider io.confluent.controlcenter.rest.MetricsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.MetricsResource will be ignored. 
control-center                | Aug 04, 2023 11:57:47 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                | WARNING: A provider io.confluent.controlcenter.rest.LicenseResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.LicenseResource will be ignored. 
control-center                | Aug 04, 2023 11:57:47 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                | WARNING: A provider io.confluent.controlcenter.rest.KafkaResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.KafkaResource will be ignored. 
control-center                | Aug 04, 2023 11:57:47 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                | WARNING: A provider io.confluent.controlcenter.rest.ClusterResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.ClusterResource will be ignored. 
control-center                | Aug 04, 2023 11:57:47 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                | WARNING: A provider io.confluent.controlcenter.rest.StatusResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.StatusResource will be ignored. 
control-center                | Aug 04, 2023 11:57:47 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                | WARNING: A provider io.confluent.controlcenter.rest.AlertsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.AlertsResource will be ignored. 
control-center                | Aug 04, 2023 11:57:47 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                | WARNING: A provider io.confluent.controlcenter.rest.ServiceHealthCheckResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.ServiceHealthCheckResource will be ignored. 
control-center                | Aug 04, 2023 11:57:47 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                | WARNING: A provider io.confluent.controlcenter.rest.HealthCheckResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.HealthCheckResource will be ignored. 
control-center                | Aug 04, 2023 11:57:47 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center                | WARNING: A provider io.confluent.controlcenter.rest.FeatureFlagResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.FeatureFlagResource will be ignored. 
control-center                | [2023-08-04 11:57:49,091] INFO Started o.e.j.s.ServletContextHandler@755057c7{/,[io.confluent.controlcenter.rest.ModifiableResource@7f2c0470],AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
control-center                | [2023-08-04 11:57:49,274] INFO Started o.e.j.s.ServletContextHandler@4d62bb8b{/api/kafka-rest-ws/MkU3OEVBNTcwNTJENDM2Qg,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
control-center                | [2023-08-04 11:57:49,369] INFO Started o.e.j.s.ServletContextHandler@70f37190{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
control-center                | [2023-08-04 11:57:49,443] INFO Started NetworkTrafficServerConnector@7ceb7a60{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:9021} (org.eclipse.jetty.server.AbstractConnector)
control-center                | [2023-08-04 11:57:49,446] INFO Started @73457ms (org.eclipse.jetty.server.Server)
data-agrigator-taskmanager-1  | 2023-08-04 11:57:52,136 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 11:57:52,138 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 11:57:59,586] WARN [creqId=4c2d0294][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:57:59.524Z(1691150279524000), length=0B, duration=60845s(60845216ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 11:57:59,587] WARN [creqId=4c2d0294][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:57:59.585Z(1691150279585000), length=0B, duration=0ns, totalDuration=61023s(61023975ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
data-agrigator-taskmanager-1  | 2023-08-04 11:58:02,173 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 11:58:02,176 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1  | 2023-08-04 11:58:12,215 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 11:58:12,218 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 11:58:17,741] WARN [creqId=8ebdd371][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:58:17.689Z(1691150297689000), length=0B, duration=50728s(50728478ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 11:58:17,742] WARN [creqId=8ebdd371][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:58:17.740Z(1691150297740000), length=0B, duration=0ns, totalDuration=50927s(50927185ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
data-agrigator-taskmanager-1  | 2023-08-04 11:58:22,266 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 11:58:22,267 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1  | 2023-08-04 11:58:32,317 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1  | 2023-08-04 11:58:32,315 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
control-center                | [2023-08-04 11:58:33,976] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | [2023-08-04 11:58:33,977] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | [2023-08-04 11:58:39,403] WARN [creqId=6305fcd2][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:58:39.387Z(1691150319387000), length=0B, duration=14470s(14470656ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 11:58:39,403] WARN [creqId=6305fcd2][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:58:39.402Z(1691150319402000), length=0B, duration=0ns, totalDuration=14550s(14550951ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
data-agrigator-taskmanager-1  | 2023-08-04 11:58:42,356 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 11:58:42,358 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1  | 2023-08-04 11:58:52,398 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 11:58:52,405 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 11:58:56,844] WARN [creqId=4db641b7][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:58:56.752Z(1691150336752000), length=0B, duration=89993s(89993363ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 11:58:56,845] WARN [creqId=4db641b7][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:58:56.842Z(1691150336842000), length=0B, duration=0ns, totalDuration=90152s(90152405ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
data-agrigator-taskmanager-1  | 2023-08-04 11:59:02,447 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 11:59:02,450 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1  | 2023-08-04 11:59:12,492 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 11:59:12,494 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 11:59:19,446] WARN [creqId=f2e5770c][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:59:19.361Z(1691150359361000), length=0B, duration=84206s(84206147ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 11:59:19,446] WARN [creqId=f2e5770c][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:59:19.445Z(1691150359445000), length=0B, duration=0ns, totalDuration=84301s(84301444ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
data-agrigator-taskmanager-1  | 2023-08-04 11:59:22,538 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 11:59:22,543 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 11:59:22,738] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] Processed 1 total records, ran 0 punctuators, and committed 1 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:59:27,504] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] Processed 16 total records, ran 0 punctuators, and committed 3 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:59:27,509] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] Processed 16 total records, ran 0 punctuators, and committed 3 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:59:27,511] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:59:27,610] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:59:27,612] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:59:27,613] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:59:27,656] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:59:27,710] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] Processed 16 total records, ran 0 punctuators, and committed 6 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:59:27,721] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:59:27,758] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:59:27,759] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 11:59:27,762] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
data-agrigator-taskmanager-1  | 2023-08-04 11:59:32,592 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1  | 2023-08-04 11:59:32,594 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
control-center                | [2023-08-04 11:59:33,969] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | [2023-08-04 11:59:33,969] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | [2023-08-04 11:59:41,076] WARN [creqId=ddcbaea4][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:59:41.048Z(1691150381048000), length=0B, duration=26504s(26504932ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 11:59:41,078] WARN [creqId=ddcbaea4][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:59:41.075Z(1691150381075000), length=0B, duration=0ns, totalDuration=26646s(26646703ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
data-agrigator-taskmanager-1  | 2023-08-04 11:59:42,632 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1  | 2023-08-04 11:59:42,628 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
postgres_db                   | 2023-08-04 11:59:44.119 UTC [22] LOG:  checkpoint starting: time
postgres_db                   | 2023-08-04 11:59:44.152 UTC [22] LOG:  checkpoint complete: wrote 3 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.008 s, sync=0.003 s, total=0.034 s; sync files=2, longest=0.002 s, average=0.002 s; distance=0 kB, estimate=0 kB
data-agrigator-taskmanager-1  | 2023-08-04 11:59:52,675 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 11:59:52,678 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 12:00:01,332] WARN [creqId=98cf1a6c][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T12:00:01.173Z(1691150401173000), length=0B, duration=157ms(157792641ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 12:00:01,333] WARN [creqId=98cf1a6c][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T12:00:01.331Z(1691150401331000), length=0B, duration=0ns, totalDuration=157ms(157916836ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
data-agrigator-taskmanager-1  | 2023-08-04 12:00:02,714 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 12:00:02,717 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 12:00:10,408] INFO ControlCenterBoundedMemoryConfig values: 
control-center                | 	rocksdb.cache.limit.strict = false
control-center                | 	rocksdb.cache.size = 16106127360
control-center                | 	rocksdb.cache.size.limit.enabled = false
control-center                | 	rocksdb.index.filter.block.ratio = 0.0
control-center                | 	rocksdb.write.buffer.cache.use = false
control-center                | 	rocksdb.write.buffer.size = 5368709120
control-center                |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center                | [2023-08-04 12:00:10,468] INFO ControlCenterBoundedMemoryConfig values: 
control-center                | 	rocksdb.cache.limit.strict = false
control-center                | 	rocksdb.cache.size = 16106127360
control-center                | 	rocksdb.cache.size.limit.enabled = false
control-center                | 	rocksdb.index.filter.block.ratio = 0.0
control-center                | 	rocksdb.write.buffer.cache.use = false
control-center                | 	rocksdb.write.buffer.size = 5368709120
control-center                |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
data-agrigator-taskmanager-1  | 2023-08-04 12:00:12,773 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 12:00:12,778 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 12:00:21,536] WARN [creqId=af43ee7e][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T12:00:21.453Z(1691150421453000), length=0B, duration=80245s(80245487ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 12:00:21,536] WARN [creqId=af43ee7e][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T12:00:21.533Z(1691150421533000), length=0B, duration=0ns, totalDuration=80514s(80514362ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
data-agrigator-taskmanager-1  | 2023-08-04 12:00:22,822 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 12:00:22,824 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1  | 2023-08-04 12:00:32,867 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 12:00:32,869 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 12:00:33,971] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | [2023-08-04 12:00:33,971] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | [2023-08-04 12:00:39,351] WARN [creqId=0318dc4a][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T12:00:39.292Z(1691150439292000), length=0B, duration=57935s(57935874ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 12:00:39,351] WARN [creqId=0318dc4a][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T12:00:39.350Z(1691150439350000), length=0B, duration=0ns, totalDuration=58024s(58024326ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
data-agrigator-taskmanager-1  | 2023-08-04 12:00:42,913 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 12:00:42,916 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1  | 2023-08-04 12:00:52,959 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 12:00:52,960 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 12:00:57,140] WARN [creqId=848e6124][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T12:00:57.116Z(1691150457116000), length=0B, duration=22489s(22489641ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 12:00:57,140] WARN [creqId=848e6124][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T12:00:57.139Z(1691150457139000), length=0B, duration=0ns, totalDuration=22733s(22733248ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
data-agrigator-taskmanager-1  | 2023-08-04 12:01:03,006 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 12:01:03,009 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1  | 2023-08-04 12:01:13,045 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 12:01:13,048 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 12:01:16,218] WARN [creqId=c8e33e94][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T12:01:16.200Z(1691150476200000), length=0B, duration=16281s(16281291ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 12:01:16,218] WARN [creqId=c8e33e94][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T12:01:16.217Z(1691150476217000), length=0B, duration=0ns, totalDuration=16403s(16403670ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
control-center                | [2023-08-04 12:01:22,797] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
data-agrigator-taskmanager-1  | 2023-08-04 12:01:23,083 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 12:01:23,086 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 12:01:27,517] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] Processed 14 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:01:27,533] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] Processed 14 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:01:27,595] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:01:27,621] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:01:27,653] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:01:27,697] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:01:27,721] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] Processed 14 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:01:27,741] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:01:27,742] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:01:27,796] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:01:27,841] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:01:27,854] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
data-agrigator-taskmanager-1  | 2023-08-04 12:01:33,124 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 12:01:33,129 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 12:01:33,664] WARN [creqId=377c301b][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T12:01:33.608Z(1691150493608000), length=0B, duration=55427s(55427605ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 12:01:33,664] WARN [creqId=377c301b][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T12:01:33.663Z(1691150493663000), length=0B, duration=0ns, totalDuration=55568s(55568466ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
control-center                | [2023-08-04 12:01:33,966] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | [2023-08-04 12:01:33,970] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
data-agrigator-taskmanager-1  | 2023-08-04 12:01:43,200 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 12:01:43,207 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 12:01:45,750] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to YZgYTx8aSQGlP9yT8BRY6A (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 12:01:45,752] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 5-cwUQMrRLuRuq67G0mKBQ (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 12:01:45,788] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                | [2023-08-04 12:01:52,580] INFO [AdminClient clientId=_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-admin] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
data-agrigator-taskmanager-1  | 2023-08-04 12:01:53,243 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 12:01:53,244 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
control-center                | [2023-08-04 12:01:55,028] WARN [creqId=e9f98315][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T12:01:54.991Z(1691150514991000), length=0B, duration=34579s(34579782ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 12:01:55,028] WARN [creqId=e9f98315][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T12:01:55.026Z(1691150515026000), length=0B, duration=0ns, totalDuration=34803s(34803486ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
data-agrigator-taskmanager-1  | 2023-08-04 12:02:03,403 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@jobmanager:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@jobmanager:6123]] Caused by: [java.net.UnknownHostException: jobmanager: Temporary failure in name resolution]
data-agrigator-taskmanager-1  | 2023-08-04 12:02:03,406 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,482 ERROR org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Fatal error occurred in TaskExecutor akka.tcp://flink@172.21.0.5:43917/user/rpc/taskmanager_0.
data-agrigator-taskmanager-1  | org.apache.flink.runtime.taskexecutor.exceptions.RegistrationTimeoutException: Could not register at the ResourceManager within the specified maximum registration duration PT5M. This indicates a problem with this instance. Terminating now.
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.registrationTimeout(TaskExecutor.java:1552) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$startRegistrationTimeout$18(TaskExecutor.java:1537) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:453) ~[flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:453) ~[flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:218) ~[flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
data-agrigator-taskmanager-1  | 	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
data-agrigator-taskmanager-1  | 	at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
data-agrigator-taskmanager-1  | 	at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
data-agrigator-taskmanager-1  | 	at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,487 ERROR org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Fatal error occurred while executing the TaskManager. Shutting it down...
data-agrigator-taskmanager-1  | org.apache.flink.runtime.taskexecutor.exceptions.RegistrationTimeoutException: Could not register at the ResourceManager within the specified maximum registration duration PT5M. This indicates a problem with this instance. Terminating now.
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.registrationTimeout(TaskExecutor.java:1552) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$startRegistrationTimeout$18(TaskExecutor.java:1537) ~[flink-dist-1.17.1.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:453) ~[flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:453) ~[flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:218) ~[flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_f38a18c7-77ba-4fb9-a496-27c76be2345f.jar:1.17.1]
data-agrigator-taskmanager-1  | 	at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
data-agrigator-taskmanager-1  | 	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
data-agrigator-taskmanager-1  | 	at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
data-agrigator-taskmanager-1  | 	at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
data-agrigator-taskmanager-1  | 	at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,506 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Stopping TaskExecutor akka.tcp://flink@172.21.0.5:43917/user/rpc/taskmanager_0.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,507 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Terminating registration attempts towards ResourceManager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,508 INFO  org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManager [] - Shutting down TaskExecutorStateChangelogStoragesManager.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,508 INFO  org.apache.flink.runtime.state.TaskExecutorChannelStateExecutorFactoryManager [] - Shutting down TaskExecutorChannelStateExecutorFactoryManager.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,534 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Stop job leader service.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,535 INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,565 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /tmp/flink-io-5a012b4d-d5a1-407a-b8a0-bf9e4e3b3557
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,569 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Shutting down the network environment and its components.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,574 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful shutdown (took 3 ms).
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,587 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful shutdown (took 11 ms).
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,609 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /tmp/flink-netty-shuffle-3c448a91-bda5-4602-8a6e-3ab77ef54fb4
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,611 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Shutting down the kvState service and its components.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,611 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Stop job leader service.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,616 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - removed file cache directory /tmp/flink-dist-cache-f90515ff-91c3-4b82-b5a6-7251f1fd0af9
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,640 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Stopped TaskExecutor akka.tcp://flink@172.21.0.5:43917/user/rpc/taskmanager_0.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,658 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,673 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,675 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,721 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,912 INFO  akka.actor.CoordinatedShutdown                               [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,911 INFO  akka.actor.CoordinatedShutdown                               [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,962 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,970 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,973 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:10,974 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:11,067 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:11,076 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:11,118 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:11,209 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Terminating TaskManagerRunner with exit code 1.
data-agrigator-taskmanager-1  | 2023-08-04 12:02:11,210 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
data-agrigator-taskmanager-1 exited with code 1
control-center                | [2023-08-04 12:02:17,672] WARN [creqId=137db640][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T12:02:17.606Z(1691150537606000), length=0B, duration=65434s(65434150ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 12:02:17,673] WARN [creqId=137db640][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T12:02:17.671Z(1691150537671000), length=0B, duration=0ns, totalDuration=65714s(65714891ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
control-center                | [2023-08-04 12:02:25,815] INFO [AdminClient clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-admin] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                | [2023-08-04 12:02:26,405] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 to 0 since the associated topicId changed from null to RFgEQoDlQFa80gJgHe9JRA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 12:02:26,406] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to QpQV-WRTTiy9FejLVOtjsA (org.apache.kafka.clients.Metadata)
control-center                | [2023-08-04 12:02:31,529] INFO 172.21.0.1 - - [04/Aug/2023:12:02:30 +0000] "GET /clusters HTTP/1.1" 200 1151 "-" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 543 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:31,669] INFO 172.21.0.1 - - [04/Aug/2023:12:02:31 +0000] "GET /dist/bootstrap-local.76a65c8.js HTTP/1.1" 200 61728 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 122 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:32,137] INFO 172.21.0.1 - - [04/Aug/2023:12:02:32 +0000] "GET /dist/manifest.json HTTP/1.1" 200 389 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 17 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:32,180] INFO 172.21.0.1 - - [04/Aug/2023:12:02:32 +0000] "GET /dist/favicon.ico HTTP/1.1" 200 33310 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 22 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:32,374] INFO 172.21.0.1 - - [04/Aug/2023:12:02:32 +0000] "GET /dist/dist/android-chrome-144x144.png HTTP/1.1" 404 399 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 186 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:32,732] INFO 172.21.0.1 - - [04/Aug/2023:12:02:32 +0000] "GET /3.0/license/payload HTTP/1.1" 200 170 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 645 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:32,800] INFO 172.21.0.1 - - [04/Aug/2023:12:02:32 +0000] "GET /2.0/feature/flags HTTP/1.1" 200 429 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 716 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:33,956] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | [2023-08-04 12:02:33,956] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | [2023-08-04 12:02:34,258] INFO [AdminClient clientId=adminclient-3] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center                | [2023-08-04 12:02:37,365] WARN [creqId=708e1b63][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T12:02:37.339Z(1691150557339000), length=0B, duration=24556s(24556361ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 12:02:37,366] WARN [creqId=708e1b63][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T12:02:37.364Z(1691150557364000), length=0B, duration=0ns, totalDuration=24702s(24702690ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
control-center                | [2023-08-04 12:02:42,207] INFO 172.21.0.1 - - [04/Aug/2023:12:02:41 +0000] "GET /clusters HTTP/1.1" 200 1151 "-" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 233 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:42,428] INFO 172.21.0.1 - - [04/Aug/2023:12:02:42 +0000] "GET /dist/bootstrap-local.76a65c8.js HTTP/1.1" 200 61728 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 114 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:42,583] INFO 172.21.0.1 - - [04/Aug/2023:12:02:42 +0000] "GET /2.0/feature/flags HTTP/1.1" 200 429 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 43 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:42,585] INFO 172.21.0.1 - - [04/Aug/2023:12:02:42 +0000] "GET /3.0/license/payload HTTP/1.1" 200 170 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 29 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:42,618] INFO 172.21.0.1 - - [04/Aug/2023:12:02:42 +0000] "GET /dist/manifest.json HTTP/1.1" 200 389 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 12 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:42,663] INFO 172.21.0.1 - - [04/Aug/2023:12:02:42 +0000] "GET /dist/favicon.ico HTTP/1.1" 200 33310 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 37 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:42,693] INFO 172.21.0.1 - - [04/Aug/2023:12:02:42 +0000] "GET /dist/dist/android-chrome-144x144.png HTTP/1.1" 404 399 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 7 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:46,085] INFO 172.21.0.1 - - [04/Aug/2023:12:02:44 +0000] "GET /dist/c3.chunk-71c32f43e29ac8156892.js HTTP/1.1" 200 1116319 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 1339 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:46,119] INFO 172.21.0.1 - - [04/Aug/2023:12:02:44 +0000] "GET /dist/c3.chunk-e71fc146570941633bfc.js HTTP/1.1" 200 1077929 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 1372 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:48,494] INFO 172.21.0.1 - - [04/Aug/2023:12:02:48 +0000] "GET /2.0/feature/flags HTTP/1.1" 200 429 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 15 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:48,990] INFO 172.21.0.1 - - [04/Aug/2023:12:02:48 +0000] "GET /2.0/clusters/kafka/display/stream-monitoring HTTP/1.1" 200 110 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 215 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:49,034] INFO 172.21.0.1 - - [04/Aug/2023:12:02:48 +0000] "GET /3.0/auth/principal HTTP/1.1" 200 30 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 258 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:49,049] INFO 172.21.0.1 - - [04/Aug/2023:12:02:48 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 234 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:49,141] INFO 172.21.0.1 - - [04/Aug/2023:12:02:48 +0000] "GET /2.0/clusters/kafka/display/cluster_management HTTP/1.1" 200 110 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 401 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:49,178] INFO 172.21.0.1 - - [04/Aug/2023:12:02:48 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 2 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 182 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:49,194] INFO 172.21.0.1 - - [04/Aug/2023:12:02:49 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 2 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 137 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:49,256] INFO 172.21.0.1 - - [04/Aug/2023:12:02:49 +0000] "GET /3.0/license HTTP/1.1" 200 445 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 114 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:49,322] INFO 172.21.0.1 - - [04/Aug/2023:12:02:48 +0000] "GET /2.0/health/status HTTP/1.1" 200 149 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 606 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:49,341] INFO 172.21.0.1 - - [04/Aug/2023:12:02:49 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 248 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:49,389] INFO 172.21.0.1 - - [04/Aug/2023:12:02:48 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 652 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:49,406] INFO 172.21.0.1 - - [04/Aug/2023:12:02:49 +0000] "GET / HTTP/1.1" 200 1151 "-" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 222 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:49,432] INFO 172.21.0.1 - - [04/Aug/2023:12:02:49 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 33 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:50,023] INFO 172.21.0.1 - - [04/Aug/2023:12:02:49 +0000] "GET /dist/bootstrap-local.76a65c8.js HTTP/1.1" 200 61728 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 73 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:50,143] INFO 172.21.0.1 - - [04/Aug/2023:12:02:50 +0000] "GET /2.0/feature/flags HTTP/1.1" 200 429 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 17 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:50,180] INFO 172.21.0.1 - - [04/Aug/2023:12:02:50 +0000] "GET /3.0/license/payload HTTP/1.1" 200 170 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 27 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:50,184] INFO 172.21.0.1 - - [04/Aug/2023:12:02:50 +0000] "GET /dist/manifest.json HTTP/1.1" 200 389 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 5 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:50,244] INFO 172.21.0.1 - - [04/Aug/2023:12:02:50 +0000] "GET /dist/dist/android-chrome-144x144.png HTTP/1.1" 404 399 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 12 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:52,641] INFO 172.21.0.1 - - [04/Aug/2023:12:02:51 +0000] "GET /dist/c3.chunk-e71fc146570941633bfc.js HTTP/1.1" 200 1077929 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 903 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:52,802] INFO 172.21.0.1 - - [04/Aug/2023:12:02:51 +0000] "GET /dist/c3.chunk-71c32f43e29ac8156892.js HTTP/1.1" 200 1116319 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 1064 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:53,409] INFO 172.21.0.1 - - [04/Aug/2023:12:02:53 +0000] "GET /2.0/feature/flags HTTP/1.1" 200 429 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 14 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:53,506] INFO 172.21.0.1 - - [04/Aug/2023:12:02:53 +0000] "GET /3.0/license HTTP/1.1" 200 445 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 31 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:53,592] INFO 172.21.0.1 - - [04/Aug/2023:12:02:53 +0000] "GET /2.0/health/status HTTP/1.1" 200 149 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 21 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:53,633] INFO 172.21.0.1 - - [04/Aug/2023:12:02:53 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 25 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:53,643] INFO 172.21.0.1 - - [04/Aug/2023:12:02:53 +0000] "GET /3.0/auth/principal HTTP/1.1" 200 30 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 44 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:53,652] INFO 172.21.0.1 - - [04/Aug/2023:12:02:53 +0000] "GET /2.0/clusters/kafka/display/stream-monitoring HTTP/1.1" 200 110 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 45 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:53,666] INFO 172.21.0.1 - - [04/Aug/2023:12:02:53 +0000] "GET /2.0/clusters/kafka/display/cluster_management HTTP/1.1" 200 110 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 59 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:53,695] INFO 172.21.0.1 - - [04/Aug/2023:12:02:53 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 48 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:53,708] INFO 172.21.0.1 - - [04/Aug/2023:12:02:53 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 2 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 75 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:53,777] INFO 172.21.0.1 - - [04/Aug/2023:12:02:53 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 2 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 21 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:53,785] INFO 172.21.0.1 - - [04/Aug/2023:12:02:53 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 34 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:53,794] INFO 172.21.0.1 - - [04/Aug/2023:12:02:53 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 48 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:54,714] INFO 172.21.0.1 - - [04/Aug/2023:12:02:54 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/controller HTTP/1.1" 200 67 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 546 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:55,054] INFO 172.21.0.1 - - [04/Aug/2023:12:02:54 +0000] "GET /2.0/metrics/maxtime HTTP/1.1" 200 2 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 916 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:55,275] INFO 172.21.0.1 - - [04/Aug/2023:12:02:54 +0000] "GET /2.0/metrics/clusters/status HTTP/1.1" 200 217 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 1112 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:55,791] WARN [creqId=57847b29][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T12:02:55.696Z(1691150575696000), length=0B, duration=94190s(94190715ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 12:02:55,792] WARN [creqId=57847b29][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T12:02:55.790Z(1691150575790000), length=0B, duration=0ns, totalDuration=94284s(94284897ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
control-center                | [2023-08-04 12:02:58,408] INFO 172.21.0.1 - - [04/Aug/2023:12:02:58 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/brokers/config HTTP/1.1" 200 4487 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 133 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:02:58,551] INFO 172.21.0.1 - - [04/Aug/2023:12:02:58 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/broker/status HTTP/1.1" 200 162 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 95 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:03:19,730] WARN [creqId=2eb389a4][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T12:03:19.708Z(1691150599708000), length=0B, duration=21547s(21547309ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 12:03:19,731] WARN [creqId=2eb389a4][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T12:03:19.729Z(1691150599729000), length=0B, duration=0ns, totalDuration=21668s(21668378ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
control-center                | [2023-08-04 12:03:22,873] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-fc02bb9d-1669-4403-9b48-1381c4309ebc-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:03:23,642] INFO 172.21.0.1 - - [04/Aug/2023:12:03:23 +0000] "GET /2.0/metrics/maxtime HTTP/1.1" 200 2 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 30 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:03:23,752] INFO 172.21.0.1 - - [04/Aug/2023:12:03:23 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/controller HTTP/1.1" 200 67 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 56 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:03:23,789] INFO 172.21.0.1 - - [04/Aug/2023:12:03:23 +0000] "GET /2.0/metrics/clusters/status HTTP/1.1" 200 217 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 136 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:03:27,541] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-6] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:03:27,558] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-8] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:03:27,598] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:03:27,667] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:03:27,679] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:03:27,699] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:03:27,738] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-1] Processed 16 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:03:27,780] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:03:27,800] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:03:27,800] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:03:27,868] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:03:27,898] INFO stream-thread [_confluent-controlcenter-7-4-1-1-294242ab-6891-4246-9408-d412e91c3d10-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center                | [2023-08-04 12:03:33,960] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | [2023-08-04 12:03:33,965] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center                | [2023-08-04 12:03:40,872] WARN [creqId=2da9216b][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T12:03:40.748Z(1691150620748000), length=0B, duration=122ms(122245317ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | [2023-08-04 12:03:40,872] WARN [creqId=2da9216b][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T12:03:40.871Z(1691150620871000), length=0B, duration=0ns, totalDuration=122ms(122483044ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center                | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center                | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center                | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center                | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center                | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center                | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center                | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center                | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireCh
control-center                | annelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center                | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center                | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center                | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center                | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center                | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center                | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center                | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center                | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center                | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center                | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center                | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center                | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center                | 	... 35 more
control-center                | [2023-08-04 12:03:53,676] INFO 172.21.0.1 - - [04/Aug/2023:12:03:53 +0000] "GET /2.0/health/status HTTP/1.1" 200 149 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 18 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:03:53,749] INFO 172.21.0.1 - - [04/Aug/2023:12:03:53 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 35 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:03:53,764] INFO 172.21.0.1 - - [04/Aug/2023:12:03:53 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 62 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:03:53,793] INFO 172.21.0.1 - - [04/Aug/2023:12:03:53 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 2 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 20 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:03:53,828] INFO 172.21.0.1 - - [04/Aug/2023:12:03:53 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 2 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 17 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:03:53,863] INFO 172.21.0.1 - - [04/Aug/2023:12:03:53 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 19 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:03:53,938] INFO 172.21.0.1 - - [04/Aug/2023:12:03:53 +0000] "GET /3.0/license HTTP/1.1" 200 445 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 37 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:03:54,017] INFO 172.21.0.1 - - [04/Aug/2023:12:03:53 +0000] "GET /2.0/metrics/maxtime HTTP/1.1" 200 2 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 40 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:03:54,077] INFO 172.21.0.1 - - [04/Aug/2023:12:03:54 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/controller HTTP/1.1" 200 67 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 40 (io.confluent.rest-utils.requests)
control-center                | [2023-08-04 12:03:54,134] INFO 172.21.0.1 - - [04/Aug/2023:12:03:53 +0000] "GET /2.0/metrics/clusters/status HTTP/1.1" 200 217 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 137 (io.confluent.rest-utils.requests)
Gracefully stopping... (press Ctrl+C again to force)
Aborting on container exit...
 Container control-center  Stopping
 Container data-agrigator-taskmanager-1  Stopping
 Container postgres_db  Stopping
 Container data-agrigator-taskmanager-1  Stopped
 Container data-agrigator-jobmanager-1  Stopping
 Container data-agrigator-jobmanager-1  Stopped
 Container postgres_db  Stopped
 Container control-center  Stopped
 Container schema-registry  Stopping
 Container schema-registry  Stopped
 Container broker  Stopping
 Container broker  Stopped
canceled
