 Network kafka_kraft_default  Creating
 Network kafka_kraft_default  Created
 Container broker  Creating
 Container broker  Created
 Container schema-registry  Creating
 Container schema-registry  Created
 Container rest-proxy  Creating
 Container connect  Creating
 Container rest-proxy  Created
 Container connect  Created
 Container control-center  Creating
 Container control-center  Created
Attaching to broker, connect, control-center, rest-proxy, schema-registry
broker           | ===> User
broker           | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
broker           | ===> Configuring ...
broker           | Running in KRaft mode...
schema-registry  | ===> User
schema-registry  | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
schema-registry  | ===> Configuring ...
connect          | ===> User
connect          | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
connect          | ===> Configuring ...
rest-proxy       | ===> User
rest-proxy       | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
rest-proxy       | ===> Configuring ...
control-center   | ===> User
control-center   | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
control-center   | ===> Configuring ...
control-center   | ===> Check if /etc/confluent-control-center is writable ...
rest-proxy       | ===> Running preflight checks ... 
rest-proxy       | ===> Check if Kafka is healthy ...
control-center   | ===> Check if /var/lib/confluent-control-center is writable ...
broker           | ===> Running preflight checks ... 
broker           | ===> Check if /var/lib/kafka/data is writable ...
schema-registry  | ===> Running preflight checks ... 
schema-registry  | ===> Check if Kafka is healthy ...
broker           | ===> Running in KRaft mode, skipping Zookeeper health check...
broker           | ===> Using provided cluster id MkU3OEVBNTcwNTJENDM2Qk ...
rest-proxy       | [2023-08-04 11:13:44,881] INFO AdminClientConfig values: 
rest-proxy       | 	auto.include.jmx.reporter = true
rest-proxy       | 	bootstrap.servers = [broker:29092]
rest-proxy       | 	client.dns.lookup = use_all_dns_ips
rest-proxy       | 	client.id = 
rest-proxy       | 	connections.max.idle.ms = 300000
rest-proxy       | 	default.api.timeout.ms = 60000
rest-proxy       | 	metadata.max.age.ms = 300000
rest-proxy       | 	metric.reporters = []
rest-proxy       | 	metrics.num.samples = 2
rest-proxy       | 	metrics.recording.level = INFO
rest-proxy       | 	metrics.sample.window.ms = 30000
rest-proxy       | 	receive.buffer.bytes = 65536
rest-proxy       | 	reconnect.backoff.max.ms = 1000
rest-proxy       | 	reconnect.backoff.ms = 50
rest-proxy       | 	request.timeout.ms = 30000
rest-proxy       | 	retries = 2147483647
rest-proxy       | 	retry.backoff.ms = 100
rest-proxy       | 	sasl.client.callback.handler.class = null
rest-proxy       | 	sasl.jaas.config = null
rest-proxy       | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
rest-proxy       | 	sasl.kerberos.min.time.before.relogin = 60000
rest-proxy       | 	sasl.kerberos.service.name = null
rest-proxy       | 	sasl.kerberos.ticket.renew.jitter = 0.05
rest-proxy       | 	sasl.kerberos.ticket.renew.window.factor = 0.8
rest-proxy       | 	sasl.login.callback.handler.class = null
rest-proxy       | 	sasl.login.class = null
rest-proxy       | 	sasl.login.connect.timeout.ms = null
rest-proxy       | 	sasl.login.read.timeout.ms = null
rest-proxy       | 	sasl.login.refresh.buffer.seconds = 300
rest-proxy       | 	sasl.login.refresh.min.period.seconds = 60
rest-proxy       | 	sasl.login.refresh.window.factor = 0.8
rest-proxy       | 	sasl.login.refresh.window.jitter = 0.05
rest-proxy       | 	sasl.login.retry.backoff.max.ms = 10000
rest-proxy       | 	sasl.login.retry.backoff.ms = 100
rest-proxy       | 	sasl.mechanism = GSSAPI
rest-proxy       | 	sasl.oauthbearer.clock.skew.seconds = 30
rest-proxy       | 	sasl.oauthbearer.expected.audience = null
rest-proxy       | 	sasl.oauthbearer.expected.issuer = null
rest-proxy       | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
rest-proxy       | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
rest-proxy       | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
rest-proxy       | 	sasl.oauthbearer.jwks.endpoint.url = null
rest-proxy       | 	sasl.oauthbearer.scope.claim.name = scope
rest-proxy       | 	sasl.oauthbearer.sub.claim.name = sub
rest-proxy       | 	sasl.oauthbearer.token.endpoint.url = null
rest-proxy       | 	security.protocol = PLAINTEXT
rest-proxy       | 	security.providers = null
rest-proxy       | 	send.buffer.bytes = 131072
rest-proxy       | 	socket.connection.setup.timeout.max.ms = 30000
rest-proxy       | 	socket.connection.setup.timeout.ms = 10000
rest-proxy       | 	ssl.cipher.suites = null
rest-proxy       | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
rest-proxy       | 	ssl.endpoint.identification.algorithm = https
rest-proxy       | 	ssl.engine.factory.class = null
rest-proxy       | 	ssl.key.password = null
rest-proxy       | 	ssl.keymanager.algorithm = SunX509
rest-proxy       | 	ssl.keystore.certificate.chain = null
rest-proxy       | 	ssl.keystore.key = null
rest-proxy       | 	ssl.keystore.location = null
rest-proxy       | 	ssl.keystore.password = null
rest-proxy       | 	ssl.keystore.type = JKS
rest-proxy       | 	ssl.protocol = TLSv1.3
rest-proxy       | 	ssl.provider = null
rest-proxy       | 	ssl.secure.random.implementation = null
rest-proxy       | 	ssl.trustmanager.algorithm = PKIX
rest-proxy       | 	ssl.truststore.certificates = null
rest-proxy       | 	ssl.truststore.location = null
rest-proxy       | 	ssl.truststore.password = null
rest-proxy       | 	ssl.truststore.type = JKS
rest-proxy       |  (org.apache.kafka.clients.admin.AdminClientConfig)
schema-registry  | [2023-08-04 11:13:44,896] INFO AdminClientConfig values: 
schema-registry  | 	auto.include.jmx.reporter = true
schema-registry  | 	bootstrap.servers = [broker:29092]
schema-registry  | 	client.dns.lookup = use_all_dns_ips
schema-registry  | 	client.id = 
schema-registry  | 	connections.max.idle.ms = 300000
schema-registry  | 	default.api.timeout.ms = 60000
schema-registry  | 	metadata.max.age.ms = 300000
schema-registry  | 	metric.reporters = []
schema-registry  | 	metrics.num.samples = 2
schema-registry  | 	metrics.recording.level = INFO
schema-registry  | 	metrics.sample.window.ms = 30000
schema-registry  | 	receive.buffer.bytes = 65536
schema-registry  | 	reconnect.backoff.max.ms = 1000
schema-registry  | 	reconnect.backoff.ms = 50
schema-registry  | 	request.timeout.ms = 30000
schema-registry  | 	retries = 2147483647
schema-registry  | 	retry.backoff.ms = 100
schema-registry  | 	sasl.client.callback.handler.class = null
schema-registry  | 	sasl.jaas.config = null
schema-registry  | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
schema-registry  | 	sasl.kerberos.min.time.before.relogin = 60000
schema-registry  | 	sasl.kerberos.service.name = null
schema-registry  | 	sasl.kerberos.ticket.renew.jitter = 0.05
schema-registry  | 	sasl.kerberos.ticket.renew.window.factor = 0.8
schema-registry  | 	sasl.login.callback.handler.class = null
schema-registry  | 	sasl.login.class = null
schema-registry  | 	sasl.login.connect.timeout.ms = null
schema-registry  | 	sasl.login.read.timeout.ms = null
schema-registry  | 	sasl.login.refresh.buffer.seconds = 300
schema-registry  | 	sasl.login.refresh.min.period.seconds = 60
schema-registry  | 	sasl.login.refresh.window.factor = 0.8
schema-registry  | 	sasl.login.refresh.window.jitter = 0.05
schema-registry  | 	sasl.login.retry.backoff.max.ms = 10000
schema-registry  | 	sasl.login.retry.backoff.ms = 100
schema-registry  | 	sasl.mechanism = GSSAPI
schema-registry  | 	sasl.oauthbearer.clock.skew.seconds = 30
schema-registry  | 	sasl.oauthbearer.expected.audience = null
schema-registry  | 	sasl.oauthbearer.expected.issuer = null
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.url = null
schema-registry  | 	sasl.oauthbearer.scope.claim.name = scope
schema-registry  | 	sasl.oauthbearer.sub.claim.name = sub
schema-registry  | 	sasl.oauthbearer.token.endpoint.url = null
schema-registry  | 	security.protocol = PLAINTEXT
schema-registry  | 	security.providers = null
schema-registry  | 	send.buffer.bytes = 131072
schema-registry  | 	socket.connection.setup.timeout.max.ms = 30000
schema-registry  | 	socket.connection.setup.timeout.ms = 10000
schema-registry  | 	ssl.cipher.suites = null
schema-registry  | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
schema-registry  | 	ssl.endpoint.identification.algorithm = https
schema-registry  | 	ssl.engine.factory.class = null
schema-registry  | 	ssl.key.password = null
schema-registry  | 	ssl.keymanager.algorithm = SunX509
schema-registry  | 	ssl.keystore.certificate.chain = null
schema-registry  | 	ssl.keystore.key = null
schema-registry  | 	ssl.keystore.location = null
schema-registry  | 	ssl.keystore.password = null
schema-registry  | 	ssl.keystore.type = JKS
schema-registry  | 	ssl.protocol = TLSv1.3
schema-registry  | 	ssl.provider = null
schema-registry  | 	ssl.secure.random.implementation = null
schema-registry  | 	ssl.trustmanager.algorithm = PKIX
schema-registry  | 	ssl.truststore.certificates = null
schema-registry  | 	ssl.truststore.location = null
schema-registry  | 	ssl.truststore.password = null
schema-registry  | 	ssl.truststore.type = JKS
schema-registry  |  (org.apache.kafka.clients.admin.AdminClientConfig)
schema-registry  | [2023-08-04 11:13:48,707] INFO Kafka version: 7.4.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2023-08-04 11:13:48,708] INFO Kafka commitId: fed9c006bfc7ba5b (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2023-08-04 11:13:48,712] INFO Kafka startTimeMs: 1691147628633 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2023-08-04 11:13:49,327] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:49,395] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:49,476] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:49,476] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:49,594] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:49,597] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:49,691] INFO Kafka version: 7.4.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
rest-proxy       | [2023-08-04 11:13:49,691] INFO Kafka commitId: fed9c006bfc7ba5b (org.apache.kafka.common.utils.AppInfoParser)
rest-proxy       | [2023-08-04 11:13:49,692] INFO Kafka startTimeMs: 1691147629629 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2023-08-04 11:13:49,820] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:49,831] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:50,111] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:50,152] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:50,209] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:50,209] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:50,352] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:50,356] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:50,420] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:50,420] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:50,638] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:50,654] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
control-center   | ===> Running preflight checks ... 
control-center   | ===> Check if Kafka is healthy ...
rest-proxy       | [2023-08-04 11:13:51,174] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:51,174] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:51,188] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:51,188] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:52,038] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:52,044] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:52,118] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:52,120] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:53,040] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:53,040] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:53,068] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:53,069] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker           | ===> Launching ... 
broker           | ===> Launching kafka ... 
schema-registry  | [2023-08-04 11:13:53,959] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:53,959] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:54,094] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:54,094] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:54,923] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:54,924] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:55,222] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:55,228] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:55,855] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:55,856] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:56,371] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:56,374] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:56,792] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:56,799] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:57,216] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:57,217] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:57,955] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:57,956] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:58,145] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:58,148] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:58,986] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:13:58,987] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:59,276] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:13:59,279] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:00,214] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:00,221] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:00,235] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:00,240] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:01,171] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:01,181] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:01,370] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:01,371] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:02,226] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:02,231] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:02,330] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:02,331] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:03,251] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:03,255] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:03,263] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:03,264] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:14:03,650] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
schema-registry  | [2023-08-04 11:14:04,382] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:04,386] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:04,487] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:04,489] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:05,535] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:05,545] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:05,613] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:05,613] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:06,486] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:06,493] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:06,745] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:06,749] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:14:07,301] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
schema-registry  | [2023-08-04 11:14:07,640] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:07,642] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:07,888] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:07,915] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:08,699] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:08,701] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:09,157] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:09,160] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:14:09,360] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
broker           | [2023-08-04 11:14:09,382] INFO Starting controller (kafka.server.ControllerServer)
connect          | ===> Running preflight checks ... 
connect          | ===> Check if Kafka is healthy ...
schema-registry  | [2023-08-04 11:14:09,834] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:09,852] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:10,027] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:10,028] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:10,985] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:10,986] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:11,169] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:11,170] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:12,110] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:12,111] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:12,317] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:12,320] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:12,953] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:12,955] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:13,344] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:13,348] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:13,812] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:13,813] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:14,401] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:14,402] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:14:14,734] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
schema-registry  | [2023-08-04 11:14:14,743] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:14,746] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:14:14,861] INFO Awaiting socket connections on broker:29093. (kafka.network.DataPlaneAcceptor)
rest-proxy       | [2023-08-04 11:14:15,327] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:14:15,322] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
rest-proxy       | [2023-08-04 11:14:15,329] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:14:15,379] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)
connect          | SLF4J: Class path contains multiple SLF4J bindings.
connect          | SLF4J: Found binding in [jar:file:/usr/share/java/cp-base-new/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
connect          | SLF4J: Found binding in [jar:file:/usr/share/java/cp-base-new/slf4j-simple-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
connect          | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
schema-registry  | [2023-08-04 11:14:15,780] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:15,781] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
connect          | SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
broker           | [2023-08-04 11:14:16,364] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:14:16,371] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:14:16,412] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Producer state recovery took 26ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
rest-proxy       | [2023-08-04 11:14:16,560] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:16,566] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:16,622] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:16,623] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:14:16,902] INFO Initialized snapshots with IDs SortedSet() from /tmp/kraft-combined-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
broker           | [2023-08-04 11:14:17,107] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
connect          | 1 [main] DEBUG io.confluent.admin.utils.cli.KafkaReadyCommand  - Arguments Namespace(zookeeper_connect=null, min_expected_brokers=1, security_protocol=PLAINTEXT, config=null, bootstrap_servers=broker:29092, timeout=40000). 
schema-registry  | [2023-08-04 11:14:17,549] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:17,550] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
connect          | 521 [main] INFO org.apache.kafka.clients.admin.AdminClientConfig  - AdminClientConfig values: 
connect          | 	bootstrap.servers = [broker:29092]
connect          | 	client.dns.lookup = use_all_dns_ips
connect          | 	client.id = 
connect          | 	connections.max.idle.ms = 300000
connect          | 	default.api.timeout.ms = 60000
connect          | 	metadata.max.age.ms = 300000
connect          | 	metric.reporters = []
connect          | 	metrics.num.samples = 2
connect          | 	metrics.recording.level = INFO
connect          | 	metrics.sample.window.ms = 30000
connect          | 	receive.buffer.bytes = 65536
connect          | 	reconnect.backoff.max.ms = 1000
connect          | 	reconnect.backoff.ms = 50
connect          | 	request.timeout.ms = 30000
connect          | 	retries = 2147483647
connect          | 	retry.backoff.ms = 100
connect          | 	sasl.client.callback.handler.class = null
connect          | 	sasl.jaas.config = null
connect          | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	sasl.kerberos.min.time.before.relogin = 60000
connect          | 	sasl.kerberos.service.name = null
connect          | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	sasl.login.callback.handler.class = null
connect          | 	sasl.login.class = null
connect          | 	sasl.login.connect.timeout.ms = null
connect          | 	sasl.login.read.timeout.ms = null
connect          | 	sasl.login.refresh.buffer.seconds = 300
connect          | 	sasl.login.refresh.min.period.seconds = 60
connect          | 	sasl.login.refresh.window.factor = 0.8
connect          | 	sasl.login.refresh.window.jitter = 0.05
connect          | 	sasl.login.retry.backoff.max.ms = 10000
connect          | 	sasl.login.retry.backoff.ms = 100
connect          | 	sasl.mechanism = GSSAPI
connect          | 	sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	sasl.oauthbearer.expected.audience = null
connect          | 	sasl.oauthbearer.expected.issuer = null
connect          | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	sasl.oauthbearer.scope.claim.name = scope
connect          | 	sasl.oauthbearer.sub.claim.name = sub
connect          | 	sasl.oauthbearer.token.endpoint.url = null
connect          | 	security.protocol = PLAINTEXT
connect          | 	security.providers = null
connect          | 	send.buffer.bytes = 131072
connect          | 	socket.connection.setup.timeout.max.ms = 30000
connect          | 	socket.connection.setup.timeout.ms = 10000
connect          | 	ssl.cipher.suites = null
connect          | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	ssl.endpoint.identification.algorithm = https
connect          | 	ssl.engine.factory.class = null
connect          | 	ssl.key.password = null
connect          | 	ssl.keymanager.algorithm = SunX509
connect          | 	ssl.keystore.certificate.chain = null
connect          | 	ssl.keystore.key = null
connect          | 	ssl.keystore.location = null
connect          | 	ssl.keystore.password = null
connect          | 	ssl.keystore.type = JKS
connect          | 	ssl.protocol = TLSv1.3
connect          | 	ssl.provider = null
connect          | 	ssl.secure.random.implementation = null
connect          | 	ssl.trustmanager.algorithm = PKIX
connect          | 	ssl.truststore.certificates = null
connect          | 	ssl.truststore.location = null
connect          | 	ssl.truststore.password = null
connect          | 	ssl.truststore.type = JKS
connect          | 
connect          | 
rest-proxy       | [2023-08-04 11:14:17,792] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:17,793] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
connect          | 1090 [main] DEBUG org.apache.kafka.clients.admin.internals.AdminMetadataManager  - [AdminClient clientId=adminclient-1] Setting bootstrap cluster metadata Cluster(id = null, nodes = [broker:29092 (id: -1 rack: null)], partitions = [], controller = null).
schema-registry  | [2023-08-04 11:14:18,481] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:18,482] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:18,821] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:18,828] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:18,806] INFO [AdminClient clientId=adminclient-1] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
schema-registry  | org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: fetchMetadata
schema-registry  | [2023-08-04 11:14:19,443] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:19,448] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:14:19,570] INFO [RaftManager nodeId=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1471) (org.apache.kafka.raft.QuorumState)
broker           | [2023-08-04 11:14:19,611] INFO [RaftManager nodeId=1] Completed transition to CandidateState(localId=1, epoch=1, retries=1, electionTimeoutMs=1947) (org.apache.kafka.raft.QuorumState)
broker           | [2023-08-04 11:14:19,718] INFO [RaftManager nodeId=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) (org.apache.kafka.raft.QuorumState)
rest-proxy       | [2023-08-04 11:14:19,758] INFO [AdminClient clientId=adminclient-1] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
rest-proxy       | org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: fetchMetadata
rest-proxy       | [2023-08-04 11:14:19,876] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:19,877] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:14:20,157] INFO [kafka-raft-outbound-request-thread]: Starting (kafka.raft.RaftSendThread)
broker           | [2023-08-04 11:14:20,210] INFO [kafka-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)
schema-registry  | [2023-08-04 11:14:20,266] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:20,269] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:14:20,553] INFO [MetadataLoader 1] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
broker           | [2023-08-04 11:14:20,655] INFO [MetadataLoader 1] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
broker           | [2023-08-04 11:14:20,756] INFO [MetadataLoader 1] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
broker           | [2023-08-04 11:14:20,793] INFO [RaftManager nodeId=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)
broker           | [2023-08-04 11:14:20,863] INFO [MetadataLoader 1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
broker           | [2023-08-04 11:14:20,981] INFO [MetadataLoader 1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
rest-proxy       | [2023-08-04 11:14:21,000] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:21,001] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:14:21,084] INFO [MetadataLoader 1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
schema-registry  | [2023-08-04 11:14:21,096] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:21,099] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:14:21,107] INFO [RaftManager nodeId=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@727155926 (org.apache.kafka.raft.KafkaRaftClient)
broker           | [2023-08-04 11:14:21,185] INFO [MetadataLoader 1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
broker           | [2023-08-04 11:14:21,237] INFO [MetadataLoader 1] handleCommit: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
broker           | [2023-08-04 11:14:21,251] INFO [Controller 1] Creating new QuorumController with clusterId MkU3OEVBNTcwNTJENDM2Qg, authorizer Optional.empty. (org.apache.kafka.controller.QuorumController)
broker           | [2023-08-04 11:14:21,253] INFO [RaftManager nodeId=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@206437109 (org.apache.kafka.raft.KafkaRaftClient)
broker           | [2023-08-04 11:14:21,268] INFO [Controller 1] Becoming the active controller at epoch 1, committed offset -1, committed epoch -1 (org.apache.kafka.controller.QuorumController)
broker           | [2023-08-04 11:14:21,286] INFO [MetadataLoader 1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
broker           | [2023-08-04 11:14:21,318] INFO [Controller 1] The metadata log appears to be empty. Appending 1 bootstrap record(s) at metadata.version 3.4-IV0 from the binary bootstrap metadata file: /tmp/kraft-combined-logs/bootstrap.checkpoint. (org.apache.kafka.controller.QuorumController)
broker           | [2023-08-04 11:14:21,329] INFO [Controller 1] Setting metadata.version to 8 (org.apache.kafka.controller.FeatureControlManager)
broker           | [2023-08-04 11:14:21,372] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2023-08-04 11:14:21,389] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2023-08-04 11:14:21,398] INFO [MetadataLoader 1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
broker           | [2023-08-04 11:14:21,398] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2023-08-04 11:14:21,416] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2023-08-04 11:14:21,516] INFO [MetadataLoader 1] handleCommit: The loader finished catching up to the current high water mark of 2 (org.apache.kafka.image.loader.MetadataLoader)
broker           | [2023-08-04 11:14:21,529] INFO [MetadataLoader 1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 1 (org.apache.kafka.image.loader.MetadataLoader)
broker           | [2023-08-04 11:14:21,593] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2023-08-04 11:14:21,654] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
broker           | [2023-08-04 11:14:21,704] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
broker           | [2023-08-04 11:14:21,720] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)
broker           | [2023-08-04 11:14:21,845] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2023-08-04 11:14:21,871] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2023-08-04 11:14:21,881] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2023-08-04 11:14:21,892] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
rest-proxy       | [2023-08-04 11:14:21,917] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
rest-proxy       | [2023-08-04 11:14:21,918] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
connect          | 4837 [main] INFO org.apache.kafka.common.utils.AppInfoParser  - Kafka version: 7.1.0-ccs
connect          | 4839 [main] INFO org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId: c86722379ab997cc
connect          | 4839 [main] INFO org.apache.kafka.common.utils.AppInfoParser  - Kafka startTimeMs: 1691147662005
connect          | 4854 [main] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Kafka admin client initialized
connect          | 4862 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Thread starting
broker           | [2023-08-04 11:14:22,150] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)
broker           | [2023-08-04 11:14:22,157] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Recorded new controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
connect          | 4996 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host broker as 192.168.224.2
connect          | 4997 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node broker:29092 (id: -1 rack: null) using address broker/192.168.224.2
connect          | 5009 [main] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Queueing Call(callName=listNodes, deadlineMs=1691147702139, tries=0, nextAllowedTryMs=0) with a timeout 30000 ms from now.
schema-registry  | [2023-08-04 11:14:22,222] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
schema-registry  | [2023-08-04 11:14:22,224] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
connect          | 5252 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with broker/192.168.224.2 disconnected
connect          | java.net.ConnectException: Connection refused
connect          | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
connect          | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
connect          | 	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
connect          | 	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
connect          | 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
connect          | 	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
connect          | 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
connect          | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
connect          | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
connect          | 	at java.base/java.lang.Thread.run(Thread.java:829)
connect          | 5280 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 disconnected.
connect          | 5306 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available.
connect          | 5342 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host broker as 192.168.224.2
connect          | 5343 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node broker:29092 (id: -1 rack: null) using address broker/192.168.224.2
connect          | 5356 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with broker/192.168.224.2 disconnected
connect          | java.net.ConnectException: Connection refused
connect          | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
connect          | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
connect          | 	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
connect          | 	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
connect          | 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
connect          | 	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
connect          | 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
connect          | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
connect          | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
connect          | 	at java.base/java.lang.Thread.run(Thread.java:829)
connect          | 5359 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 disconnected.
connect          | 5359 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available.
connect          | 5563 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host broker as 192.168.224.2
connect          | 5563 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node broker:29092 (id: -1 rack: null) using address broker/192.168.224.2
connect          | 5568 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Connection with broker/192.168.224.2 disconnected
connect          | java.net.ConnectException: Connection refused
connect          | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
connect          | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
connect          | 	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
connect          | 	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:224)
connect          | 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:526)
connect          | 	at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
connect          | 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
connect          | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1400)
connect          | 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1331)
connect          | 	at java.base/java.lang.Thread.run(Thread.java:829)
connect          | 5572 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 disconnected.
connect          | 5572 [kafka-admin-client-thread | adminclient-1] WARN org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Connection to node -1 (broker/192.168.224.2:29092) could not be established. Broker may not be available.
broker           | [2023-08-04 11:14:22,782] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
broker           | [2023-08-04 11:14:22,785] INFO Awaiting socket connections on broker:29092. (kafka.network.DataPlaneAcceptor)
broker           | [2023-08-04 11:14:22,824] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
broker           | [2023-08-04 11:14:22,827] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
broker           | [2023-08-04 11:14:22,829] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
broker           | [2023-08-04 11:14:22,860] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
broker           | [2023-08-04 11:14:22,924] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Starting (kafka.server.BrokerToControllerRequestThread)
broker           | [2023-08-04 11:14:22,928] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Recorded new controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
broker           | [2023-08-04 11:14:23,017] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2023-08-04 11:14:23,029] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2023-08-04 11:14:23,032] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2023-08-04 11:14:23,041] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
connect          | 5880 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host broker as 192.168.224.2
connect          | 5881 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node broker:29092 (id: -1 rack: null) using address broker/192.168.224.2
connect          | 5884 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node -1
broker           | [2023-08-04 11:14:23,310] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2023-08-04 11:14:23,311] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2023-08-04 11:14:24,512] INFO [RaftManager nodeId=1] Registered the listener kafka.server.metadata.BrokerMetadataListener@543782074 (org.apache.kafka.raft.KafkaRaftClient)
broker           | [2023-08-04 11:14:24,542] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Starting (kafka.server.BrokerToControllerRequestThread)
broker           | [2023-08-04 11:14:24,551] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Recorded new controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
broker           | [2023-08-04 11:14:24,761] INFO [BrokerLifecycleManager id=1] Incarnation OIYaHV0SRFOoCO0TPpFeyw of broker 1 in cluster MkU3OEVBNTcwNTJENDM2Qg is now STARTING. (kafka.server.BrokerLifecycleManager)
broker           | [2023-08-04 11:14:25,253] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2023-08-04 11:14:25,301] INFO [BrokerServer id=1] Waiting for broker metadata to catch up. (kafka.server.BrokerServer)
connect          | 9659 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Completed connection to node -1. Fetching API versions.
connect          | 9659 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating API versions fetch from node -1.
connect          | 10084 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=adminclient-1, correlationId=0) and timeout 3600000 to node -1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='7.1.0-ccs')
broker           | [2023-08-04 11:14:27,859] INFO [Controller 1] Registered new broker: RegisterBrokerRecord(brokerId=1, isMigratingZkBroker=false, incarnationId=OIYaHV0SRFOoCO0TPpFeyw, brokerEpoch=14, endPoints=[BrokerEndpoint(name='PLAINTEXT', host='broker', port=29092, securityProtocol=0), BrokerEndpoint(name='PLAINTEXT_HOST', host='localhost', port=9092, securityProtocol=0)], features=[BrokerFeature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=8)], rack=null, fenced=true, inControlledShutdown=false) (org.apache.kafka.controller.ClusterControlManager)
broker           | [2023-08-04 11:14:27,918] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 14 (kafka.server.BrokerLifecycleManager)
broker           | [2023-08-04 11:14:27,959] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
broker           | [2023-08-04 11:14:27,991] INFO [BrokerLifecycleManager id=1] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
broker           | [2023-08-04 11:14:27,999] INFO [BrokerMetadataListener id=1] Starting to publish metadata events at offset 14. (kafka.server.metadata.BrokerMetadataListener)
broker           | [2023-08-04 11:14:28,029] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=14, epoch=1) with metadata.version 3.4-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
broker           | [2023-08-04 11:14:28,040] INFO Loading logs from log dirs ArraySeq(/tmp/kraft-combined-logs) (kafka.log.LogManager)
broker           | [2023-08-04 11:14:28,054] INFO Attempting recovery for all logs in /tmp/kraft-combined-logs since no clean shutdown file was found (kafka.log.LogManager)
broker           | [2023-08-04 11:14:28,113] INFO Loaded 0 logs in 73ms. (kafka.log.LogManager)
broker           | [2023-08-04 11:14:28,116] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
broker           | [2023-08-04 11:14:28,127] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
broker           | [2023-08-04 11:14:28,208] INFO Starting the log cleaner (kafka.log.LogCleaner)
broker           | [2023-08-04 11:14:28,675] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
broker           | [2023-08-04 11:14:28,697] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
broker           | [2023-08-04 11:14:28,700] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:14:28,713] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:14:28,722] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
broker           | [2023-08-04 11:14:28,733] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
broker           | [2023-08-04 11:14:28,741] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 8 at offset OffsetAndEpoch(offset=14, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)
broker           | [2023-08-04 11:14:28,781] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
schema-registry  | [2023-08-04 11:14:28,839] ERROR Error while getting broker list. (io.confluent.admin.utils.ClusterStatus)
schema-registry  | java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: listNodes
schema-registry  | 	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
schema-registry  | 	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
schema-registry  | 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
schema-registry  | 	at io.confluent.admin.utils.ClusterStatus.isKafkaReady(ClusterStatus.java:147)
schema-registry  | 	at io.confluent.admin.utils.cli.KafkaReadyCommand.main(KafkaReadyCommand.java:149)
schema-registry  | Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: listNodes
broker           | [2023-08-04 11:14:28,849] INFO KafkaConfig values: 
broker           | 	advertised.listeners = PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
broker           | 	alter.config.policy.class.name = null
broker           | 	alter.log.dirs.replication.quota.window.num = 11
broker           | 	alter.log.dirs.replication.quota.window.size.seconds = 1
broker           | 	authorizer.class.name = 
broker           | 	auto.create.topics.enable = true
broker           | 	auto.include.jmx.reporter = true
broker           | 	auto.leader.rebalance.enable = true
broker           | 	background.threads = 10
broker           | 	broker.heartbeat.interval.ms = 2000
broker           | 	broker.id = 1
broker           | 	broker.id.generation.enable = true
broker           | 	broker.rack = null
broker           | 	broker.session.timeout.ms = 9000
broker           | 	client.quota.callback.class = null
broker           | 	compression.type = producer
broker           | 	connection.failed.authentication.delay.ms = 100
broker           | 	connections.max.idle.ms = 600000
broker           | 	connections.max.reauth.ms = 0
broker           | 	control.plane.listener.name = null
broker           | 	controlled.shutdown.enable = true
broker           | 	controlled.shutdown.max.retries = 3
broker           | 	controlled.shutdown.retry.backoff.ms = 5000
broker           | 	controller.listener.names = CONTROLLER
broker           | 	controller.quorum.append.linger.ms = 25
broker           | 	controller.quorum.election.backoff.max.ms = 1000
broker           | 	controller.quorum.election.timeout.ms = 1000
broker           | 	controller.quorum.fetch.timeout.ms = 2000
broker           | 	controller.quorum.request.timeout.ms = 2000
broker           | 	controller.quorum.retry.backoff.ms = 20
broker           | 	controller.quorum.voters = [1@broker:29093]
broker           | 	controller.quota.window.num = 11
broker           | 	controller.quota.window.size.seconds = 1
broker           | 	controller.socket.timeout.ms = 30000
broker           | 	create.topic.policy.class.name = null
broker           | 	default.replication.factor = 1
broker           | 	delegation.token.expiry.check.interval.ms = 3600000
broker           | 	delegation.token.expiry.time.ms = 86400000
broker           | 	delegation.token.master.key = null
broker           | 	delegation.token.max.lifetime.ms = 604800000
broker           | 	delegation.token.secret.key = null
broker           | 	delete.records.purgatory.purge.interval.requests = 1
broker           | 	delete.topic.enable = true
broker           | 	early.start.listeners = null
broker           | 	fetch.max.bytes = 57671680
broker           | 	fetch.purgatory.purge.interval.requests = 1000
broker           | 	group.initial.rebalance.delay.ms = 0
broker           | 	group.max.session.timeout.ms = 1800000
broker           | 	group.max.size = 2147483647
broker           | 	group.min.session.timeout.ms = 6000
broker           | 	initial.broker.registration.timeout.ms = 60000
broker           | 	inter.broker.listener.name = PLAINTEXT
broker           | 	inter.broker.protocol.version = 3.4-IV0
broker           | 	kafka.metrics.polling.interval.secs = 10
broker           | 	kafka.metrics.reporters = []
broker           | 	leader.imbalance.check.interval.seconds = 300
broker           | 	leader.imbalance.per.broker.percentage = 10
broker           | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
broker           | 	listeners = PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092
broker           | 	log.cleaner.backoff.ms = 15000
broker           | 	log.cleaner.dedupe.buffer.size = 134217728
broker           | 	log.cleaner.delete.retention.ms = 86400000
broker           | 	log.cleaner.enable = true
broker           | 	log.cleaner.io.buffer.load.factor = 0.9
broker           | 	log.cleaner.io.buffer.size = 524288
broker           | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
broker           | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
broker           | 	log.cleaner.min.cleanable.ratio = 0.5
broker           | 	log.cleaner.min.compaction.lag.ms = 0
broker           | 	log.cleaner.threads = 1
broker           | 	log.cleanup.policy = [delete]
broker           | 	log.dir = /tmp/kafka-logs
broker           | 	log.dirs = /tmp/kraft-combined-logs
broker           | 	log.flush.interval.messages = 9223372036854775807
broker           | 	log.flush.interval.ms = null
broker           | 	log.flush.offset.checkpoint.interval.ms = 60000
broker           | 	log.flush.scheduler.interval.ms = 9223372036854775807
broker           | 	log.flush.start.offset.checkpoint.interval.ms = 60000
broker           | 	log.index.interval.bytes = 4096
broker           | 	log.index.size.max.bytes = 10485760
broker           | 	log.message.downconversion.enable = true
broker           | 	log.message.format.version = 3.0-IV1
broker           | 	log.message.timestamp.difference.max.ms = 9223372036854775807
broker           | 	log.message.timestamp.type = CreateTime
broker           | 	log.preallocate = false
broker           | 	log.retention.bytes = -1
broker           | 	log.retention.check.interval.ms = 300000
broker           | 	log.retention.hours = 168
broker           | 	log.retention.minutes = null
broker           | 	log.retention.ms = null
broker           | 	log.roll.hours = 168
broker           | 	log.roll.jitter.hours = 0
broker           | 	log.roll.jitter.ms = null
broker           | 	log.roll.ms = null
broker           | 	log.segment.bytes = 1073741824
broker           | 	log.segment.delete.delay.ms = 60000
broker           | 	max.connection.creation.rate = 2147483647
broker           | 	max.connections = 2147483647
broker           | 	max.connections.per.ip = 2147483647
broker           | 	max.connections.per.ip.overrides = 
broker           | 	max.incremental.fetch.session.cache.slots = 1000
broker           | 	message.max.bytes = 1048588
broker           | 	metadata.log.dir = null
broker           | 	metadata.log.max.record.bytes.between.snapshots = 20971520
broker           | 	metadata.log.max.snapshot.interval.ms = 3600000
broker           | 	metadata.log.segment.bytes = 1073741824
broker           | 	metadata.log.segment.min.bytes = 8388608
broker           | 	metadata.log.segment.ms = 604800000
broker           | 	metadata.max.idle.interval.ms = 500
broker           | 	metadata.max.retention.bytes = 104857600
broker           | 	metadata.max.retention.ms = 604800000
broker           | 	metric.reporters = []
broker           | 	metrics.num.samples = 2
broker           | 	metrics.recording.level = INFO
broker           | 	metrics.sample.window.ms = 30000
broker           | 	min.insync.replicas = 1
broker           | 	node.id = 1
broker           | 	num.io.threads = 8
broker           | 	num.network.threads = 3
broker           | 	num.partitions = 1
broker           | 	num.recovery.threads.per.data.dir = 1
broker           | 	num.replica.alter.log.dirs.threads = null
broker           | 	num.replica.fetchers = 1
broker           | 	offset.metadata.max.bytes = 4096
broker           | 	offsets.commit.required.acks = -1
broker           | 	offsets.commit.timeout.ms = 5000
broker           | 	offsets.load.buffer.size = 5242880
broker           | 	offsets.retention.check.interval.ms = 600000
broker           | 	offsets.retention.minutes = 10080
broker           | 	offsets.topic.compression.codec = 0
broker           | 	offsets.topic.num.partitions = 50
broker           | 	offsets.topic.replication.factor = 1
broker           | 	offsets.topic.segment.bytes = 104857600
broker           | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
broker           | 	password.encoder.iterations = 4096
broker           | 	password.encoder.key.length = 128
broker           | 	password.encoder.keyfactory.algorithm = null
broker           | 	password.encoder.old.secret = null
broker           | 	password.encoder.secret = null
broker           | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
broker           | 	process.roles = [broker, controller]
broker           | 	producer.id.expiration.check.interval.ms = 600000
broker           | 	producer.id.expiration.ms = 86400000
broker           | 	producer.purgatory.purge.interval.requests = 1000
broker           | 	queued.max.request.bytes = -1
broker           | 	queued.max.requests = 500
broker           | 	quota.window.num = 11
broker           | 	quota.window.size.seconds = 1
broker           | 	remote.log.index.file.cache.total.size.bytes = 1073741824
broker           | 	remote.log.manager.task.interval.ms = 30000
broker           | 	remote.log.manager.task.retry.backoff.max.ms = 30000
broker           | 	remote.log.manager.task.retry.backoff.ms = 500
broker           | 	remote.log.manager.task.retry.jitter = 0.2
broker           | 	remote.log.manager.thread.pool.size = 10
broker           | 	remote.log.metadata.manager.class.name = null
broker           | 	remote.log.metadata.manager.class.path = null
broker           | 	remote.log.metadata.manager.impl.prefix = null
broker           | 	remote.log.metadata.manager.listener.name = null
broker           | 	remote.log.reader.max.pending.tasks = 100
broker           | 	remote.log.reader.threads = 10
broker           | 	remote.log.storage.manager.class.name = null
broker           | 	remote.log.storage.manager.class.path = null
broker           | 	remote.log.storage.manager.impl.prefix = null
broker           | 	remote.log.storage.system.enable = false
broker           | 	replica.fetch.backoff.ms = 1000
broker           | 	replica.fetch.max.bytes = 1048576
broker           | 	replica.fetch.min.bytes = 1
broker           | 	replica.fetch.response.max.bytes = 10485760
broker           | 	replica.fetch.wait.max.ms = 500
broker           | 	replica.high.watermark.checkpoint.interval.ms = 5000
broker           | 	replica.lag.time.max.ms = 30000
broker           | 	replica.selector.class = null
broker           | 	replica.socket.receive.buffer.bytes = 65536
broker           | 	replica.socket.timeout.ms = 30000
broker           | 	replication.quota.window.num = 11
broker           | 	replication.quota.window.size.seconds = 1
broker           | 	request.timeout.ms = 30000
broker           | 	reserved.broker.max.id = 1000
broker           | 	sasl.client.callback.handler.class = null
broker           | 	sasl.enabled.mechanisms = [GSSAPI]
broker           | 	sasl.jaas.config = null
broker           | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
broker           | 	sasl.kerberos.min.time.before.relogin = 60000
broker           | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
broker           | 	sasl.kerberos.service.name = null
broker           | 	sasl.kerberos.ticket.renew.jitter = 0.05
broker           | 	sasl.kerberos.ticket.renew.window.factor = 0.8
broker           | 	sasl.login.callback.handler.class = null
broker           | 	sasl.login.class = null
broker           | 	sasl.login.connect.timeout.ms = null
broker           | 	sasl.login.read.timeout.ms = null
broker           | 	sasl.login.refresh.buffer.seconds = 300
broker           | 	sasl.login.refresh.min.period.seconds = 60
broker           | 	sasl.login.refresh.window.factor = 0.8
broker           | 	sasl.login.refresh.window.jitter = 0.05
broker           | 	sasl.login.retry.backoff.max.ms = 10000
broker           | 	sasl.login.retry.backoff.ms = 100
broker           | 	sasl.mechanism.controller.protocol = GSSAPI
broker           | 	sasl.mechanism.inter.broker.protocol = GSSAPI
broker           | 	sasl.oauthbearer.clock.skew.seconds = 30
broker           | 	sasl.oauthbearer.expected.audience = null
broker           | 	sasl.oauthbearer.expected.issuer = null
broker           | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
broker           | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
broker           | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
broker           | 	sasl.oauth
broker           | bearer.jwks.endpoint.url = null
broker           | 	sasl.oauthbearer.scope.claim.name = scope
broker           | 	sasl.oauthbearer.sub.claim.name = sub
broker           | 	sasl.oauthbearer.token.endpoint.url = null
broker           | 	sasl.server.callback.handler.class = null
broker           | 	sasl.server.max.receive.size = 524288
broker           | 	security.inter.broker.protocol = PLAINTEXT
broker           | 	security.providers = null
broker           | 	socket.connection.setup.timeout.max.ms = 30000
broker           | 	socket.connection.setup.timeout.ms = 10000
broker           | 	socket.listen.backlog.size = 50
broker           | 	socket.receive.buffer.bytes = 102400
broker           | 	socket.request.max.bytes = 104857600
broker           | 	socket.send.buffer.bytes = 102400
broker           | 	ssl.cipher.suites = []
broker           | 	ssl.client.auth = none
broker           | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
broker           | 	ssl.endpoint.identification.algorithm = https
broker           | 	ssl.engine.factory.class = null
broker           | 	ssl.key.password = null
broker           | 	ssl.keymanager.algorithm = SunX509
broker           | 	ssl.keystore.certificate.chain = null
broker           | 	ssl.keystore.key = null
broker           | 	ssl.keystore.location = null
broker           | 	ssl.keystore.password = null
broker           | 	ssl.keystore.type = JKS
broker           | 	ssl.principal.mapping.rules = DEFAULT
broker           | 	ssl.protocol = TLSv1.3
broker           | 	ssl.provider = null
broker           | 	ssl.secure.random.implementation = null
broker           | 	ssl.trustmanager.algorithm = PKIX
broker           | 	ssl.truststore.certificates = null
broker           | 	ssl.truststore.location = null
broker           | 	ssl.truststore.password = null
broker           | 	ssl.truststore.type = JKS
broker           | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
broker           | 	transaction.max.timeout.ms = 900000
broker           | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
broker           | 	transaction.state.log.load.buffer.size = 5242880
broker           | 	transaction.state.log.min.isr = 1
broker           | 	transaction.state.log.num.partitions = 50
broker           | 	transaction.state.log.replication.factor = 1
broker           | 	transaction.state.log.segment.bytes = 104857600
broker           | 	transactional.id.expiration.ms = 604800000
broker           | 	unclean.leader.election.enable = false
broker           | 	zookeeper.clientCnxnSocket = null
broker           | 	zookeeper.connect = 
broker           | 	zookeeper.connection.timeout.ms = null
broker           | 	zookeeper.max.in.flight.requests = 10
broker           | 	zookeeper.metadata.migration.enable = false
broker           | 	zookeeper.session.timeout.ms = 18000
broker           | 	zookeeper.set.acl = false
broker           | 	zookeeper.ssl.cipher.suites = null
broker           | 	zookeeper.ssl.client.enable = false
broker           | 	zookeeper.ssl.crl.enable = false
broker           | 	zookeeper.ssl.enabled.protocols = null
broker           | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
broker           | 	zookeeper.ssl.keystore.location = null
broker           | 	zookeeper.ssl.keystore.password = null
broker           | 	zookeeper.ssl.keystore.type = null
broker           | 	zookeeper.ssl.ocsp.enable = false
broker           | 	zookeeper.ssl.protocol = TLSv1.2
broker           | 	zookeeper.ssl.truststore.location = null
broker           | 	zookeeper.ssl.truststore.password = null
broker           | 	zookeeper.ssl.truststore.type = null
broker           |  (kafka.server.KafkaConfig)
broker           | [2023-08-04 11:14:28,917] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
broker           | [2023-08-04 11:14:28,945] INFO [Controller 1] The request from broker 1 to unfence has been granted because it has caught up with the offset of it's register broker record 14. (org.apache.kafka.controller.BrokerHeartbeatManager)
broker           | [2023-08-04 11:14:29,127] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
broker           | [2023-08-04 11:14:29,148] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)
broker           | [2023-08-04 11:14:29,175] INFO Kafka version: 7.4.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
broker           | [2023-08-04 11:14:29,177] INFO Kafka commitId: fed9c006bfc7ba5bf7d2dee840e041d1a851d903 (org.apache.kafka.common.utils.AppInfoParser)
broker           | [2023-08-04 11:14:29,186] INFO Kafka startTimeMs: 1691147669154 (org.apache.kafka.common.utils.AppInfoParser)
broker           | [2023-08-04 11:14:29,302] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)
connect          | 12285 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Received API_VERSIONS response from node -1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=adminclient-1, correlationId=0): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=55, minVersion=0, maxVersion=1), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=64, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[SupportedFeatureKey(name='metadata.version', minVersion=1, maxVersion=8)], finalizedFeaturesEpoch=16, finalizedFeatures=[FinalizedFeatureKey(name='metadata.version', maxVersionLevel=8, minVersionLevel=8)])
schema-registry  | [2023-08-04 11:14:29,849] INFO Expected 1 brokers but found only 0. Trying to query Kafka for metadata again ... (io.confluent.admin.utils.ClusterStatus)
schema-registry  | [2023-08-04 11:14:29,852] ERROR Expected 1 brokers but found only 0. Brokers found []. (io.confluent.admin.utils.ClusterStatus)
schema-registry  | Using log4j config /etc/schema-registry/log4j.properties
connect          | 13403 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node -1 has finalized features epoch: 16, finalized features: [FinalizedFeatureKey(name='metadata.version', maxVersionLevel=8, minVersionLevel=8)], supported features: [SupportedFeatureKey(name='metadata.version', minVersion=1, maxVersion=8)], API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): UNSUPPORTED, StopReplica(5): UNSUPPORTED, UpdateMetadata(6): UNSUPPORTED, ControlledShutdown(7): UNSUPPORTED, OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 7], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 4], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 2], CreateAcls(30): 0 to 3 [usable: 2], DeleteAcls(31): 0 to 3 [usable: 2], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 2], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): UNSUPPORTED, RenewDelegationToken(39): UNSUPPORTED, ExpireDelegationToken(40): UNSUPPORTED, DescribeDelegationToken(41): UNSUPPORTED, DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): UNSUPPORTED, AlterUserScramCredentials(51): UNSUPPORTED, DescribeQuorum(55): 0 to 1 [usable: 0], AlterIsr(56): UNSUPPORTED, UpdateFeatures(57): 0 to 1 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): 0 [usable: 0], DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): UNSUPPORTED).
connect          | 13431 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Sending MetadataRequestData(topics=[], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to broker:29092 (id: -1 rack: null). correlationId=1, timeoutMs=21459
connect          | 13461 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=adminclient-1, correlationId=1) and timeout 21459 to node -1: MetadataRequestData(topics=[], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false)
rest-proxy       | [2023-08-04 11:14:30,709] ERROR Error while getting broker list. (io.confluent.admin.utils.ClusterStatus)
rest-proxy       | java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: listNodes
rest-proxy       | 	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
rest-proxy       | 	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
rest-proxy       | 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
rest-proxy       | 	at io.confluent.admin.utils.ClusterStatus.isKafkaReady(ClusterStatus.java:147)
rest-proxy       | 	at io.confluent.admin.utils.cli.KafkaReadyCommand.main(KafkaReadyCommand.java:149)
rest-proxy       | Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: listNodes
connect          | 13784 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Received METADATA response from node -1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=adminclient-1, correlationId=1): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=1, host='broker', port=29092, rack=null)], clusterId='MkU3OEVBNTcwNTJENDM2Qg', controllerId=1, topics=[], clusterAuthorizedOperations=-2147483648)
connect          | 13851 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.admin.internals.AdminMetadataManager  - [AdminClient clientId=adminclient-1] Updating cluster metadata to Cluster(id = MkU3OEVBNTcwNTJENDM2Qg, nodes = [broker:29092 (id: 1 rack: null)], partitions = [], controller = broker:29092 (id: 1 rack: null))
connect          | 13859 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.ClientUtils  - Resolved host broker as 192.168.224.2
connect          | 13859 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating connection to node broker:29092 (id: 1 rack: null) using address broker/192.168.224.2
connect          | 13899 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.common.network.Selector  - [AdminClient clientId=adminclient-1] Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node 1
connect          | 13899 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Completed connection to node 1. Fetching API versions.
connect          | 13900 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Initiating API versions fetch from node 1.
connect          | 13901 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=adminclient-1, correlationId=2) and timeout 3600000 to node 1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='7.1.0-ccs')
connect          | 13960 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Received API_VERSIONS response from node 1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=adminclient-1, correlationId=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=9), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=5), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=3), ApiVersion(apiKey=30, minVersion=0, maxVersion=3), ApiVersion(apiKey=31, minVersion=0, maxVersion=3), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=4), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=55, minVersion=0, maxVersion=1), ApiVersion(apiKey=57, minVersion=0, maxVersion=1), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=64, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[SupportedFeatureKey(name='metadata.version', minVersion=1, maxVersion=8)], finalizedFeaturesEpoch=21, finalizedFeatures=[FinalizedFeatureKey(name='metadata.version', maxVersionLevel=8, minVersionLevel=8)])
connect          | 13975 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Node 1 has finalized features epoch: 21, finalized features: [FinalizedFeatureKey(name='metadata.version', maxVersionLevel=8, minVersionLevel=8)], supported features: [SupportedFeatureKey(name='metadata.version', minVersion=1, maxVersion=8)], API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): UNSUPPORTED, StopReplica(5): UNSUPPORTED, UpdateMetadata(6): UNSUPPORTED, ControlledShutdown(7): UNSUPPORTED, OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 9 [usable: 7], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 5 [usable: 4], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 3 [usable: 2], CreateAcls(30): 0 to 3 [usable: 2], DeleteAcls(31): 0 to 3 [usable: 2], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 4 [usable: 2], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): UNSUPPORTED, RenewDelegationToken(39): UNSUPPORTED, ExpireDelegationToken(40): UNSUPPORTED, DescribeDelegationToken(41): UNSUPPORTED, DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): UNSUPPORTED, AlterUserScramCredentials(51): UNSUPPORTED, DescribeQuorum(55): 0 to 1 [usable: 0], AlterIsr(56): UNSUPPORTED, UpdateFeatures(57): 0 to 1 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], UnregisterBroker(64): 0 [usable: 0], DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): UNSUPPORTED).
connect          | 13977 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.admin.KafkaAdminClient  - [AdminClient clientId=adminclient-1] Sending DescribeClusterRequestData(includeClusterAuthorizedOperations=false) to broker:29092 (id: 1 rack: null). correlationId=3, timeoutMs=29826
connect          | 13988 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Sending DESCRIBE_CLUSTER request with header RequestHeader(apiKey=DESCRIBE_CLUSTER, apiVersion=0, clientId=adminclient-1, correlationId=3) and timeout 29826 to node 1: DescribeClusterRequestData(includeClusterAuthorizedOperations=false)
connect          | 14112 [kafka-admin-client-thread | adminclient-1] DEBUG org.apache.kafka.clients.NetworkClient  - [AdminClient clientId=adminclient-1] Received DESCRIBE_CLUSTER response from node 1 for request with header RequestHeader(apiKey=DESCRIBE_CLUSTER, apiVersion=0, clientId=adminclient-1, correlationId=3): DescribeClusterResponseData(throttleTimeMs=0, errorCode=0, errorMessage=null, clusterId='MkU3OEVBNTcwNTJENDM2Qg', controllerId=1, brokers=[DescribeClusterBroker(brokerId=1, host='broker', port=29092, rack=null)], clusterAuthorizedOperations=-2147483648)
connect          | 14120 [main] DEBUG io.confluent.admin.utils.ClusterStatus  - Broker list: [broker:29092 (id: 1 rack: null)]
rest-proxy       | [2023-08-04 11:14:31,735] INFO Expected 1 brokers but found only 0. Trying to query Kafka for metadata again ... (io.confluent.admin.utils.ClusterStatus)
rest-proxy       | [2023-08-04 11:14:31,735] ERROR Expected 1 brokers but found only 0. Brokers found []. (io.confluent.admin.utils.ClusterStatus)
schema-registry exited with code 1
control-center   | Using log4j config /etc/cp-base-new/log4j.properties
rest-proxy       | Using log4j config /etc/kafka-rest/log4j.properties
control-center   | ===> Launching ... 
control-center   | ===> Launching control-center ... 
connect          | ===> Launching ... 
connect          | ===> Launching kafka-connect ... 
rest-proxy exited with code 1
control-center   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
connect          | [2023-08-04 11:14:45,241] INFO WorkerInfo values: 
connect          | 	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/var/log/kafka, -Dlog4j.configuration=file:/etc/kafka/connect-log4j.properties, -Dlog4j.config.dir=/etc/kafka
connect          | 	jvm.spec = Azul Systems, Inc., OpenJDK 64-Bit Server VM, 11.0.14.1, 11.0.14.1+1-LTS
connect          | 	jvm.classpath = /usr/share/java/monitoring-interceptors/monitoring-interceptors-7.4.1.jar:/usr/share/java/confluent-security/connect/tink-1.6.0.jar:/usr/share/java/confluent-security/connect/snakeyaml-1.27.jar:/usr/share/java/confluent-security/connect/aws-java-sdk-s3-1.11.988.jar:/usr/share/java/confluent-security/connect/error_prone_annotations-2.5.1.jar:/usr/share/java/confluent-security/connect/httpclient-4.5.13.jar:/usr/share/java/confluent-security/connect/kotlin-stdlib-jdk8-1.5.31.jar:/usr/share/java/confluent-security/connect/confluent-connect-secret-registry-plugin-7.1.0.jar:/usr/share/java/confluent-security/connect/commons-logging-1.2.jar:/usr/share/java/confluent-security/connect/jetty-client-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/jetty-server-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/netty-transport-classes-epoll-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/google-api-services-storage-v1-rev20210127-1.32.1.jar:/usr/share/java/confluent-security/connect/kafka-connect-json-schema-converter-7.1.0.jar:/usr/share/java/confluent-security/connect/netty-transport-sctp-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/scala-reflect-2.13.5.jar:/usr/share/java/confluent-security/connect/netty-codec-mqtt-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/netty-transport-rxtx-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/google-cloud-core-http-2.1.3.jar:/usr/share/java/confluent-security/connect/zookeeper-3.6.3.jar:/usr/share/java/confluent-security/connect/jackson-datatype-jdk8-2.12.3.jar:/usr/share/java/confluent-security/connect/common-utils-7.1.0.jar:/usr/share/java/confluent-security/connect/google-http-client-apache-v2-1.40.0.jar:/usr/share/java/confluent-security/connect/kotlin-scripting-compiler-embeddable-1.3.50.jar:/usr/share/java/confluent-security/connect/bcpg-jdk15on-1.68.jar:/usr/share/java/confluent-security/connect/javax.ws.rs-api-2.1.1.jar:/usr/share/java/confluent-security/connect/asm-tree-9.2.jar:/usr/share/java/confluent-security/connect/asm-analysis-9.2.jar:/usr/share/java/confluent-security/connect/google-http-client-1.40.0.jar:/usr/share/java/confluent-security/connect/google-auth-library-credentials-1.1.0.jar:/usr/share/java/confluent-security/connect/security-extensions-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/netty-resolver-dns-native-macos-4.1.73.Final-osx-aarch_64.jar:/usr/share/java/confluent-security/connect/netty-handler-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/netty-codec-memcache-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/jersey-container-servlet-2.34.jar:/usr/share/java/confluent-security/connect/kotlin-scripting-jvm-1.4.21.jar:/usr/share/java/confluent-security/connect/netty-codec-http-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/common-config-7.1.0.jar:/usr/share/java/confluent-security/connect/checker-qual-3.8.0.jar:/usr/share/java/confluent-security/connect/netty-resolver-dns-classes-macos-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/jetty-alpn-java-server-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/netty-tcnative-boringssl-static-2.0.46.Final.jar:/usr/share/java/confluent-security/connect/jetty-jndi-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/annotations-13.0.jar:/usr/share/java/confluent-security/connect/jetty-servlets-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/kafka-avro-serializer-7.1.0.jar:/usr/share/java/confluent-security/connect/metrics-core-2.2.0.jar:/usr/share/java/confluent-security/connect/jetty-xml-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/netty-all-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/commons-codec-1.13.jar:/usr/share/java/confluent-security/connect/netty-handler-proxy-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/commons-collections-3.2.2.jar:/usr/share/java/confluent-security/connect/netty-common-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/auto-service-1.0-rc7.jar:/usr/share/java/confluent-security/connect/jackson-dataformat-cbor-2.12.3.jar:/usr/share/java/confluent-security/connect/netty-codec-dns-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/jetty-webapp-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/threetenbp-1.5.1.jar:/usr/share/java/confluent-security/connect/jersey-common-2.34.jar:/usr/share/java/confluent-security/connect/google-oauth-client-1.32.1.jar:/usr/share/java/confluent-security/connect/api-common-2.0.2.jar:/usr/share/java/confluent-security/connect/jakarta.el-api-4.0.0.jar:/usr/share/java/confluent-security/connect/proto-google-common-protos-2.5.1.jar:/usr/share/java/confluent-security/connect/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/share/java/confluent-security/connect/javax-websocket-client-impl-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/okio-jvm-3.0.0.jar:/usr/share/java/confluent-security/connect/kotlin-scripting-common-1.4.21.jar:/usr/share/java/confluent-security/connect/kafka-json-serializer-7.1.0.jar:/usr/share/java/confluent-security/connect/kafka-schema-serializer-7.1.0.jar:/usr/share/java/confluent-security/connect/internal-rest-server-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/kafka-raft-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/aopalliance-repackaged-2.6.1.jar:/usr/share/java/confluent-security/connect/scala-collection-compat_2.13-2.4.4.jar:/usr/share/java/confluent-security/connect/hk2-locator-2.6.1.jar:/usr/share/java/confluent-security/connect/scala-library-2.13.5.jar:/usr/share/java/confluent-security/connect/javax.json-1.0.4.jar:/usr/share/java/confluent-security/connect/google-auth-library-oauth2-http-1.1.0.jar:/usr/share/java/confluent-security/connect/jakarta.validation-api-2.0.2.jar:/usr/share/java/confluent-security/connect/bcprov-jdk15on-1.68.jar:/usr/share/java/confluent-security/connect/confluent-security-plugins-common-7.1.0.jar:/usr/share/java/confluent-security/connect/j2objc-annotations-1.3.jar:/usr/share/java/confluent-security/connect/jackson-jaxrs-base-2.12.3.jar:/usr/share/java/confluent-security/connect/netty-codec-stomp-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/json-20201115.jar:/usr/share/java/confluent-security/connect/ion-java-1.0.2.jar:/usr/share/java/confluent-security/connect/google-api-services-cloudkms-v1-rev108-1.25.0.jar:/usr/share/java/confluent-security/connect/jackson-dataformat-yaml-2.12.3.jar:/usr/share/java/confluent-security/connect/websocket-client-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/antlr-runtime-3.5.2.jar:/usr/share/java/confluent-security/connect/netty-codec-xml-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/jetty-alpn-server-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/activation-1.1.1.jar:/usr/share/java/confluent-security/connect/org.everit.json.schema-1.12.2.jar:/usr/share/java/confluent-security/connect/jakarta.ws.rs-api-2.1.6.jar:/usr/share/java/confluent-security/connect/netty-transport-udt-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/jetty-util-ajax-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/google-http-client-gson-1.40.0.jar:/usr/share/java/confluent-security/connect/paranamer-2.8.jar:/usr/share/java/confluent-security/connect/kafka-connect-avro-converter-7.1.0.jar:/usr/share/java/confluent-security/connect/asm-9.2.jar:/usr/share/java/confluent-security/connect/jmespath-java-1.11.988.jar:/usr/share/java/co
connect          | nfluent-security/connect/scala-java8-compat_2.13-1.0.0.jar:/usr/share/java/confluent-security/connect/kafka-metadata-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/jboss-logging-3.3.2.Final.jar:/usr/share/java/confluent-security/connect/commons-validator-1.6.jar:/usr/share/java/confluent-security/connect/jose4j-0.7.8.jar:/usr/share/java/confluent-security/connect/netty-buffer-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/joda-time-2.10.8.jar:/usr/share/java/confluent-security/connect/tink-gcpkms-1.6.0.jar:/usr/share/java/confluent-security/connect/kafka-json-schema-provider-7.1.0.jar:/usr/share/java/confluent-security/connect/jbcrypt-0.4.jar:/usr/share/java/confluent-security/connect/hibernate-validator-6.1.7.Final.jar:/usr/share/java/confluent-security/connect/jackson-datatype-guava-2.12.3.jar:/usr/share/java/confluent-security/connect/wire-runtime-jvm-4.0.0.jar:/usr/share/java/confluent-security/connect/jetty-util-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/auto-common-0.10.jar:/usr/share/java/confluent-security/connect/hk2-utils-2.6.1.jar:/usr/share/java/confluent-security/connect/confluent-connect-security-plugin-7.1.0.jar:/usr/share/java/confluent-security/connect/metrics-core-4.1.12.1.jar:/usr/share/java/confluent-security/connect/authorizer-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/opencensus-api-0.28.0.jar:/usr/share/java/confluent-security/connect/netty-transport-native-kqueue-4.1.73.Final-osx-aarch_64.jar:/usr/share/java/confluent-security/connect/jackson-module-jaxb-annotations-2.12.3.jar:/usr/share/java/confluent-security/connect/websocket-servlet-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/netty-codec-haproxy-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/wire-schema-jvm-4.0.0.jar:/usr/share/java/confluent-security/connect/bcpkix-jdk15on-1.68.jar:/usr/share/java/confluent-security/connect/jackson-dataformat-csv-2.12.3.jar:/usr/share/java/confluent-security/connect/commons-digester-1.8.1.jar:/usr/share/java/confluent-security/connect/netty-tcnative-classes-2.0.46.Final.jar:/usr/share/java/confluent-security/connect/aws-java-sdk-sts-1.11.988.jar:/usr/share/java/confluent-security/connect/aws-java-sdk-core-1.11.988.jar:/usr/share/java/confluent-security/connect/commons-cli-1.4.jar:/usr/share/java/confluent-security/connect/swagger-annotations-2.1.10.jar:/usr/share/java/confluent-security/connect/kafka-protobuf-types-7.1.0.jar:/usr/share/java/confluent-security/connect/failureaccess-1.0.1.jar:/usr/share/java/confluent-security/connect/jakarta.inject-2.6.1.jar:/usr/share/java/confluent-security/connect/kafka-protobuf-serializer-7.1.0.jar:/usr/share/java/confluent-security/connect/javassist-3.25.0-GA.jar:/usr/share/java/confluent-security/connect/netty-transport-native-unix-common-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/kafka-connect-protobuf-converter-7.1.0.jar:/usr/share/java/confluent-security/connect/jersey-container-servlet-core-2.34.jar:/usr/share/java/confluent-security/connect/confluent-serializers-new-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/jetty-io-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/rest-utils-7.1.0.jar:/usr/share/java/confluent-security/connect/javax-websocket-server-impl-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/kafka-schema-registry-client-7.1.0.jar:/usr/share/java/confluent-security/connect/netty-transport-classes-kqueue-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/kafka-server-common-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/kotlin-stdlib-1.4.21.jar:/usr/share/java/confluent-security/connect/confluent-licensing-new-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/websocket-api-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/argparse4j-0.7.0.jar:/usr/share/java/confluent-security/connect/netty-resolver-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/jopt-simple-5.0.4.jar:/usr/share/java/confluent-security/connect/netty-transport-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/kafka-protobuf-provider-7.1.0.jar:/usr/share/java/confluent-security/connect/audience-annotations-0.5.0.jar:/usr/share/java/confluent-security/connect/jackson-jaxrs-json-provider-2.12.3.jar:/usr/share/java/confluent-security/connect/icu4j-61.1.jar:/usr/share/java/confluent-security/connect/zookeeper-jute-3.6.3.jar:/usr/share/java/confluent-security/connect/websocket-server-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/scala-logging_2.13-3.9.3.jar:/usr/share/java/confluent-security/connect/gax-2.4.1.jar:/usr/share/java/confluent-security/connect/netty-codec-redis-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/commons-lang3-3.12.0.jar:/usr/share/java/confluent-security/connect/kafka-json-schema-serializer-7.1.0.jar:/usr/share/java/confluent-security/connect/kafka-connect-avro-data-7.1.0.jar:/usr/share/java/confluent-security/connect/javax.servlet-api-4.0.1.jar:/usr/share/java/confluent-security/connect/jersey-server-2.34.jar:/usr/share/java/confluent-security/connect/netty-codec-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/proto-google-iam-v1-1.1.0.jar:/usr/share/java/confluent-security/connect/websocket-common-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/rest-authorizer-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/netty-codec-socks-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/hk2-api-2.6.1.jar:/usr/share/java/confluent-security/connect/protobuf-java-util-3.17.3.jar:/usr/share/java/confluent-security/connect/jersey-bean-validation-2.34.jar:/usr/share/java/confluent-security/connect/aws-java-sdk-kms-1.11.988.jar:/usr/share/java/confluent-security/connect/jsr305-3.0.2.jar:/usr/share/java/confluent-security/connect/jetty-jaas-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/auto-service-annotations-1.0-rc7.jar:/usr/share/java/confluent-security/connect/kafka-storage-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/jetty-servlet-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/validation-api-2.0.1.Final.jar:/usr/share/java/confluent-security/connect/avro-1.11.0.jar:/usr/share/java/confluent-security/connect/kafka-secret-registry-client-7.1.0.jar:/usr/share/java/confluent-security/connect/flatbuffers-java-1.9.0.jar:/usr/share/java/confluent-security/connect/protobuf-java-3.17.3.jar:/usr/share/java/confluent-security/connect/jersey-hk2-2.34.jar:/usr/share/java/confluent-security/connect/jakarta.activation-api-1.2.1.jar:/usr/share/java/confluent-security/connect/asm-commons-9.2.jar:/usr/share/java/confluent-security/connect/jetty-continuation-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/javax.websocket-client-api-1.0.jar:/usr/share/java/confluent-security/connect/jersey-client-2.34.jar:/usr/share/java/confluent-security/connect/opencensus-contrib-http-util-0.28.0.jar:/usr/share/java/confluent-security/connect/kotlin-stdlib-jdk7-1.5.31.jar:/usr/share/java/confluent-security/connect/antlr4-4.9.2.jar:/usr/share/java/confluent-security/connect/jackson-annotations-2.12.3.jar:/usr/share/java/confluent-security/connect/kafka-storage-api-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/classmate-1.3.4.jar:/usr/share/java/confluent-security/connect/gson-2.8.6.jar:/usr/share/java/confluent-security/connect/kotlinx-coroutines-core-1.3.7.jar:/usr/share/java/confluent-security/connect/jakarta.el-3.0.3.jar:/usr/share/java/confluent-security/connect/netty-codec-http2-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/google-cloud-core-2.1.3.jar:/usr/share/java/confluent-security/connect/gax-httpjson-0.89.1.jar:/usr/share/java/confluent-security/connect/netty-resolver-dns-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/jakarta.annotation-api-1.3.5.jar:/usr/share/java/confluent-security/connect/jetty-http-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/kafka-secret-registry-7.1.0.jar:/usr/share/java/confluent-security/connect/ST4-4.3.jar:/usr/share/java/confluent-security/connect/org.abego.treelayout.core-1.0.3.jar:/usr/share/java/con
connect          | fluent-security/connect/re2j-1.6.jar:/usr/share/java/confluent-security/connect/http2-server-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/jackson-module-parameter-names-2.12.3.jar:/usr/share/java/confluent-security/connect/http2-hpack-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/httpcore-4.4.13.jar:/usr/share/java/confluent-security/connect/google-api-client-1.32.1.jar:/usr/share/java/confluent-security/connect/antlr4-runtime-4.9.2.jar:/usr/share/java/confluent-security/connect/netty-transport-native-kqueue-4.1.73.Final-osx-x86_64.jar:/usr/share/java/confluent-security/connect/handy-uri-templates-2.1.8.jar:/usr/share/java/confluent-security/connect/jetty-jmx-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/http2-common-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/google-http-client-jackson2-1.40.0.jar:/usr/share/java/confluent-security/connect/guava-30.1.1-jre.jar:/usr/share/java/confluent-security/connect/netty-codec-smtp-4.1.73.Final.jar:/usr/share/java/confluent-security/connect/grpc-context-1.40.1.jar:/usr/share/java/confluent-security/connect/google-http-client-appengine-1.40.0.jar:/usr/share/java/confluent-security/connect/jackson-core-2.12.3.jar:/usr/share/java/confluent-security/connect/kotlin-script-runtime-1.4.21.jar:/usr/share/java/confluent-security/connect/jetty-annotations-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/kafka-client-plugins-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/osgi-resource-locator-1.0.3.jar:/usr/share/java/confluent-security/connect/jackson-module-scala_2.13-2.12.3.jar:/usr/share/java/confluent-security/connect/jackson-datatype-jsr310-2.12.3.jar:/usr/share/java/confluent-security/connect/javax.websocket-api-1.0.jar:/usr/share/java/confluent-security/connect/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/usr/share/java/confluent-security/connect/jetty-plus-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/kafka_2.13-7.1.0-ce.jar:/usr/share/java/confluent-security/connect/netty-resolver-dns-native-macos-4.1.73.Final-osx-x86_64.jar:/usr/share/java/confluent-security/connect/google-cloud-storage-2.1.2.jar:/usr/share/java/confluent-security/connect/jaxb-api-2.3.0.jar:/usr/share/java/confluent-security/connect/classgraph-4.8.21.jar:/usr/share/java/confluent-security/connect/commons-compress-1.21.jar:/usr/share/java/confluent-security/connect/auto-value-annotations-1.8.2.jar:/usr/share/java/confluent-security/connect/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/usr/share/java/confluent-security/connect/kotlin-stdlib-common-1.5.31.jar:/usr/share/java/confluent-security/connect/jetty-security-9.4.44.v20210927.jar:/usr/share/java/confluent-security/connect/jackson-datatype-joda-2.12.3.jar:/usr/share/java/confluent-security/connect/javax.annotation-api-1.3.2.jar:/usr/share/java/confluent-security/connect/broker-plugins-7.1.0-ce-test.jar:/usr/share/java/confluent-security/connect/netty-transport-native-epoll-4.1.73.Final-linux-x86_64.jar:/usr/share/java/confluent-security/connect/jakarta.xml.bind-api-2.3.2.jar:/usr/share/java/confluent-security/connect/netty-transport-native-epoll-4.1.73.Final-linux-aarch_64.jar:/usr/share/java/kafka/tink-1.6.0.jar:/usr/share/java/kafka/snakeyaml-1.27.jar:/usr/share/java/kafka/aws-java-sdk-s3-1.11.988.jar:/usr/share/java/kafka/rbac-7.1.0-ce.jar:/usr/share/java/kafka/azure-storage-blob-12.12.0.jar:/usr/share/java/kafka/cloudevents-kafka-2.0.0.jar:/usr/share/java/kafka/httpclient-4.5.13.jar:/usr/share/java/kafka/kotlin-stdlib-jdk8-1.5.31.jar:/usr/share/java/kafka/connect-runtime-7.1.0-ce.jar:/usr/share/java/kafka/trogdor-7.1.0-ce.jar:/usr/share/java/kafka/commons-logging-1.2.jar:/usr/share/java/kafka/jetty-client-9.4.44.v20210927.jar:/usr/share/java/kafka/jetty-server-9.4.44.v20210927.jar:/usr/share/java/kafka/netty-transport-classes-epoll-4.1.73.Final.jar:/usr/share/java/kafka/google-api-services-storage-v1-rev20210127-1.32.1.jar:/usr/share/java/kafka/kafka-streams-7.1.0-ce.jar:/usr/share/java/kafka/netty-transport-sctp-4.1.73.Final.jar:/usr/share/java/kafka/KeePassJava2-dom-2.1.4.jar:/usr/share/java/kafka/reactor-netty-http-1.0.7.jar:/usr/share/java/kafka/confluent-log4j-1.2.17-cp10.jar:/usr/share/java/kafka/netty-codec-mqtt-4.1.73.Final.jar:/usr/share/java/kafka/netty-transport-rxtx-4.1.73.Final.jar:/usr/share/java/kafka/google-cloud-core-http-2.1.3.jar:/usr/share/java/kafka/zookeeper-3.6.3.jar:/usr/share/java/kafka/jackson-datatype-jdk8-2.12.3.jar:/usr/share/java/kafka/zipkin-reporter-brave-2.16.3.jar:/usr/share/java/kafka/google-http-client-apache-v2-1.40.0.jar:/usr/share/java/kafka/simpleclient_tracer_otel-0.12.0.jar:/usr/share/java/kafka/lz4-java-1.8.0.jar:/usr/share/java/kafka/scala-reflect-2.13.6.jar:/usr/share/java/kafka/kafka.jar:/usr/share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/share/java/kafka/core-1.54.0.0.jar:/usr/share/java/kafka/confluent-audit-7.1.0-ce.jar:/usr/share/java/kafka/google-http-client-1.40.0.jar:/usr/share/java/kafka/telemetry-client-1.745.0.jar:/usr/share/java/kafka/brave-instrumentation-http-5.13.3.jar:/usr/share/java/kafka/google-auth-library-credentials-1.1.0.jar:/usr/share/java/kafka/security-extensions-7.1.0-ce.jar:/usr/share/java/kafka/netty-resolver-dns-native-macos-4.1.73.Final-osx-aarch_64.jar:/usr/share/java/kafka/netty-handler-4.1.73.Final.jar:/usr/share/java/kafka/netty-codec-memcache-4.1.73.Final.jar:/usr/share/java/kafka/jersey-container-servlet-2.34.jar:/usr/share/java/kafka/netty-codec-http-4.1.73.Final.jar:/usr/share/java/kafka/snappy-java-1.1.8.4.jar:/usr/share/java/kafka/kafka-clients-7.1.0-ce.jar:/usr/share/java/kafka/checker-qual-3.8.0.jar:/usr/share/java/kafka/netty-resolver-dns-classes-macos-4.1.73.Final.jar:/usr/share/java/kafka/netty-transport-native-epoll-4.1.73.Final.jar:/usr/share/java/kafka/lang-tag-1.5.jar:/usr/share/java/kafka/netty-tcnative-boringssl-static-2.0.46.Final.jar:/usr/share/java/kafka/simpleclient_tracer_common-0.12.0.jar:/usr/share/java/kafka/snakeyaml-1.29.jar:/usr/share/java/kafka/annotations-13.0.jar:/usr/share/java/kafka/msal4j-1.10.1.jar:/usr/share/java/kafka/azure-identity-1.3.3.jar:/usr/share/java/kafka/jetty-servlets-9.4.44.v20210927.jar:/usr/share/java/kafka/jcip-annotations-1.0.jar:/usr/share/java/kafka/metrics-core-2.2.0.jar:/usr/share/java/kafka/zipkin-reporter-2.16.3.jar:/usr/share/java/kafka/netty-all-4.1.73.Final.jar:/usr/share/java/kafka/maven-artifact-3.8.1.jar:/usr/share/java/kafka/telemetry-events-api-7.1.0-ce.jar:/usr/share/java/kafka/netty-handler-proxy-4.1.73.Final.jar:/usr/share/java/kafka/netty-common-4.1.73.Final.jar:/usr/share/java/kafka/auto-service-1.0-rc7.jar:/usr/share/java/kafka/jackson-dataformat-cbor-2.12.3.jar:/usr/share/java/kafka/netty-codec-dns-4.1.73.Final.jar:/usr/share/java/kafka/threetenbp-1.5.1.jar:/usr/share/java/kafka/jersey-common-2.34.jar:/usr/share/java/kafka/accessors-smart-2.4.7.jar:/usr/share/java/kafka/cloudevents-core-2.0.0.jar:/usr/share/java/kafka/google-oauth-client-1.32.1.jar:/usr/share/java/kafka/api-common-2.0.2.jar:/usr/share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/share/java/kafka/okio-jvm-3.0.0.jar:/usr/share/java/kafka/httpcore-4.4.14.jar:/usr/share/java/kafka/telemetry-events-7.1.0-ce.jar:/usr/share/java/kafka/kotlin-stdlib-1.5.31.jar:/usr/share/java/kafka/internal-rest-server-7.1.0-ce.jar:/usr/share/java/kafka/kafka-raft-7.1.0-ce.jar:/usr/share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/share/java/kafka/scala-collection-compat_2.13-2.4.4.jar:/usr/share/java/kafka/hk2-locator-2.6.1.jar:/usr/share/java/kafka/javax.json-1.0.4.jar:/usr/share/java/kafka/google-auth-library-oauth2-http-1.1.0.jar:/usr/share/java/kafka/simpleclient-0.12.0.jar:/usr/share/java/kafka/msal4j-persistence-extension-1.1.0.jar:/usr/share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/share/java/kafka/connect-json-7.1.0-ce.jar:/usr/share/java/kafka/scala-library-2.13.6.jar:/usr/share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/share/java/kafka/swagger-annotations-1.6.3.jar:/usr/share/java/kafka/reflections-0.9.12.jar:/usr/share/java/kafka/j2objc-annotations-1.3.jar:/usr/share/java/kafka/jackson-jaxrs-base-2.12.3.
connect          | jar:/usr/share/java/kafka/slf4j-api-1.7.32.jar:/usr/share/java/kafka/netty-codec-stomp-4.1.73.Final.jar:/usr/share/java/kafka/ion-java-1.0.2.jar:/usr/share/java/kafka/google-api-services-cloudkms-v1-rev108-1.25.0.jar:/usr/share/java/kafka/jackson-dataformat-yaml-2.12.3.jar:/usr/share/java/kafka/kafka-streams-scala_2.13-7.1.0-ce.jar:/usr/share/java/kafka/antlr-runtime-3.5.2.jar:/usr/share/java/kafka/netty-codec-xml-4.1.73.Final.jar:/usr/share/java/kafka/jackson-databind-2.12.3.jar:/usr/share/java/kafka/azure-core-1.18.0.jar:/usr/share/java/kafka/activation-1.1.1.jar:/usr/share/java/kafka/connect-api-7.1.0-ce.jar:/usr/share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/share/java/kafka/database-2.1.4.jar:/usr/share/java/kafka/netty-transport-udt-4.1.73.Final.jar:/usr/share/java/kafka/cloudevents-api-2.0.0.jar:/usr/share/java/kafka/jetty-util-ajax-9.4.44.v20210927.jar:/usr/share/java/kafka/google-http-client-gson-1.40.0.jar:/usr/share/java/kafka/joda-time-2.9.9.jar:/usr/share/java/kafka/paranamer-2.8.jar:/usr/share/java/kafka/commons-codec-1.15.jar:/usr/share/java/kafka/jmespath-java-1.11.988.jar:/usr/share/java/kafka/scala-java8-compat_2.13-1.0.0.jar:/usr/share/java/kafka/reactor-core-3.4.6.jar:/usr/share/java/kafka/kafka-metadata-7.1.0-ce.jar:/usr/share/java/kafka/content-type-2.1.jar:/usr/share/java/kafka/jose4j-0.7.8.jar:/usr/share/java/kafka/netty-buffer-4.1.73.Final.jar:/usr/share/java/kafka/plexus-utils-3.2.1.jar:/usr/share/java/kafka/tink-gcpkms-1.6.0.jar:/usr/share/java/kafka/kafka-tools-7.1.0-ce.jar:/usr/share/java/kafka/KeePassJava2-kdbx-2.1.4.jar:/usr/share/java/kafka/client-java-api-14.0.0.jar:/usr/share/java/kafka/jbcrypt-0.4.jar:/usr/share/java/kafka/slf4j-api-1.7.21.jar:/usr/share/java/kafka/jsr305-3.0.1.jar:/usr/share/java/kafka/jetty-util-9.4.44.v20210927.jar:/usr/share/java/kafka/connector-datapreview-extension-7.1.0-ce.jar:/usr/share/java/kafka/auto-common-0.10.jar:/usr/share/java/kafka/hk2-utils-2.6.1.jar:/usr/share/java/kafka/KeePassJava2-2.1.4.jar:/usr/share/java/kafka/metrics-core-4.1.12.1.jar:/usr/share/java/kafka/error_prone_annotations-2.3.4.jar:/usr/share/java/kafka/authorizer-7.1.0-ce.jar:/usr/share/java/kafka/rocksdbjni-6.22.1.1.jar:/usr/share/java/kafka/simpleclient_httpserver-0.12.0.jar:/usr/share/java/kafka/opencensus-api-0.28.0.jar:/usr/share/java/kafka/netty-transport-native-kqueue-4.1.73.Final-osx-aarch_64.jar:/usr/share/java/kafka/jackson-module-jaxb-annotations-2.12.3.jar:/usr/share/java/kafka/gson-fire-1.8.5.jar:/usr/share/java/kafka/jcip-annotations-1.0-1.jar:/usr/share/java/kafka/netty-codec-haproxy-4.1.73.Final.jar:/usr/share/java/kafka/logging-interceptor-4.9.1.jar:/usr/share/java/kafka/cloudevents-json-jackson-2.0.0.jar:/usr/share/java/kafka/jackson-dataformat-csv-2.12.3.jar:/usr/share/java/kafka/netty-tcnative-classes-2.0.46.Final.jar:/usr/share/java/kafka/aws-java-sdk-sts-1.11.988.jar:/usr/share/java/kafka/auth-providers-7.1.0-ce.jar:/usr/share/java/kafka/aws-java-sdk-core-1.11.988.jar:/usr/share/java/kafka/simpleclient_common-0.12.0.jar:/usr/share/java/kafka/commons-cli-1.4.jar:/usr/share/java/kafka/ce-sbk_2.13-7.1.0-ce.jar:/usr/share/java/kafka/failureaccess-1.0.1.jar:/usr/share/java/kafka/brave-5.13.3.jar:/usr/share/java/kafka/stax2-api-4.2.1.jar:/usr/share/java/kafka/jakarta.inject-2.6.1.jar:/usr/share/java/kafka/netty-transport-native-unix-common-4.1.73.Final.jar:/usr/share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/share/java/kafka/confluent-serializers-new-7.1.0-ce.jar:/usr/share/java/kafka/jetty-io-9.4.44.v20210927.jar:/usr/share/java/kafka/client-java-14.0.0.jar:/usr/share/java/kafka/netty-transport-classes-kqueue-4.1.73.Final.jar:/usr/share/java/kafka/kafka-server-common-7.1.0-ce.jar:/usr/share/java/kafka/connect-transforms-7.1.0-ce.jar:/usr/share/java/kafka/connect-mirror-client-7.1.0-ce.jar:/usr/share/java/kafka/confluent-licensing-new-7.1.0-ce.jar:/usr/share/java/kafka/argparse4j-0.7.0.jar:/usr/share/java/kafka/netty-resolver-4.1.73.Final.jar:/usr/share/java/kafka/annotations-15.0.jar:/usr/share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/usr/share/java/kafka/jopt-simple-5.0.4.jar:/usr/share/java/kafka/netty-transport-4.1.73.Final.jar:/usr/share/java/kafka/commons-math3-3.6.1.jar:/usr/share/java/kafka/connect-basic-auth-extension-7.1.0-ce.jar:/usr/share/java/kafka/audience-annotations-0.5.0.jar:/usr/share/java/kafka/KeePassJava2-kdb-2.1.4.jar:/usr/share/java/kafka/jackson-jaxrs-json-provider-2.12.3.jar:/usr/share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/share/java/kafka/client-java-proto-14.0.0.jar:/usr/share/java/kafka/icu4j-61.1.jar:/usr/share/java/kafka/kafka-log4j-appender-7.1.0-ce.jar:/usr/share/java/kafka/zookeeper-jute-3.6.3.jar:/usr/share/java/kafka/scala-logging_2.13-3.9.3.jar:/usr/share/java/kafka/logredactor-metrics-1.0.10.jar:/usr/share/java/kafka/gax-2.4.1.jar:/usr/share/java/kafka/netty-codec-redis-4.1.73.Final.jar:/usr/share/java/kafka/aalto-xml-1.0.0.jar:/usr/share/java/kafka/reactor-netty-core-1.0.7.jar:/usr/share/java/kafka/jersey-server-2.34.jar:/usr/share/java/kafka/simpleclient_tracer_otel_agent-0.12.0.jar:/usr/share/java/kafka/netty-codec-4.1.73.Final.jar:/usr/share/java/kafka/proto-google-iam-v1-1.1.0.jar:/usr/share/java/kafka/rest-authorizer-7.1.0-ce.jar:/usr/share/java/kafka/netty-codec-socks-4.1.73.Final.jar:/usr/share/java/kafka/hk2-api-2.6.1.jar:/usr/share/java/kafka/protobuf-java-util-3.17.3.jar:/usr/share/java/kafka/bcpkix-fips-1.0.3.jar:/usr/share/java/kafka/aws-java-sdk-kms-1.11.988.jar:/usr/share/java/kafka/jsr305-3.0.2.jar:/usr/share/java/kafka/kafka-shell-7.1.0-ce.jar:/usr/share/java/kafka/azure-core-http-netty-1.10.1.jar:/usr/share/java/kafka/broker-plugins-7.1.0-ce.jar:/usr/share/java/kafka/connect-mirror-7.1.0-ce.jar:/usr/share/java/kafka/auto-service-annotations-1.0-rc7.jar:/usr/share/java/kafka/kafka-storage-7.1.0-ce.jar:/usr/share/java/kafka/jetty-servlet-9.4.44.v20210927.jar:/usr/share/java/kafka/confluent-resource-names-7.1.0-ce.jar:/usr/share/java/kafka/commons-io-2.11.0.jar:/usr/share/java/kafka/telemetry-api-1.745.0.jar:/usr/share/java/kafka/flatbuffers-java-1.9.0.jar:/usr/share/java/kafka/protobuf-java-3.17.3.jar:/usr/share/java/kafka/jersey-hk2-2.34.jar:/usr/share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/share/java/kafka/jetty-continuation-9.4.44.v20210927.jar:/usr/share/java/kafka/jersey-client-2.34.jar:/usr/share/java/kafka/kafka-streams-test-utils-7.1.0-ce.jar:/usr/share/java/kafka/opencensus-contrib-http-util-0.28.0.jar:/usr/share/java/kafka/kotlin-stdlib-jdk7-1.5.31.jar:/usr/share/java/kafka/antlr4-4.9.2.jar:/usr/share/java/kafka/jackson-annotations-2.12.3.jar:/usr/share/java/kafka/proto-google-common-protos-2.5.0.jar:/usr/share/java/kafka/kafka-storage-api-7.1.0-ce.jar:/usr/share/java/kafka/gson-2.8.6.jar:/usr/share/java/kafka/netty-codec-http2-4.1.73.Final.jar:/usr/share/java/kafka/commons-collections4-4.4.jar:/usr/share/java/kafka/google-cloud-core-2.1.3.jar:/usr/share/java/kafka/oauth2-oidc-sdk-9.7.jar:/usr/share/java/kafka/gax-httpjson-0.89.1.jar:/usr/share/java/kafka/netty-resolver-dns-4.1.73.Final.jar:/usr/share/java/kafka/connect-ce-logs-7.1.0-ce.jar:/usr/share/java/kafka/netty-handler-proxy-4.1.65.Final.jar:/usr/share/java/kafka/minimal-json-0.9.5.jar:/usr/share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/share/java/kafka/jetty-http-9.4.44.v20210927.jar:/usr/share/java/kafka/ST4-4.3.jar:/usr/share/java/kafka/okhttp-4.9.1.jar:/usr/share/java/kafka/org.abego.treelayout.core-1.0.3.jar:/usr/share/java/kafka/re2j-1.6.jar:/usr/share/java/kafka/slf4j-api-1.7.30.jar:/usr/share/java/kafka/json-smart-2.4.7.jar:/usr/share/java/kafka/httpcore-4.4.13.jar:/usr/share/java/kafka/jline-3.12.1.jar:/usr/share/java/kafka/woodstox-core-6.2.4.jar:/usr/share/java/kafka/KeePassJava2-jaxb-2.1.4.jar:/usr/share/java/kafka/bctls-fips-1.0.10.jar:/usr/share/java/kafka/google-api-client-1.32.1.jar:/usr/share/java/kafka/antlr4-runtime-4.9.2.jar:/usr/share/java/kafka/zstd-jni-1.5.0-4.jar:/usr/share/java/kafka/azure-storage-common-12.12.0.jar:/usr/share/java/kafka/netty-transport-native-kqueue-4.1.73.Final-osx-x86_64.jar:/usr/share/java/kafka/KeePassJava2-simple-2.1.4.jar:/usr/share/java/kafka/google-http-client-
connect          | jackson2-1.40.0.jar:/usr/share/java/kafka/netty-codec-smtp-4.1.73.Final.jar:/usr/share/java/kafka/logredactor-1.0.10.jar:/usr/share/java/kafka/grpc-context-1.40.1.jar:/usr/share/java/kafka/google-http-client-appengine-1.40.0.jar:/usr/share/java/kafka/javassist-3.27.0-GA.jar:/usr/share/java/kafka/jackson-core-2.12.3.jar:/usr/share/java/kafka/kafka-streams-examples-7.1.0-ce.jar:/usr/share/java/kafka/kafka-client-plugins-7.1.0-ce.jar:/usr/share/java/kafka/annotations-3.0.1.jar:/usr/share/java/kafka/jackson-dataformat-xml-2.12.3.jar:/usr/share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/share/java/kafka/jackson-module-scala_2.13-2.12.3.jar:/usr/share/java/kafka/jackson-datatype-jsr310-2.12.3.jar:/usr/share/java/kafka/nimbus-jose-jwt-9.9.3.jar:/usr/share/java/kafka/kafka_2.13-7.1.0-ce.jar:/usr/share/java/kafka/netty-resolver-dns-native-macos-4.1.73.Final-osx-x86_64.jar:/usr/share/java/kafka/reactor-netty-http-brave-1.0.7.jar:/usr/share/java/kafka/asm-9.1.jar:/usr/share/java/kafka/zipkin-2.23.2.jar:/usr/share/java/kafka/google-cloud-storage-2.1.2.jar:/usr/share/java/kafka/jaxb-api-2.3.0.jar:/usr/share/java/kafka/reactor-netty-1.0.7.jar:/usr/share/java/kafka/reactive-streams-1.0.3.jar:/usr/share/java/kafka/jna-5.6.0.jar:/usr/share/java/kafka/commons-compress-1.21.jar:/usr/share/java/kafka/guava-30.0-jre.jar:/usr/share/java/kafka/jackson-dataformat-properties-2.12.3.jar:/usr/share/java/kafka/auto-value-annotations-1.8.2.jar:/usr/share/java/kafka/bc-fips-1.0.2.jar:/usr/share/java/kafka/kotlin-stdlib-common-1.5.31.jar:/usr/share/java/kafka/jetty-security-9.4.44.v20210927.jar:/usr/share/java/kafka/commons-lang3-3.11.jar:/usr/share/java/kafka/javax.annotation-api-1.3.2.jar:/usr/share/java/kafka/opencensus-proto-0.2.0.jar:/usr/share/java/kafka/azure-storage-internal-avro-12.0.5.jar:/usr/share/java/kafka/checker-qual-3.5.0.jar:/usr/share/java/kafka/netty-transport-native-epoll-4.1.73.Final-linux-x86_64.jar:/usr/share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/share/java/kafka/jna-platform-5.6.0.jar:/usr/share/java/kafka/netty-transport-native-epoll-4.1.73.Final-linux-aarch_64.jar:/usr/share/java/confluent-common/common-utils-7.1.0.jar:/usr/share/java/confluent-common/common-config-7.1.0.jar:/usr/share/java/confluent-common/build-tools-7.1.0.jar:/usr/share/java/confluent-common/common-metrics-7.1.0.jar:/usr/share/java/confluent-common/slf4j-api-1.7.30.jar:/usr/share/java/kafka-serde-tools/error_prone_annotations-2.5.1.jar:/usr/share/java/kafka-serde-tools/kotlin-stdlib-jdk8-1.5.31.jar:/usr/share/java/kafka-serde-tools/commons-logging-1.2.jar:/usr/share/java/kafka-serde-tools/kafka-connect-json-schema-converter-7.1.0.jar:/usr/share/java/kafka-serde-tools/jackson-datatype-jdk8-2.12.3.jar:/usr/share/java/kafka-serde-tools/kotlin-scripting-compiler-embeddable-1.3.50.jar:/usr/share/java/kafka-serde-tools/kafka-streams-protobuf-serde-7.1.0.jar:/usr/share/java/kafka-serde-tools/kotlin-scripting-jvm-1.4.21.jar:/usr/share/java/kafka-serde-tools/checker-qual-3.8.0.jar:/usr/share/java/kafka-serde-tools/annotations-13.0.jar:/usr/share/java/kafka-serde-tools/kafka-avro-serializer-7.1.0.jar:/usr/share/java/kafka-serde-tools/commons-collections-3.2.2.jar:/usr/share/java/kafka-serde-tools/proto-google-common-protos-2.5.1.jar:/usr/share/java/kafka-serde-tools/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/share/java/kafka-serde-tools/okio-jvm-3.0.0.jar:/usr/share/java/kafka-serde-tools/kotlin-scripting-common-1.4.21.jar:/usr/share/java/kafka-serde-tools/kafka-json-serializer-7.1.0.jar:/usr/share/java/kafka-serde-tools/kafka-schema-serializer-7.1.0.jar:/usr/share/java/kafka-serde-tools/scala-library-2.13.5.jar:/usr/share/java/kafka-serde-tools/kafka-streams-json-schema-serde-7.1.0.jar:/usr/share/java/kafka-serde-tools/j2objc-annotations-1.3.jar:/usr/share/java/kafka-serde-tools/json-20201115.jar:/usr/share/java/kafka-serde-tools/jackson-databind-2.12.3.jar:/usr/share/java/kafka-serde-tools/org.everit.json.schema-1.12.2.jar:/usr/share/java/kafka-serde-tools/kafka-connect-avro-converter-7.1.0.jar:/usr/share/java/kafka-serde-tools/kafka-streams-7.1.0-ccs.jar:/usr/share/java/kafka-serde-tools/commons-validator-1.6.jar:/usr/share/java/kafka-serde-tools/kafka-streams-avro-serde-7.1.0.jar:/usr/share/java/kafka-serde-tools/joda-time-2.10.8.jar:/usr/share/java/kafka-serde-tools/kafka-json-schema-provider-7.1.0.jar:/usr/share/java/kafka-serde-tools/jackson-datatype-guava-2.12.3.jar:/usr/share/java/kafka-serde-tools/wire-runtime-jvm-4.0.0.jar:/usr/share/java/kafka-serde-tools/rocksdbjni-6.22.1.1.jar:/usr/share/java/kafka-serde-tools/wire-schema-jvm-4.0.0.jar:/usr/share/java/kafka-serde-tools/commons-digester-1.8.1.jar:/usr/share/java/kafka-serde-tools/swagger-annotations-2.1.10.jar:/usr/share/java/kafka-serde-tools/kafka-protobuf-types-7.1.0.jar:/usr/share/java/kafka-serde-tools/failureaccess-1.0.1.jar:/usr/share/java/kafka-serde-tools/re2j-1.3.jar:/usr/share/java/kafka-serde-tools/kafka-protobuf-serializer-7.1.0.jar:/usr/share/java/kafka-serde-tools/kafka-connect-protobuf-converter-7.1.0.jar:/usr/share/java/kafka-serde-tools/kafka-schema-registry-client-7.1.0.jar:/usr/share/java/kafka-serde-tools/kotlin-stdlib-1.4.21.jar:/usr/share/java/kafka-serde-tools/kafka-protobuf-provider-7.1.0.jar:/usr/share/java/kafka-serde-tools/kafka-json-schema-serializer-7.1.0.jar:/usr/share/java/kafka-serde-tools/kafka-connect-avro-data-7.1.0.jar:/usr/share/java/kafka-serde-tools/protobuf-java-util-3.17.3.jar:/usr/share/java/kafka-serde-tools/jsr305-3.0.2.jar:/usr/share/java/kafka-serde-tools/validation-api-2.0.1.Final.jar:/usr/share/java/kafka-serde-tools/avro-1.11.0.jar:/usr/share/java/kafka-serde-tools/protobuf-java-3.17.3.jar:/usr/share/java/kafka-serde-tools/kotlin-stdlib-jdk7-1.5.31.jar:/usr/share/java/kafka-serde-tools/jackson-annotations-2.12.3.jar:/usr/share/java/kafka-serde-tools/gson-2.8.6.jar:/usr/share/java/kafka-serde-tools/kotlinx-coroutines-core-1.3.7.jar:/usr/share/java/kafka-serde-tools/slf4j-api-1.7.30.jar:/usr/share/java/kafka-serde-tools/jackson-module-parameter-names-2.12.3.jar:/usr/share/java/kafka-serde-tools/handy-uri-templates-2.1.8.jar:/usr/share/java/kafka-serde-tools/guava-30.1.1-jre.jar:/usr/share/java/kafka-serde-tools/jackson-core-2.12.3.jar:/usr/share/java/kafka-serde-tools/kotlin-script-runtime-1.4.21.jar:/usr/share/java/kafka-serde-tools/jackson-datatype-jsr310-2.12.3.jar:/usr/share/java/kafka-serde-tools/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/usr/share/java/kafka-serde-tools/classgraph-4.8.21.jar:/usr/share/java/kafka-serde-tools/commons-compress-1.21.jar:/usr/share/java/kafka-serde-tools/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/usr/share/java/kafka-serde-tools/kotlin-stdlib-common-1.5.31.jar:/usr/share/java/kafka-serde-tools/jackson-datatype-joda-2.12.3.jar:/usr/share/java/monitoring-interceptors/monitoring-interceptors-7.1.0.jar:/usr/bin/../ce-broker-plugins/build/libs/*:/usr/bin/../ce-broker-plugins/build/dependant-libs/*:/usr/bin/../ce-auth-providers/build/libs/*:/usr/bin/../ce-auth-providers/build/dependant-libs/*:/usr/bin/../ce-rest-server/build/libs/*:/usr/bin/../ce-rest-server/build/dependant-libs/*:/usr/bin/../ce-audit/build/libs/*:/usr/bin/../ce-audit/build/dependant-libs/*:/usr/bin/../share/java/kafka/tink-1.6.0.jar:/usr/bin/../share/java/kafka/snakeyaml-1.27.jar:/usr/bin/../share/java/kafka/aws-java-sdk-s3-1.11.988.jar:/usr/bin/../share/java/kafka/rbac-7.1.0-ce.jar:/usr/bin/../share/java/kafka/azure-storage-blob-12.12.0.jar:/usr/bin/../share/java/kafka/cloudevents-kafka-2.0.0.jar:/usr/bin/../share/java/kafka/httpclient-4.5.13.jar:/usr/bin/../share/java/kafka/kotlin-stdlib-jdk8-1.5.31.jar:/usr/bin/../share/java/kafka/connect-runtime-7.1.0-ce.jar:/usr/bin/../share/java/kafka/trogdor-7.1.0-ce.jar:/usr/bin/../share/java/kafka/commons-logging-1.2.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/netty-transport-classes-epoll-4.1.73.Final.jar:/usr/bin/../share/java/kafka/google-api-services-storage-v1-rev20210127-1.32.1.jar:/usr/bin/../share/java/kafka/kafka-streams-7.1.0-ce.jar:/usr/bin/../share/java/kafk
connect          | a/netty-transport-sctp-4.1.73.Final.jar:/usr/bin/../share/java/kafka/KeePassJava2-dom-2.1.4.jar:/usr/bin/../share/java/kafka/reactor-netty-http-1.0.7.jar:/usr/bin/../share/java/kafka/confluent-log4j-1.2.17-cp10.jar:/usr/bin/../share/java/kafka/netty-codec-mqtt-4.1.73.Final.jar:/usr/bin/../share/java/kafka/netty-transport-rxtx-4.1.73.Final.jar:/usr/bin/../share/java/kafka/google-cloud-core-http-2.1.3.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/kafka/zipkin-reporter-brave-2.16.3.jar:/usr/bin/../share/java/kafka/google-http-client-apache-v2-1.40.0.jar:/usr/bin/../share/java/kafka/simpleclient_tracer_otel-0.12.0.jar:/usr/bin/../share/java/kafka/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.6.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/core-1.54.0.0.jar:/usr/bin/../share/java/kafka/confluent-audit-7.1.0-ce.jar:/usr/bin/../share/java/kafka/google-http-client-1.40.0.jar:/usr/bin/../share/java/kafka/telemetry-client-1.745.0.jar:/usr/bin/../share/java/kafka/brave-instrumentation-http-5.13.3.jar:/usr/bin/../share/java/kafka/google-auth-library-credentials-1.1.0.jar:/usr/bin/../share/java/kafka/security-extensions-7.1.0-ce.jar:/usr/bin/../share/java/kafka/netty-resolver-dns-native-macos-4.1.73.Final-osx-aarch_64.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.73.Final.jar:/usr/bin/../share/java/kafka/netty-codec-memcache-4.1.73.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/netty-codec-http-4.1.73.Final.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/kafka/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/kafka/checker-qual-3.8.0.jar:/usr/bin/../share/java/kafka/netty-resolver-dns-classes-macos-4.1.73.Final.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.73.Final.jar:/usr/bin/../share/java/kafka/lang-tag-1.5.jar:/usr/bin/../share/java/kafka/netty-tcnative-boringssl-static-2.0.46.Final.jar:/usr/bin/../share/java/kafka/simpleclient_tracer_common-0.12.0.jar:/usr/bin/../share/java/kafka/snakeyaml-1.29.jar:/usr/bin/../share/java/kafka/annotations-13.0.jar:/usr/bin/../share/java/kafka/msal4j-1.10.1.jar:/usr/bin/../share/java/kafka/azure-identity-1.3.3.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/jcip-annotations-1.0.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/zipkin-reporter-2.16.3.jar:/usr/bin/../share/java/kafka/netty-all-4.1.73.Final.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.1.jar:/usr/bin/../share/java/kafka/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/kafka/netty-handler-proxy-4.1.73.Final.jar:/usr/bin/../share/java/kafka/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/kafka/auto-service-1.0-rc7.jar:/usr/bin/../share/java/kafka/jackson-dataformat-cbor-2.12.3.jar:/usr/bin/../share/java/kafka/netty-codec-dns-4.1.73.Final.jar:/usr/bin/../share/java/kafka/threetenbp-1.5.1.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/accessors-smart-2.4.7.jar:/usr/bin/../share/java/kafka/cloudevents-core-2.0.0.jar:/usr/bin/../share/java/kafka/google-oauth-client-1.32.1.jar:/usr/bin/../share/java/kafka/api-common-2.0.2.jar:/usr/bin/../share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/kafka/okio-jvm-3.0.0.jar:/usr/bin/../share/java/kafka/httpcore-4.4.14.jar:/usr/bin/../share/java/kafka/telemetry-events-7.1.0-ce.jar:/usr/bin/../share/java/kafka/kotlin-stdlib-1.5.31.jar:/usr/bin/../share/java/kafka/internal-rest-server-7.1.0-ce.jar:/usr/bin/../share/java/kafka/kafka-raft-7.1.0-ce.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.4.4.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/javax.json-1.0.4.jar:/usr/bin/../share/java/kafka/google-auth-library-oauth2-http-1.1.0.jar:/usr/bin/../share/java/kafka/simpleclient-0.12.0.jar:/usr/bin/../share/java/kafka/msal4j-persistence-extension-1.1.0.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/connect-json-7.1.0-ce.jar:/usr/bin/../share/java/kafka/scala-library-2.13.6.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/swagger-annotations-1.6.3.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/j2objc-annotations-1.3.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.32.jar:/usr/bin/../share/java/kafka/netty-codec-stomp-4.1.73.Final.jar:/usr/bin/../share/java/kafka/ion-java-1.0.2.jar:/usr/bin/../share/java/kafka/google-api-services-cloudkms-v1-rev108-1.25.0.jar:/usr/bin/../share/java/kafka/jackson-dataformat-yaml-2.12.3.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.1.0-ce.jar:/usr/bin/../share/java/kafka/antlr-runtime-3.5.2.jar:/usr/bin/../share/java/kafka/netty-codec-xml-4.1.73.Final.jar:/usr/bin/../share/java/kafka/jackson-databind-2.12.3.jar:/usr/bin/../share/java/kafka/azure-core-1.18.0.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/connect-api-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/database-2.1.4.jar:/usr/bin/../share/java/kafka/netty-transport-udt-4.1.73.Final.jar:/usr/bin/../share/java/kafka/cloudevents-api-2.0.0.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/google-http-client-gson-1.40.0.jar:/usr/bin/../share/java/kafka/joda-time-2.9.9.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/commons-codec-1.15.jar:/usr/bin/../share/java/kafka/jmespath-java-1.11.988.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.0.jar:/usr/bin/../share/java/kafka/reactor-core-3.4.6.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.1.0-ce.jar:/usr/bin/../share/java/kafka/content-type-2.1.jar:/usr/bin/../share/java/kafka/jose4j-0.7.8.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/tink-gcpkms-1.6.0.jar:/usr/bin/../share/java/kafka/kafka-tools-7.1.0-ce.jar:/usr/bin/../share/java/kafka/KeePassJava2-kdbx-2.1.4.jar:/usr/bin/../share/java/kafka/client-java-api-14.0.0.jar:/usr/bin/../share/java/kafka/jbcrypt-0.4.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.21.jar:/usr/bin/../share/java/kafka/jsr305-3.0.1.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/connector-datapreview-extension-7.1.0-ce.jar:/usr/bin/../share/java/kafka/auto-common-0.10.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/KeePassJava2-2.1.4.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/error_prone_annotations-2.3.4.jar:/usr/bin/../share/java/kafka/authorizer-7.1.0-ce.jar:/usr/bin/../share/java/kafka/rocksdbjni-6.22.1.1.jar:/usr/bin/../share/java/kafka/simpleclient_httpserver-0.12.0.jar:/usr/bin/../share/java/kafka/opencensus-api-0.28.0.jar:/usr/bin/../share/java/kafka/netty-transport-native-kqueue-4.1.73.Final-osx-aarch_64.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/gson-fire-1.8.5.jar:/usr/bin/../share/java/kafka/jcip-annotations-1.0-1.jar:/usr/bin/../share/java/kafka/netty-codec-haproxy-4.1.73.Final.jar:/usr/bin/../share/java/kafka/logging-interceptor-4.9.1.jar:/usr/bin/../share/java/kafka/cloudevents-json-jackson-2.0.0.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.12.3.jar:/usr/bin/../share/java/kafka/netty-tcnative-classes-2.0.46.Final.jar:/usr/bin/../share/java/kafka/aws-java-sdk-sts-1.11.988.jar:/usr/bin/../share/java/kafka/auth-providers-7.1.0-ce.jar:/usr/bin/../share/java/kafka/aws-java-sdk-core-1.11.988.jar:/usr/bin/../share/java/kafka/simpleclient_common-0.12.0.jar:/usr/bin/../share/java/kafka/commons-cli-1
connect          | .4.jar:/usr/bin/../share/java/kafka/ce-sbk_2.13-7.1.0-ce.jar:/usr/bin/../share/java/kafka/failureaccess-1.0.1.jar:/usr/bin/../share/java/kafka/brave-5.13.3.jar:/usr/bin/../share/java/kafka/stax2-api-4.2.1.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.73.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/confluent-serializers-new-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/client-java-14.0.0.jar:/usr/bin/../share/java/kafka/netty-transport-classes-kqueue-4.1.73.Final.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.1.0-ce.jar:/usr/bin/../share/java/kafka/connect-transforms-7.1.0-ce.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.1.0-ce.jar:/usr/bin/../share/java/kafka/confluent-licensing-new-7.1.0-ce.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/kafka/annotations-15.0.jar:/usr/bin/../share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/kafka/commons-math3-3.6.1.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.1.0-ce.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/KeePassJava2-kdb-2.1.4.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/client-java-proto-14.0.0.jar:/usr/bin/../share/java/kafka/icu4j-61.1.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.1.0-ce.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.3.jar:/usr/bin/../share/java/kafka/logredactor-metrics-1.0.10.jar:/usr/bin/../share/java/kafka/gax-2.4.1.jar:/usr/bin/../share/java/kafka/netty-codec-redis-4.1.73.Final.jar:/usr/bin/../share/java/kafka/aalto-xml-1.0.0.jar:/usr/bin/../share/java/kafka/reactor-netty-core-1.0.7.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/simpleclient_tracer_otel_agent-0.12.0.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/kafka/proto-google-iam-v1-1.1.0.jar:/usr/bin/../share/java/kafka/rest-authorizer-7.1.0-ce.jar:/usr/bin/../share/java/kafka/netty-codec-socks-4.1.73.Final.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/kafka/bcpkix-fips-1.0.3.jar:/usr/bin/../share/java/kafka/aws-java-sdk-kms-1.11.988.jar:/usr/bin/../share/java/kafka/jsr305-3.0.2.jar:/usr/bin/../share/java/kafka/kafka-shell-7.1.0-ce.jar:/usr/bin/../share/java/kafka/azure-core-http-netty-1.10.1.jar:/usr/bin/../share/java/kafka/broker-plugins-7.1.0-ce.jar:/usr/bin/../share/java/kafka/connect-mirror-7.1.0-ce.jar:/usr/bin/../share/java/kafka/auto-service-annotations-1.0-rc7.jar:/usr/bin/../share/java/kafka/kafka-storage-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/confluent-resource-names-7.1.0-ce.jar:/usr/bin/../share/java/kafka/commons-io-2.11.0.jar:/usr/bin/../share/java/kafka/telemetry-api-1.745.0.jar:/usr/bin/../share/java/kafka/flatbuffers-java-1.9.0.jar:/usr/bin/../share/java/kafka/protobuf-java-3.17.3.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.1.0-ce.jar:/usr/bin/../share/java/kafka/opencensus-contrib-http-util-0.28.0.jar:/usr/bin/../share/java/kafka/kotlin-stdlib-jdk7-1.5.31.jar:/usr/bin/../share/java/kafka/antlr4-4.9.2.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/proto-google-common-protos-2.5.0.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.1.0-ce.jar:/usr/bin/../share/java/kafka/gson-2.8.6.jar:/usr/bin/../share/java/kafka/netty-codec-http2-4.1.73.Final.jar:/usr/bin/../share/java/kafka/commons-collections4-4.4.jar:/usr/bin/../share/java/kafka/google-cloud-core-2.1.3.jar:/usr/bin/../share/java/kafka/oauth2-oidc-sdk-9.7.jar:/usr/bin/../share/java/kafka/gax-httpjson-0.89.1.jar:/usr/bin/../share/java/kafka/netty-resolver-dns-4.1.73.Final.jar:/usr/bin/../share/java/kafka/connect-ce-logs-7.1.0-ce.jar:/usr/bin/../share/java/kafka/netty-handler-proxy-4.1.65.Final.jar:/usr/bin/../share/java/kafka/minimal-json-0.9.5.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/ST4-4.3.jar:/usr/bin/../share/java/kafka/okhttp-4.9.1.jar:/usr/bin/../share/java/kafka/org.abego.treelayout.core-1.0.3.jar:/usr/bin/../share/java/kafka/re2j-1.6.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/json-smart-2.4.7.jar:/usr/bin/../share/java/kafka/httpcore-4.4.13.jar:/usr/bin/../share/java/kafka/jline-3.12.1.jar:/usr/bin/../share/java/kafka/woodstox-core-6.2.4.jar:/usr/bin/../share/java/kafka/KeePassJava2-jaxb-2.1.4.jar:/usr/bin/../share/java/kafka/bctls-fips-1.0.10.jar:/usr/bin/../share/java/kafka/google-api-client-1.32.1.jar:/usr/bin/../share/java/kafka/antlr4-runtime-4.9.2.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/kafka/azure-storage-common-12.12.0.jar:/usr/bin/../share/java/kafka/netty-transport-native-kqueue-4.1.73.Final-osx-x86_64.jar:/usr/bin/../share/java/kafka/KeePassJava2-simple-2.1.4.jar:/usr/bin/../share/java/kafka/google-http-client-jackson2-1.40.0.jar:/usr/bin/../share/java/kafka/netty-codec-smtp-4.1.73.Final.jar:/usr/bin/../share/java/kafka/logredactor-1.0.10.jar:/usr/bin/../share/java/kafka/grpc-context-1.40.1.jar:/usr/bin/../share/java/kafka/google-http-client-appengine-1.40.0.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/jackson-core-2.12.3.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.1.0-ce.jar:/usr/bin/../share/java/kafka/kafka-client-plugins-7.1.0-ce.jar:/usr/bin/../share/java/kafka/annotations-3.0.1.jar:/usr/bin/../share/java/kafka/jackson-dataformat-xml-2.12.3.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.12.3.jar:/usr/bin/../share/java/kafka/jackson-datatype-jsr310-2.12.3.jar:/usr/bin/../share/java/kafka/nimbus-jose-jwt-9.9.3.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.1.0-ce.jar:/usr/bin/../share/java/kafka/netty-resolver-dns-native-macos-4.1.73.Final-osx-x86_64.jar:/usr/bin/../share/java/kafka/reactor-netty-http-brave-1.0.7.jar:/usr/bin/../share/java/kafka/asm-9.1.jar:/usr/bin/../share/java/kafka/zipkin-2.23.2.jar:/usr/bin/../share/java/kafka/google-cloud-storage-2.1.2.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/reactor-netty-1.0.7.jar:/usr/bin/../share/java/kafka/reactive-streams-1.0.3.jar:/usr/bin/../share/java/kafka/jna-5.6.0.jar:/usr/bin/../share/java/kafka/commons-compress-1.21.jar:/usr/bin/../share/java/kafka/guava-30.0-jre.jar:/usr/bin/../share/java/kafka/jackson-dataformat-properties-2.12.3.jar:/usr/bin/../share/java/kafka/auto-value-annotations-1.8.2.jar:/usr/bin/../share/java/kafka/bc-fips-1.0.2.jar:/usr/bin/../share/java/kafka/kotlin-stdlib-common-1.5.31.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/commons-lang3-3.11.jar:/usr/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/kafka/opencensus-proto-0.2.0.jar:/usr/bin/../share/java/kafka/azure-storage-internal-avro-12.0.5.jar:/usr/bin/../share/java/kafka/checker-qual-3.5.0.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.73.Final-linux-x86_64.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/jna-platform-5.6.0.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.
connect          | 1.73.Final-linux-aarch_64.jar:/usr/bin/../share/java/confluent-metadata-service/snakeyaml-1.27.jar:/usr/bin/../share/java/confluent-metadata-service/rbac-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/jose4j-0.7.2.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-kafka-2.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-classes-epoll-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/common-utils-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/auto-value-annotations-1.8.1.jar:/usr/bin/../share/java/confluent-metadata-service/lz4-java-1.8.0.jar:/usr/bin/../share/java/confluent-metadata-service/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/confluent-metadata-service/confluent-audit-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-client-1.745.0.jar:/usr/bin/../share/java/confluent-metadata-service/commons-lang3-3.8.1.jar:/usr/bin/../share/java/confluent-metadata-service/security-extensions-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/jcommander-1.72.jar:/usr/bin/../share/java/confluent-metadata-service/rbac-common-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/netty-codec-http-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/confluent-metadata-service/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/metrics-core-2.2.0.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/netty-handler-proxy-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-common-2.34.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-core-2.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.el-api-4.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-events-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/rbac-api-server-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/javax.json-1.0.4.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/confluent-metadata-service/confluent-security-plugins-common-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-dataformat-yaml-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/antlr-runtime-3.5.2.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-api-2.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/jbcrypt-0.4.jar:/usr/bin/../share/java/confluent-metadata-service/jul-to-slf4j-1.7.30.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-metadata-service/authorizer-client-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/authorizer-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/bsh-2.0b6.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-json-jackson-2.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/auth-providers-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/concurrent-trees-2.6.1.jar:/usr/bin/../share/java/confluent-metadata-service/testng-6.14.3.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-proxy-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-native-unix-common-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-classes-kqueue-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/kafka-server-common-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/usr/bin/../share/java/confluent-metadata-service/icu4j-61.1.jar:/usr/bin/../share/java/confluent-metadata-service/javax.servlet-api-4.0.1.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-server-2.34.jar:/usr/bin/../share/java/confluent-metadata-service/rest-authorizer-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/netty-codec-socks-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/confluent-metadata-service/ce-kafka-http-server-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/bcpkix-fips-1.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-bean-validation-2.34.jar:/usr/bin/../share/java/confluent-metadata-service/confluent-resource-names-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-api-1.745.0.jar:/usr/bin/../share/java/confluent-metadata-service/protobuf-java-3.17.3.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-client-2.34.jar:/usr/bin/../share/java/confluent-metadata-service/antlr4-4.9.2.jar:/usr/bin/../share/java/confluent-metadata-service/gson-2.8.6.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.el-3.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/confluent-metadata-service/ST4-4.3.jar:/usr/bin/../share/java/confluent-metadata-service/org.abego.treelayout.core-1.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/bctls-fips-1.0.10.jar:/usr/bin/../share/java/confluent-metadata-service/antlr4-runtime-4.9.2.jar:/usr/bin/../share/java/confluent-metadata-service/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-native-kqueue-4.1.73.Final-osx-x86_64.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-core-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/kafka-client-plugins-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/annotations-3.0.1.jar:/usr/bin/../share/java/confluent-metadata-service/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-datatype-jsr310-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/bc-fips-1.0.2.1.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-dataformat-properties-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-metadata-service/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/confluent-metadata-service/opencensus-proto-0.2.0.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-native-epoll-4.1.73.Final-linux-x86_64.jar:/usr/bin/../share/java/rest-utils/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/rest-utils/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/lz4-java-1.8.0.jar:/usr/bin/../share/java/rest-utils/asm-tree-9.2.jar:/usr/bin/../share/java/rest-utils/asm-analysis-9.2.jar:/usr/bin/../share/java/rest-utils/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/rest-utils/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/rest-utils/checker-qual-3.8.0.jar:/usr/bin/../share/java/rest-utils/jetty-alpn-java-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-jndi-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-xml-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/jetty-webapp-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jersey-common-2.34.jar:/usr/bin/../share/java/rest-utils/jakarta.el-api-4.0.0.jar:/usr/bin/../share/java/rest-utils/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/rest-utils/javax-websocket-client-impl-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/rest-utils/hk2-locator-2.6.1.jar:/usr/bin/../shar
connect          | e/java/rest-utils/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/rest-utils/j2objc-annotations-1.3.jar:/usr/bin/../share/java/rest-utils/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/rest-utils/websocket-client-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jackson-databind-2.12.3.jar:/usr/bin/../share/java/rest-utils/jetty-alpn-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/activation-1.1.1.jar:/usr/bin/../share/java/rest-utils/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/rest-utils/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/asm-9.2.jar:/usr/bin/../share/java/rest-utils/jboss-logging-3.3.2.Final.jar:/usr/bin/../share/java/rest-utils/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/hibernate-validator-6.1.7.Final.jar:/usr/bin/../share/java/rest-utils/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/hk2-utils-2.6.1.jar:/usr/bin/../share/java/rest-utils/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/rest-utils/websocket-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/failureaccess-1.0.1.jar:/usr/bin/../share/java/rest-utils/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/rest-utils/javassist-3.25.0-GA.jar:/usr/bin/../share/java/rest-utils/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/rest-utils/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/rest-utils-7.1.0.jar:/usr/bin/../share/java/rest-utils/javax-websocket-server-impl-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/kafka-clients-7.1.0-ccs.jar:/usr/bin/../share/java/rest-utils/websocket-api-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/rest-utils/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/rest-utils/websocket-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jersey-server-2.34.jar:/usr/bin/../share/java/rest-utils/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/websocket-common-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/hk2-api-2.6.1.jar:/usr/bin/../share/java/rest-utils/jersey-bean-validation-2.34.jar:/usr/bin/../share/java/rest-utils/jsr305-3.0.2.jar:/usr/bin/../share/java/rest-utils/jetty-jaas-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jersey-hk2-2.34.jar:/usr/bin/../share/java/rest-utils/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/rest-utils/asm-commons-9.2.jar:/usr/bin/../share/java/rest-utils/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/javax.websocket-client-api-1.0.jar:/usr/bin/../share/java/rest-utils/jersey-client-2.34.jar:/usr/bin/../share/java/rest-utils/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/rest-utils/classmate-1.3.4.jar:/usr/bin/../share/java/rest-utils/jakarta.el-3.0.3.jar:/usr/bin/../share/java/rest-utils/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/rest-utils/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/http2-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/http2-hpack-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/rest-utils/jetty-jmx-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/http2-common-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/guava-30.1.1-jre.jar:/usr/bin/../share/java/rest-utils/jackson-core-2.12.3.jar:/usr/bin/../share/java/rest-utils/jetty-annotations-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/rest-utils/javax.websocket-api-1.0.jar:/usr/bin/../share/java/rest-utils/jetty-plus-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jaxb-api-2.3.0.jar:/usr/bin/../share/java/rest-utils/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/rest-utils/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/confluent-common/common-utils-7.1.0.jar:/usr/bin/../share/java/confluent-common/common-config-7.1.0.jar:/usr/bin/../share/java/confluent-common/build-tools-7.1.0.jar:/usr/bin/../share/java/confluent-common/common-metrics-7.1.0.jar:/usr/bin/../share/java/confluent-common/slf4j-api-1.7.30.jar:/usr/bin/../share/java/ce-kafka-http-server/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/common-utils-7.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/lz4-java-1.8.0.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-tree-9.2.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-analysis-9.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/ce-kafka-http-server/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/ce-kafka-http-server/checker-qual-3.8.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-alpn-java-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-jndi-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-xml-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-webapp-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-common-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.el-api-4.0.0.jar:/usr/bin/../share/java/ce-kafka-http-server/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/ce-kafka-http-server/javax-websocket-client-impl-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/hk2-locator-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/ce-kafka-http-server/j2objc-annotations-1.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-client-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-databind-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-alpn-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/activation-1.1.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-9.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jboss-logging-3.3.2.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/hibernate-validator-6.1.7.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/hk2-utils-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/failureaccess-1.0.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/javassist-3.25.0-GA.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/rest-utils-7.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/javax-websocket-server-impl-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-api-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-resolver-4.1.73
connect          | .Final.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-server-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-common-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/hk2-api-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/ce-kafka-http-server-7.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-bean-validation-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/jsr305-3.0.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-jaas-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-hk2-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-commons-9.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.websocket-client-api-1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-client-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/classmate-1.3.4.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.el-3.0.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/http2-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/slf4j-api-1.7.30.jar:/usr/bin/../share/java/ce-kafka-http-server/http2-hpack-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-jmx-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/http2-common-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/guava-30.1.1-jre.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-core-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-annotations-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.websocket-api-1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-plus-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jaxb-api-2.3.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/ce-kafka-rest-servlet/ce-kafka-rest-servlet-7.1.0.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/common-utils-7.1.0.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/lz4-java-1.8.0.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/ce-kafka-rest-extensions-7.1.0.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/slf4j-api-1.7.30.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/kafka-rest-lib/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-server-common-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-jdk8-1.5.31.jar:/usr/bin/../share/java/kafka-rest-lib/commons-logging-1.2.jar:/usr/bin/../share/java/kafka-rest-lib/scala-reflect-2.13.5.jar:/usr/bin/../share/java/kafka-rest-lib/confluent-log4j-1.2.17-cp10.jar:/usr/bin/../share/java/kafka-rest-lib/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/common-utils-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-compiler-embeddable-1.3.50.jar:/usr/bin/../share/java/kafka-rest-lib/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-jvm-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/common-config-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/kafka-rest-lib/checker-qual-3.8.0.jar:/usr/bin/../share/java/kafka-rest-lib/annotations-13.0.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-avro-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka-rest-lib/commons-collections-3.2.2.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-common-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/proto-google-common-protos-2.5.1.jar:/usr/bin/../share/java/kafka-rest-lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/kafka-rest-lib/okio-jvm-3.0.0.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-common-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-json-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-schema-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/scala-collection-compat_2.13-2.4.4.jar:/usr/bin/../share/java/kafka-rest-lib/scala-library-2.13.5.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-rest-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/j2objc-annotations-1.3.jar:/usr/bin/../share/java/kafka-rest-lib/json-20201115.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-storage-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-metadata-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-raft-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/org.everit.json.schema-1.12.2.jar:/usr/bin/../share/java/kafka-rest-lib/paranamer-2.8.jar:/usr/bin/../share/java/kafka-rest-lib/scala-java8-compat_2.13-1.0.0.jar:/usr/bin/../share/java/kafka-rest-lib/commons-validator-1.6.jar:/usr/bin/../share/java/kafka-rest-lib/jose4j-0.7.8.jar:/usr/bin/../share/java/kafka-rest-lib/joda-time-2.10.8.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-json-schema-provider-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-guava-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-storage-api-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/spotbugs-annotations-4.3.0.jar:/usr/bin/../share/java/kafka-rest-lib/wire-runtime-jvm-4.0.0.jar:/usr/bin/../share/java/kafka-rest-lib/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka-rest-lib/wire-schema-jvm-4.0.0.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-dataformat-csv-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/commons-digester-1.8.1.jar:/usr/bin/../share/java/kafka-rest-lib/commons-cli-1.4.jar:/usr/bin/../share/java/kafka-rest-lib/swagger-annotations-2.1.10.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-protobuf-types-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/failureaccess-1.0.1.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-protobuf-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/vavr-match-0.10.2.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-schema-registry-client-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-clients-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka-rest-lib/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-protobuf-provider-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka-rest-lib/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka-rest-lib/scala-logging_2.13-3.9.3.jar:/usr/bin/../share/java/kafka-rest-lib/logredactor-metrics-1.0.10.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-json-schema-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/kafka-re
connect          | st-lib/jsr305-3.0.2.jar:/usr/bin/../share/java/kafka-rest-lib/validation-api-2.0.1.Final.jar:/usr/bin/../share/java/kafka-rest-lib/resilience4j-ratelimiter-1.7.1.jar:/usr/bin/../share/java/kafka-rest-lib/avro-1.11.0.jar:/usr/bin/../share/java/kafka-rest-lib/protobuf-java-3.17.3.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-jdk7-1.5.31.jar:/usr/bin/../share/java/kafka-rest-lib/auto-value-annotations-1.7.2.jar:/usr/bin/../share/java/kafka-rest-lib/gson-2.8.6.jar:/usr/bin/../share/java/kafka-rest-lib/kotlinx-coroutines-core-1.3.7.jar:/usr/bin/../share/java/kafka-rest-lib/resilience4j-core-1.7.1.jar:/usr/bin/../share/java/kafka-rest-lib/minimal-json-0.9.5.jar:/usr/bin/../share/java/kafka-rest-lib/re2j-1.6.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-module-parameter-names-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/kafka_2.13-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/kafka-rest-lib/handy-uri-templates-2.1.8.jar:/usr/bin/../share/java/kafka-rest-lib/guava-30.1.1-jre.jar:/usr/bin/../share/java/kafka-rest-lib/logredactor-1.0.10.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-core-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-script-runtime-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-module-scala_2.13-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-jsr310-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/usr/bin/../share/java/kafka-rest-lib/vavr-0.10.2.jar:/usr/bin/../share/java/kafka-rest-lib/classgraph-4.8.21.jar:/usr/bin/../share/java/kafka-rest-lib/commons-compress-1.21.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-joda-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/snakeyaml-1.27.jar:/usr/bin/../share/java/confluent-security/kafka-rest/cloudevents-kafka-2.0.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-runtime-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/lz4-java-1.8.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/telemetry-client-1.745.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/security-extensions-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-handler-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-codec-http-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/checker-qual-3.8.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/metrics-core-2.2.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/maven-artifact-3.8.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-common-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-transport-native-kqueue-4.1.65.Final-osx-x86_64.jar:/usr/bin/../share/java/confluent-security/kafka-rest/cloudevents-core-2.0.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/confluent-security/kafka-rest/telemetry-events-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-transport-native-epoll-4.1.65.Final-linux-x86_64.jar:/usr/bin/../share/java/confluent-security/kafka-rest/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/hk2-locator-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javax.json-1.0.4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-json-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/bcprov-jdk15on-1.68.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-security-plugins-common-7.1.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/reflections-0.9.12.jar:/usr/bin/../share/java/confluent-security/kafka-rest/j2objc-annotations-1.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-dataformat-yaml-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/antlr-runtime-3.5.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-databind-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/activation-1.1.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-api-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/confluent-security/kafka-rest/cloudevents-api-2.0.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jose4j-0.7.8.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/plexus-utils-3.2.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-tools-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jbcrypt-0.4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/hk2-utils-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/authorizer-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/cloudevents-json-jackson-2.0.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/bcpkix-jdk15on-1.68.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-tcnative-classes-2.0.46.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/swagger-annotations-2.1.10.jar:/usr/bin/../share/java/confluent-security/kafka-rest/failureaccess-1.0.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javassist-3.25.0-GA.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-serializers-new-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-schema-registry-client-7.1.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-transforms-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-licensing-new-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/argparse4j-0.7.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/icu4j-61.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-log4j-appender-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/commons-lang3-3.12.0.jar:/usr/bin/../share/java/confluent-security/kafk
connect          | a-rest/javax.servlet-api-4.0.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-server-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/rest-authorizer-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/hk2-api-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jsr305-3.0.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-resource-names-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/telemetry-api-1.745.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/avro-1.11.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/protobuf-java-3.17.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-codec-socks-4.1.65.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-hk2-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-kafka-rest-security-plugin-7.1.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-client-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/antlr4-4.9.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/gson-2.8.6.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-transport-native-unix-common-4.1.65.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-ce-logs-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-handler-proxy-4.1.65.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/ST4-4.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/org.abego.treelayout.core-1.0.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/antlr4-runtime-4.9.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/guava-30.1.1-jre.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-core-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-client-plugins-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/annotations-3.0.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jaxb-api-2.3.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/commons-compress-1.21.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/opencensus-proto-0.2.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/broker-plugins-7.1.0-ce-test.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-jdk8-1.5.31.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-logging-1.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/common-utils-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-compiler-embeddable-1.3.50.jar:/usr/bin/../share/java/confluent-security/schema-validator/lz4-java-1.8.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-jvm-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/schema-validator/checker-qual-3.8.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/annotations-13.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-avro-serializer-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-collections-3.2.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-common-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/proto-google-common-protos-2.5.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/confluent-security/schema-validator/okio-jvm-3.0.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-common-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-schema-serializer-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/scala-library-2.13.5.jar:/usr/bin/../share/java/confluent-security/schema-validator/j2objc-annotations-1.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/json-20201115.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-databind-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/org.everit.json.schema-1.12.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-validator-1.6.jar:/usr/bin/../share/java/confluent-security/schema-validator/joda-time-2.10.8.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-json-schema-provider-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-guava-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/wire-runtime-jvm-4.0.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/wire-schema-jvm-4.0.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-digester-1.8.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/swagger-annotations-2.1.10.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-protobuf-types-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/failureaccess-1.0.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-schema-registry-client-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/caffeine-2.8.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-protobuf-provider-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/jsr305-3.0.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/validation-api-2.0.1.Final.jar:/usr/bin/../share/java/confluent-security/schema-validator/avro-1.11.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/protobuf-java-3.17.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-jdk7-1.5.31.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/gson-2.8.6.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlinx-coroutines-core-1.3.7.jar:/usr/bin/../share/java/confluent-security/schema-validator/confluent-schema-registry-validator-plugin-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/re2j-1.6.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-module-parameter-names-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/confluent-security/schema-validator/handy-uri-templates-2.1.8.jar:/usr/bin/../share/java/confluent-security/schema-validator/guav
connect          | a-30.1.1-jre.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-core-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-script-runtime-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-jsr310-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/usr/bin/../share/java/confluent-security/schema-validator/classgraph-4.8.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-compress-1.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-joda-2.12.3.jar:/usr/bin/../support-metrics-client/build/dependant-libs-2.13.6/*:/usr/bin/../support-metrics-client/build/libs/*:/usr/bin/../share/java/confluent-telemetry/confluent-metrics-7.1.0-ce.jar:/usr/share/java/support-metrics-client/*
connect          | 	os.spec = Linux, amd64, 5.19.0-50-generic
connect          | 	os.vcpus = 4
connect          |  (org.apache.kafka.connect.runtime.WorkerInfo)
connect          | [2023-08-04 11:14:45,544] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectDistributed)
connect          | [2023-08-04 11:14:45,855] INFO Loading plugin from: /usr/share/java/cp-base-new (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
control-center   | SLF4J: Class path contains multiple SLF4J bindings.
control-center   | SLF4J: Found binding in [jar:file:/usr/share/java/acl/acl-7.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
control-center   | SLF4J: Found binding in [jar:file:/usr/share/java/confluent-control-center/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
control-center   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
control-center   | SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]
control-center   | [2023-08-04 11:14:53,354] WARN Invalid value 1 for configuration confluent.controlcenter.internal.topics.replication: Value must be at least 3 (io.confluent.controlcenter.ControlCenterConfig)
control-center   | [2023-08-04 11:14:53,370] WARN Invalid value 1 for configuration confluent.controlcenter.internal.topics.replication: Value must be at least 3 (io.confluent.controlcenter.ControlCenterConfig)
control-center   | [2023-08-04 11:14:53,388] INFO ControlCenterConfig values: 
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	confluent.controlcenter.alert.cluster.down.autocreate = false
control-center   | 	confluent.controlcenter.alert.cluster.down.send.rate = 12
control-center   | 	confluent.controlcenter.alert.cluster.down.to.email = 
control-center   | 	confluent.controlcenter.alert.cluster.down.to.pagerduty.integrationkey = 
control-center   | 	confluent.controlcenter.alert.cluster.down.to.webhookurl.slack = 
control-center   | 	confluent.controlcenter.alert.max.trigger.events = 1000
control-center   | 	confluent.controlcenter.armeria.healthcheck.force.http1 = false
control-center   | 	confluent.controlcenter.auth.bearer.issuer = Confluent
control-center   | 	confluent.controlcenter.auth.bearer.roles.claim = 
control-center   | 	confluent.controlcenter.auth.restricted.roles = []
control-center   | 	confluent.controlcenter.auth.session.expiration.ms = 0
control-center   | 	confluent.controlcenter.broker.config.edit.enable = true
control-center   | 	confluent.controlcenter.command.streams.start.timeout = 300000
control-center   | 	confluent.controlcenter.command.topic = _confluent-command
control-center   | 	confluent.controlcenter.command.topic.replication = 1
control-center   | 	confluent.controlcenter.command.topic.retention.ms = 86400000
control-center   | 	confluent.controlcenter.command.topic.segment.bytes = 1073741824
control-center   | 	confluent.controlcenter.connect.connect.alias.name = 
control-center   | 	confluent.controlcenter.connect.healthcheck.endpoint = /v1/metadata/id
control-center   | 	confluent.controlcenter.consumer.metadata.timeout.ms = 15000
control-center   | 	confluent.controlcenter.consumers.view.enable = true
control-center   | 	confluent.controlcenter.data.dir = /var/lib/confluent-control-center
control-center   | 	confluent.controlcenter.deprecated.views.enable = false
control-center   | 	confluent.controlcenter.disk.skew.warning.min.bytes = 1073741824
control-center   | 	confluent.controlcenter.embedded.kafkarest.enable = true
control-center   | 	confluent.controlcenter.hostedmonitoring.enable = false
control-center   | 	confluent.controlcenter.id = 1
control-center   | 	confluent.controlcenter.internal.streams.start.timeout = 21600000
control-center   | 	confluent.controlcenter.internal.topics.changelog.segment.bytes = 134217728
control-center   | 	confluent.controlcenter.internal.topics.partitions = 1
control-center   | 	confluent.controlcenter.internal.topics.replication = 1
control-center   | 	confluent.controlcenter.internal.topics.retention.bytes = -1
control-center   | 	confluent.controlcenter.internal.topics.retention.ms = 604800000
control-center   | 	confluent.controlcenter.ksql.enable = true
control-center   | 	confluent.controlcenter.ksql.ksql.alias.name = 
control-center   | 	confluent.controlcenter.license.manager = _confluent-controlcenter-license-manager-7-4-1
control-center   | 	confluent.controlcenter.license.manager.enable = true
control-center   | 	confluent.controlcenter.mail.bounce.address = 
control-center   | 	confluent.controlcenter.mail.enabled = false
control-center   | 	confluent.controlcenter.mail.from = c3@confluent.io
control-center   | 	confluent.controlcenter.mail.host.name = localhost
control-center   | 	confluent.controlcenter.mail.password = [hidden]
control-center   | 	confluent.controlcenter.mail.port = 587
control-center   | 	confluent.controlcenter.mail.ssl.checkserveridentity = false
control-center   | 	confluent.controlcenter.mail.ssl.port = 465
control-center   | 	confluent.controlcenter.mail.starttls.required = false
control-center   | 	confluent.controlcenter.mail.username = 
control-center   | 	confluent.controlcenter.mds.client.idle.timeout = null
control-center   | 	confluent.controlcenter.mds.client.max.requests.queued.per.destination = null
control-center   | 	confluent.controlcenter.mode.enable = all
control-center   | 	confluent.controlcenter.name = _confluent-controlcenter
control-center   | 	confluent.controlcenter.private.installer.id = 
control-center   | 	confluent.controlcenter.proactive.support.ui.cta.enable = true
control-center   | 	confluent.controlcenter.purge.stale.cluster.enable = false
control-center   | 	confluent.controlcenter.request.buffer.size.bytes = 10000
control-center   | 	confluent.controlcenter.rest.advertised.url = 
control-center   | 	confluent.controlcenter.rest.compression.enable = true
control-center   | 	confluent.controlcenter.rest.csrf.prevention.enable = false
control-center   | 	confluent.controlcenter.rest.csrf.prevention.token.endpoint = /csrf
control-center   | 	confluent.controlcenter.rest.csrf.prevention.token.expiration.minutes = 30
control-center   | 	confluent.controlcenter.rest.hsts.enable = true
control-center   | 	confluent.controlcenter.rest.nosniff.prevention.enable = true
control-center   | 	confluent.controlcenter.rest.port = 9021
control-center   | 	confluent.controlcenter.rest.proxy.alias.name = 
control-center   | 	confluent.controlcenter.sbk.ui.enable = true
control-center   | 	confluent.controlcenter.schema.registry.enable = true
control-center   | 	confluent.controlcenter.schema.registry.schema.registry.alias.name = 
control-center   | 	confluent.controlcenter.schema.registry.url = [http://schema-registry:8081]
control-center   | 	confluent.controlcenter.service.healthcheck.interval.sec = 20
control-center   | 	confluent.controlcenter.streams.cache.max.bytes.buffering = 1073741824
control-center   | 	confluent.controlcenter.streams.consumer.session.timeout.ms = 60000
control-center   | 	confluent.controlcenter.streams.num.stream.threads = 12
control-center   | 	confluent.controlcenter.streams.producer.compression.type = lz4
control-center   | 	confluent.controlcenter.streams.producer.delivery.timeout.ms = 2147483647
control-center   | 	confluent.controlcenter.streams.producer.linger.ms = 500
control-center   | 	confluent.controlcenter.streams.producer.max.block.ms = 9223372036854775807
control-center   | 	confluent.controlcenter.streams.producer.retries = 2147483647
control-center   | 	confluent.controlcenter.streams.producer.retry.backoff.ms = 100
control-center   | 	confluent.controlcenter.streams.task.timeout.ms = 0
control-center   | 	confluent.controlcenter.streams.upgrade.from = 2.3
control-center   | 	confluent.controlcenter.topic.inspection.enable = true
control-center   | 	confluent.controlcenter.topic.inspection.message.max.bytes = 1048576
control-center   | 	confluent.controlcenter.trigger.active-controller-count.enable = false
control-center   | 	confluent.controlcenter.ui.acl.kafkarest.enable = false
control-center   | 	confluent.controlcenter.ui.autoupdate.enable = true
control-center   | 	confluent.controlcenter.ui.basepath = /
control-center   | 	confluent.controlcenter.ui.broker.kafkarest.enable = false
control-center   | 	confluent.controlcenter.ui.brokersettings.kafkarest.enable = true
control-center   | 	confluent.controlcenter.ui.consumer.group.kafkarest.enable = false
control-center   | 	confluent.controlcenter.ui.controller.chart.enable = false
control-center   | 	confluent.controlcenter.ui.data.expired.threshold = 120
control-center   | 	confluent.controlcenter.ui.external.css.files = null
control-center   | 	confluent.controlcenter.ui.external.js.files = null
control-center   | 	confluent.controlcenter.ui.replicator.monitoring.enable = true
control-center   | 	confluent.controlcenter.ui.topic.kafkarest.enable = false
control-center   | 	confluent.controlcenter.usage.data.collection.enable = true
control-center   | 	confluent.controlcenter.use.default.jvm.truststore = false
control-center   | 	confluent.controlcenter.use.default.os.truststore = false
control-center   | 	confluent.controlcenter.webhook.enabled = true
control-center   | 	confluent.license = 
control-center   | 	confluent.metadata.basic.auth.user.info = [hidden]
control-center   | 	confluent.metadata.bootstrap.server.urls = []
control-center   | 	confluent.metadata.cluster.registry.enable = false
control-center   | 	confluent.metadata.cluster.registry.merge.configuration.enable = true
control-center   | 	confluent.metrics.broker.count.staleness.threshold.ms = 120000
control-center   | 	confluent.metrics.topic = _confluent-metrics
control-center   | 	confluent.metrics.topic.config.validate = false
control-center   | 	confluent.metrics.topic.max.message.bytes = 10485760
control-center   | 	confluent.metrics.topic.partitions = 12
control-center   | 	confluent.metrics.topic.replication = 1
control-center   | 	confluent.metrics.topic.retention.bytes = -1
control-center   | 	confluent.metrics.topic.retention.ms = 259200000
control-center   | 	confluent.metrics.topic.skip.backlog.minutes = 15
control-center   | 	confluent.monitoring.interceptor.topic = _confluent-monitoring
control-center   | 	confluent.monitoring.interceptor.topic.config.validate = false
control-center   | 	confluent.monitoring.interceptor.topic.partitions = 1
control-center   | 	confluent.monitoring.interceptor.topic.replication = 1
control-center   | 	confluent.monitoring.interceptor.topic.retention.bytes = -1
control-center   | 	confluent.monitoring.interceptor.topic.retention.ms = 259200000
control-center   | 	confluent.monitoring.interceptor.topic.skip.backlog.minutes = 15
control-center   | 	confluent.support.metrics.enable = true
control-center   | 	confluent.support.metrics.segment.endpoint = https://analytics-api.confluent.io
control-center   | 	confluent.support.metrics.segment.id = MORqDG61F2eE5mfxAXVqpEblmFG18nbv
control-center   | 	public.key.path = 
control-center   | 	zookeeper.connect = 
control-center   |  (io.confluent.controlcenter.ControlCenterConfig)
connect          | [2023-08-04 11:15:03,751] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/cp-base-new/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:03,772] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:03,774] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:03,785] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:03,786] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:03,787] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:03,815] INFO Loading plugin from: /usr/share/java/confluent-control-center (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
control-center   | [2023-08-04 11:15:05,632] INFO Capturing metrics for topic names _confluent-monitoring _confluent-metrics (io.confluent.controlcenter.streams.WindowExtractor)
control-center   | [2023-08-04 11:15:05,683] INFO transformerStore=MonitoringVerifierStore (io.confluent.controlcenter.streams.StreamsModule)
control-center   | [2023-08-04 11:15:05,717] INFO transformerStore=MonitoringTriggerStore (io.confluent.controlcenter.streams.StreamsModule)
control-center   | [2023-08-04 11:15:05,717] INFO transformerStore=TriggerActionsStore (io.confluent.controlcenter.streams.StreamsModule)
control-center   | [2023-08-04 11:15:05,718] INFO transformerStore=TriggerEventsStore (io.confluent.controlcenter.streams.StreamsModule)
control-center   | [2023-08-04 11:15:05,720] INFO transformerStore=AlertHistoryStore (io.confluent.controlcenter.streams.StreamsModule)
control-center   | [2023-08-04 11:15:05,821] INFO StreamsConfig values: 
control-center   | 	acceptable.recovery.lag = 10000
control-center   | 	application.id = _confluent-controlcenter-7-4-1-1
control-center   | 	application.server = 
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffered.records.per.partition = 100
control-center   | 	built.in.metrics.version = latest
control-center   | 	cache.max.bytes.buffering = 1073741824
control-center   | 	client.id = 
control-center   | 	commit.interval.ms = 30000
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndContinueExceptionHandler
control-center   | 	default.dsl.store = rocksDB
control-center   | 	default.key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
control-center   | 	default.list.key.serde.inner = null
control-center   | 	default.list.key.serde.type = null
control-center   | 	default.list.value.serde.inner = null
control-center   | 	default.list.value.serde.type = null
control-center   | 	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
control-center   | 	default.timestamp.extractor = class io.confluent.controlcenter.streams.WindowExtractor
control-center   | 	default.value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
control-center   | 	max.task.idle.ms = 0
control-center   | 	max.warmup.replicas = 2
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	num.standby.replicas = 0
control-center   | 	num.stream.threads = 12
control-center   | 	poll.ms = 100
control-center   | 	probing.rebalance.interval.ms = 600000
control-center   | 	processing.guarantee = at_least_once
control-center   | 	rack.aware.assignment.tags = []
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	repartition.purge.interval.ms = 30000
control-center   | 	replication.factor = 1
control-center   | 	request.timeout.ms = 40000
control-center   | 	retries = 0
control-center   | 	retry.backoff.ms = 100
control-center   | 	rocksdb.config.setter = class io.confluent.controlcenter.streams.RocksDBConfigurator
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	send.buffer.bytes = 131072
control-center   | 	state.cleanup.delay.ms = 600000
control-center   | 	state.dir = /var/lib/confluent-control-center/1/kafka-streams
control-center   | 	statestore.cache.max.bytes = 10485760
control-center   | 	task.timeout.ms = 0
control-center   | 	topology.optimization = all
control-center   | 	upgrade.from = 2.3
control-center   | 	window.size.ms = null
control-center   | 	windowed.inner.class.serde = null
control-center   | 	windowstore.changelog.additional.retention.ms = 86400000
control-center   |  (org.apache.kafka.streams.StreamsConfig)
control-center   | [2023-08-04 11:15:06,159] INFO ProducerConfig values: 
control-center   | 	acks = -1
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	batch.size = 16384
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffer.memory = 33554432
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = confluent-control-center-heartbeat-sender-1
control-center   | 	compression.type = lz4
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	delivery.timeout.ms = 2147483647
control-center   | 	enable.idempotence = true
control-center   | 	interceptor.classes = []
control-center   | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   | 	linger.ms = 500
control-center   | 	max.block.ms = 9223372036854775807
control-center   | 	max.in.flight.requests.per.connection = 1
control-center   | 	max.request.size = 10485760
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metadata.max.idle.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partitioner.adaptive.partitioning.enable = true
control-center   | 	partitioner.availability.timeout.ms = 0
control-center   | 	partitioner.class = null
control-center   | 	partitioner.ignore.keys = false
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 500
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	transaction.timeout.ms = 60000
control-center   | 	transactional.id = null
control-center   | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center   | [2023-08-04 11:15:06,669] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center   | [2023-08-04 11:15:07,938] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:07,939] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:07,940] INFO Kafka startTimeMs: 1691147707889 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:09,280] INFO Fetching bootstrap cluster id (io.confluent.controlcenter.BootstrapClusterIdSupplier)
control-center   | [2023-08-04 11:15:09,329] INFO AdminClientConfig values: 
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = 
control-center   | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	confluent.use.controller.listener = false
control-center   | 	connections.max.idle.ms = 300000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center   | [2023-08-04 11:15:10,191] WARN These configurations '[consumer.session.timeout.ms, producer.max.block.ms, producer.retries, upgrade.from, producer.retry.backoff.ms, producer.linger.ms, producer.delivery.timeout.ms, task.timeout.ms, cache.max.bytes.buffering, producer.compression.type, num.stream.threads]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center   | [2023-08-04 11:15:10,192] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:10,192] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:10,192] INFO Kafka startTimeMs: 1691147710192 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:16,527] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:15:16,680] INFO Bootstrap cluster id MkU3OEVBNTcwNTJENDM2Qg (io.confluent.controlcenter.BootstrapClusterIdSupplier)
control-center   | [2023-08-04 11:15:16,737] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] ProducerId set to 0 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center   | [2023-08-04 11:15:22,078] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center   | 	metrics.jmx.prefix = confluent.controlcenter
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = true
control-center   | 	port = 9021
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:15:22,092] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center   | 	metrics.jmx.prefix = confluent.controlcenter
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = true
control-center   | 	port = 9021
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:15:22,130] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = []
control-center   | 	metrics.jmx.prefix = rest-utils
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = false
control-center   | 	port = 8080
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:15:22,181] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = []
control-center   | 	metrics.jmx.prefix = rest-utils
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = false
control-center   | 	port = 8080
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:15:22,214] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = []
control-center   | 	metrics.jmx.prefix = rest-utils
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = false
control-center   | 	port = 8080
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:15:22,222] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = []
control-center   | 	metrics.jmx.prefix = rest-utils
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = false
control-center   | 	port = 8080
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:15:22,225] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = []
control-center   | 	metrics.jmx.prefix = rest-utils
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = false
control-center   | 	port = 8080
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:15:22,417] INFO com.linecorp.armeria.verboseExceptions: rate-limit=10 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:22,459] INFO com.linecorp.armeria.preferredIpV4Addresses:  (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:22,641] INFO com.linecorp.armeria.verboseSocketExceptions: false (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:22,642] INFO com.linecorp.armeria.verboseResponses: false (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:22,643] INFO com.linecorp.armeria.warnNettyVersions: true (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:22,907] WARN Inconsistent Netty versions detected: {netty-buffer=netty-buffer-4.1.86.Final.cde0e2d050, netty-codec=netty-codec-4.1.86.Final.cde0e2d050, netty-codec-dns=netty-codec-dns-4.1.86.Final.cde0e2d050, netty-codec-http=netty-codec-http-4.1.86.Final.cde0e2d050, netty-codec-http2=netty-codec-http2-4.1.86.Final.cde0e2d (repository: dirty), netty-codec-socks=netty-codec-socks-4.1.86.Final.cde0e2d (repository: dirty), netty-common=netty-common-4.1.86.Final.cde0e2d050, netty-handler=netty-handler-4.1.86.Final.cde0e2d050, netty-handler-proxy=netty-handler-proxy-4.1.92.Final.acc3525 (repository: dirty), netty-resolver=netty-resolver-4.1.86.Final.cde0e2d050, netty-resolver-dns=netty-resolver-dns-4.1.86.Final.cde0e2d050, netty-resolver-dns-classes-macos=netty-resolver-dns-classes-macos-4.1.86.Final.cde0e2d050, netty-resolver-dns-native-macos=netty-resolver-dns-native-macos-4.1.86.Final.cde0e2d050, netty-transport=netty-transport-4.1.86.Final.cde0e2d050, netty-transport-classes-epoll=netty-transport-classes-epoll-4.1.86.Final.cde0e2d (repository: dirty), netty-transport-classes-kqueue=netty-transport-classes-kqueue-4.1.86.Final.cde0e2d050, netty-transport-native-epoll=netty-transport-native-epoll-4.1.86.Final.cde0e2d (repository: dirty), netty-transport-native-kqueue=netty-transport-native-kqueue-4.1.86.Final.cde0e2d050, netty-transport-native-unix-common=netty-transport-native-unix-common-4.1.86.Final.cde0e2d050} This means 1) you specified Netty versions inconsistently in your build or 2) the Netty JARs in the classpath were repackaged or shaded incorrectly. Specify the '-Dcom.linecorp.armeria.warnNettyVersions=false' JVM option to disable this warning at the risk of unexpected Netty behavior, if you think it is a false positive. (com.linecorp.armeria.internal.common.util.TransportTypeProvider)
control-center   | [2023-08-04 11:15:23,674] INFO com.linecorp.armeria.useEpoll: false (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,680] INFO com.linecorp.armeria.transportType: nio (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,750] INFO com.linecorp.armeria.maxNumConnections: 2147483647 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,752] INFO com.linecorp.armeria.numCommonWorkers: 8 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,753] INFO com.linecorp.armeria.numCommonBlockingTaskThreads: 200 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,776] INFO com.linecorp.armeria.defaultMaxRequestLength: 10485760 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,787] INFO com.linecorp.armeria.defaultMaxResponseLength: 10485760 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,788] INFO com.linecorp.armeria.defaultRequestTimeoutMillis: 10000 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,790] INFO com.linecorp.armeria.defaultResponseTimeoutMillis: 15000 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,798] INFO com.linecorp.armeria.defaultConnectTimeoutMillis: 3200 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,800] INFO com.linecorp.armeria.defaultWriteTimeoutMillis: 1000 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,801] INFO com.linecorp.armeria.defaultServerIdleTimeoutMillis: 15000 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,803] INFO com.linecorp.armeria.defaultClientIdleTimeoutMillis: 10000 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,805] INFO com.linecorp.armeria.defaultPingIntervalMillis: 0 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,824] INFO com.linecorp.armeria.defaultMaxServerNumRequestsPerConnection: 0 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,830] INFO com.linecorp.armeria.defaultMaxClientNumRequestsPerConnection: 0 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,837] INFO com.linecorp.armeria.defaultMaxServerConnectionAgeMillis: 0 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,850] INFO com.linecorp.armeria.defaultMaxClientConnectionAgeMillis: 0 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,867] INFO com.linecorp.armeria.defaultServerConnectionDrainDurationMicros: 1000000 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,878] INFO com.linecorp.armeria.defaultHttp2InitialConnectionWindowSize: 1048576 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,908] INFO com.linecorp.armeria.defaultHttp2InitialStreamWindowSize: 1048576 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,923] INFO com.linecorp.armeria.defaultHttp2MaxFrameSize: 16384 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,926] INFO com.linecorp.armeria.defaultHttp2MaxStreamsPerConnection: 2147483647 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,934] INFO com.linecorp.armeria.defaultHttp2MaxHeaderListSize: 8192 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,951] INFO com.linecorp.armeria.defaultHttp1MaxInitialLineLength: 4096 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,971] INFO com.linecorp.armeria.defaultHttp1MaxHeaderSize: 8192 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,974] INFO com.linecorp.armeria.defaultHttp1MaxChunkSize: 8192 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:23,996] INFO com.linecorp.armeria.defaultUseHttp2Preface: true (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,000] INFO com.linecorp.armeria.defaultUseHttp1Pipelining: false (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,005] INFO com.linecorp.armeria.defaultBackoffSpec: exponential=200:10000,jitter=0.2 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,009] INFO com.linecorp.armeria.defaultMaxTotalAttempts: 10 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,020] INFO com.linecorp.armeria.routeCache: maximumSize=4096 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,021] INFO com.linecorp.armeria.routeDecoratorCache: maximumSize=4096 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,025] INFO com.linecorp.armeria.parsedPathCache: maximumSize=4096 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,026] INFO com.linecorp.armeria.headerValueCache: maximumSize=4096 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,034] INFO com.linecorp.armeria.cachedHeaders: :authority,:scheme,:method,accept-encoding,content-type (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,036] INFO com.linecorp.armeria.fileServiceCache: maximumSize=1024 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,044] INFO com.linecorp.armeria.dnsCacheSpec: maximumSize=4096 (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,049] INFO com.linecorp.armeria.annotatedServiceExceptionVerbosity: unhandled (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,062] INFO com.linecorp.armeria.useJdkDnsResolver: false (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,068] INFO com.linecorp.armeria.reportBlockedEventLoop: true (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,069] INFO com.linecorp.armeria.validateHeaders: true (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,071] INFO com.linecorp.armeria.tlsAllowUnsafeCiphers: false (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,073] INFO com.linecorp.armeria.transientServiceOptions:  (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,140] INFO com.linecorp.armeria.useLegacyRouteDecoratorOrdering: false (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,141] INFO com.linecorp.armeria.useDefaultSocketOptions: true (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:24,148] INFO Using nio (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:25,810] INFO IPv6: disabled (from /proc/sys/net/ipv6/conf/all/disable_ipv6) (com.linecorp.armeria.common.util.SystemInfo)
control-center   | [2023-08-04 11:15:27,066] INFO com.linecorp.armeria.useOpenSsl: true (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:29,712] INFO Using OpenSSL: BoringSSL, 0x1010107f (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:29,713] INFO com.linecorp.armeria.dumpOpenSslInfo: false (default) (com.linecorp.armeria.common.Flags)
control-center   | [2023-08-04 11:15:32,277] INFO HV000001: Hibernate Validator null (org.hibernate.validator.internal.util.Version)
control-center   | [2023-08-04 11:15:34,789] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center   | 	metrics.jmx.prefix = confluent.controlcenter
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = true
control-center   | 	port = 9021
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:15:35,958] INFO getPersistentStoreTopicNames=[_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center   | [2023-08-04 11:15:36,036] INFO getLruStoreTopicNames=[_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center   | [2023-08-04 11:15:36,043] INFO getWindowedStoreTopicNames=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center   | [2023-08-04 11:15:36,062] INFO getLogAppendTimeIntermediateTopicNames=[_confluent-controlcenter-7-4-1-1-cluster-rekey, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store] (io.confluent.controlcenter.ControlCenterModule)
control-center   | [2023-08-04 11:15:36,087] INFO intermediateTopics=[_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center   | [2023-08-04 11:15:36,109] INFO StreamsConfig values: 
control-center   | 	acceptable.recovery.lag = 10000
control-center   | 	application.id = _confluent-controlcenter-7-4-1-1-command
control-center   | 	application.server = 
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffered.records.per.partition = 1000
control-center   | 	built.in.metrics.version = latest
control-center   | 	cache.max.bytes.buffering = 0
control-center   | 	client.id = 
control-center   | 	commit.interval.ms = 30000
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
control-center   | 	default.dsl.store = rocksDB
control-center   | 	default.key.serde = null
control-center   | 	default.list.key.serde.inner = null
control-center   | 	default.list.key.serde.type = null
control-center   | 	default.list.value.serde.inner = null
control-center   | 	default.list.value.serde.type = null
control-center   | 	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
control-center   | 	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
control-center   | 	default.value.serde = null
control-center   | 	max.task.idle.ms = 0
control-center   | 	max.warmup.replicas = 2
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	num.standby.replicas = 0
control-center   | 	num.stream.threads = 1
control-center   | 	poll.ms = 100
control-center   | 	probing.rebalance.interval.ms = 600000
control-center   | 	processing.guarantee = at_least_once
control-center   | 	rack.aware.assignment.tags = []
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	repartition.purge.interval.ms = 30000
control-center   | 	replication.factor = -1
control-center   | 	request.timeout.ms = 40000
control-center   | 	retries = 0
control-center   | 	retry.backoff.ms = 100
control-center   | 	rocksdb.config.setter = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	send.buffer.bytes = 131072
control-center   | 	state.cleanup.delay.ms = 600000
control-center   | 	state.dir = /var/lib/confluent-control-center/1/cp-command
control-center   | 	statestore.cache.max.bytes = 10485760
control-center   | 	task.timeout.ms = 0
control-center   | 	topology.optimization = all
control-center   | 	upgrade.from = 2.3
control-center   | 	window.size.ms = null
control-center   | 	windowed.inner.class.serde = null
control-center   | 	windowstore.changelog.additional.retention.ms = 86400000
control-center   |  (org.apache.kafka.streams.StreamsConfig)
control-center   | [2023-08-04 11:15:36,306] WARN Deprecated config cache.max.bytes.buffering is set, and will be used; we suggest setting the new config statestore.cache.max.bytes instead as deprecated cache.max.bytes.buffering would be removed in the future. (org.apache.kafka.streams.internals.StreamsConfigUtils)
control-center   | [2023-08-04 11:15:36,537] INFO No process id found on disk, got fresh process id 131145ad-beaf-40ab-9f2f-4f67571c7c4c (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center   | [2023-08-04 11:15:37,400] INFO AdminClientConfig values: 
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-admin
control-center   | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	confluent.use.controller.listener = false
control-center   | 	connections.max.idle.ms = 300000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center   | [2023-08-04 11:15:37,534] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:37,534] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:37,547] INFO Kafka startTimeMs: 1691147737534 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:37,557] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c] Kafka Streams version: 7.4.1-ce (org.apache.kafka.streams.KafkaStreams)
control-center   | [2023-08-04 11:15:37,558] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c] Kafka Streams commit ID: 96cc303d3f85bf31 (org.apache.kafka.streams.KafkaStreams)
control-center   | [2023-08-04 11:15:37,850] WARN Deprecated config cache.max.bytes.buffering is set, and will be used; we suggest setting the new config statestore.cache.max.bytes instead as deprecated cache.max.bytes.buffering would be removed in the future. (org.apache.kafka.streams.internals.StreamsConfigUtils)
control-center   | [2023-08-04 11:15:37,938] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:15:37,971] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = true
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-restore-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = null
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 300000
control-center   | 	max.poll.records = 1000
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:15:38,559] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:38,560] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:38,560] INFO Kafka startTimeMs: 1691147738543 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:38,689] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:15:38,718] INFO ProducerConfig values: 
control-center   | 	acks = -1
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	batch.size = 16384
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffer.memory = 33554432
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-producer
control-center   | 	compression.type = lz4
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	delivery.timeout.ms = 2147483647
control-center   | 	enable.idempotence = true
control-center   | 	interceptor.classes = []
control-center   | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   | 	linger.ms = 500
control-center   | 	max.block.ms = 9223372036854775807
control-center   | 	max.in.flight.requests.per.connection = 5
control-center   | 	max.request.size = 10485760
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metadata.max.idle.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partitioner.adaptive.partitioning.enable = true
control-center   | 	partitioner.availability.timeout.ms = 0
control-center   | 	partitioner.class = null
control-center   | 	partitioner.ignore.keys = false
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	transaction.timeout.ms = 60000
control-center   | 	transactional.id = null
control-center   | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center   | [2023-08-04 11:15:38,755] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center   | [2023-08-04 11:15:38,987] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:38,988] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:38,988] INFO Kafka startTimeMs: 1691147738987 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:39,156] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:15:39,161] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:15:39,174] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-producer] ProducerId set to 1 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center   | [2023-08-04 11:15:39,216] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = false
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = _confluent-controlcenter-7-4-1-1-command
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 300000
control-center   | 	max.poll.records = 1000
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:15:39,402] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:15:39,402] WARN stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:15:39,655] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:39,657] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:39,659] INFO Kafka startTimeMs: 1691147739654 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:39,732] INFO ProducerConfig values: 
control-center   | 	acks = -1
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	batch.size = 16384
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffer.memory = 33554432
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = c3-command
control-center   | 	compression.type = lz4
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	delivery.timeout.ms = 2147483647
control-center   | 	enable.idempotence = true
control-center   | 	interceptor.classes = []
control-center   | 	key.serializer = class io.confluent.serializers.ProtoSerde
control-center   | 	linger.ms = 500
control-center   | 	max.block.ms = 9223372036854775807
control-center   | 	max.in.flight.requests.per.connection = 5
control-center   | 	max.request.size = 10485760
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metadata.max.idle.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partitioner.adaptive.partitioning.enable = true
control-center   | 	partitioner.availability.timeout.ms = 0
control-center   | 	partitioner.class = null
control-center   | 	partitioner.ignore.keys = false
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	transaction.timeout.ms = 60000
control-center   | 	transactional.id = null
control-center   | 	value.serializer = class io.confluent.serializers.ProtoSerde
control-center   |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center   | [2023-08-04 11:15:39,743] INFO [Producer clientId=c3-command] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center   | [2023-08-04 11:15:39,822] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:39,828] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:39,829] INFO Kafka startTimeMs: 1691147739821 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:39,919] INFO [Producer clientId=c3-command] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:15:39,924] INFO [Producer clientId=c3-command] ProducerId set to 2 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center   | [2023-08-04 11:15:40,029] INFO getPersistentStoreTopicNames=[_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center   | [2023-08-04 11:15:40,033] INFO getLruStoreTopicNames=[_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center   | [2023-08-04 11:15:40,052] INFO getWindowedStoreTopicNames=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center   | [2023-08-04 11:15:40,057] INFO getLogAppendTimeIntermediateTopicNames=[_confluent-controlcenter-7-4-1-1-cluster-rekey, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store] (io.confluent.controlcenter.ControlCenterModule)
control-center   | [2023-08-04 11:15:40,060] INFO intermediateTopics=[_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition] (io.confluent.controlcenter.ControlCenterModule)
control-center   | WARNING: An illegal reflective access operation has occurred
control-center   | WARNING: Illegal reflective access by retrofit2.Platform (file:/usr/share/java/acl/acl-7.4.1.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class,int)
control-center   | WARNING: Please consider reporting this to the maintainers of retrofit2.Platform
control-center   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
control-center   | WARNING: All illegal access operations will be denied in a future release
control-center   | 
control-center   | 
control-center   | [2023-08-04 11:15:43,119] INFO StreamsConfig values: 
control-center   | 	acceptable.recovery.lag = 10000
control-center   | 	application.id = _confluent-controlcenter-7-4-1-1-command
control-center   | 	application.server = 
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffered.records.per.partition = 1000
control-center   | 	built.in.metrics.version = latest
control-center   | 	cache.max.bytes.buffering = 0
control-center   | 	client.id = 
control-center   | 	commit.interval.ms = 30000
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
control-center   | 	default.dsl.store = rocksDB
control-center   | 	default.key.serde = null
control-center   | 	default.list.key.serde.inner = null
control-center   | 	default.list.key.serde.type = null
control-center   | 	default.list.value.serde.inner = null
control-center   | 	default.list.value.serde.type = null
control-center   | 	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
control-center   | 	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
control-center   | 	default.value.serde = null
control-center   | 	max.task.idle.ms = 0
control-center   | 	max.warmup.replicas = 2
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	num.standby.replicas = 0
control-center   | 	num.stream.threads = 1
control-center   | 	poll.ms = 100
control-center   | 	probing.rebalance.interval.ms = 600000
control-center   | 	processing.guarantee = at_least_once
control-center   | 	rack.aware.assignment.tags = []
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	repartition.purge.interval.ms = 30000
control-center   | 	replication.factor = -1
control-center   | 	request.timeout.ms = 40000
control-center   | 	retries = 0
control-center   | 	retry.backoff.ms = 100
control-center   | 	rocksdb.config.setter = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	send.buffer.bytes = 131072
control-center   | 	state.cleanup.delay.ms = 600000
control-center   | 	state.dir = /var/lib/confluent-control-center/1/cp-command
control-center   | 	statestore.cache.max.bytes = 10485760
control-center   | 	task.timeout.ms = 0
control-center   | 	topology.optimization = all
control-center   | 	upgrade.from = 2.3
control-center   | 	window.size.ms = null
control-center   | 	windowed.inner.class.serde = null
control-center   | 	windowstore.changelog.additional.retention.ms = 86400000
control-center   |  (org.apache.kafka.streams.StreamsConfig)
control-center   | [2023-08-04 11:15:43,245] INFO AdminClientConfig values: 
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1
control-center   | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	confluent.use.controller.listener = false
control-center   | 	connections.max.idle.ms = 300000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center   | [2023-08-04 11:15:43,322] WARN These configurations '[replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center   | [2023-08-04 11:15:43,329] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:43,329] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:43,330] INFO Kafka startTimeMs: 1691147743328 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:43,644] INFO App info kafka.admin.client for _confluent-controlcenter-license-manager-7-4-1-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:43,764] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:15:43,771] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:15:43,779] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:15:43,832] INFO Starting License Store (io.confluent.license.LicenseStore)
control-center   | [2023-08-04 11:15:43,833] INFO Starting KafkaBasedLog with topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
control-center   | [2023-08-04 11:15:43,835] INFO AdminClientConfig values: 
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1
control-center   | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	confluent.use.controller.listener = false
control-center   | 	connections.max.idle.ms = 300000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center   | [2023-08-04 11:15:43,953] WARN These configurations '[replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center   | [2023-08-04 11:15:43,954] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:43,956] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:43,967] INFO Kafka startTimeMs: 1691147743954 (org.apache.kafka.common.utils.AppInfoParser)
broker           | [2023-08-04 11:15:44,465] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:15:44,480] INFO [Controller 1] Created topic _confluent-command with topic ID AFaY0a_lSiO7tlDgMwz2qw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:15:44,482] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-command'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:15:44,483] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-command'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:15:44,485] INFO [Controller 1] Created partition _confluent-command-0 with topic ID AFaY0a_lSiO7tlDgMwz2qw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
control-center   | [2023-08-04 11:15:44,587] INFO App info kafka.admin.client for _confluent-controlcenter-license-manager-7-4-1-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
broker           | [2023-08-04 11:15:44,591] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:15:44,609] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-command-0) (kafka.server.ReplicaFetcherManager)
control-center   | [2023-08-04 11:15:44,615] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:15:44,636] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:15:44,639] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:15:44,644] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = true
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1-global-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = null
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 300000
control-center   | 	max.poll.records = 1000
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 120000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
broker           | [2023-08-04 11:15:44,656] INFO [Broker id=1] Creating new partition _confluent-command-0 with topic id AFaY0a_lSiO7tlDgMwz2qw. (state.change.logger)
control-center   | [2023-08-04 11:15:44,703] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:44,704] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:44,706] INFO Kafka startTimeMs: 1691147744703 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:44,861] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
broker           | [2023-08-04 11:15:44,932] INFO [LogLoader partition=_confluent-command-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
control-center   | [2023-08-04 11:15:44,935] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:15:44,936] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:15:44,936] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:15:44,965] INFO App info kafka.consumer for _confluent-controlcenter-license-manager-7-4-1-1-global-consumer unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:44,978] INFO ProducerConfig values: 
control-center   | 	acks = -1
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	batch.size = 16384
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffer.memory = 33554432
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1
control-center   | 	compression.type = lz4
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	delivery.timeout.ms = 2147483647
control-center   | 	enable.idempotence = false
control-center   | 	interceptor.classes = []
control-center   | 	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
control-center   | 	linger.ms = 500
control-center   | 	max.block.ms = 9223372036854775807
control-center   | 	max.in.flight.requests.per.connection = 1
control-center   | 	max.request.size = 10485760
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metadata.max.idle.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partitioner.adaptive.partitioning.enable = true
control-center   | 	partitioner.availability.timeout.ms = 0
control-center   | 	partitioner.class = null
control-center   | 	partitioner.ignore.keys = false
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
broker           | [2023-08-04 11:15:44,983] INFO Created log for partition _confluent-command-0 in /tmp/kraft-combined-logs/_confluent-command-0 with properties {cleanup.policy=compact, min.insync.replicas=1} (kafka.log.LogManager)
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	transaction.timeout.ms = 60000
control-center   | 	transactional.id = null
control-center   | 	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
control-center   |  (org.apache.kafka.clients.producer.ProducerConfig)
broker           | [2023-08-04 11:15:45,002] INFO [Partition _confluent-command-0 broker=1] No checkpointed highwatermark is found for partition _confluent-command-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:15:45,015] INFO [Partition _confluent-command-0 broker=1] Log loaded for partition _confluent-command-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:15:45,073] INFO [Broker id=1] Leader _confluent-command-0 with topic id Some(AFaY0a_lSiO7tlDgMwz2qw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center   | [2023-08-04 11:15:45,090] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:45,092] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:45,093] INFO Kafka startTimeMs: 1691147745090 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:45,115] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = true
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = earliest
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1-global-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = null
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 300000
control-center   | 	max.poll.records = 1000
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 120000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
broker           | [2023-08-04 11:15:45,184] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-command with new configuration : cleanup.policy -> compact,min.insync.replicas -> 1 (kafka.server.metadata.DynamicConfigPublisher)
control-center   | [2023-08-04 11:15:45,186] INFO [Producer clientId=_confluent-controlcenter-license-manager-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:15:45,203] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:45,209] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:45,210] INFO Kafka startTimeMs: 1691147745202 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:45,240] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:15:45,247] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Assigned to partition(s): _confluent-command-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:15:45,272] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Seeking to earliest offset of partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:15:45,323] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-command-0 to 0 since the associated topicId changed from null to AFaY0a_lSiO7tlDgMwz2qw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:15:45,496] INFO Finished reading KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
control-center   | [2023-08-04 11:15:45,498] INFO Started KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
control-center   | [2023-08-04 11:15:45,499] INFO Started License Store (io.confluent.license.LicenseStore)
control-center   | [2023-08-04 11:15:46,603] INFO AdminClientConfig values: 
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1
control-center   | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	confluent.use.controller.listener = false
control-center   | 	connections.max.idle.ms = 300000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center   | [2023-08-04 11:15:46,626] WARN These configurations '[replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center   | [2023-08-04 11:15:46,630] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:46,632] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:46,633] INFO Kafka startTimeMs: 1691147746628 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:46,773] INFO App info kafka.admin.client for _confluent-controlcenter-license-manager-7-4-1-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:46,793] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:15:46,793] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:15:46,794] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:15:47,615] INFO AdminClientConfig values: 
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-license-manager-7-4-1-1
control-center   | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	confluent.use.controller.listener = false
control-center   | 	connections.max.idle.ms = 300000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center   | [2023-08-04 11:15:47,630] WARN These configurations '[replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center   | [2023-08-04 11:15:47,630] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:47,630] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:47,631] INFO Kafka startTimeMs: 1691147747630 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:47,717] INFO App info kafka.admin.client for _confluent-controlcenter-license-manager-7-4-1-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:15:47,748] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:15:47,748] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:15:47,750] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:15:47,751] INFO License for single cluster, single node (io.confluent.license.LicenseManager)
control-center   | [2023-08-04 11:15:47,758] INFO License: Free Tier license for Confluent Enterprise. (io.confluent.controlcenter.license.LicenseModule)
control-center   | [2023-08-04 11:15:47,759] INFO License: Free Tier license for Confluent Enterprise. (io.confluent.controlcenter.license.LicenseModule)
control-center   | [2023-08-04 11:15:47,786] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center   | 	metrics.jmx.prefix = confluent.controlcenter
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = true
control-center   | 	port = 9021
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:15:52,400] INFO Reflections took 4527 ms to scan 5 urls, producing 215 keys and 847 values  (org.reflections.Reflections)
control-center   | [2023-08-04 11:15:52,709] INFO EventEmitterConfig values: 
control-center   |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
control-center   | [2023-08-04 11:15:52,716] INFO EventEmitterConfig values: 
control-center   |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
control-center   | [2023-08-04 11:15:52,897] INFO Linux CPU collector enabled: true (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center   | [2023-08-04 11:15:52,899] INFO Using cpu metric: io\.confluent\.kafka\.server/server/linux_system_cpu_utilization (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center   | [2023-08-04 11:15:53,048] INFO ConfluentTelemetryConfig values: 
control-center   | 	confluent.telemetry.api.key = null
control-center   | 	confluent.telemetry.api.secret = null
control-center   | 	confluent.telemetry.debug.enabled = false
control-center   | 	confluent.telemetry.enabled = false
control-center   | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
control-center   | 	confluent.telemetry.events.enable = true
control-center   | 	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*
control-center   | 	confluent.telemetry.metrics.collector.interval.ms = 60000
control-center   | 	confluent.telemetry.metrics.collector.slo.enabled = false
control-center   | 	confluent.telemetry.proxy.password = null
control-center   | 	confluent.telemetry.proxy.url = null
control-center   | 	confluent.telemetry.proxy.username = null
control-center   |  (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center   | [2023-08-04 11:15:53,073] INFO VolumeMetricsCollectorConfig values: 
control-center   | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
control-center   |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
control-center   | [2023-08-04 11:15:53,143] INFO HttpExporterConfig values: 
control-center   | 	api.key = null
control-center   | 	api.secret = null
control-center   | 	buffer.batch.duration.max.ms = null
control-center   | 	buffer.batch.items.max = null
control-center   | 	buffer.inflight.submissions.max = null
control-center   | 	buffer.pending.batches.max = null
control-center   | 	client.attempts.max = null
control-center   | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center   | 	client.compression = null
control-center   | 	client.connect.timeout.ms = null
control-center   | 	client.contentType = null
control-center   | 	client.request.timeout.ms = null
control-center   | 	client.retry.delay.seconds = null
control-center   | 	enabled = false
control-center   | 	events.enabled = true
control-center   | 	metrics.enabled = true
control-center   | 	metrics.include = null
control-center   | 	proxy.password = null
control-center   | 	proxy.url = null
control-center   | 	proxy.username = null
control-center   | 	type = http
control-center   |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
control-center   | [2023-08-04 11:15:53,154] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center   | [2023-08-04 11:15:53,165] INFO RemoteConfigConfiguration values: 
control-center   | 	enabled = true
control-center   | 	polling.interval.ms = 60000
control-center   |  (io.confluent.shaded.io.confluent.telemetry.config.remote.RemoteConfigConfiguration)
control-center   | [2023-08-04 11:15:53,360] WARN Ignoring redefinition of existing telemetry label controlcenter.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
control-center   | [2023-08-04 11:15:53,376] INFO ConfluentTelemetryConfig values: 
control-center   | 	confluent.telemetry.api.key = null
control-center   | 	confluent.telemetry.api.secret = null
control-center   | 	confluent.telemetry.debug.enabled = false
control-center   | 	confluent.telemetry.enabled = false
control-center   | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
control-center   | 	confluent.telemetry.events.enable = true
control-center   | 	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.controlcenter/.*(metrics_input_topic_progress|monitoring_input_topic_progress|misconfigured_topics|missing_topic_configurations|broker_log_persistent_dir|cluster_offline|streams_status|total_lag|request_latency|response_size|response_rate).*
control-center   | 	confluent.telemetry.metrics.collector.interval.ms = 60000
control-center   | 	confluent.telemetry.metrics.collector.slo.enabled = false
control-center   | 	confluent.telemetry.proxy.password = null
control-center   | 	confluent.telemetry.proxy.url = null
control-center   | 	confluent.telemetry.proxy.username = null
control-center   |  (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center   | [2023-08-04 11:15:53,377] INFO VolumeMetricsCollectorConfig values: 
control-center   | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
control-center   |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
control-center   | [2023-08-04 11:15:53,378] INFO HttpExporterConfig values: 
control-center   | 	api.key = null
control-center   | 	api.secret = null
control-center   | 	buffer.batch.duration.max.ms = null
control-center   | 	buffer.batch.items.max = null
control-center   | 	buffer.inflight.submissions.max = null
control-center   | 	buffer.pending.batches.max = null
control-center   | 	client.attempts.max = null
control-center   | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center   | 	client.compression = null
control-center   | 	client.connect.timeout.ms = null
control-center   | 	client.contentType = null
control-center   | 	client.request.timeout.ms = null
control-center   | 	client.retry.delay.seconds = null
control-center   | 	enabled = false
control-center   | 	events.enabled = true
control-center   | 	metrics.enabled = true
control-center   | 	metrics.include = null
control-center   | 	proxy.password = null
control-center   | 	proxy.url = null
control-center   | 	proxy.username = null
control-center   | 	type = http
control-center   |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
control-center   | [2023-08-04 11:15:53,379] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center   | [2023-08-04 11:15:53,380] INFO RemoteConfigConfiguration values: 
control-center   | 	enabled = true
control-center   | 	polling.interval.ms = 60000
control-center   |  (io.confluent.shaded.io.confluent.telemetry.config.remote.RemoteConfigConfiguration)
control-center   | [2023-08-04 11:15:53,380] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
control-center   | [2023-08-04 11:15:53,428] INFO EventLoggerConfig values: 
control-center   | 	event.logger.cloudevent.codec = structured
control-center   | 	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter
control-center   |  (io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig)
control-center   | [2023-08-04 11:15:53,497] INFO HttpExporterConfig values: 
control-center   | 	api.key = null
control-center   | 	api.secret = null
control-center   | 	buffer.batch.duration.max.ms = null
control-center   | 	buffer.batch.items.max = null
control-center   | 	buffer.inflight.submissions.max = null
control-center   | 	buffer.pending.batches.max = null
control-center   | 	client.attempts.max = null
control-center   | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center   | 	client.compression = null
control-center   | 	client.connect.timeout.ms = null
control-center   | 	client.request.timeout.ms = null
control-center   | 	client.retry.delay.seconds = null
control-center   | 	enabled = false
control-center   | 	events.enabled = true
control-center   | 	metrics.enabled = true
control-center   | 	proxy.password = null
control-center   | 	proxy.url = null
control-center   | 	proxy.username = null
control-center   | 	type = http
control-center   |  (io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig)
connect          | [2023-08-04 11:15:58,900] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-control-center/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,901] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,902] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,902] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,903] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,903] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,904] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,904] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,905] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,905] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,905] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,905] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,906] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,906] INFO Added plugin 'io.confluent.connect.json.JsonSchemaConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,906] INFO Added plugin 'io.confluent.connect.protobuf.ProtobufConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,906] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,906] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,907] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,907] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,907] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,908] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,911] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,911] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,912] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,912] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,912] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,912] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,912] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,913] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,913] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,913] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,913] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,913] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,913] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,914] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,914] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,914] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,914] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,914] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,914] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,915] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,915] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,915] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,915] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,916] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,916] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,917] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,918] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,919] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,921] INFO Added plugin 'io.confluent.kafka.secretregistry.client.config.provider.SecretConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,923] INFO Added plugin 'io.confluent.connect.security.ConnectSecurityExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:15:58,940] INFO Loading plugin from: /usr/share/java/schema-registry (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
control-center   | [2023-08-04 11:15:59,375] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter)
control-center   | [2023-08-04 11:15:59,800] INFO Logging initialized @77249ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
control-center   | [2023-08-04 11:16:00,237] INFO CONTROL CENTER UI
control-center   | 
control-center   | By using Control Center, subject to any license you may have with Confluent, you agree to the Confluent Data Protection Agreement.  In particular, please note that the version check feature of Control Center is enabled.
control-center   | 
control-center   | With this enabled, this instance is configured to collect and report certain data (version information, time stamped session IDs, instance ID, instance uptime, license key for subscription customers, IP address, and other product data)  to Confluent, Inc. ("Confluent") or its parent, subsidiaries, affiliates or service providers every hour.  By proceeding with `confluent.support.metrics.enable=true`, you agree to all such collection, transfer and use of Version information by Confluent. You can turn the version check feature off by setting `confluent.support.metrics.enable=false` in the Control Center configuration and restarting Control Center.  See the Confluent Enterprise documentation for further information.
control-center   |  (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | 
control-center   | [2023-08-04 11:16:00,381] INFO Starting Control Center version=7.4.1 (io.confluent.controlcenter.application.AllControlCenter)
control-center   | [2023-08-04 11:16:00,449] INFO topicListings=[(name=_confluent-command, topicId=AFaY0a_lSiO7tlDgMwz2qw, internal=false)] (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:00,457] INFO missingTopics=[_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, _confluent-metrics, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-cluster-rekey, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, _confluent-monitoring, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition] (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:00,460] INFO extantTopics=[_confluent-command] (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:00,502] INFO checking topicDescription=(name=_confluent-command, internal=false, partitions=(partition=0, leader=broker:29092 (id: 1 rack: null), replicas=broker:29092 (id: 1 rack: null), isr=broker:29092 (id: 1 rack: null)), authorizedOperations=null) (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:00,502] INFO found topic=_confluent-command with partitions=1 (io.confluent.controlcenter.KafkaHelper)
broker           | [2023-08-04 11:16:00,559] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:00,594] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition with topic ID FBg1W8OSQXWAXSTUaAxC7Q. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:00,600] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,611] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,614] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,619] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,632] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,636] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,638] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,639] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,640] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with topic ID FBg1W8OSQXWAXSTUaAxC7Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:00,677] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:00,684] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:00,687] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with topic id FBg1W8OSQXWAXSTUaAxC7Q. (state.change.logger)
broker           | [2023-08-04 11:16:00,757] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:00,776] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog with topic ID 1llU1igJTl2WtB6Ombk9uQ. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:00,781] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,793] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,794] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,796] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,803] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,802] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:00,828] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,832] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,834] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,837] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with topic ID 1llU1igJTl2WtB6Ombk9uQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:00,846] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:00,865] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:00,870] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:00,872] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with topic id Some(FBg1W8OSQXWAXSTUaAxC7Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:00,924] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:00,931] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog with topic ID S495LVuvRsuHe_-TfpTCVA. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:00,939] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,940] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,942] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,942] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,943] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,943] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,944] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,944] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:00,944] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with topic ID S495LVuvRsuHe_-TfpTCVA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:00,962] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:01,001] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:01,006] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:01,008] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with topic id 1llU1igJTl2WtB6Ombk9uQ. (state.change.logger)
broker           | [2023-08-04 11:16:01,058] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:01,099] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,115] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog with topic ID 9SrPkJvdQRKHmFqHX1yx4w. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,119] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,128] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,129] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,132] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,134] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,140] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,142] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,144] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,109] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:01,159] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with topic ID 9SrPkJvdQRKHmFqHX1yx4w and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,185] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:01,187] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:01,189] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with topic id Some(1llU1igJTl2WtB6Ombk9uQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:01,200] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:01,221] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,222] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:01,227] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:01,231] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with topic id S495LVuvRsuHe_-TfpTCVA. (state.change.logger)
broker           | [2023-08-04 11:16:01,223] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition with topic ID aFs6ffa2RL65YNQHKdNm3A. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,256] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,256] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,257] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,258] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,258] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,261] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,277] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,278] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,279] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with topic ID aFs6ffa2RL65YNQHKdNm3A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,357] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:01,379] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,380] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog with topic ID SXqs9z_IT0ycq4w-So2PtQ. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,381] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,385] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,388] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:01,398] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:01,398] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:01,399] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with topic id Some(S495LVuvRsuHe_-TfpTCVA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:01,388] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,404] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,424] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,425] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,426] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,427] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,428] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with topic ID SXqs9z_IT0ycq4w-So2PtQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,421] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:01,439] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:01,440] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:01,441] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with topic id 9SrPkJvdQRKHmFqHX1yx4w. (state.change.logger)
broker           | [2023-08-04 11:16:01,461] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:01,492] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:01,496] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:01,497] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:01,498] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with topic id Some(9SrPkJvdQRKHmFqHX1yx4w) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:01,514] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,520] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog with topic ID HoP6Lp2hR0mutBINDVd0SA. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,527] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,528] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,528] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,529] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,530] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,530] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,531] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,539] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,540] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with topic ID HoP6Lp2hR0mutBINDVd0SA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,524] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:01,630] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:01,631] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:01,639] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with topic id aFs6ffa2RL65YNQHKdNm3A. (state.change.logger)
broker           | [2023-08-04 11:16:01,689] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,695] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition with topic ID RkotBavzTyeTSSFDELtuFA. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,696] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,697] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,699] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,700] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,712] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,716] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,718] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,720] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,729] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with topic ID RkotBavzTyeTSSFDELtuFA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,726] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:01,753] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:01,755] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:01,756] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:01,757] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with topic id Some(aFs6ffa2RL65YNQHKdNm3A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:01,782] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:01,792] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:01,793] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:01,793] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with topic id SXqs9z_IT0ycq4w-So2PtQ. (state.change.logger)
broker           | [2023-08-04 11:16:01,815] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,842] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store with topic ID iJLqc_VsS6GtzjQhq4pkTQ. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,842] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,843] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,844] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,845] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,846] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,847] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with topic ID iJLqc_VsS6GtzjQhq4pkTQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,881] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:01,894] INFO Created log for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:01,903] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:01,915] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:01,917] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with topic id Some(SXqs9z_IT0ycq4w-So2PtQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:01,937] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,941] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog with topic ID O3OKZPZ5RfOh-XbP8Y4yyQ. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:01,942] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,943] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,943] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,944] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,945] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,953] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,972] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,973] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:01,975] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with topic ID O3OKZPZ5RfOh-XbP8Y4yyQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,023] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:02,032] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:02,043] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:02,044] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with topic id HoP6Lp2hR0mutBINDVd0SA. (state.change.logger)
broker           | [2023-08-04 11:16:02,114] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:02,120] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,126] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog with topic ID NZXeFHK0TkaUSTnQorwW-A. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,127] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,128] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,128] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,129] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,135] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,136] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,137] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,140] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:02,151] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:02,148] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,156] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with topic ID NZXeFHK0TkaUSTnQorwW-A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,159] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:02,161] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with topic id Some(HoP6Lp2hR0mutBINDVd0SA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:02,181] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:02,192] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:02,198] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:02,207] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with topic id RkotBavzTyeTSSFDELtuFA. (state.change.logger)
broker           | [2023-08-04 11:16:02,240] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:02,261] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,292] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition with topic ID pssqfmbBQmKndkFz4zchXw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,293] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,293] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,294] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,294] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,295] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,296] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,297] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,298] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,280] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:02,311] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with topic ID pssqfmbBQmKndkFz4zchXw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,313] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:02,379] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:02,380] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with topic id Some(RkotBavzTyeTSSFDELtuFA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:02,396] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:02,405] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:02,408] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:02,414] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with topic id iJLqc_VsS6GtzjQhq4pkTQ. (state.change.logger)
broker           | [2023-08-04 11:16:02,472] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:02,479] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,492] INFO [Controller 1] Created topic _confluent-metrics with topic ID YkgFeJSTQIKRJZkpXXhRjg. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,492] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,492] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:02,493] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration max.message.bytes to 10485760 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,504] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,506] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,507] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration retention.ms to 259200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,509] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,510] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-metrics'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,512] INFO [Controller 1] Created partition _confluent-metrics-0 with topic ID YkgFeJSTQIKRJZkpXXhRjg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,513] INFO [Controller 1] Created partition _confluent-metrics-1 with topic ID YkgFeJSTQIKRJZkpXXhRjg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,514] INFO [Controller 1] Created partition _confluent-metrics-2 with topic ID YkgFeJSTQIKRJZkpXXhRjg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,515] INFO [Controller 1] Created partition _confluent-metrics-3 with topic ID YkgFeJSTQIKRJZkpXXhRjg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,517] INFO [Controller 1] Created partition _confluent-metrics-4 with topic ID YkgFeJSTQIKRJZkpXXhRjg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,518] INFO [Controller 1] Created partition _confluent-metrics-5 with topic ID YkgFeJSTQIKRJZkpXXhRjg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,514] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:02,523] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:02,524] INFO [Controller 1] Created partition _confluent-metrics-6 with topic ID YkgFeJSTQIKRJZkpXXhRjg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,526] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with topic id Some(iJLqc_VsS6GtzjQhq4pkTQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:02,528] INFO [Controller 1] Created partition _confluent-metrics-7 with topic ID YkgFeJSTQIKRJZkpXXhRjg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,534] INFO [Controller 1] Created partition _confluent-metrics-8 with topic ID YkgFeJSTQIKRJZkpXXhRjg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,541] INFO [Controller 1] Created partition _confluent-metrics-9 with topic ID YkgFeJSTQIKRJZkpXXhRjg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,544] INFO [Controller 1] Created partition _confluent-metrics-10 with topic ID YkgFeJSTQIKRJZkpXXhRjg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,548] INFO [Controller 1] Created partition _confluent-metrics-11 with topic ID YkgFeJSTQIKRJZkpXXhRjg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,555] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:02,572] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:02,575] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:02,576] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with topic id O3OKZPZ5RfOh-XbP8Y4yyQ. (state.change.logger)
broker           | [2023-08-04 11:16:02,629] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,630] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog with topic ID qP4lTE8ATsmQfTcF8slryw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,631] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,632] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,635] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,653] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,654] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,655] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,656] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,657] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,657] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with topic ID qP4lTE8ATsmQfTcF8slryw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,662] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:02,711] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:02,742] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-cluster-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,745] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:02,758] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-cluster-rekey with topic ID RNee_9E6QoWuyPfJUZqYdg. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,772] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-cluster-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,773] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-cluster-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,774] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-cluster-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,775] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-cluster-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,776] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-cluster-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,777] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 with topic ID RNee_9E6QoWuyPfJUZqYdg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,778] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:02,780] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with topic id Some(O3OKZPZ5RfOh-XbP8Y4yyQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:02,791] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:02,800] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:02,819] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:02,828] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with topic id NZXeFHK0TkaUSTnQorwW-A. (state.change.logger)
broker           | [2023-08-04 11:16:02,836] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,840] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition with topic ID sAkAJV7KS4-GAs2bkdIKRA. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,841] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,843] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,845] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,848] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,864] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,868] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,873] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,878] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,879] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with topic ID sAkAJV7KS4-GAs2bkdIKRA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,937] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:02,954] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,956] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog with topic ID nzdecc30ShSdQcB04frvog. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,957] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,958] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,959] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,971] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,978] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,982] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,988] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,990] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:02,991] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with topic ID nzdecc30ShSdQcB04frvog and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:02,982] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:03,009] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:03,010] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:03,011] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with topic id Some(NZXeFHK0TkaUSTnQorwW-A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:03,034] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:03,041] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:03,058] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:03,069] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,074] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition with topic ID UygM9SIITY-KoEx4I2w6zw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,075] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,077] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,079] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,081] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,082] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,088] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,090] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with topic ID UygM9SIITY-KoEx4I2w6zw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,085] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with topic id pssqfmbBQmKndkFz4zchXw. (state.change.logger)
broker           | [2023-08-04 11:16:03,246] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,247] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog with topic ID 4A9-74zvQouR64zqhXh9FA. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,251] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,253] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,255] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,258] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,267] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,273] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,275] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,276] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,278] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with topic ID 4A9-74zvQouR64zqhXh9FA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,280] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:03,294] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:03,296] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:03,297] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:03,300] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with topic id Some(pssqfmbBQmKndkFz4zchXw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:03,352] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:03,366] INFO [Broker id=1] Transitioning 12 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:03,392] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-metrics-11, _confluent-metrics-9, _confluent-metrics-10, _confluent-metrics-7, _confluent-metrics-8, _confluent-metrics-5, _confluent-metrics-6, _confluent-metrics-3, _confluent-metrics-4, _confluent-metrics-1, _confluent-metrics-2, _confluent-metrics-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:03,394] INFO [Broker id=1] Creating new partition _confluent-metrics-11 with topic id YkgFeJSTQIKRJZkpXXhRjg. (state.change.logger)
broker           | [2023-08-04 11:16:03,430] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,478] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog with topic ID gFRdYcjWQsGSJ47CHoXyqw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,478] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,479] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,479] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,479] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,479] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,491] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,492] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,493] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,488] INFO [LogLoader partition=_confluent-metrics-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:03,493] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with topic ID gFRdYcjWQsGSJ47CHoXyqw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,506] INFO Created log for partition _confluent-metrics-11 in /tmp/kraft-combined-logs/_confluent-metrics-11 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:03,512] INFO [Partition _confluent-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-11 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:03,513] INFO [Partition _confluent-metrics-11 broker=1] Log loaded for partition _confluent-metrics-11 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:03,514] INFO [Broker id=1] Leader _confluent-metrics-11 with topic id Some(YkgFeJSTQIKRJZkpXXhRjg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:03,531] INFO [Broker id=1] Creating new partition _confluent-metrics-9 with topic id YkgFeJSTQIKRJZkpXXhRjg. (state.change.logger)
broker           | [2023-08-04 11:16:03,599] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,603] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition with topic ID pAGnLBBpRCSB7OJ5_G2CJA. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,604] INFO [LogLoader partition=_confluent-metrics-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:03,637] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,642] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,647] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,648] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,650] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,652] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,654] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,655] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,659] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with topic ID pAGnLBBpRCSB7OJ5_G2CJA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,657] INFO Created log for partition _confluent-metrics-9 in /tmp/kraft-combined-logs/_confluent-metrics-9 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:03,664] INFO [Partition _confluent-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-9 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:03,664] INFO [Partition _confluent-metrics-9 broker=1] Log loaded for partition _confluent-metrics-9 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:03,665] INFO [Broker id=1] Leader _confluent-metrics-9 with topic id Some(YkgFeJSTQIKRJZkpXXhRjg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:03,687] INFO [Broker id=1] Creating new partition _confluent-metrics-10 with topic id YkgFeJSTQIKRJZkpXXhRjg. (state.change.logger)
broker           | [2023-08-04 11:16:03,794] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,795] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition with topic ID yh8_-dziTp-s7IDqy8w3Rw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,799] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,803] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,806] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,811] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,813] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,814] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,815] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,818] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,819] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with topic ID yh8_-dziTp-s7IDqy8w3Rw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,831] INFO [LogLoader partition=_confluent-metrics-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:03,884] INFO Created log for partition _confluent-metrics-10 in /tmp/kraft-combined-logs/_confluent-metrics-10 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:03,886] INFO [Partition _confluent-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-10 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:03,898] INFO [Partition _confluent-metrics-10 broker=1] Log loaded for partition _confluent-metrics-10 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:03,899] INFO [Broker id=1] Leader _confluent-metrics-10 with topic id Some(YkgFeJSTQIKRJZkpXXhRjg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:03,911] INFO [Broker id=1] Creating new partition _confluent-metrics-7 with topic id YkgFeJSTQIKRJZkpXXhRjg. (state.change.logger)
broker           | [2023-08-04 11:16:03,974] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,983] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey with topic ID SR4XL91XSTmhEPn2HtkHYQ. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:03,993] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,994] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,996] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:03,996] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,006] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,010] INFO [LogLoader partition=_confluent-metrics-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:04,035] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with topic ID SR4XL91XSTmhEPn2HtkHYQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,044] INFO Created log for partition _confluent-metrics-7 in /tmp/kraft-combined-logs/_confluent-metrics-7 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:04,046] INFO [Partition _confluent-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-7 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:04,048] INFO [Partition _confluent-metrics-7 broker=1] Log loaded for partition _confluent-metrics-7 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:04,050] INFO [Broker id=1] Leader _confluent-metrics-7 with topic id Some(YkgFeJSTQIKRJZkpXXhRjg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:04,078] INFO [Broker id=1] Creating new partition _confluent-metrics-8 with topic id YkgFeJSTQIKRJZkpXXhRjg. (state.change.logger)
broker           | [2023-08-04 11:16:04,119] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,121] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey with topic ID Y_GSqt8QROagfczJg3r40g. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,124] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,125] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,128] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,130] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,132] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,134] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with topic ID Y_GSqt8QROagfczJg3r40g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,183] INFO [LogLoader partition=_confluent-metrics-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:04,214] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,217] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey with topic ID f6T0fWnJTtClUuyvYTjZBw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,221] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,223] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,225] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,228] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,229] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,231] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with topic ID f6T0fWnJTtClUuyvYTjZBw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,236] INFO Created log for partition _confluent-metrics-8 in /tmp/kraft-combined-logs/_confluent-metrics-8 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:04,243] INFO [Partition _confluent-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-8 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:04,244] INFO [Partition _confluent-metrics-8 broker=1] Log loaded for partition _confluent-metrics-8 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:04,265] INFO [Broker id=1] Leader _confluent-metrics-8 with topic id Some(YkgFeJSTQIKRJZkpXXhRjg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:04,298] INFO [Broker id=1] Creating new partition _confluent-metrics-5 with topic id YkgFeJSTQIKRJZkpXXhRjg. (state.change.logger)
broker           | [2023-08-04 11:16:04,313] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,314] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog with topic ID JdPg8CHIR2eSEEpP-4pgKA. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,325] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,326] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,328] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,329] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,330] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,332] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,334] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,336] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,341] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with topic ID JdPg8CHIR2eSEEpP-4pgKA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,392] INFO [LogLoader partition=_confluent-metrics-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:04,424] INFO Created log for partition _confluent-metrics-5 in /tmp/kraft-combined-logs/_confluent-metrics-5 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:04,429] INFO [Partition _confluent-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-5 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:04,431] INFO [Partition _confluent-metrics-5 broker=1] Log loaded for partition _confluent-metrics-5 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:04,436] INFO [Broker id=1] Leader _confluent-metrics-5 with topic id Some(YkgFeJSTQIKRJZkpXXhRjg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:04,446] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,455] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition with topic ID Pmp7XBFVTWm7zuSANCQ_8g. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,456] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,457] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,460] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,461] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,467] INFO [Broker id=1] Creating new partition _confluent-metrics-6 with topic id YkgFeJSTQIKRJZkpXXhRjg. (state.change.logger)
broker           | [2023-08-04 11:16:04,473] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,480] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,481] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,484] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,485] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with topic ID Pmp7XBFVTWm7zuSANCQ_8g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,508] INFO [LogLoader partition=_confluent-metrics-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:04,530] INFO Created log for partition _confluent-metrics-6 in /tmp/kraft-combined-logs/_confluent-metrics-6 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:04,532] INFO [Partition _confluent-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-6 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:04,533] INFO [Partition _confluent-metrics-6 broker=1] Log loaded for partition _confluent-metrics-6 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:04,534] INFO [Broker id=1] Leader _confluent-metrics-6 with topic id Some(YkgFeJSTQIKRJZkpXXhRjg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:04,545] INFO [Broker id=1] Creating new partition _confluent-metrics-3 with topic id YkgFeJSTQIKRJZkpXXhRjg. (state.change.logger)
broker           | [2023-08-04 11:16:04,600] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,604] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition with topic ID RifsNDOQSCG4j2fz_vLz5g. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,604] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,608] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,608] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,609] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,611] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,613] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,615] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,616] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,617] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with topic ID RifsNDOQSCG4j2fz_vLz5g and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,636] INFO [LogLoader partition=_confluent-metrics-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:04,685] INFO Created log for partition _confluent-metrics-3 in /tmp/kraft-combined-logs/_confluent-metrics-3 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:04,695] INFO [Partition _confluent-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-3 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:04,698] INFO [Partition _confluent-metrics-3 broker=1] Log loaded for partition _confluent-metrics-3 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:04,705] INFO [Broker id=1] Leader _confluent-metrics-3 with topic id Some(YkgFeJSTQIKRJZkpXXhRjg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:04,711] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,714] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog with topic ID oBNDr9MwSwyfz3UNlKj0Aw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,716] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,720] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,721] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,722] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,723] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,725] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,726] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,728] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,731] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with topic ID oBNDr9MwSwyfz3UNlKj0Aw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,744] INFO [Broker id=1] Creating new partition _confluent-metrics-4 with topic id YkgFeJSTQIKRJZkpXXhRjg. (state.change.logger)
broker           | [2023-08-04 11:16:04,810] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,814] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition with topic ID WdjVN2t-SM21TyDTgxyLhw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,816] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,817] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,819] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,820] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,824] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,828] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,829] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,830] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,836] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with topic ID WdjVN2t-SM21TyDTgxyLhw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,851] INFO [LogLoader partition=_confluent-metrics-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:04,886] INFO Created log for partition _confluent-metrics-4 in /tmp/kraft-combined-logs/_confluent-metrics-4 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:04,887] INFO [Partition _confluent-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-4 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:04,888] INFO [Partition _confluent-metrics-4 broker=1] Log loaded for partition _confluent-metrics-4 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:04,889] INFO [Broker id=1] Leader _confluent-metrics-4 with topic id Some(YkgFeJSTQIKRJZkpXXhRjg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:04,905] INFO [Broker id=1] Creating new partition _confluent-metrics-1 with topic id YkgFeJSTQIKRJZkpXXhRjg. (state.change.logger)
broker           | [2023-08-04 11:16:04,922] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,936] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition with topic ID 44Yp-2aPRK-xKM9zKUCEJw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:04,938] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,939] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,942] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,944] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,945] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,951] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,955] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,956] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:04,968] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with topic ID 44Yp-2aPRK-xKM9zKUCEJw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:05,258] INFO [LogLoader partition=_confluent-metrics-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:05,314] INFO Created log for partition _confluent-metrics-1 in /tmp/kraft-combined-logs/_confluent-metrics-1 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:05,329] INFO [Partition _confluent-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-1 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:05,336] INFO [Partition _confluent-metrics-1 broker=1] Log loaded for partition _confluent-metrics-1 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:05,333] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:05,403] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog with topic ID HSq3iWH_Q0G8WAozBnXiFQ. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:05,403] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,404] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,404] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,404] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,404] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,404] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,404] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,405] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,405] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with topic ID HSq3iWH_Q0G8WAozBnXiFQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:05,414] INFO [Broker id=1] Leader _confluent-metrics-1 with topic id Some(YkgFeJSTQIKRJZkpXXhRjg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:05,459] INFO [Broker id=1] Creating new partition _confluent-metrics-2 with topic id YkgFeJSTQIKRJZkpXXhRjg. (state.change.logger)
broker           | [2023-08-04 11:16:05,550] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:05,556] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog with topic ID I-AspoMlSU-I5nUZzPsT2Q. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:05,560] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,564] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,565] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,568] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,569] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,570] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,571] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,576] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,577] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with topic ID I-AspoMlSU-I5nUZzPsT2Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:05,590] INFO [LogLoader partition=_confluent-metrics-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:05,680] INFO Created log for partition _confluent-metrics-2 in /tmp/kraft-combined-logs/_confluent-metrics-2 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:05,680] INFO [Partition _confluent-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-2 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:05,681] INFO [Partition _confluent-metrics-2 broker=1] Log loaded for partition _confluent-metrics-2 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:05,681] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:05,692] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog with topic ID xpuJp62xRjKbInyyPCX1lQ. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:05,692] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,699] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,682] INFO [Broker id=1] Leader _confluent-metrics-2 with topic id Some(YkgFeJSTQIKRJZkpXXhRjg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:05,703] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,707] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,707] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,708] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,709] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,714] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,715] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with topic ID xpuJp62xRjKbInyyPCX1lQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:05,723] INFO [Broker id=1] Creating new partition _confluent-metrics-0 with topic id YkgFeJSTQIKRJZkpXXhRjg. (state.change.logger)
broker           | [2023-08-04 11:16:05,823] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:05,827] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey with topic ID RXByHOqASZ2CbNrZqK_AMw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:05,828] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,833] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,842] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,843] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,844] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:05,844] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with topic ID RXByHOqASZ2CbNrZqK_AMw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:05,837] INFO [LogLoader partition=_confluent-metrics-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:05,888] INFO Created log for partition _confluent-metrics-0 in /tmp/kraft-combined-logs/_confluent-metrics-0 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:05,929] INFO [Partition _confluent-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:05,933] INFO [Partition _confluent-metrics-0 broker=1] Log loaded for partition _confluent-metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:05,938] INFO [Broker id=1] Leader _confluent-metrics-0 with topic id Some(YkgFeJSTQIKRJZkpXXhRjg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:05,956] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-metrics with new configuration : cleanup.policy -> delete,max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:05,973] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:05,979] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:05,980] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with topic id qP4lTE8ATsmQfTcF8slryw. (state.change.logger)
broker           | [2023-08-04 11:16:06,014] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,020] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition with topic ID 3eA5D2OKTTmHRKZTjiQWtA. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,027] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,028] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,029] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,039] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,040] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,047] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,049] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,052] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,059] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with topic ID 3eA5D2OKTTmHRKZTjiQWtA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,043] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:06,093] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:06,105] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:06,111] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:06,112] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with topic id Some(qP4lTE8ATsmQfTcF8slryw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:06,128] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:06,136] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='604800000'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,156] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey with topic ID KabcuEMhTCOz9nkNgpIsHg. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,157] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:06,157] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,159] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,167] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,168] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey'): set configuration retention.ms to 604800000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,168] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,171] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with topic ID KabcuEMhTCOz9nkNgpIsHg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,177] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-cluster-rekey-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:06,178] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 with topic id RNee_9E6QoWuyPfJUZqYdg. (state.change.logger)
broker           | [2023-08-04 11:16:06,220] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-cluster-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:06,229] INFO Created log for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-cluster-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:06,234] INFO [Partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:06,234] INFO [Partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:06,237] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-cluster-rekey-0 with topic id Some(RNee_9E6QoWuyPfJUZqYdg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:06,316] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,317] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition with topic ID dlujEO5uTCeuzLFQQNtU0A. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,320] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,320] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,332] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,332] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,333] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,335] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,336] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,338] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,338] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with topic ID dlujEO5uTCeuzLFQQNtU0A and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,337] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-cluster-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:06,345] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:06,346] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:06,346] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with topic id sAkAJV7KS4-GAs2bkdIKRA. (state.change.logger)
broker           | [2023-08-04 11:16:06,412] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:06,444] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:06,455] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:06,457] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,475] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition with topic ID e66bfUqWTZWX8U4X4Ipjsw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,475] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,476] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,476] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,477] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,484] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,503] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,504] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,505] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,506] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with topic ID e66bfUqWTZWX8U4X4Ipjsw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,512] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:06,513] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with topic id Some(sAkAJV7KS4-GAs2bkdIKRA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:06,526] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:06,568] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,570] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog with topic ID 88agc7ovSmm9apNE4HN69Q. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,572] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,573] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,576] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,577] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,579] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,587] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,588] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,589] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,589] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with topic ID 88agc7ovSmm9apNE4HN69Q and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,594] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:06,601] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:06,602] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with topic id nzdecc30ShSdQcB04frvog. (state.change.logger)
broker           | [2023-08-04 11:16:06,674] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,676] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition with topic ID xT2oJSg6R6ONg7UwEIUFtA. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,678] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,678] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,679] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,679] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,680] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,680] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,681] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,684] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,685] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with topic ID xT2oJSg6R6ONg7UwEIUFtA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,689] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:06,716] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:06,728] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:06,733] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:06,748] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with topic id Some(nzdecc30ShSdQcB04frvog) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:06,757] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,764] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog with topic ID Trp1P4PASmedQMblOoysPQ. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,764] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,764] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,765] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,765] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,765] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,766] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,766] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,767] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,767] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with topic ID Trp1P4PASmedQMblOoysPQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,784] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:06,791] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:06,816] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:06,818] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with topic id UygM9SIITY-KoEx4I2w6zw. (state.change.logger)
broker           | [2023-08-04 11:16:06,818] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,828] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog with topic ID VHDKH6yPRKekarvjYHIjyw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,832] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,833] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,834] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,836] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,843] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,844] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,845] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,848] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,852] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with topic ID VHDKH6yPRKekarvjYHIjyw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,914] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,915] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog with topic ID No9nQpMqSCWKHXxTpgGjHA. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,916] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,929] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,930] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,930] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,932] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,933] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,934] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,937] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:06,938] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with topic ID No9nQpMqSCWKHXxTpgGjHA and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:06,941] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:06,953] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:06,985] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-monitoring', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete'), CreateableTopicConfig(name='message.timestamp.type', value='LogAppendTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:07,019] INFO [Controller 1] Created topic _confluent-monitoring with topic ID 8fgQicUSRVi7xJcKYztVRQ. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:07,021] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration cleanup.policy to delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,022] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration message.timestamp.type to LogAppendTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,023] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,023] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration retention.ms to 259200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,024] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,037] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-monitoring'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,038] INFO [Controller 1] Created partition _confluent-monitoring-0 with topic ID 8fgQicUSRVi7xJcKYztVRQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:07,042] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:07,046] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:07,048] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with topic id Some(UygM9SIITY-KoEx4I2w6zw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:07,076] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition with new configuration : cleanup.policy -> delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 604800000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:07,111] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:07,114] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:07,116] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with topic id 4A9-74zvQouR64zqhXh9FA. (state.change.logger)
broker           | [2023-08-04 11:16:07,118] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:07,124] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition with topic ID w47q9kvPRoGucrcFjBQ9fw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:07,127] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,135] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,136] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,137] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,138] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,139] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,141] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,141] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,142] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with topic ID w47q9kvPRoGucrcFjBQ9fw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:07,229] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact,delete'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='60566400000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='60566400000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:07,243] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition with topic ID 7J5ZVEciQBa64RcXLSLhrw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:07,247] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration cleanup.policy to compact,delete (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,249] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,248] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:07,256] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,257] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,258] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,259] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,259] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,260] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition'): set configuration delete.retention.ms to 60566400000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,261] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with topic ID 7J5ZVEciQBa64RcXLSLhrw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:07,266] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:07,276] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:07,277] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:07,277] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with topic id Some(4A9-74zvQouR64zqhXh9FA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:07,322] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='691200000'), CreateableTopicConfig(name='segment.bytes', value='134217728'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='691200000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:07,327] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition with topic ID 6s9RSwQ_RiqRp339_OPjrw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:07,328] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,329] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,335] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,336] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,338] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration segment.bytes to 134217728 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,339] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,340] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,341] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition'): set configuration delete.retention.ms to 691200000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,342] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with topic ID 6s9RSwQ_RiqRp339_OPjrw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:07,344] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:07,412] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='432000000'), CreateableTopicConfig(name='segment.bytes', value='67108864'), CreateableTopicConfig(name='message.timestamp.difference.max.ms', value='9223372036854775807'), CreateableTopicConfig(name='retention.bytes', value='-1'), CreateableTopicConfig(name='delete.retention.ms', value='432000000')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:07,413] INFO [Controller 1] Created topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition with topic ID I_0cUFAkR3umlIgUR1kaaw. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:07,415] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,417] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,418] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,419] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,420] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration segment.bytes to 67108864 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,422] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration message.timestamp.difference.max.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,423] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,424] INFO [Controller 1] ConfigResource(type=TOPIC, name='_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition'): set configuration delete.retention.ms to 432000000 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:07,426] INFO [Controller 1] Created partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with topic ID I_0cUFAkR3umlIgUR1kaaw and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:07,454] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:07,464] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
control-center   | [2023-08-04 11:16:07,463] INFO describing topics=[_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, _confluent-metrics, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-cluster-rekey, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, _confluent-monitoring, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition] (io.confluent.controlcenter.KafkaHelper)
broker           | [2023-08-04 11:16:07,491] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with topic id gFRdYcjWQsGSJ47CHoXyqw. (state.change.logger)
broker           | [2023-08-04 11:16:07,590] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:07,621] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:07,630] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:07,651] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:07,656] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with topic id Some(gFRdYcjWQsGSJ47CHoXyqw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:07,667] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:07,674] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:07,675] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:07,686] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with topic id pAGnLBBpRCSB7OJ5_G2CJA. (state.change.logger)
broker           | [2023-08-04 11:16:07,761] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:07,774] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:07,789] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:07,799] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:07,812] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with topic id Some(pAGnLBBpRCSB7OJ5_G2CJA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:07,827] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:07,845] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:07,846] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:07,848] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with topic id yh8_-dziTp-s7IDqy8w3Rw. (state.change.logger)
broker           | [2023-08-04 11:16:07,951] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:07,976] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:07,979] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:07,979] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:07,980] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with topic id Some(yh8_-dziTp-s7IDqy8w3Rw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:07,995] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:08,026] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:08,034] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:08,043] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with topic id SR4XL91XSTmhEPn2HtkHYQ. (state.change.logger)
broker           | [2023-08-04 11:16:08,094] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:08,142] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:08,146] INFO [Partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:08,146] INFO [Partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:08,147] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with topic id Some(SR4XL91XSTmhEPn2HtkHYQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:08,167] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:08,173] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:08,174] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:08,175] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with topic id Y_GSqt8QROagfczJg3r40g. (state.change.logger)
broker           | [2023-08-04 11:16:08,207] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:08,219] INFO Created log for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:08,226] INFO [Partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:08,226] INFO [Partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:08,228] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with topic id Some(Y_GSqt8QROagfczJg3r40g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:08,244] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:08,258] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:08,259] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:08,259] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with topic id f6T0fWnJTtClUuyvYTjZBw. (state.change.logger)
broker           | [2023-08-04 11:16:08,290] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:08,309] INFO Created log for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:08,322] INFO [Partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:08,324] INFO [Partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:08,325] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with topic id Some(f6T0fWnJTtClUuyvYTjZBw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:08,357] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:08,363] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:08,376] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:08,378] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with topic id JdPg8CHIR2eSEEpP-4pgKA. (state.change.logger)
broker           | [2023-08-04 11:16:08,444] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:08,491] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:08,511] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:08,512] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:08,523] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with topic id Some(JdPg8CHIR2eSEEpP-4pgKA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:08,548] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:08,583] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:08,585] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:08,589] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with topic id Pmp7XBFVTWm7zuSANCQ_8g. (state.change.logger)
broker           | [2023-08-04 11:16:08,665] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:08,692] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:08,715] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:08,719] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:08,722] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with topic id Some(Pmp7XBFVTWm7zuSANCQ_8g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:08,734] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:08,739] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:08,747] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:08,749] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with topic id RifsNDOQSCG4j2fz_vLz5g. (state.change.logger)
broker           | [2023-08-04 11:16:08,789] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:08,811] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:08,814] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:08,816] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:08,820] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with topic id Some(RifsNDOQSCG4j2fz_vLz5g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:08,840] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:08,846] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:08,867] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:08,869] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with topic id oBNDr9MwSwyfz3UNlKj0Aw. (state.change.logger)
broker           | [2023-08-04 11:16:08,904] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:08,930] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:08,963] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:08,974] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:08,977] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with topic id Some(oBNDr9MwSwyfz3UNlKj0Aw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:08,992] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:09,006] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:09,014] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:09,015] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with topic id WdjVN2t-SM21TyDTgxyLhw. (state.change.logger)
broker           | [2023-08-04 11:16:09,073] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:09,092] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:09,110] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:09,114] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:09,117] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with topic id Some(WdjVN2t-SM21TyDTgxyLhw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:09,136] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:09,179] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:09,179] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:09,181] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with topic id 44Yp-2aPRK-xKM9zKUCEJw. (state.change.logger)
broker           | [2023-08-04 11:16:09,212] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:09,225] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:09,228] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:09,229] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:09,232] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with topic id Some(44Yp-2aPRK-xKM9zKUCEJw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:09,256] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:09,270] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:09,278] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:09,279] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with topic id HSq3iWH_Q0G8WAozBnXiFQ. (state.change.logger)
broker           | [2023-08-04 11:16:09,333] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:09,338] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:09,356] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:09,367] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:09,371] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with topic id Some(HSq3iWH_Q0G8WAozBnXiFQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:09,383] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:09,394] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:09,407] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:09,413] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with topic id I-AspoMlSU-I5nUZzPsT2Q. (state.change.logger)
broker           | [2023-08-04 11:16:09,457] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:09,466] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:09,496] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:09,497] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:09,498] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with topic id Some(I-AspoMlSU-I5nUZzPsT2Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:09,520] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:09,530] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:09,532] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:09,533] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with topic id xpuJp62xRjKbInyyPCX1lQ. (state.change.logger)
broker           | [2023-08-04 11:16:09,585] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:09,593] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:09,615] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:09,628] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:09,629] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with topic id Some(xpuJp62xRjKbInyyPCX1lQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:09,670] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:09,680] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:09,682] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:09,684] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with topic id RXByHOqASZ2CbNrZqK_AMw. (state.change.logger)
broker           | [2023-08-04 11:16:09,722] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:09,737] INFO Created log for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:09,752] INFO [Partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:09,759] INFO [Partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:09,760] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with topic id Some(RXByHOqASZ2CbNrZqK_AMw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:09,777] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:09,795] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:09,813] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:09,821] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with topic id 3eA5D2OKTTmHRKZTjiQWtA. (state.change.logger)
broker           | [2023-08-04 11:16:09,858] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:09,883] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:09,902] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:09,915] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:09,922] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with topic id Some(3eA5D2OKTTmHRKZTjiQWtA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:09,938] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:09,947] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:09,950] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:09,952] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with topic id KabcuEMhTCOz9nkNgpIsHg. (state.change.logger)
broker           | [2023-08-04 11:16:09,997] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:10,009] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:10,015] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,023] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,025] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with topic id Some(KabcuEMhTCOz9nkNgpIsHg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:10,043] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:10,058] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:10,058] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:10,059] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with topic id dlujEO5uTCeuzLFQQNtU0A. (state.change.logger)
broker           | [2023-08-04 11:16:10,098] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:10,109] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:10,117] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,119] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,120] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with topic id Some(dlujEO5uTCeuzLFQQNtU0A) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:10,134] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:10,139] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:10,143] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:10,144] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with topic id e66bfUqWTZWX8U4X4Ipjsw. (state.change.logger)
broker           | [2023-08-04 11:16:10,179] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:10,187] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:10,193] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,194] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,195] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with topic id Some(e66bfUqWTZWX8U4X4Ipjsw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:10,212] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:10,216] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:10,218] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:10,219] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with topic id 88agc7ovSmm9apNE4HN69Q. (state.change.logger)
broker           | [2023-08-04 11:16:10,260] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:10,276] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:10,279] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,280] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,281] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with topic id Some(88agc7ovSmm9apNE4HN69Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:10,303] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:10,323] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:10,325] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:10,327] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with topic id xT2oJSg6R6ONg7UwEIUFtA. (state.change.logger)
broker           | [2023-08-04 11:16:10,360] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:10,373] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:10,376] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,377] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,378] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with topic id Some(xT2oJSg6R6ONg7UwEIUFtA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:10,412] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:10,441] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:10,442] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:10,453] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with topic id Trp1P4PASmedQMblOoysPQ. (state.change.logger)
broker           | [2023-08-04 11:16:10,502] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:10,508] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:10,529] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,530] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,534] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with topic id Some(Trp1P4PASmedQMblOoysPQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:10,546] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:10,557] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:10,559] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:10,564] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with topic id VHDKH6yPRKekarvjYHIjyw. (state.change.logger)
broker           | [2023-08-04 11:16:10,623] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:10,649] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:10,662] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,671] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,672] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with topic id Some(VHDKH6yPRKekarvjYHIjyw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:10,684] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:10,692] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:10,707] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:10,709] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with topic id No9nQpMqSCWKHXxTpgGjHA. (state.change.logger)
broker           | [2023-08-04 11:16:10,745] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:10,770] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:10,804] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,807] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,808] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with topic id Some(No9nQpMqSCWKHXxTpgGjHA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:10,844] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:10,849] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:10,868] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-monitoring-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:10,869] INFO [Broker id=1] Creating new partition _confluent-monitoring-0 with topic id 8fgQicUSRVi7xJcKYztVRQ. (state.change.logger)
broker           | [2023-08-04 11:16:10,920] INFO [LogLoader partition=_confluent-monitoring-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:10,931] INFO Created log for partition _confluent-monitoring-0 in /tmp/kraft-combined-logs/_confluent-monitoring-0 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:10,990] INFO [Partition _confluent-monitoring-0 broker=1] No checkpointed highwatermark is found for partition _confluent-monitoring-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,991] INFO [Partition _confluent-monitoring-0 broker=1] Log loaded for partition _confluent-monitoring-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:10,992] INFO [Broker id=1] Leader _confluent-monitoring-0 with topic id Some(8fgQicUSRVi7xJcKYztVRQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:11,010] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-monitoring with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 259200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:11,030] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:11,040] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:11,042] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with topic id w47q9kvPRoGucrcFjBQ9fw. (state.change.logger)
broker           | [2023-08-04 11:16:11,072] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:11,087] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:11,092] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:11,094] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:11,097] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with topic id Some(w47q9kvPRoGucrcFjBQ9fw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:11,120] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:11,125] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:11,131] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:11,132] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with topic id 7J5ZVEciQBa64RcXLSLhrw. (state.change.logger)
broker           | [2023-08-04 11:16:11,153] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:11,176] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:11,183] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:11,183] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:11,184] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with topic id Some(7J5ZVEciQBa64RcXLSLhrw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:11,214] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:11,221] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:11,232] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:11,240] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with topic id 6s9RSwQ_RiqRp339_OPjrw. (state.change.logger)
broker           | [2023-08-04 11:16:11,320] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:11,328] INFO Created log for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:11,343] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:11,350] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:11,352] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with topic id Some(6s9RSwQ_RiqRp339_OPjrw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:11,384] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:11,398] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:11,400] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:11,401] INFO [Broker id=1] Creating new partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with topic id I_0cUFAkR3umlIgUR1kaaw. (state.change.logger)
broker           | [2023-08-04 11:16:11,482] INFO [LogLoader partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:11,485] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 in /tmp/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:11,507] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:11,509] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:11,511] INFO [Broker id=1] Leader _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with topic id Some(I_0cUFAkR3umlIgUR1kaaw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:11,526] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
connect          | [2023-08-04 11:16:15,127] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/schema-registry/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:16:15,144] INFO Loading plugin from: /usr/share/java/acl (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
control-center   | [2023-08-04 11:16:22,757] INFO describing topics=[_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, _confluent-metrics, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-cluster-rekey, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, _confluent-monitoring, _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition] (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,838] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,839] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,847] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,848] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,849] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,849] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,850] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,850] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,852] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,854] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,854] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,854] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,855] INFO create=success topic=TopicInfo{name=_confluent-metrics, partitions=12, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,855] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,855] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-cluster-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,858] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,871] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,871] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,872] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,872] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,872] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,873] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,887] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,888] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,888] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,888] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,889] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,889] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,889] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,890] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,890] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,890] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,890] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,899] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,900] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,900] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,901] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,908] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,909] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,915] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,922] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,923] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,925] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,928] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,931] INFO create=success topic=TopicInfo{name=_confluent-monitoring, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,933] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,935] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,936] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,937] INFO create=success topic=TopicInfo{name=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition, partitions=1, replication=1} (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:22,941] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = true
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = latest
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = will-delete-this
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = _confluent-controlcenter-7-4-1-1
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:23,003] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:23,005] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:23,006] INFO Kafka startTimeMs: 1691147783003 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:23,007] INFO Setting offsets for topic=_confluent-monitoring (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:23,040] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:23,103] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Assigned to partition(s): _confluent-monitoring-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:23,103] INFO found 1 topicPartitions for topic=_confluent-monitoring (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:23,162] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 8fgQicUSRVi7xJcKYztVRQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:23,206] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-monitoring-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
broker           | [2023-08-04 11:16:23,301] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
broker           | [2023-08-04 11:16:23,317] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='__consumer_offsets', numPartitions=50, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='compression.type', value='producer'), CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='segment.bytes', value='104857600')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,317] INFO [Controller 1] Created topic __consumer_offsets with topic ID 6FR4V_fAS7Gu-9aw7Irmcg. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,332] INFO [Controller 1] ConfigResource(type=TOPIC, name='__consumer_offsets'): set configuration compression.type to producer (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:23,332] INFO [Controller 1] ConfigResource(type=TOPIC, name='__consumer_offsets'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:23,333] INFO [Controller 1] ConfigResource(type=TOPIC, name='__consumer_offsets'): set configuration segment.bytes to 104857600 (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:16:23,333] INFO [Controller 1] Created partition __consumer_offsets-0 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,333] INFO [Controller 1] Created partition __consumer_offsets-1 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,334] INFO [Controller 1] Created partition __consumer_offsets-2 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,334] INFO [Controller 1] Created partition __consumer_offsets-3 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,334] INFO [Controller 1] Created partition __consumer_offsets-4 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,335] INFO [Controller 1] Created partition __consumer_offsets-5 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,336] INFO [Controller 1] Created partition __consumer_offsets-6 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,336] INFO [Controller 1] Created partition __consumer_offsets-7 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,336] INFO [Controller 1] Created partition __consumer_offsets-8 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,343] INFO [Controller 1] Created partition __consumer_offsets-9 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,343] INFO [Controller 1] Created partition __consumer_offsets-10 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,344] INFO [Controller 1] Created partition __consumer_offsets-11 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,344] INFO [Controller 1] Created partition __consumer_offsets-12 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,344] INFO [Controller 1] Created partition __consumer_offsets-13 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,344] INFO [Controller 1] Created partition __consumer_offsets-14 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,345] INFO [Controller 1] Created partition __consumer_offsets-15 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,345] INFO [Controller 1] Created partition __consumer_offsets-16 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,345] INFO [Controller 1] Created partition __consumer_offsets-17 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,345] INFO [Controller 1] Created partition __consumer_offsets-18 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,346] INFO [Controller 1] Created partition __consumer_offsets-19 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,346] INFO [Controller 1] Created partition __consumer_offsets-20 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,346] INFO [Controller 1] Created partition __consumer_offsets-21 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,346] INFO [Controller 1] Created partition __consumer_offsets-22 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,347] INFO [Controller 1] Created partition __consumer_offsets-23 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,364] INFO [Controller 1] Created partition __consumer_offsets-24 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,365] INFO [Controller 1] Created partition __consumer_offsets-25 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,365] INFO [Controller 1] Created partition __consumer_offsets-26 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,365] INFO [Controller 1] Created partition __consumer_offsets-27 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,366] INFO [Controller 1] Created partition __consumer_offsets-28 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,366] INFO [Controller 1] Created partition __consumer_offsets-29 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,366] INFO [Controller 1] Created partition __consumer_offsets-30 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,380] INFO [Controller 1] Created partition __consumer_offsets-31 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,388] INFO [Controller 1] Created partition __consumer_offsets-32 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,388] INFO [Controller 1] Created partition __consumer_offsets-33 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,391] INFO [Controller 1] Created partition __consumer_offsets-34 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,392] INFO [Controller 1] Created partition __consumer_offsets-35 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,392] INFO [Controller 1] Created partition __consumer_offsets-36 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,393] INFO [Controller 1] Created partition __consumer_offsets-37 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,398] INFO [Controller 1] Created partition __consumer_offsets-38 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,401] INFO [Controller 1] Created partition __consumer_offsets-39 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,403] INFO [Controller 1] Created partition __consumer_offsets-40 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,405] INFO [Controller 1] Created partition __consumer_offsets-41 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,406] INFO [Controller 1] Created partition __consumer_offsets-42 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,409] INFO [Controller 1] Created partition __consumer_offsets-43 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,413] INFO [Controller 1] Created partition __consumer_offsets-44 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,414] INFO [Controller 1] Created partition __consumer_offsets-45 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,414] INFO [Controller 1] Created partition __consumer_offsets-46 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,416] INFO [Controller 1] Created partition __consumer_offsets-47 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,417] INFO [Controller 1] Created partition __consumer_offsets-48 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,418] INFO [Controller 1] Created partition __consumer_offsets-49 with topic ID 6FR4V_fAS7Gu-9aw7Irmcg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:16:23,456] INFO [Broker id=1] Transitioning 50 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:16:23,457] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:16:23,457] INFO [Broker id=1] Creating new partition __consumer_offsets-13 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:23,483] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:23,488] INFO Created log for partition __consumer_offsets-13 in /tmp/kraft-combined-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:23,497] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:23,498] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:23,507] INFO [Broker id=1] Leader __consumer_offsets-13 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:23,536] INFO [Broker id=1] Creating new partition __consumer_offsets-46 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:23,541] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:23,588] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:23,599] INFO Created log for partition __consumer_offsets-46 in /tmp/kraft-combined-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:23,599] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:23,600] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:23,601] INFO [Broker id=1] Leader __consumer_offsets-46 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:23,612] INFO [Broker id=1] Creating new partition __consumer_offsets-9 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:23,650] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:23,654] INFO Created log for partition __consumer_offsets-9 in /tmp/kraft-combined-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:23,655] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:23,656] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:23,657] INFO [Broker id=1] Leader __consumer_offsets-9 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:23,696] INFO [Broker id=1] Creating new partition __consumer_offsets-42 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:23,742] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:23,742] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:23,743] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:23,743] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:23,753] INFO Created log for partition __consumer_offsets-42 in /tmp/kraft-combined-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:23,754] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:23,755] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:23,755] INFO [Broker id=1] Leader __consumer_offsets-42 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:23,775] INFO [Broker id=1] Creating new partition __consumer_offsets-21 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:23,795] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:23,798] INFO Created log for partition __consumer_offsets-21 in /tmp/kraft-combined-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:23,798] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:23,798] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:23,799] INFO [Broker id=1] Leader __consumer_offsets-21 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:23,816] INFO [Broker id=1] Creating new partition __consumer_offsets-17 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:23,847] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:16:23,866] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:23,871] INFO Created log for partition __consumer_offsets-17 in /tmp/kraft-combined-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:23,871] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:23,871] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:23,872] INFO [Broker id=1] Leader __consumer_offsets-17 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center   | [2023-08-04 11:16:23,895] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:23,896] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:23,907] INFO [Broker id=1] Creating new partition __consumer_offsets-30 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:23,911] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:23,931] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:23,935] INFO Created log for partition __consumer_offsets-30 in /tmp/kraft-combined-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:23,936] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:23,937] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:23,938] INFO [Broker id=1] Leader __consumer_offsets-30 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:23,966] INFO [Broker id=1] Creating new partition __consumer_offsets-26 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:24,052] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
control-center   | [2023-08-04 11:16:24,052] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:24,058] INFO Created log for partition __consumer_offsets-26 in /tmp/kraft-combined-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:24,075] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,075] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,076] INFO [Broker id=1] Leader __consumer_offsets-26 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:24,105] INFO [Broker id=1] Creating new partition __consumer_offsets-5 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:24,124] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:24,125] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:24,133] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:24,170] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:24,188] INFO Created log for partition __consumer_offsets-5 in /tmp/kraft-combined-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:24,189] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,192] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,192] INFO [Broker id=1] Leader __consumer_offsets-5 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:24,227] INFO [Broker id=1] Creating new partition __consumer_offsets-38 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:24,239] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:16:24,248] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:24,252] INFO Created log for partition __consumer_offsets-38 in /tmp/kraft-combined-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:24,253] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,254] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,255] INFO [Broker id=1] Leader __consumer_offsets-38 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:24,264] INFO [Broker id=1] Creating new partition __consumer_offsets-1 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:24,281] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:24,289] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:24,345] INFO Created log for partition __consumer_offsets-1 in /tmp/kraft-combined-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:24,359] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,361] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,363] INFO [Broker id=1] Leader __consumer_offsets-1 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center   | [2023-08-04 11:16:24,379] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:24,380] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:24,380] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:24,404] INFO [Broker id=1] Creating new partition __consumer_offsets-34 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:24,455] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:24,457] INFO Created log for partition __consumer_offsets-34 in /tmp/kraft-combined-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:24,458] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,458] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,464] INFO [Broker id=1] Leader __consumer_offsets-34 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center   | [2023-08-04 11:16:24,482] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:16:24,508] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:24,509] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:24,509] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:24,508] INFO [Broker id=1] Creating new partition __consumer_offsets-16 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:24,547] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:24,552] INFO Created log for partition __consumer_offsets-16 in /tmp/kraft-combined-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:24,552] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,553] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,554] INFO [Broker id=1] Leader __consumer_offsets-16 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:24,570] INFO [Broker id=1] Creating new partition __consumer_offsets-45 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:24,587] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:24,599] INFO Created log for partition __consumer_offsets-45 in /tmp/kraft-combined-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:24,599] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,599] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,600] INFO [Broker id=1] Leader __consumer_offsets-45 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:24,608] INFO [Broker id=1] Creating new partition __consumer_offsets-12 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:24,625] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:24,672] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
control-center   | [2023-08-04 11:16:24,680] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:24,680] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:24,680] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:24,687] INFO Created log for partition __consumer_offsets-12 in /tmp/kraft-combined-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:24,688] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,689] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,690] INFO [Broker id=1] Leader __consumer_offsets-12 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:24,705] INFO [Broker id=1] Creating new partition __consumer_offsets-41 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:24,740] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:24,747] INFO Created log for partition __consumer_offsets-41 in /tmp/kraft-combined-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:24,748] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,750] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,752] INFO [Broker id=1] Leader __consumer_offsets-41 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:24,762] INFO [Broker id=1] Creating new partition __consumer_offsets-24 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:24,786] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:16:24,805] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:24,805] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:24,805] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:24,814] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:24,821] INFO Created log for partition __consumer_offsets-24 in /tmp/kraft-combined-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:24,827] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,836] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,838] INFO [Broker id=1] Leader __consumer_offsets-24 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:24,854] INFO [Broker id=1] Creating new partition __consumer_offsets-20 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:24,886] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:24,891] INFO Created log for partition __consumer_offsets-20 in /tmp/kraft-combined-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:24,895] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,898] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,904] INFO [Broker id=1] Leader __consumer_offsets-20 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:24,913] INFO [Broker id=1] Creating new partition __consumer_offsets-49 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:24,917] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:24,939] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:24,940] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:24,940] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:24,958] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:24,974] INFO Created log for partition __consumer_offsets-49 in /tmp/kraft-combined-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:24,977] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,979] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:24,980] INFO [Broker id=1] Leader __consumer_offsets-49 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:24,996] INFO [Broker id=1] Creating new partition __consumer_offsets-0 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:25,041] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:16:25,054] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:25,056] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:25,057] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:25,065] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:25,074] INFO Created log for partition __consumer_offsets-0 in /tmp/kraft-combined-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:25,074] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,075] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,078] INFO [Broker id=1] Leader __consumer_offsets-0 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:25,092] INFO [Broker id=1] Creating new partition __consumer_offsets-29 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:25,125] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:25,131] INFO Created log for partition __consumer_offsets-29 in /tmp/kraft-combined-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:25,133] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,133] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,135] INFO [Broker id=1] Leader __consumer_offsets-29 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:25,146] INFO [Broker id=1] Creating new partition __consumer_offsets-25 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:25,167] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:25,205] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:25,205] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:25,206] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:25,218] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:25,223] INFO Created log for partition __consumer_offsets-25 in /tmp/kraft-combined-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:25,230] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,239] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,241] INFO [Broker id=1] Leader __consumer_offsets-25 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:25,254] INFO [Broker id=1] Creating new partition __consumer_offsets-8 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:25,301] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
control-center   | [2023-08-04 11:16:25,307] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:16:25,329] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:25,329] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:25,329] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:25,322] INFO Created log for partition __consumer_offsets-8 in /tmp/kraft-combined-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:25,334] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,335] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,335] INFO [Broker id=1] Leader __consumer_offsets-8 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:25,354] INFO [Broker id=1] Creating new partition __consumer_offsets-37 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:25,385] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:25,406] INFO Created log for partition __consumer_offsets-37 in /tmp/kraft-combined-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:25,406] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,406] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,420] INFO [Broker id=1] Leader __consumer_offsets-37 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:25,432] INFO [Broker id=1] Creating new partition __consumer_offsets-4 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:25,436] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:25,468] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:25,470] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:25,471] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:25,472] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:25,499] INFO Created log for partition __consumer_offsets-4 in /tmp/kraft-combined-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:25,503] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,503] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,505] INFO [Broker id=1] Leader __consumer_offsets-4 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:25,519] INFO [Broker id=1] Creating new partition __consumer_offsets-33 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:25,548] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:25,552] INFO Created log for partition __consumer_offsets-33 in /tmp/kraft-combined-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:25,553] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,554] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,555] INFO [Broker id=1] Leader __consumer_offsets-33 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:25,567] INFO [Broker id=1] Creating new partition __consumer_offsets-15 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:25,579] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:16:25,604] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:25,605] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:25,605] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:25,618] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:25,635] INFO Created log for partition __consumer_offsets-15 in /tmp/kraft-combined-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:25,637] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,639] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,640] INFO [Broker id=1] Leader __consumer_offsets-15 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:25,652] INFO [Broker id=1] Creating new partition __consumer_offsets-48 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:25,702] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:25,720] INFO Created log for partition __consumer_offsets-48 in /tmp/kraft-combined-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:25,721] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,721] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,722] INFO [Broker id=1] Leader __consumer_offsets-48 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center   | [2023-08-04 11:16:25,716] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:25,738] INFO [Broker id=1] Creating new partition __consumer_offsets-11 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:25,748] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:25,749] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:25,749] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:25,774] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:25,790] INFO Created log for partition __consumer_offsets-11 in /tmp/kraft-combined-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:25,792] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,794] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,803] INFO [Broker id=1] Leader __consumer_offsets-11 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:25,811] INFO [Broker id=1] Creating new partition __consumer_offsets-44 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:25,831] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:25,840] INFO Created log for partition __consumer_offsets-44 in /tmp/kraft-combined-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:25,842] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,843] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,845] INFO [Broker id=1] Leader __consumer_offsets-44 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center   | [2023-08-04 11:16:25,850] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:16:25,859] INFO [Broker id=1] Creating new partition __consumer_offsets-23 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:25,870] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:25,871] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:25,871] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:25,894] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:25,899] INFO Created log for partition __consumer_offsets-23 in /tmp/kraft-combined-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:25,899] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,901] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,904] INFO [Broker id=1] Leader __consumer_offsets-23 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:25,917] INFO [Broker id=1] Creating new partition __consumer_offsets-19 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:25,934] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:25,937] INFO Created log for partition __consumer_offsets-19 in /tmp/kraft-combined-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:25,938] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,938] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:25,939] INFO [Broker id=1] Leader __consumer_offsets-19 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:25,951] INFO [Broker id=1] Creating new partition __consumer_offsets-32 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:25,976] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:25,989] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:25,994] INFO Created log for partition __consumer_offsets-32 in /tmp/kraft-combined-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:25,998] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,000] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,017] INFO [Broker id=1] Leader __consumer_offsets-32 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:26,037] INFO [Broker id=1] Creating new partition __consumer_offsets-28 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:26,042] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:26,052] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:26,053] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:26,089] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:26,097] INFO Created log for partition __consumer_offsets-28 in /tmp/kraft-combined-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:26,098] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,098] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,098] INFO [Broker id=1] Leader __consumer_offsets-28 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:26,131] INFO [Broker id=1] Creating new partition __consumer_offsets-7 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:26,153] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
control-center   | [2023-08-04 11:16:26,155] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:16:26,199] INFO Created log for partition __consumer_offsets-7 in /tmp/kraft-combined-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:26,201] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
control-center   | [2023-08-04 11:16:26,204] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:26,205] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:26,205] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:26,201] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,212] INFO [Broker id=1] Leader __consumer_offsets-7 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:26,230] INFO [Broker id=1] Creating new partition __consumer_offsets-40 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:26,286] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:26,290] INFO Created log for partition __consumer_offsets-40 in /tmp/kraft-combined-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:26,295] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,298] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,303] INFO [Broker id=1] Leader __consumer_offsets-40 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:26,316] INFO [Broker id=1] Creating new partition __consumer_offsets-3 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:26,325] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:26,351] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:26,368] INFO Created log for partition __consumer_offsets-3 in /tmp/kraft-combined-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:26,369] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
control-center   | [2023-08-04 11:16:26,372] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:26,373] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:26,374] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:26,387] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,388] INFO [Broker id=1] Leader __consumer_offsets-3 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:26,413] INFO [Broker id=1] Creating new partition __consumer_offsets-36 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:26,434] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:26,438] INFO Created log for partition __consumer_offsets-36 in /tmp/kraft-combined-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:26,439] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,440] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,442] INFO [Broker id=1] Leader __consumer_offsets-36 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:26,454] INFO [Broker id=1] Creating new partition __consumer_offsets-47 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:26,474] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
control-center   | [2023-08-04 11:16:26,476] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:16:26,505] INFO Created log for partition __consumer_offsets-47 in /tmp/kraft-combined-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:26,505] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,505] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,506] INFO [Broker id=1] Leader __consumer_offsets-47 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center   | [2023-08-04 11:16:26,506] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:26,509] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:26,511] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:26,522] INFO [Broker id=1] Creating new partition __consumer_offsets-14 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:26,537] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:26,540] INFO Created log for partition __consumer_offsets-14 in /tmp/kraft-combined-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:26,540] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,540] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,541] INFO [Broker id=1] Leader __consumer_offsets-14 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:26,564] INFO [Broker id=1] Creating new partition __consumer_offsets-43 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:26,582] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:26,585] INFO Created log for partition __consumer_offsets-43 in /tmp/kraft-combined-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:26,586] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,586] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,586] INFO [Broker id=1] Leader __consumer_offsets-43 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:26,597] INFO [Broker id=1] Creating new partition __consumer_offsets-10 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:26,606] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:26,609] INFO Created log for partition __consumer_offsets-10 in /tmp/kraft-combined-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:26,611] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,612] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,614] INFO [Broker id=1] Leader __consumer_offsets-10 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:26,624] INFO [Broker id=1] Creating new partition __consumer_offsets-22 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:26,618] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:26,638] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:26,659] INFO Created log for partition __consumer_offsets-22 in /tmp/kraft-combined-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:26,661] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,663] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,664] INFO [Broker id=1] Leader __consumer_offsets-22 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:26,677] INFO [Broker id=1] Creating new partition __consumer_offsets-18 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:26,720] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:26,721] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:26,721] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:26,743] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:26,760] INFO Created log for partition __consumer_offsets-18 in /tmp/kraft-combined-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:26,760] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,761] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,762] INFO [Broker id=1] Leader __consumer_offsets-18 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:26,782] INFO [Broker id=1] Creating new partition __consumer_offsets-31 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:26,798] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:26,801] INFO Created log for partition __consumer_offsets-31 in /tmp/kraft-combined-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:26,802] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,803] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,805] INFO [Broker id=1] Leader __consumer_offsets-31 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:26,818] INFO [Broker id=1] Creating new partition __consumer_offsets-27 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:26,822] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:16:26,833] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:26,835] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:26,836] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:26,841] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:26,847] INFO Created log for partition __consumer_offsets-27 in /tmp/kraft-combined-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:26,849] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,851] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,853] INFO [Broker id=1] Leader __consumer_offsets-27 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:26,872] INFO [Broker id=1] Creating new partition __consumer_offsets-39 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:26,908] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:26,915] INFO Created log for partition __consumer_offsets-39 in /tmp/kraft-combined-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:26,915] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,916] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,917] INFO [Broker id=1] Leader __consumer_offsets-39 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:26,931] INFO [Broker id=1] Creating new partition __consumer_offsets-6 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
control-center   | [2023-08-04 11:16:26,945] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:26,972] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:26,979] INFO Created log for partition __consumer_offsets-6 in /tmp/kraft-combined-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:26,980] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,980] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:26,980] INFO [Broker id=1] Leader __consumer_offsets-6 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
control-center   | [2023-08-04 11:16:26,993] WARN [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Offset commit failed on partition _confluent-monitoring-0 at offset 0: This is not the correct coordinator. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:26,993] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:26,993] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:26,996] INFO [Broker id=1] Creating new partition __consumer_offsets-35 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:27,017] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:27,022] INFO Created log for partition __consumer_offsets-35 in /tmp/kraft-combined-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:27,022] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:27,023] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:27,023] INFO [Broker id=1] Leader __consumer_offsets-35 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:27,034] INFO [Broker id=1] Creating new partition __consumer_offsets-2 with topic id 6FR4V_fAS7Gu-9aw7Irmcg. (state.change.logger)
broker           | [2023-08-04 11:16:27,049] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:16:27,051] INFO Created log for partition __consumer_offsets-2 in /tmp/kraft-combined-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2023-08-04 11:16:27,052] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:27,052] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:16:27,052] INFO [Broker id=1] Leader __consumer_offsets-2 with topic id Some(6FR4V_fAS7Gu-9aw7Irmcg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:16:27,071] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,073] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,089] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,089] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,089] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,089] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,089] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,089] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,089] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,089] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,089] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,090] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,090] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,090] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,090] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,090] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,090] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,090] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,090] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,090] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,090] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,090] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,090] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,090] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,091] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,091] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,091] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,091] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,091] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,091] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,091] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,091] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,092] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,092] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,092] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,092] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,093] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,093] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,093] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,093] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,093] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,093] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,093] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:27,094] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
broker           | [2023-08-04 11:16:27,094] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,100] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,100] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,100] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,101] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,101] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,101] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,101] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,101] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,101] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,102] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,102] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,102] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,102] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,102] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,102] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,103] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,103] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,103] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,103] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,103] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,103] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,104] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,104] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,104] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,104] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,104] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,105] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,105] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,105] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,106] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,106] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,106] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,121] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,121] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,121] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,122] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,122] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,122] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,122] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:27,113] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:27,114] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:27,114] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:27,123] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,123] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,123] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,123] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,126] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,127] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,127] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,128] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,128] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,128] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,128] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,128] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,129] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,130] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,130] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,131] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:27,131] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,134] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
broker           | [2023-08-04 11:16:27,184] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 108 milliseconds for epoch 0, of which 37 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,187] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 98 milliseconds for epoch 0, of which 97 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,189] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 100 milliseconds for epoch 0, of which 99 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,190] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 101 milliseconds for epoch 0, of which 100 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,191] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 102 milliseconds for epoch 0, of which 102 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,192] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 102 milliseconds for epoch 0, of which 102 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,193] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 103 milliseconds for epoch 0, of which 103 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,194] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 104 milliseconds for epoch 0, of which 104 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,195] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 105 milliseconds for epoch 0, of which 104 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,200] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 110 milliseconds for epoch 0, of which 105 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,201] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 111 milliseconds for epoch 0, of which 110 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,201] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 111 milliseconds for epoch 0, of which 111 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,202] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 111 milliseconds for epoch 0, of which 111 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,203] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 111 milliseconds for epoch 0, of which 111 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,204] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 113 milliseconds for epoch 0, of which 112 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,204] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 113 milliseconds for epoch 0, of which 113 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,206] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 114 milliseconds for epoch 0, of which 113 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,220] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 127 milliseconds for epoch 0, of which 113 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,221] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 128 milliseconds for epoch 0, of which 127 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,223] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 130 milliseconds for epoch 0, of which 129 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,225] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 131 milliseconds for epoch 0, of which 131 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
control-center   | [2023-08-04 11:16:27,232] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:27,258] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 158 milliseconds for epoch 0, of which 126 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,259] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 159 milliseconds for epoch 0, of which 159 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,267] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 166 milliseconds for epoch 0, of which 159 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,271] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 170 milliseconds for epoch 0, of which 169 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,277] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 176 milliseconds for epoch 0, of which 175 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,280] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 178 milliseconds for epoch 0, of which 176 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,282] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 180 milliseconds for epoch 0, of which 179 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,283] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 181 milliseconds for epoch 0, of which 181 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,284] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 181 milliseconds for epoch 0, of which 181 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,285] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 182 milliseconds for epoch 0, of which 182 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,295] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 192 milliseconds for epoch 0, of which 183 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,297] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 193 milliseconds for epoch 0, of which 192 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,299] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 195 milliseconds for epoch 0, of which 194 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,300] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 195 milliseconds for epoch 0, of which 194 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,301] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 196 milliseconds for epoch 0, of which 196 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,302] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 196 milliseconds for epoch 0, of which 196 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,303] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 183 milliseconds for epoch 0, of which 183 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,304] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 183 milliseconds for epoch 0, of which 183 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,305] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 183 milliseconds for epoch 0, of which 182 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,308] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 186 milliseconds for epoch 0, of which 184 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,313] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 190 milliseconds for epoch 0, of which 189 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,323] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 200 milliseconds for epoch 0, of which 196 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,326] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 198 milliseconds for epoch 0, of which 197 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,331] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 201 milliseconds for epoch 0, of which 199 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,332] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 204 milliseconds for epoch 0, of which 203 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,333] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 205 milliseconds for epoch 0, of which 204 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,333] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 203 milliseconds for epoch 0, of which 203 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,336] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 206 milliseconds for epoch 0, of which 205 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2023-08-04 11:16:27,338] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 207 milliseconds for epoch 0, of which 207 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
control-center   | [2023-08-04 11:16:27,620] INFO Setting offsets for topic=_confluent-metrics (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:27,693] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Assigned to partition(s): _confluent-metrics-11, _confluent-metrics-9, _confluent-metrics-10, _confluent-metrics-7, _confluent-metrics-8, _confluent-metrics-5, _confluent-metrics-6, _confluent-metrics-3, _confluent-metrics-4, _confluent-metrics-1, _confluent-metrics-2, _confluent-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:27,696] INFO found 12 topicPartitions for topic=_confluent-metrics (io.confluent.controlcenter.KafkaHelper)
control-center   | [2023-08-04 11:16:27,710] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:27,714] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:27,715] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:27,716] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:27,717] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:27,718] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:27,719] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:27,723] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:27,724] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:27,725] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:27,726] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:27,728] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:27,753] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:27,754] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:27,757] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:27,757] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:27,758] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:27,759] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:27,760] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:27,762] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:27,763] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:27,765] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:27,767] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:27,769] INFO [Consumer clientId=will-delete-this, groupId=_confluent-controlcenter-7-4-1-1] Seeking to latest offset of partition _confluent-metrics-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:27,816] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:16:27,818] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:16:27,819] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:16:27,874] INFO App info kafka.consumer for will-delete-this unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:27,883] INFO action=starting topology=command (io.confluent.controlcenter.application.AllControlCenter)
control-center   | [2023-08-04 11:16:27,889] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c] State transition from CREATED to REBALANCING (org.apache.kafka.streams.KafkaStreams)
control-center   | [2023-08-04 11:16:27,915] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c] Started 1 stream threads (org.apache.kafka.streams.KafkaStreams)
control-center   | [2023-08-04 11:16:27,916] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:27,964] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:27,960] INFO waiting for streams to be in running state. Current state is REBALANCING (io.confluent.command.kafka.CommandStore)
control-center   | [2023-08-04 11:16:27,985] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Subscribed to topic(s): _confluent-command (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:28,084] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Resetting the last seen epoch of partition _confluent-command-0 to 0 since the associated topicId changed from null to AFaY0a_lSiO7tlDgMwz2qw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:28,085] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:28,086] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:28,108] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:28,210] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
broker           | [2023-08-04 11:16:28,266] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1-command in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer-3ecb2160-1f2a-4f66-82ea-e87d2951c5dd and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:28,381] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer-3ecb2160-1f2a-4f66-82ea-e87d2951c5dd (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:28,384] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:28,384] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:28,456] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1-command in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member _confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer-3ecb2160-1f2a-4f66-82ea-e87d2951c5dd with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:28,590] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1-command generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:28,601] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Successfully joined group with generation Generation{generationId=1, memberId='_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer-3ecb2160-1f2a-4f66-82ea-e87d2951c5dd', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:28,655] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer] Skipping the repartition topic validation since there are no repartition topics. (org.apache.kafka.streams.processor.internals.RepartitionTopics)
control-center   | [2023-08-04 11:16:28,808] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Found no committed offset for partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:28,820] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer] All members participating in this rebalance: 
control-center   | 131145ad-beaf-40ab-9f2f-4f67571c7c4c: [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer-3ecb2160-1f2a-4f66-82ea-e87d2951c5dd]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:28,886] INFO Decided on assignment: {131145ad-beaf-40ab-9f2f-4f67571c7c4c=[activeTasks: ([0_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([0_0=0]) clientTags: ([]) capacity: 1 assigned: 1]} with no followup probing rebalance. (org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor)
control-center   | [2023-08-04 11:16:28,888] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer] Assigned tasks [0_0] including stateful [0_0] to clients as: 
control-center   | 131145ad-beaf-40ab-9f2f-4f67571c7c4c=[activeTasks: ([0_0]) standbyTasks: ([])]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:28,948] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer] Client 131145ad-beaf-40ab-9f2f-4f67571c7c4c per-consumer assignment:
control-center   | 	prev owned active {}
control-center   | 	prev owned standby {_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer-3ecb2160-1f2a-4f66-82ea-e87d2951c5dd=[]}
control-center   | 	assigned active {_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer-3ecb2160-1f2a-4f66-82ea-e87d2951c5dd=[0_0]}
control-center   | 	revoking active {}
control-center   | 	assigned standby {}
control-center   |  (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:28,949] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:28,952] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Finished assignment for group at generation 1: {_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer-3ecb2160-1f2a-4f66-82ea-e87d2951c5dd=Assignment(partitions=[_confluent-command-0], userDataSize=52)} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:28,980] INFO waiting for streams to be in running state. Current state is REBALANCING (io.confluent.command.kafka.CommandStore)
broker           | [2023-08-04 11:16:29,242] INFO [GroupCoordinator 1]: Assignment received from leader _confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer-3ecb2160-1f2a-4f66-82ea-e87d2951c5dd for group _confluent-controlcenter-7-4-1-1-command for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:29,431] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Successfully synced group in generation Generation{generationId=1, memberId='_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer-3ecb2160-1f2a-4f66-82ea-e87d2951c5dd', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:29,446] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Notifying assignor about the new Assignment(partitions=[_confluent-command-0], userDataSize=52) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:29,449] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:29,467] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] Handle new assignment with:
control-center   | 	New active tasks: [0_0]
control-center   | 	New standby tasks: []
control-center   | 	Existing active tasks: []
control-center   | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center   | [2023-08-04 11:16:29,646] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Adding newly assigned partitions: _confluent-command-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:29,648] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:29,788] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Found no committed offset for partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:29,987] INFO waiting for streams to be in running state. Current state is REBALANCING (io.confluent.command.kafka.CommandStore)
control-center   | [2023-08-04 11:16:30,848] INFO Opening store commander in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore)
control-center   | [2023-08-04 11:16:30,901] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] stream-task [0_0] State store commander did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-command-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:30,902] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] task [0_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:30,988] INFO waiting for streams to be in running state. Current state is REBALANCING (io.confluent.command.kafka.CommandStore)
control-center   | [2023-08-04 11:16:31,030] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] End offset for changelog _confluent-command-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:31,032] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-restore-consumer, groupId=null] Assigned to partition(s): _confluent-command-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:31,042] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:31,076] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-command-0 to 0 since the associated topicId changed from null to AFaY0a_lSiO7tlDgMwz2qw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:31,078] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:31,235] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] Finished restoring changelog _confluent-command-0 to store commander with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:31,237] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] Setting topic '_confluent-command' to consume from earliest offset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:31,237] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Seeking to earliest offset of partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:31,245] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Found no committed offset for partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:31,260] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] task [0_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:31,263] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] Restoration took 1613 ms for all tasks [0_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:31,264] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:31,276] INFO stream-client [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c] State transition from REBALANCING to RUNNING (org.apache.kafka.streams.KafkaStreams)
control-center   | [2023-08-04 11:16:31,281] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Requesting the log end offset for _confluent-command-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:31,988] INFO Streams state is RUNNING (io.confluent.command.kafka.CommandStore)
control-center   | [2023-08-04 11:16:33,035] INFO action=started topology=command (io.confluent.controlcenter.application.AllControlCenter)
control-center   | [2023-08-04 11:16:33,035] INFO action=starting operation=command-migration  (io.confluent.controlcenter.application.AllControlCenter)
control-center   | [2023-08-04 11:16:33,062] INFO action=completed operation=command-migration (io.confluent.controlcenter.application.AllControlCenter)
control-center   | [2023-08-04 11:16:33,062] INFO action=starting topology=monitoring (io.confluent.controlcenter.application.AllControlCenter)
control-center   | [2023-08-04 11:16:33,154] WARN Deprecated config cache.max.bytes.buffering is set, and will be used; we suggest setting the new config statestore.cache.max.bytes instead as deprecated cache.max.bytes.buffering would be removed in the future. (org.apache.kafka.streams.internals.StreamsConfigUtils)
control-center   | [2023-08-04 11:16:33,280] INFO No process id found on disk, got fresh process id 8bb38cc0-eb32-4be5-be61-027cb1582feb (org.apache.kafka.streams.processor.internals.StateDirectory)
control-center   | [2023-08-04 11:16:33,292] INFO AdminClientConfig values: 
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-admin
control-center   | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	confluent.use.controller.listener = false
control-center   | 	connections.max.idle.ms = 300000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center   | [2023-08-04 11:16:33,321] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,324] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,325] INFO Kafka startTimeMs: 1691147793321 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,328] INFO stream-client [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb] Kafka Streams version: 7.4.1-ce (org.apache.kafka.streams.KafkaStreams)
control-center   | [2023-08-04 11:16:33,329] INFO stream-client [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb] Kafka Streams commit ID: 96cc303d3f85bf31 (org.apache.kafka.streams.KafkaStreams)
control-center   | [2023-08-04 11:16:33,333] WARN Deprecated config cache.max.bytes.buffering is set, and will be used; we suggest setting the new config statestore.cache.max.bytes instead as deprecated cache.max.bytes.buffering would be removed in the future. (org.apache.kafka.streams.internals.StreamsConfigUtils)
control-center   | [2023-08-04 11:16:33,336] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:33,343] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = true
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-restore-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = null
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:33,380] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,382] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,383] INFO Kafka startTimeMs: 1691147793380 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,389] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:33,393] INFO ProducerConfig values: 
control-center   | 	acks = -1
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	batch.size = 16384
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffer.memory = 33554432
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-producer
control-center   | 	compression.type = lz4
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	delivery.timeout.ms = 2147483647
control-center   | 	enable.idempotence = true
control-center   | 	interceptor.classes = []
control-center   | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   | 	linger.ms = 500
control-center   | 	max.block.ms = 9223372036854775807
control-center   | 	max.in.flight.requests.per.connection = 5
control-center   | 	max.request.size = 10485760
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metadata.max.idle.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partitioner.adaptive.partitioning.enable = true
control-center   | 	partitioner.availability.timeout.ms = 0
control-center   | 	partitioner.class = null
control-center   | 	partitioner.ignore.keys = false
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	transaction.timeout.ms = 60000
control-center   | 	transactional.id = null
control-center   | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center   | [2023-08-04 11:16:33,396] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center   | [2023-08-04 11:16:33,412] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,430] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,433] INFO Kafka startTimeMs: 1691147793411 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,439] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:33,460] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = false
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = _confluent-controlcenter-7-4-1-1
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:33,489] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:33,535] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:33,547] WARN stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:33,556] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-producer] ProducerId set to 3 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center   | [2023-08-04 11:16:33,567] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,572] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,572] INFO Kafka startTimeMs: 1691147793566 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,584] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:33,588] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = true
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-restore-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = null
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:33,627] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,627] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,629] INFO Kafka startTimeMs: 1691147793627 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,630] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:33,633] INFO ProducerConfig values: 
control-center   | 	acks = -1
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	batch.size = 16384
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffer.memory = 33554432
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-producer
control-center   | 	compression.type = lz4
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	delivery.timeout.ms = 2147483647
control-center   | 	enable.idempotence = true
control-center   | 	interceptor.classes = []
control-center   | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   | 	linger.ms = 500
control-center   | 	max.block.ms = 9223372036854775807
control-center   | 	max.in.flight.requests.per.connection = 5
control-center   | 	max.request.size = 10485760
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metadata.max.idle.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partitioner.adaptive.partitioning.enable = true
control-center   | 	partitioner.availability.timeout.ms = 0
control-center   | 	partitioner.class = null
control-center   | 	partitioner.ignore.keys = false
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	transaction.timeout.ms = 60000
control-center   | 	transactional.id = null
control-center   | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center   | [2023-08-04 11:16:33,636] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center   | [2023-08-04 11:16:33,690] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,698] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,698] INFO Kafka startTimeMs: 1691147793690 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,700] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:33,703] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = false
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = _confluent-controlcenter-7-4-1-1
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:33,773] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:33,782] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:33,787] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-producer] ProducerId set to 4 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center   | [2023-08-04 11:16:33,783] WARN stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:33,800] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,800] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,800] INFO Kafka startTimeMs: 1691147793800 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,827] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:33,830] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = true
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-restore-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = null
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:33,851] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,851] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,851] INFO Kafka startTimeMs: 1691147793850 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,852] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:33,855] INFO ProducerConfig values: 
control-center   | 	acks = -1
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	batch.size = 16384
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffer.memory = 33554432
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-producer
control-center   | 	compression.type = lz4
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	delivery.timeout.ms = 2147483647
control-center   | 	enable.idempotence = true
control-center   | 	interceptor.classes = []
control-center   | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   | 	linger.ms = 500
control-center   | 	max.block.ms = 9223372036854775807
control-center   | 	max.in.flight.requests.per.connection = 5
control-center   | 	max.request.size = 10485760
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metadata.max.idle.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partitioner.adaptive.partitioning.enable = true
control-center   | 	partitioner.availability.timeout.ms = 0
control-center   | 	partitioner.class = null
control-center   | 	partitioner.ignore.keys = false
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	transaction.timeout.ms = 60000
control-center   | 	transactional.id = null
control-center   | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center   | [2023-08-04 11:16:33,871] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center   | [2023-08-04 11:16:33,904] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,905] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,906] INFO Kafka startTimeMs: 1691147793903 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:33,909] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:33,949] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:33,957] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = false
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = _confluent-controlcenter-7-4-1-1
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:33,968] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-producer] ProducerId set to 5 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center   | [2023-08-04 11:16:33,988] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:33,992] WARN stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:34,002] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,004] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,004] INFO Kafka startTimeMs: 1691147794002 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,010] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:34,016] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = true
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-restore-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = null
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:34,041] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,041] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,041] INFO Kafka startTimeMs: 1691147794041 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,042] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:34,045] INFO ProducerConfig values: 
control-center   | 	acks = -1
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	batch.size = 16384
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffer.memory = 33554432
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-producer
control-center   | 	compression.type = lz4
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	delivery.timeout.ms = 2147483647
control-center   | 	enable.idempotence = true
control-center   | 	interceptor.classes = []
control-center   | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   | 	linger.ms = 500
control-center   | 	max.block.ms = 9223372036854775807
control-center   | 	max.in.flight.requests.per.connection = 5
control-center   | 	max.request.size = 10485760
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metadata.max.idle.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partitioner.adaptive.partitioning.enable = true
control-center   | 	partitioner.availability.timeout.ms = 0
control-center   | 	partitioner.class = null
control-center   | 	partitioner.ignore.keys = false
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	transaction.timeout.ms = 60000
control-center   | 	transactional.id = null
control-center   | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center   | [2023-08-04 11:16:34,047] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center   | [2023-08-04 11:16:34,074] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,076] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,077] INFO Kafka startTimeMs: 1691147794074 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,079] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:34,081] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = false
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = _confluent-controlcenter-7-4-1-1
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:34,126] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:34,131] WARN stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:34,150] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,153] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,156] INFO Kafka startTimeMs: 1691147794150 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,169] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:34,183] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-producer] ProducerId set to 6 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center   | [2023-08-04 11:16:34,180] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:34,189] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = true
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-restore-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = null
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:34,211] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,212] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,212] INFO Kafka startTimeMs: 1691147794211 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,213] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:34,216] INFO ProducerConfig values: 
control-center   | 	acks = -1
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	batch.size = 16384
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffer.memory = 33554432
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-producer
control-center   | 	compression.type = lz4
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	delivery.timeout.ms = 2147483647
control-center   | 	enable.idempotence = true
control-center   | 	interceptor.classes = []
control-center   | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   | 	linger.ms = 500
control-center   | 	max.block.ms = 9223372036854775807
control-center   | 	max.in.flight.requests.per.connection = 5
control-center   | 	max.request.size = 10485760
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metadata.max.idle.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partitioner.adaptive.partitioning.enable = true
control-center   | 	partitioner.availability.timeout.ms = 0
control-center   | 	partitioner.class = null
control-center   | 	partitioner.ignore.keys = false
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	transaction.timeout.ms = 60000
control-center   | 	transactional.id = null
control-center   | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center   | [2023-08-04 11:16:34,222] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center   | [2023-08-04 11:16:34,260] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,271] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,275] INFO Kafka startTimeMs: 1691147794259 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,286] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:34,298] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = false
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = _confluent-controlcenter-7-4-1-1
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:34,333] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:34,365] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-producer] ProducerId set to 7 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center   | [2023-08-04 11:16:34,374] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:34,384] WARN stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:34,394] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,394] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,394] INFO Kafka startTimeMs: 1691147794394 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,399] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:34,401] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = true
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-restore-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = null
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:34,420] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,420] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,421] INFO Kafka startTimeMs: 1691147794420 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,426] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:34,428] INFO ProducerConfig values: 
control-center   | 	acks = -1
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	batch.size = 16384
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffer.memory = 33554432
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-producer
control-center   | 	compression.type = lz4
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	delivery.timeout.ms = 2147483647
control-center   | 	enable.idempotence = true
control-center   | 	interceptor.classes = []
control-center   | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   | 	linger.ms = 500
control-center   | 	max.block.ms = 9223372036854775807
control-center   | 	max.in.flight.requests.per.connection = 5
control-center   | 	max.request.size = 10485760
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metadata.max.idle.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partitioner.adaptive.partitioning.enable = true
control-center   | 	partitioner.availability.timeout.ms = 0
control-center   | 	partitioner.class = null
control-center   | 	partitioner.ignore.keys = false
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	transaction.timeout.ms = 60000
control-center   | 	transactional.id = null
control-center   | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center   | [2023-08-04 11:16:34,431] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center   | [2023-08-04 11:16:34,468] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,472] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,482] INFO Kafka startTimeMs: 1691147794468 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,499] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:34,513] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = false
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = _confluent-controlcenter-7-4-1-1
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:34,541] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:34,551] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-producer] ProducerId set to 8 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center   | [2023-08-04 11:16:34,554] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:34,559] WARN stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:34,585] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,585] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,585] INFO Kafka startTimeMs: 1691147794585 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,596] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:34,598] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = true
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-restore-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = null
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:34,613] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,613] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,613] INFO Kafka startTimeMs: 1691147794613 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,614] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:34,618] INFO ProducerConfig values: 
control-center   | 	acks = -1
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	batch.size = 16384
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffer.memory = 33554432
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-producer
control-center   | 	compression.type = lz4
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	delivery.timeout.ms = 2147483647
control-center   | 	enable.idempotence = true
control-center   | 	interceptor.classes = []
control-center   | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   | 	linger.ms = 500
control-center   | 	max.block.ms = 9223372036854775807
control-center   | 	max.in.flight.requests.per.connection = 5
control-center   | 	max.request.size = 10485760
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metadata.max.idle.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partitioner.adaptive.partitioning.enable = true
control-center   | 	partitioner.availability.timeout.ms = 0
control-center   | 	partitioner.class = null
control-center   | 	partitioner.ignore.keys = false
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	transaction.timeout.ms = 60000
control-center   | 	transactional.id = null
control-center   | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center   | [2023-08-04 11:16:34,622] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center   | [2023-08-04 11:16:34,802] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,802] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,802] INFO Kafka startTimeMs: 1691147794802 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,803] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:34,807] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = false
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = _confluent-controlcenter-7-4-1-1
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:34,827] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:34,828] WARN stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:34,839] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,842] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,843] INFO Kafka startTimeMs: 1691147794839 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,851] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:34,852] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = true
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-restore-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = null
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:34,853] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:34,864] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-producer] ProducerId set to 9 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center   | [2023-08-04 11:16:34,901] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,913] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,913] INFO Kafka startTimeMs: 1691147794901 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:34,914] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:34,941] INFO ProducerConfig values: 
control-center   | 	acks = -1
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	batch.size = 16384
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffer.memory = 33554432
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-producer
control-center   | 	compression.type = lz4
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	delivery.timeout.ms = 2147483647
control-center   | 	enable.idempotence = true
control-center   | 	interceptor.classes = []
control-center   | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   | 	linger.ms = 500
control-center   | 	max.block.ms = 9223372036854775807
control-center   | 	max.in.flight.requests.per.connection = 5
control-center   | 	max.request.size = 10485760
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metadata.max.idle.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partitioner.adaptive.partitioning.enable = true
control-center   | 	partitioner.availability.timeout.ms = 0
control-center   | 	partitioner.class = null
control-center   | 	partitioner.ignore.keys = false
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	transaction.timeout.ms = 60000
control-center   | 	transactional.id = null
control-center   | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center   | [2023-08-04 11:16:34,955] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center   | [2023-08-04 11:16:35,036] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,037] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,037] INFO Kafka startTimeMs: 1691147795036 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,038] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:35,044] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = false
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = _confluent-controlcenter-7-4-1-1
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:35,129] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:35,130] WARN stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:35,132] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:35,150] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-producer] ProducerId set to 10 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center   | [2023-08-04 11:16:35,165] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,173] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,174] INFO Kafka startTimeMs: 1691147795165 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,192] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:35,206] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = true
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-restore-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = null
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:35,236] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,237] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,237] INFO Kafka startTimeMs: 1691147795236 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,242] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:35,245] INFO ProducerConfig values: 
control-center   | 	acks = -1
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	batch.size = 16384
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffer.memory = 33554432
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-producer
control-center   | 	compression.type = lz4
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	delivery.timeout.ms = 2147483647
control-center   | 	enable.idempotence = true
control-center   | 	interceptor.classes = []
control-center   | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   | 	linger.ms = 500
control-center   | 	max.block.ms = 9223372036854775807
control-center   | 	max.in.flight.requests.per.connection = 5
control-center   | 	max.request.size = 10485760
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metadata.max.idle.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partitioner.adaptive.partitioning.enable = true
control-center   | 	partitioner.availability.timeout.ms = 0
control-center   | 	partitioner.class = null
control-center   | 	partitioner.ignore.keys = false
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	transaction.timeout.ms = 60000
control-center   | 	transactional.id = null
control-center   | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center   | [2023-08-04 11:16:35,260] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center   | [2023-08-04 11:16:35,272] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,272] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,272] INFO Kafka startTimeMs: 1691147795272 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,275] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:35,279] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = false
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = _confluent-controlcenter-7-4-1-1
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:35,361] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:35,376] WARN stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:35,376] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:35,405] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-producer] ProducerId set to 11 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center   | [2023-08-04 11:16:35,409] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,417] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,417] INFO Kafka startTimeMs: 1691147795409 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,439] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:35,458] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = true
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-restore-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = null
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:35,524] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,528] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,535] INFO Kafka startTimeMs: 1691147795524 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,538] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:35,542] INFO ProducerConfig values: 
control-center   | 	acks = -1
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	batch.size = 16384
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffer.memory = 33554432
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-producer
control-center   | 	compression.type = lz4
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	delivery.timeout.ms = 2147483647
control-center   | 	enable.idempotence = true
control-center   | 	interceptor.classes = []
control-center   | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   | 	linger.ms = 500
control-center   | 	max.block.ms = 9223372036854775807
control-center   | 	max.in.flight.requests.per.connection = 5
control-center   | 	max.request.size = 10485760
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metadata.max.idle.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partitioner.adaptive.partitioning.enable = true
control-center   | 	partitioner.availability.timeout.ms = 0
control-center   | 	partitioner.class = null
control-center   | 	partitioner.ignore.keys = false
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	transaction.timeout.ms = 60000
control-center   | 	transactional.id = null
control-center   | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center   | [2023-08-04 11:16:35,569] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center   | [2023-08-04 11:16:35,616] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,618] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,619] INFO Kafka startTimeMs: 1691147795615 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,621] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:35,627] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = false
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = _confluent-controlcenter-7-4-1-1
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:35,726] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:35,741] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-producer] ProducerId set to 12 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center   | [2023-08-04 11:16:35,777] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:35,779] WARN stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:35,784] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,787] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,788] INFO Kafka startTimeMs: 1691147795784 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,806] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:35,810] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = true
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-restore-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = null
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:35,860] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,864] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,866] INFO Kafka startTimeMs: 1691147795860 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,868] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:35,870] INFO ProducerConfig values: 
control-center   | 	acks = -1
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	batch.size = 16384
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffer.memory = 33554432
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-producer
control-center   | 	compression.type = lz4
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	delivery.timeout.ms = 2147483647
control-center   | 	enable.idempotence = true
control-center   | 	interceptor.classes = []
control-center   | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   | 	linger.ms = 500
control-center   | 	max.block.ms = 9223372036854775807
control-center   | 	max.in.flight.requests.per.connection = 5
control-center   | 	max.request.size = 10485760
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metadata.max.idle.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partitioner.adaptive.partitioning.enable = true
control-center   | 	partitioner.availability.timeout.ms = 0
control-center   | 	partitioner.class = null
control-center   | 	partitioner.ignore.keys = false
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	transaction.timeout.ms = 60000
control-center   | 	transactional.id = null
control-center   | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center   | [2023-08-04 11:16:35,881] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center   | [2023-08-04 11:16:35,897] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,898] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,899] INFO Kafka startTimeMs: 1691147795897 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,900] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:35,902] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = false
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = _confluent-controlcenter-7-4-1-1
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:35,928] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:35,930] WARN stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:35,957] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,980] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,981] INFO Kafka startTimeMs: 1691147795947 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:35,992] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,013] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-producer] ProducerId set to 13 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center   | [2023-08-04 11:16:35,998] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] Creating restore consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,016] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = true
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-restore-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = null
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:36,120] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:36,120] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:36,120] INFO Kafka startTimeMs: 1691147796120 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:36,123] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] Creating thread producer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,125] INFO ProducerConfig values: 
control-center   | 	acks = -1
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	batch.size = 16384
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	buffer.memory = 33554432
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-producer
control-center   | 	compression.type = lz4
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	delivery.timeout.ms = 2147483647
control-center   | 	enable.idempotence = true
control-center   | 	interceptor.classes = []
control-center   | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   | 	linger.ms = 500
control-center   | 	max.block.ms = 9223372036854775807
control-center   | 	max.in.flight.requests.per.connection = 5
control-center   | 	max.request.size = 10485760
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metadata.max.idle.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partitioner.adaptive.partitioning.enable = true
control-center   | 	partitioner.availability.timeout.ms = 0
control-center   | 	partitioner.class = null
control-center   | 	partitioner.ignore.keys = false
control-center   | 	receive.buffer.bytes = 32768
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	transaction.timeout.ms = 60000
control-center   | 	transactional.id = null
control-center   | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
control-center   |  (org.apache.kafka.clients.producer.ProducerConfig)
control-center   | [2023-08-04 11:16:36,140] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-producer] Instantiated an idempotent producer. (org.apache.kafka.clients.producer.KafkaProducer)
control-center   | [2023-08-04 11:16:36,216] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:36,216] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:36,217] INFO Kafka startTimeMs: 1691147796215 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:36,218] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] Creating consumer client (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,221] INFO ConsumerConfig values: 
control-center   | 	allow.auto.create.topics = false
control-center   | 	auto.commit.interval.ms = 5000
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	auto.offset.reset = none
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	check.crcs = true
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer
control-center   | 	client.rack = 
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	connections.max.idle.ms = 540000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	enable.auto.commit = false
control-center   | 	exclude.internal.topics = true
control-center   | 	fetch.max.bytes = 52428800
control-center   | 	fetch.max.wait.ms = 500
control-center   | 	fetch.min.bytes = 1
control-center   | 	group.id = _confluent-controlcenter-7-4-1-1
control-center   | 	group.instance.id = null
control-center   | 	heartbeat.interval.ms = 3000
control-center   | 	interceptor.classes = []
control-center   | 	internal.leave.group.on.close = false
control-center   | 	internal.throw.on.fetch.stable.offset.unsupported = false
control-center   | 	isolation.level = read_uncommitted
control-center   | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   | 	max.partition.fetch.bytes = 1048576
control-center   | 	max.poll.interval.ms = 21600000
control-center   | 	max.poll.records = 100
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	session.timeout.ms = 60000
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
control-center   |  (org.apache.kafka.clients.consumer.ConsumerConfig)
control-center   | [2023-08-04 11:16:36,326] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,365] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-producer] ProducerId set to 14 with epoch 0 (org.apache.kafka.clients.producer.internals.TransactionManager)
control-center   | [2023-08-04 11:16:36,367] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer] Eager rebalancing protocol is enabled now for upgrade from 2.3.x (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:36,368] WARN stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer] The eager rebalancing protocol is deprecated and will stop being supported in a future release. Please be prepared to remove the 'upgrade.from' config soon. (org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration)
control-center   | [2023-08-04 11:16:36,375] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:36,377] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:36,378] INFO Kafka startTimeMs: 1691147796375 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:36,408] INFO stream-client [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb] State transition from CREATED to REBALANCING (org.apache.kafka.streams.KafkaStreams)
control-center   | [2023-08-04 11:16:36,413] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,414] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,417] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,418] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,443] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,430] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:36,446] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,449] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:36,454] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:36,511] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to e66bfUqWTZWX8U4X4Ipjsw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,627] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,626] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,659] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,624] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,665] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,623] INFO stream-client [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb] Started 12 stream threads (org.apache.kafka.streams.KafkaStreams)
control-center   | [2023-08-04 11:16:36,603] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,596] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,675] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,593] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,570] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,560] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,678] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,678] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,553] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to e66bfUqWTZWX8U4X4Ipjsw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,685] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to Y_GSqt8QROagfczJg3r40g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,551] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] Starting (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,694] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 8fgQicUSRVi7xJcKYztVRQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,538] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to e66bfUqWTZWX8U4X4Ipjsw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,695] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to yh8_-dziTp-s7IDqy8w3Rw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,695] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to Y_GSqt8QROagfczJg3r40g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,695] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to iJLqc_VsS6GtzjQhq4pkTQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,695] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 8fgQicUSRVi7xJcKYztVRQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,695] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,678] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,676] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,658] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,640] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to Y_GSqt8QROagfczJg3r40g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,711] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 8fgQicUSRVi7xJcKYztVRQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,712] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to yh8_-dziTp-s7IDqy8w3Rw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,705] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,705] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:36,705] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:36,705] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:36,704] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:36,704] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:36,695] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to yh8_-dziTp-s7IDqy8w3Rw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,921] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to iJLqc_VsS6GtzjQhq4pkTQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,922] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,924] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,933] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,934] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,935] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,936] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,937] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,962] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,963] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,964] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,965] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,965] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,966] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to f6T0fWnJTtClUuyvYTjZBw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,968] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to RXByHOqASZ2CbNrZqK_AMw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,968] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to SR4XL91XSTmhEPn2HtkHYQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,972] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 44Yp-2aPRK-xKM9zKUCEJw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,976] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to RifsNDOQSCG4j2fz_vLz5g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,977] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to UygM9SIITY-KoEx4I2w6zw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,979] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to aFs6ffa2RL65YNQHKdNm3A (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,984] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to KabcuEMhTCOz9nkNgpIsHg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,985] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 7J5ZVEciQBa64RcXLSLhrw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,880] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to e66bfUqWTZWX8U4X4Ipjsw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,878] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:36,729] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:36,729] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:36,719] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:36,718] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Subscribed to topic(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey, _confluent-metrics, _confluent-monitoring (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:36,718] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:36,714] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to iJLqc_VsS6GtzjQhq4pkTQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,711] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,042] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,045] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,049] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,052] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,053] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,054] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,055] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,059] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,060] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,061] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,065] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,067] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,068] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to f6T0fWnJTtClUuyvYTjZBw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,069] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to RXByHOqASZ2CbNrZqK_AMw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,023] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to e66bfUqWTZWX8U4X4Ipjsw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,117] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to Y_GSqt8QROagfczJg3r40g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,117] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 8fgQicUSRVi7xJcKYztVRQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,020] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to Y_GSqt8QROagfczJg3r40g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,120] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to yh8_-dziTp-s7IDqy8w3Rw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,020] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:37,014] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:37,014] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:36,977] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:37,168] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:37,175] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:37,190] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to e66bfUqWTZWX8U4X4Ipjsw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,120] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to iJLqc_VsS6GtzjQhq4pkTQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,223] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,118] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to SR4XL91XSTmhEPn2HtkHYQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,044] INFO tocheck=[Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=AlertHistoryStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=Group, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=TriggerEventsStore, rollup=false}, Store{name=group-aggregate-store, rollup=true}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:37,246] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:37,043] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,249] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,250] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,251] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,252] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,252] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,253] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,254] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,257] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,260] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,261] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to f6T0fWnJTtClUuyvYTjZBw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,261] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to RXByHOqASZ2CbNrZqK_AMw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,264] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to SR4XL91XSTmhEPn2HtkHYQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,265] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 44Yp-2aPRK-xKM9zKUCEJw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,265] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to RifsNDOQSCG4j2fz_vLz5g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,266] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to UygM9SIITY-KoEx4I2w6zw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,268] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to aFs6ffa2RL65YNQHKdNm3A (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,277] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to KabcuEMhTCOz9nkNgpIsHg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,279] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 7J5ZVEciQBa64RcXLSLhrw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,281] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,290] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,237] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to Y_GSqt8QROagfczJg3r40g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,303] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 8fgQicUSRVi7xJcKYztVRQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,226] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,308] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,308] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,309] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,309] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,309] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,314] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,315] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,316] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,318] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,320] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,327] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to f6T0fWnJTtClUuyvYTjZBw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,328] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to RXByHOqASZ2CbNrZqK_AMw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,329] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to SR4XL91XSTmhEPn2HtkHYQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,331] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 44Yp-2aPRK-xKM9zKUCEJw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,332] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to RifsNDOQSCG4j2fz_vLz5g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,333] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to UygM9SIITY-KoEx4I2w6zw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,335] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to aFs6ffa2RL65YNQHKdNm3A (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,343] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to KabcuEMhTCOz9nkNgpIsHg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,218] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to e66bfUqWTZWX8U4X4Ipjsw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,345] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to Y_GSqt8QROagfczJg3r40g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,348] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 8fgQicUSRVi7xJcKYztVRQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,350] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to yh8_-dziTp-s7IDqy8w3Rw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,354] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to iJLqc_VsS6GtzjQhq4pkTQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,205] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 8fgQicUSRVi7xJcKYztVRQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,359] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,205] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to e66bfUqWTZWX8U4X4Ipjsw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,361] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to Y_GSqt8QROagfczJg3r40g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,177] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:37,171] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,171] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:37,374] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 8fgQicUSRVi7xJcKYztVRQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,395] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to yh8_-dziTp-s7IDqy8w3Rw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,397] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to iJLqc_VsS6GtzjQhq4pkTQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,405] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,405] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,406] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to e66bfUqWTZWX8U4X4Ipjsw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,410] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to Y_GSqt8QROagfczJg3r40g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,360] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,359] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to yh8_-dziTp-s7IDqy8w3Rw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,346] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 7J5ZVEciQBa64RcXLSLhrw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,338] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:37,335] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to yh8_-dziTp-s7IDqy8w3Rw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,449] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to iJLqc_VsS6GtzjQhq4pkTQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,451] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,278] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to e66bfUqWTZWX8U4X4Ipjsw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,270] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to e66bfUqWTZWX8U4X4Ipjsw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,453] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to Y_GSqt8QROagfczJg3r40g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,267] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 44Yp-2aPRK-xKM9zKUCEJw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,453] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to Y_GSqt8QROagfczJg3r40g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,451] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,465] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,444] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,468] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,468] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,468] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,468] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,468] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,469] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,470] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,470] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,470] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,471] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to f6T0fWnJTtClUuyvYTjZBw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,471] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to RXByHOqASZ2CbNrZqK_AMw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,471] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to SR4XL91XSTmhEPn2HtkHYQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,471] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 44Yp-2aPRK-xKM9zKUCEJw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,472] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to RifsNDOQSCG4j2fz_vLz5g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,472] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to UygM9SIITY-KoEx4I2w6zw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,440] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to iJLqc_VsS6GtzjQhq4pkTQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,411] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 8fgQicUSRVi7xJcKYztVRQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,478] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to yh8_-dziTp-s7IDqy8w3Rw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,479] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to iJLqc_VsS6GtzjQhq4pkTQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,483] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,406] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,497] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,497] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,497] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,497] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,497] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,497] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,497] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,497] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,497] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,497] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to f6T0fWnJTtClUuyvYTjZBw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,497] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to RXByHOqASZ2CbNrZqK_AMw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,497] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to SR4XL91XSTmhEPn2HtkHYQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,497] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 44Yp-2aPRK-xKM9zKUCEJw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,498] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to RifsNDOQSCG4j2fz_vLz5g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,498] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to UygM9SIITY-KoEx4I2w6zw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,498] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to aFs6ffa2RL65YNQHKdNm3A (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,498] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to KabcuEMhTCOz9nkNgpIsHg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,498] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 7J5ZVEciQBa64RcXLSLhrw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,404] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to e66bfUqWTZWX8U4X4Ipjsw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,499] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to Y_GSqt8QROagfczJg3r40g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,500] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 8fgQicUSRVi7xJcKYztVRQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,500] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to yh8_-dziTp-s7IDqy8w3Rw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,500] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to iJLqc_VsS6GtzjQhq4pkTQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,500] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,501] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,501] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,501] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,501] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,501] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,502] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,502] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,502] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,502] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,502] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,503] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,503] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to f6T0fWnJTtClUuyvYTjZBw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,503] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to RXByHOqASZ2CbNrZqK_AMw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,399] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:37,509] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to SR4XL91XSTmhEPn2HtkHYQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,509] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 44Yp-2aPRK-xKM9zKUCEJw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,510] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to RifsNDOQSCG4j2fz_vLz5g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,506] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,513] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,492] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,520] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,492] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,479] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,478] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,530] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,531] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,532] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,536] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,473] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to aFs6ffa2RL65YNQHKdNm3A (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,541] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to KabcuEMhTCOz9nkNgpIsHg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,461] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 8fgQicUSRVi7xJcKYztVRQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,542] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 7J5ZVEciQBa64RcXLSLhrw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,545] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,454] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,555] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,558] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,454] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 8fgQicUSRVi7xJcKYztVRQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,453] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to RifsNDOQSCG4j2fz_vLz5g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,560] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to UygM9SIITY-KoEx4I2w6zw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,563] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to yh8_-dziTp-s7IDqy8w3Rw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,564] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to iJLqc_VsS6GtzjQhq4pkTQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,565] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,566] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,546] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,542] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to yh8_-dziTp-s7IDqy8w3Rw (org.apache.kafka.clients.Metadata)
broker           | [2023-08-04 11:16:37,557] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer-79350b4a-0a76-4baf-94c8-735c98919f40 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:37,527] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,570] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,572] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,578] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,579] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,580] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,581] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,582] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,583] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,585] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
broker           | [2023-08-04 11:16:37,590] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer-bca356e0-f66d-4c3f-b454-d74e29dfdef6 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:37,512] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to UygM9SIITY-KoEx4I2w6zw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,510] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,595] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to aFs6ffa2RL65YNQHKdNm3A (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,596] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to KabcuEMhTCOz9nkNgpIsHg (org.apache.kafka.clients.Metadata)
broker           | [2023-08-04 11:16:37,609] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer-cc2d2982-77f6-48d2-b91f-066c626cbd58 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:37,617] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 7J5ZVEciQBa64RcXLSLhrw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,620] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,625] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,583] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,648] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,649] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,577] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,575] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,656] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,657] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,657] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,657] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,659] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,661] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
broker           | [2023-08-04 11:16:37,665] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer-da0b159a-557c-48a5-8475-833ada88b018 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:37,665] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,667] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,667] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,571] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to iJLqc_VsS6GtzjQhq4pkTQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,668] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,669] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,670] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,566] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,672] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,672] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,673] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,674] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,677] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,678] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,680] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,680] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to f6T0fWnJTtClUuyvYTjZBw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,564] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to aFs6ffa2RL65YNQHKdNm3A (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,678] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,681] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,682] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,682] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,683] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,684] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,684] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,685] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,685] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,686] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to f6T0fWnJTtClUuyvYTjZBw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,687] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to RXByHOqASZ2CbNrZqK_AMw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,688] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to SR4XL91XSTmhEPn2HtkHYQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,693] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 44Yp-2aPRK-xKM9zKUCEJw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,697] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to RifsNDOQSCG4j2fz_vLz5g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,698] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to UygM9SIITY-KoEx4I2w6zw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,699] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to aFs6ffa2RL65YNQHKdNm3A (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,699] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to KabcuEMhTCOz9nkNgpIsHg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,701] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 7J5ZVEciQBa64RcXLSLhrw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,675] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer-da0b159a-557c-48a5-8475-833ada88b018 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,650] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,714] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,716] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to f6T0fWnJTtClUuyvYTjZBw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,722] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to RXByHOqASZ2CbNrZqK_AMw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,724] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to SR4XL91XSTmhEPn2HtkHYQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,724] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 44Yp-2aPRK-xKM9zKUCEJw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,628] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,734] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to f6T0fWnJTtClUuyvYTjZBw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,736] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to RXByHOqASZ2CbNrZqK_AMw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,738] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to SR4XL91XSTmhEPn2HtkHYQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,740] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 44Yp-2aPRK-xKM9zKUCEJw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,742] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to RifsNDOQSCG4j2fz_vLz5g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,744] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to UygM9SIITY-KoEx4I2w6zw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,754] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to aFs6ffa2RL65YNQHKdNm3A (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,757] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to KabcuEMhTCOz9nkNgpIsHg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,758] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 7J5ZVEciQBa64RcXLSLhrw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,616] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer-79350b4a-0a76-4baf-94c8-735c98919f40 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,615] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer-cc2d2982-77f6-48d2-b91f-066c626cbd58 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,762] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,614] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer-bca356e0-f66d-4c3f-b454-d74e29dfdef6 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,762] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,763] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,763] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,762] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,764] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,740] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to RifsNDOQSCG4j2fz_vLz5g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,802] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to UygM9SIITY-KoEx4I2w6zw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,802] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to aFs6ffa2RL65YNQHKdNm3A (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,722] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,713] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
broker           | [2023-08-04 11:16:37,805] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 0 (__consumer_offsets-5) (reason: Adding new member _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer-79350b4a-0a76-4baf-94c8-735c98919f40 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:37,713] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,681] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to f6T0fWnJTtClUuyvYTjZBw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,807] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to RXByHOqASZ2CbNrZqK_AMw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,808] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to SR4XL91XSTmhEPn2HtkHYQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,808] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 44Yp-2aPRK-xKM9zKUCEJw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,808] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to RifsNDOQSCG4j2fz_vLz5g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,808] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to UygM9SIITY-KoEx4I2w6zw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,808] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to aFs6ffa2RL65YNQHKdNm3A (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,808] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to KabcuEMhTCOz9nkNgpIsHg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,809] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 7J5ZVEciQBa64RcXLSLhrw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,811] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,681] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to RXByHOqASZ2CbNrZqK_AMw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,846] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to SR4XL91XSTmhEPn2HtkHYQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,847] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 44Yp-2aPRK-xKM9zKUCEJw (org.apache.kafka.clients.Metadata)
broker           | [2023-08-04 11:16:37,853] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1 generation 1 (__consumer_offsets-5) with 3 members (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:37,681] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to KabcuEMhTCOz9nkNgpIsHg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,857] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 7J5ZVEciQBa64RcXLSLhrw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,862] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,871] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,874] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:37,845] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,814] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,818] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to KabcuEMhTCOz9nkNgpIsHg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,890] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 7J5ZVEciQBa64RcXLSLhrw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,816] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,779] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,764] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,894] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,927] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:37,926] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in CompletingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer-e891406f-82dd-47f9-80ee-af3c7b1bfc86 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:37,881] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=1, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer-cc2d2982-77f6-48d2-b91f-066c626cbd58', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,859] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to RifsNDOQSCG4j2fz_vLz5g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:37,910] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,909] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=1, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer-bca356e0-f66d-4c3f-b454-d74e29dfdef6', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,930] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,902] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:37,988] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 1 (__consumer_offsets-5) (reason: Adding new member _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer-da0b159a-557c-48a5-8475-833ada88b018 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:37,900] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=1, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer-79350b4a-0a76-4baf-94c8-735c98919f40', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,899] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:38,017] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:38,068] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer-f1125336-cd8c-4bc8-aebe-ea2de793d5c7 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:38,005] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=1, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer-bca356e0-f66d-4c3f-b454-d74e29dfdef6', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,977] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer-e891406f-82dd-47f9-80ee-af3c7b1bfc86 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:37,934] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to UygM9SIITY-KoEx4I2w6zw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:38,078] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:38,091] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to aFs6ffa2RL65YNQHKdNm3A (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:38,101] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to KabcuEMhTCOz9nkNgpIsHg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:38,101] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 7J5ZVEciQBa64RcXLSLhrw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:38,102] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
broker           | [2023-08-04 11:16:38,104] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer-6d4a4ab1-803c-46cf-94ab-2b0a96f625be and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:38,117] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,114] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer-f1125336-cd8c-4bc8-aebe-ea2de793d5c7 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,136] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,137] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,113] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,112] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,138] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:38,132] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer-370b82e1-4272-4e2f-8f37-aaf497aaf68b and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:16:38,141] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer-212917c7-9b1c-48ca-a306-3f46e72b8c04 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:38,164] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer-370b82e1-4272-4e2f-8f37-aaf497aaf68b (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,165] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,112] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer-6d4a4ab1-803c-46cf-94ab-2b0a96f625be (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,167] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,167] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,168] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,168] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,167] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer-212917c7-9b1c-48ca-a306-3f46e72b8c04 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,225] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,229] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:38,248] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer-4391ff38-48f7-456b-8df0-f8043a585d5c and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:38,250] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:38,254] INFO tocheck=[Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=AlertHistoryStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=Group, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=TriggerEventsStore, rollup=false}, Store{name=group-aggregate-store, rollup=true}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:38,255] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
broker           | [2023-08-04 11:16:38,309] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer-386bc794-6c1e-42a2-a92f-04bc5c8a13a2 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:38,314] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer-4391ff38-48f7-456b-8df0-f8043a585d5c (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,317] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,318] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,328] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer-386bc794-6c1e-42a2-a92f-04bc5c8a13a2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,333] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,334] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,340] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group is rebalancing, so a rejoin is needed.' (RebalanceInProgressException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,342] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:38,353] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer-e4627d28-a5a6-482b-818a-9ee15a431451 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:38,380] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: need to re-join with the given member-id: _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer-e4627d28-a5a6-482b-818a-9ee15a431451 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,383] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,383] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,394] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=1, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer-cc2d2982-77f6-48d2-b91f-066c626cbd58', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,396] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group is rebalancing, so a rejoin is needed.' (RebalanceInProgressException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,397] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,590] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer] All members participating in this rebalance: 
control-center   | 8bb38cc0-eb32-4be5-be61-027cb1582feb: [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer-bca356e0-f66d-4c3f-b454-d74e29dfdef6, _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer-cc2d2982-77f6-48d2-b91f-066c626cbd58, _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer-79350b4a-0a76-4baf-94c8-735c98919f40]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:38,631] INFO Decided on assignment: {8bb38cc0-eb32-4be5-be61-027cb1582feb=[activeTasks: ([0_0, 1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 10_0, 10_1, 10_2, 10_3, 10_4, 10_5, 10_6, 10_7, 10_8, 10_9, 10_10, 10_11, 11_0, 12_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([1_0=0, 2_0=0, 3_0=0, 4_0=0, 5_0=0, 6_0=0, 7_0=0, 8_0=0, 9_0=0, 11_0=0, 12_0=0]) clientTags: ([]) capacity: 3 assigned: 24]} with no followup probing rebalance. (org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor)
control-center   | [2023-08-04 11:16:38,636] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer] Assigned tasks [1_0, 10_9, 10_8, 2_0, 10_7, 3_0, 10_6, 4_0, 10_5, 5_0, 10_4, 6_0, 10_3, 7_0, 10_2, 8_0, 10_1, 9_0, 10_0, 11_0, 12_0, 10_11, 0_0, 10_10] including stateful [1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 11_0, 12_0] to clients as: 
control-center   | 8bb38cc0-eb32-4be5-be61-027cb1582feb=[activeTasks: ([0_0, 1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 10_0, 10_1, 10_2, 10_3, 10_4, 10_5, 10_6, 10_7, 10_8, 10_9, 10_10, 10_11, 11_0, 12_0]) standbyTasks: ([])]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:38,650] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer] Client 8bb38cc0-eb32-4be5-be61-027cb1582feb per-consumer assignment:
control-center   | 	prev owned active {}
control-center   | 	prev owned standby {_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer-bca356e0-f66d-4c3f-b454-d74e29dfdef6=[], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer-cc2d2982-77f6-48d2-b91f-066c626cbd58=[], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer-79350b4a-0a76-4baf-94c8-735c98919f40=[]}
control-center   | 	assigned active {_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer-bca356e0-f66d-4c3f-b454-d74e29dfdef6=[1_0, 10_8, 4_0, 10_5, 7_0, 10_2, 11_0, 0_0], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer-cc2d2982-77f6-48d2-b91f-066c626cbd58=[10_9, 2_0, 10_6, 5_0, 10_3, 8_0, 10_0, 12_0], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer-79350b4a-0a76-4baf-94c8-735c98919f40=[3_0, 10_7, 6_0, 10_4, 9_0, 10_1, 10_11, 10_10]}
control-center   | 	revoking active {}
control-center   | 	assigned standby {}
control-center   |  (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:38,655] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer] Finished stable assignment of tasks, no followup rebalances required. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:38,657] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Finished assignment for group at generation 1: {_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer-bca356e0-f66d-4c3f-b454-d74e29dfdef6=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, _confluent-metrics-2, _confluent-metrics-5, _confluent-metrics-8, _confluent-monitoring-0], userDataSize=136), _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer-cc2d2982-77f6-48d2-b91f-066c626cbd58=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, _confluent-metrics-0, _confluent-metrics-3, _confluent-metrics-6, _confluent-metrics-9], userDataSize=136), _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer-79350b4a-0a76-4baf-94c8-735c98919f40=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, _confluent-metrics-1, _confluent-metrics-4, _confluent-metrics-7, _confluent-metrics-10, _confluent-metrics-11], userDataSize=160)} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,686] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=1, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer-79350b4a-0a76-4baf-94c8-735c98919f40', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,691] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Request joining group due to: rebalance failed due to 'The group is rebalancing, so a rejoin is needed.' (RebalanceInProgressException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,696] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:38,742] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1 generation 2 (__consumer_offsets-5) with 12 members (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:38,760] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer-bca356e0-f66d-4c3f-b454-d74e29dfdef6', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,793] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer-6d4a4ab1-803c-46cf-94ab-2b0a96f625be', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,795] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer-386bc794-6c1e-42a2-a92f-04bc5c8a13a2', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,837] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer-f1125336-cd8c-4bc8-aebe-ea2de793d5c7', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,842] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer-cc2d2982-77f6-48d2-b91f-066c626cbd58', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,876] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer-4391ff38-48f7-456b-8df0-f8043a585d5c', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,880] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer-370b82e1-4272-4e2f-8f37-aaf497aaf68b', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,875] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer-e4627d28-a5a6-482b-818a-9ee15a431451', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,874] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer-da0b159a-557c-48a5-8475-833ada88b018', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,888] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer-212917c7-9b1c-48ca-a306-3f46e72b8c04', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,916] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer-e891406f-82dd-47f9-80ee-af3c7b1bfc86', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:38,927] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully joined group with generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer-79350b4a-0a76-4baf-94c8-735c98919f40', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,206] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer] All members participating in this rebalance: 
control-center   | 8bb38cc0-eb32-4be5-be61-027cb1582feb: [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer-4391ff38-48f7-456b-8df0-f8043a585d5c, _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer-212917c7-9b1c-48ca-a306-3f46e72b8c04, _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer-e891406f-82dd-47f9-80ee-af3c7b1bfc86, _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer-f1125336-cd8c-4bc8-aebe-ea2de793d5c7, _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer-bca356e0-f66d-4c3f-b454-d74e29dfdef6, _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer-da0b159a-557c-48a5-8475-833ada88b018, _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer-370b82e1-4272-4e2f-8f37-aaf497aaf68b, _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer-cc2d2982-77f6-48d2-b91f-066c626cbd58, _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer-6d4a4ab1-803c-46cf-94ab-2b0a96f625be, _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer-79350b4a-0a76-4baf-94c8-735c98919f40, _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer-386bc794-6c1e-42a2-a92f-04bc5c8a13a2, _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer-e4627d28-a5a6-482b-818a-9ee15a431451]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:39,210] INFO Decided on assignment: {8bb38cc0-eb32-4be5-be61-027cb1582feb=[activeTasks: ([0_0, 1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 10_0, 10_1, 10_2, 10_3, 10_4, 10_5, 10_6, 10_7, 10_8, 10_9, 10_10, 10_11, 11_0, 12_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([1_0=0, 2_0=0, 3_0=0, 4_0=0, 5_0=0, 6_0=0, 7_0=0, 8_0=0, 9_0=0, 11_0=0, 12_0=0]) clientTags: ([]) capacity: 12 assigned: 24]} with no followup probing rebalance. (org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor)
control-center   | [2023-08-04 11:16:39,212] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer] Assigned tasks [1_0, 10_9, 10_8, 2_0, 10_7, 3_0, 10_6, 4_0, 10_5, 5_0, 10_4, 6_0, 10_3, 7_0, 10_2, 8_0, 10_1, 9_0, 10_0, 11_0, 12_0, 10_11, 0_0, 10_10] including stateful [1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 11_0, 12_0] to clients as: 
control-center   | 8bb38cc0-eb32-4be5-be61-027cb1582feb=[activeTasks: ([0_0, 1_0, 2_0, 3_0, 4_0, 5_0, 6_0, 7_0, 8_0, 9_0, 10_0, 10_1, 10_2, 10_3, 10_4, 10_5, 10_6, 10_7, 10_8, 10_9, 10_10, 10_11, 11_0, 12_0]) standbyTasks: ([])]. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:39,233] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer] Client 8bb38cc0-eb32-4be5-be61-027cb1582feb per-consumer assignment:
control-center   | 	prev owned active {}
control-center   | 	prev owned standby {_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer-4391ff38-48f7-456b-8df0-f8043a585d5c=[], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer-212917c7-9b1c-48ca-a306-3f46e72b8c04=[], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer-e891406f-82dd-47f9-80ee-af3c7b1bfc86=[], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer-f1125336-cd8c-4bc8-aebe-ea2de793d5c7=[], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer-bca356e0-f66d-4c3f-b454-d74e29dfdef6=[], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer-da0b159a-557c-48a5-8475-833ada88b018=[], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer-370b82e1-4272-4e2f-8f37-aaf497aaf68b=[], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer-cc2d2982-77f6-48d2-b91f-066c626cbd58=[], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer-6d4a4ab1-803c-46cf-94ab-2b0a96f625be=[], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer-79350b4a-0a76-4baf-94c8-735c98919f40=[], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer-386bc794-6c1e-42a2-a92f-04bc5c8a13a2=[], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer-e4627d28-a5a6-482b-818a-9ee15a431451=[]}
control-center   | 	assigned active {_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer-4391ff38-48f7-456b-8df0-f8043a585d5c=[1_0, 0_0], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer-212917c7-9b1c-48ca-a306-3f46e72b8c04=[2_0, 10_0], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer-e891406f-82dd-47f9-80ee-af3c7b1bfc86=[3_0, 10_1], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer-f1125336-cd8c-4bc8-aebe-ea2de793d5c7=[4_0, 10_2], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer-bca356e0-f66d-4c3f-b454-d74e29dfdef6=[5_0, 10_3], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer-da0b159a-557c-48a5-8475-833ada88b018=[6_0, 10_4], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer-370b82e1-4272-4e2f-8f37-aaf497aaf68b=[10_5, 7_0], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer-cc2d2982-77f6-48d2-b91f-066c626cbd58=[10_6, 8_0], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer-6d4a4ab1-803c-46cf-94ab-2b0a96f625be=[10_7, 9_0], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer-79350b4a-0a76-4baf-94c8-735c98919f40=[10_8, 11_0], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer-386bc794-6c1e-42a2-a92f-04bc5c8a13a2=[10_9, 12_0], _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer-e4627d28-a5a6-482b-818a-9ee15a431451=[10_11, 10_10]}
control-center   | 	revoking active {}
control-center   | 	assigned standby {}
control-center   |  (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:39,239] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer] Finished stable assignment of tasks, no followup rebalances required. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:39,240] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Finished assignment for group at generation 2: {_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer-4391ff38-48f7-456b-8df0-f8043a585d5c=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, _confluent-monitoring-0], userDataSize=64), _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer-e891406f-82dd-47f9-80ee-af3c7b1bfc86=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, _confluent-metrics-1], userDataSize=64), _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer-bca356e0-f66d-4c3f-b454-d74e29dfdef6=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, _confluent-metrics-3], userDataSize=64), _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer-370b82e1-4272-4e2f-8f37-aaf497aaf68b=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, _confluent-metrics-5], userDataSize=64), _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer-6d4a4ab1-803c-46cf-94ab-2b0a96f625be=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, _confluent-metrics-7], userDataSize=88), _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer-386bc794-6c1e-42a2-a92f-04bc5c8a13a2=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, _confluent-metrics-9], userDataSize=64), _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer-f1125336-cd8c-4bc8-aebe-ea2de793d5c7=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-metrics-2], userDataSize=64), _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer-da0b159a-557c-48a5-8475-833ada88b018=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, _confluent-metrics-4], userDataSize=64), _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer-cc2d2982-77f6-48d2-b91f-066c626cbd58=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, _confluent-metrics-6], userDataSize=64), _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer-e4627d28-a5a6-482b-818a-9ee15a431451=Assignment(partitions=[_confluent-metrics-10, _confluent-metrics-11], userDataSize=64), _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer-79350b4a-0a76-4baf-94c8-735c98919f40=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, _confluent-metrics-8], userDataSize=64), _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer-212917c7-9b1c-48ca-a306-3f46e72b8c04=Assignment(partitions=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-metrics-0], userDataSize=64)} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
broker           | [2023-08-04 11:16:39,253] INFO [GroupCoordinator 1]: Assignment received from leader _confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer-79350b4a-0a76-4baf-94c8-735c98919f40 for group _confluent-controlcenter-7-4-1-1 for generation 2. The group has 12 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
control-center   | [2023-08-04 11:16:39,263] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:39,265] INFO tocheck=[Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=AlertHistoryStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=Group, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=TriggerEventsStore, rollup=false}, Store{name=group-aggregate-store, rollup=true}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:39,270] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:39,299] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer-386bc794-6c1e-42a2-a92f-04bc5c8a13a2', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,302] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, _confluent-metrics-9], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,309] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:39,320] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer-f1125336-cd8c-4bc8-aebe-ea2de793d5c7', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,308] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer-bca356e0-f66d-4c3f-b454-d74e29dfdef6', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,333] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, _confluent-metrics-3], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,344] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:39,346] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] Handle new assignment with:
control-center   | 	New active tasks: [5_0, 10_3]
control-center   | 	New standby tasks: []
control-center   | 	Existing active tasks: []
control-center   | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center   | [2023-08-04 11:16:39,308] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer-cc2d2982-77f6-48d2-b91f-066c626cbd58', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,353] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, _confluent-metrics-6], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,355] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:39,357] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] Handle new assignment with:
control-center   | 	New active tasks: [10_6, 8_0]
control-center   | 	New standby tasks: []
control-center   | 	Existing active tasks: []
control-center   | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center   | [2023-08-04 11:16:39,345] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer-4391ff38-48f7-456b-8df0-f8043a585d5c', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,335] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] Handle new assignment with:
control-center   | 	New active tasks: [10_9, 12_0]
control-center   | 	New standby tasks: []
control-center   | 	Existing active tasks: []
control-center   | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center   | [2023-08-04 11:16:39,335] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-metrics-2], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,379] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer-370b82e1-4272-4e2f-8f37-aaf497aaf68b', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,310] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer-6d4a4ab1-803c-46cf-94ab-2b0a96f625be', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,384] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, _confluent-monitoring-0], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,385] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:39,385] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] Handle new assignment with:
control-center   | 	New active tasks: [1_0, 0_0]
control-center   | 	New standby tasks: []
control-center   | 	Existing active tasks: []
control-center   | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center   | [2023-08-04 11:16:39,379] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:39,389] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] Handle new assignment with:
control-center   | 	New active tasks: [4_0, 10_2]
control-center   | 	New standby tasks: []
control-center   | 	Existing active tasks: []
control-center   | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center   | [2023-08-04 11:16:39,357] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer-e891406f-82dd-47f9-80ee-af3c7b1bfc86', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,401] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer-e4627d28-a5a6-482b-818a-9ee15a431451', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,417] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-metrics-10, _confluent-metrics-11], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,396] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, _confluent-metrics-7], userDataSize=88) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,388] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, _confluent-metrics-5], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,423] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:39,428] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] Handle new assignment with:
control-center   | 	New active tasks: [10_5, 7_0]
control-center   | 	New standby tasks: []
control-center   | 	Existing active tasks: []
control-center   | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center   | [2023-08-04 11:16:39,429] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:39,430] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer-da0b159a-557c-48a5-8475-833ada88b018', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,431] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, _confluent-metrics-1], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,432] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:39,432] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] Handle new assignment with:
control-center   | 	New active tasks: [3_0, 10_1]
control-center   | 	New standby tasks: []
control-center   | 	Existing active tasks: []
control-center   | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center   | [2023-08-04 11:16:39,433] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, _confluent-metrics-4], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,435] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:39,440] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:39,430] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] Handle new assignment with:
control-center   | 	New active tasks: [10_7, 9_0]
control-center   | 	New standby tasks: []
control-center   | 	Existing active tasks: []
control-center   | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center   | [2023-08-04 11:16:39,520] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] Handle new assignment with:
control-center   | 	New active tasks: [10_11, 10_10]
control-center   | 	New standby tasks: []
control-center   | 	Existing active tasks: []
control-center   | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center   | [2023-08-04 11:16:39,464] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer-79350b4a-0a76-4baf-94c8-735c98919f40', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,530] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, _confluent-metrics-8], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,531] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:39,532] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] Handle new assignment with:
control-center   | 	New active tasks: [10_8, 11_0]
control-center   | 	New standby tasks: []
control-center   | 	Existing active tasks: []
control-center   | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center   | [2023-08-04 11:16:39,443] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] Handle new assignment with:
control-center   | 	New active tasks: [10_4, 6_0]
control-center   | 	New standby tasks: []
control-center   | 	Existing active tasks: []
control-center   | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center   | [2023-08-04 11:16:39,443] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Successfully synced group in generation Generation{generationId=2, memberId='_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer-212917c7-9b1c-48ca-a306-3f46e72b8c04', protocol='stream'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,546] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Notifying assignor about the new Assignment(partitions=[_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-metrics-0], userDataSize=64) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,547] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer] No followup rebalance was requested, resetting the rebalance schedule. (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
control-center   | [2023-08-04 11:16:39,548] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] Handle new assignment with:
control-center   | 	New active tasks: [2_0, 10_0]
control-center   | 	New standby tasks: []
control-center   | 	Existing active tasks: []
control-center   | 	Existing standby tasks: [] (org.apache.kafka.streams.processor.internals.TaskManager)
control-center   | [2023-08-04 11:16:39,560] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, _confluent-metrics-9 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,569] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:39,606] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,750] INFO ControlCenterBoundedMemoryConfig values: 
control-center   | 	rocksdb.cache.limit.strict = false
control-center   | 	rocksdb.cache.size = 16106127360
control-center   | 	rocksdb.cache.size.limit.enabled = false
control-center   | 	rocksdb.index.filter.block.ratio = 0.0
control-center   | 	rocksdb.write.buffer.cache.use = false
control-center   | 	rocksdb.write.buffer.size = 5368709120
control-center   |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center   | [2023-08-04 11:16:39,784] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, _confluent-metrics-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,784] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:39,798] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,813] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-metrics-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,809] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-metrics-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,843] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:39,843] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:39,824] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, _confluent-metrics-6 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,853] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:39,852] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, _confluent-metrics-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,852] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,851] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, _confluent-metrics-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,898] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,907] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:39,885] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,858] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] task [10_6] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:39,906] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:39,926] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,943] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:39,982] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] task [10_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:39,991] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] task [10_1] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,121] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, _confluent-metrics-8 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:40,122] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:40,141] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, _confluent-monitoring-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:40,142] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:40,147] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, _confluent-metrics-5 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:40,147] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:40,186] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] stream-task [5_0] State store MonitoringMessageAggregatorWindows-THREE_HOURS did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:40,188] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] task [5_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,190] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] task [10_3] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,211] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:40,238] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] task [10_5] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,235] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:40,156] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] stream-task [3_0] State store MonitoringMessageAggregatorWindows-ONE_MINUTE did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:40,255] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] task [3_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,237] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:40,264] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-metrics-10, _confluent-metrics-11 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:40,267] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:40,260] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] task [0_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,241] INFO ControlCenterBoundedMemoryConfig values: 
control-center   | 	rocksdb.cache.limit.strict = false
control-center   | 	rocksdb.cache.size = 16106127360
control-center   | 	rocksdb.cache.size.limit.enabled = false
control-center   | 	rocksdb.index.filter.block.ratio = 0.0
control-center   | 	rocksdb.write.buffer.cache.use = false
control-center   | 	rocksdb.write.buffer.size = 5368709120
control-center   |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center   | [2023-08-04 11:16:40,298] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:40,310] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] task [10_10] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,310] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] task [10_11] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,332] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Adding newly assigned partitions: _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, _confluent-metrics-7 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:40,332] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] State transition from STARTING to PARTITIONS_ASSIGNED (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:40,549] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] task [10_3] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,570] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] task [10_10] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,608] INFO tocheck=[Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=AlertHistoryStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=Group, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=TriggerEventsStore, rollup=false}, Store{name=group-aggregate-store, rollup=true}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:40,617] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:40,586] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] stream-task [4_0] State store aggregatedTopicPartitionTableWindows-THREE_HOURS did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:40,619] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] stream-task [4_0] Initializing to the starting offset for changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 of in-memory state store group-aggregate-store-THREE_HOURS (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:40,622] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] task [4_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,623] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] task [10_2] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,585] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] task [10_1] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,580] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:40,636] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:40,638] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:40,643] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] task [10_7] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,670] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] stream-task [11_0] State store MetricsAggregateStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:40,747] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] task [11_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,751] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] task [10_8] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,743] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] stream-task [6_0] State store MonitoringStream-ONE_MINUTE did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:40,762] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] task [6_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,765] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] task [10_4] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,741] INFO ControlCenterBoundedMemoryConfig values: 
control-center   | 	rocksdb.cache.limit.strict = false
control-center   | 	rocksdb.cache.size = 16106127360
control-center   | 	rocksdb.cache.size.limit.enabled = false
control-center   | 	rocksdb.index.filter.block.ratio = 0.0
control-center   | 	rocksdb.write.buffer.cache.use = false
control-center   | 	rocksdb.write.buffer.size = 5368709120
control-center   |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center   | [2023-08-04 11:16:40,707] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:40,773] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:40,696] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] task [10_11] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,687] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] stream-task [8_0] State store Group-ONE_MINUTE did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:40,787] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] stream-task [8_0] State store Group-THREE_HOURS did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:40,789] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] task [8_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,798] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] Restoration took 517 ms for all tasks [10_11, 10_10] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:40,800] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:40,803] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-10 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:40,821] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-11 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:40,817] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] stream-task [7_0] State store MonitoringStream-THREE_HOURS did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:40,828] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] task [7_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,838] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] task [10_2] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,849] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] stream-task [2_0] State store aggregatedTopicPartitionTableWindows-ONE_MINUTE did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:40,941] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] stream-task [2_0] Initializing to the starting offset for changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 of in-memory state store group-aggregate-store-ONE_MINUTE (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:40,941] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] task [2_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,945] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:40,946] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:40,915] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] task [10_6] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,904] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:40,884] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] task [10_4] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,729] INFO ControlCenterBoundedMemoryConfig values: 
control-center   | 	rocksdb.cache.limit.strict = false
control-center   | 	rocksdb.cache.size = 16106127360
control-center   | 	rocksdb.cache.size.limit.enabled = false
control-center   | 	rocksdb.index.filter.block.ratio = 0.0
control-center   | 	rocksdb.write.buffer.cache.use = false
control-center   | 	rocksdb.write.buffer.size = 5368709120
control-center   |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center   | [2023-08-04 11:16:40,858] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] task [10_8] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:40,855] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] task [10_5] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:41,065] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-10 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:41,065] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-11 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:41,072] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] task [10_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:41,074] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to HoP6Lp2hR0mutBINDVd0SA (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:41,076] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:41,147] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:41,184] INFO Restore started for store [MonitoringMessageAggregatorWindows-THREE_HOURS] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:41,200] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] End offset for changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:41,201] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] End offset for changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:41,202] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:41,215] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:41,216] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:41,249] INFO ControlCenterBoundedMemoryConfig values: 
control-center   | 	rocksdb.cache.limit.strict = false
control-center   | 	rocksdb.cache.size = 16106127360
control-center   | 	rocksdb.cache.size.limit.enabled = false
control-center   | 	rocksdb.index.filter.block.ratio = 0.0
control-center   | 	rocksdb.write.buffer.cache.use = false
control-center   | 	rocksdb.write.buffer.size = 5368709120
control-center   |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center   | [2023-08-04 11:16:41,251] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] stream-task [12_0] State store TriggerActionsStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:41,269] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] stream-task [12_0] State store TriggerEventsStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:41,276] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] stream-task [12_0] State store AlertHistoryStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:41,279] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] task [12_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:41,279] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] task [10_9] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:41,289] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 to store MonitoringMessageAggregatorWindows-THREE_HOURS with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:41,290] INFO Restore completed for store [MonitoringMessageAggregatorWindows-THREE_HOURS] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:41,309] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to VHDKH6yPRKekarvjYHIjyw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:41,318] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:41,316] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:41,326] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:41,330] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] task [10_9] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:41,360] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] End offset for changelog _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:41,361] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] End offset for changelog _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:41,367] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:41,388] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:41,397] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:41,448] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] End offset for changelog _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:41,449] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to qP4lTE8ATsmQfTcF8slryw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:41,452] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:41,453] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to 88agc7ovSmm9apNE4HN69Q (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:41,454] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:41,484] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:41,497] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] task [5_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:41,519] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:41,536] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:41,543] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:41,546] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] stream-task [9_0] State store KSTREAM-OUTERTHIS-0000000105-store did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:41,546] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] stream-task [9_0] State store KSTREAM-OUTEROTHER-0000000106-store did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:41,546] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] stream-task [9_0] State store MonitoringTriggerStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:41,546] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] task [9_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:41,514] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to Trp1P4PASmedQMblOoysPQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:41,531] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] Restoration took 1746 ms for all tasks [5_0, 10_3] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:41,565] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:41,569] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:41,582] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-3 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:41,584] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to 9SrPkJvdQRKHmFqHX1yx4w (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:41,585] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:41,601] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:41,602] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:41,605] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:41,629] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:41,632] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 to 0 since the associated topicId changed from null to xpuJp62xRjKbInyyPCX1lQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:41,824] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:41,781] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] End offset for changelog _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:41,780] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to gFRdYcjWQsGSJ47CHoXyqw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:41,764] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] task [10_7] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:41,763] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] stream-task [1_0] State store MonitoringVerifierStore did not find checkpoint offset, hence would default to the starting offset at changelog _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:41,756] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] End offset for changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:41,725] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:41,675] INFO Restore started for store [MonitoringMessageAggregatorWindows-ONE_MINUTE] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:41,633] INFO tocheck=[Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=AlertHistoryStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=Group, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=TriggerEventsStore, rollup=false}, Store{name=group-aggregate-store, rollup=true}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:41,839] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] End offset for changelog _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:41,844] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] End offset for changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:41,843] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:41,844] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] End offset for changelog _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:41,848] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:41,849] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:41,850] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:41,855] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:41,849] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:41,961] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:41,973] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:41,948] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 to store MonitoringMessageAggregatorWindows-ONE_MINUTE with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:41,987] INFO Restore completed for store [MonitoringMessageAggregatorWindows-ONE_MINUTE] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:41,927] INFO Restore started for store [Group-THREE_HOURS] with topic-partition [_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,019] INFO Restore started for store [Group-ONE_MINUTE] with topic-partition [_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:41,922] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:41,884] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-3 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:41,873] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] stream-task [1_0] Initializing to the starting offset for changelog _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 of in-memory state store aggregate-topic-partition-store (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:42,034] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] stream-task [1_0] Initializing to the starting offset for changelog _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 of in-memory state store monitoring-aggregate-rekey-store (org.apache.kafka.streams.processor.internals.ProcessorStateManager)
control-center   | [2023-08-04 11:16:42,043] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] task [1_0] Initialized (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:41,975] INFO Restore started for store [aggregatedTopicPartitionTableWindows-THREE_HOURS] with topic-partition [_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:41,973] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to 1llU1igJTl2WtB6Ombk9uQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,048] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,053] INFO Restore started for store [group-aggregate-store-THREE_HOURS] with topic-partition [_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,066] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,080] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:42,081] INFO Restore started for store [MetricsAggregateStore] with topic-partition [_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,108] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to I-AspoMlSU-I5nUZzPsT2Q (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,109] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to JdPg8CHIR2eSEEpP-4pgKA (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,111] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,114] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 to 0 since the associated topicId changed from null to oBNDr9MwSwyfz3UNlKj0Aw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,115] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 to 0 since the associated topicId changed from null to S495LVuvRsuHe_-TfpTCVA (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,115] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 to 0 since the associated topicId changed from null to SXqs9z_IT0ycq4w-So2PtQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,116] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,129] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] Finished restoring changelog _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 to store Group-ONE_MINUTE with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,130] INFO Restore completed for store [Group-ONE_MINUTE] and topic-partition [_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,130] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] Finished restoring changelog _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 to store Group-THREE_HOURS with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,130] INFO Restore completed for store [Group-THREE_HOURS] and topic-partition [_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,156] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] Finished restoring changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 to store aggregatedTopicPartitionTableWindows-THREE_HOURS with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,158] INFO Restore completed for store [aggregatedTopicPartitionTableWindows-THREE_HOURS] and topic-partition [_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,159] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] Finished restoring changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 to store group-aggregate-store-THREE_HOURS with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,169] INFO Restore completed for store [group-aggregate-store-THREE_HOURS] and topic-partition [_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,190] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 to store MetricsAggregateStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,198] INFO Restore completed for store [MetricsAggregateStore] and topic-partition [_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,219] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,220] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,225] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] End offset for changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,226] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] End offset for changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,226] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,238] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,228] INFO Restore started for store [MonitoringStream-ONE_MINUTE] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,244] INFO Restore started for store [MonitoringStream-THREE_HOURS] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,252] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,261] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,265] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,261] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,253] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,278] INFO Restore started for store [aggregatedTopicPartitionTableWindows-ONE_MINUTE] with topic-partition [_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,279] INFO Restore started for store [group-aggregate-store-ONE_MINUTE] with topic-partition [_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,252] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:42,293] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] task [8_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:42,265] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,320] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] task [4_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:42,325] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] Restoration took 2458 ms for all tasks [4_0, 10_2] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,326] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,327] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,328] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-2 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,313] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] task [0_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:42,298] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:42,335] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:42,295] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] task [3_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:42,343] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] Restoration took 2424 ms for all tasks [10_1, 3_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,344] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,345] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,346] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-1 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,325] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] Restoration took 2472 ms for all tasks [8_0, 10_6] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,358] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,359] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,373] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 to store MonitoringStream-ONE_MINUTE with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,378] INFO Restore completed for store [MonitoringStream-ONE_MINUTE] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,381] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] Finished restoring changelog _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 to store aggregatedTopicPartitionTableWindows-ONE_MINUTE with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,391] INFO Restore completed for store [aggregatedTopicPartitionTableWindows-ONE_MINUTE] and topic-partition [_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,407] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] Finished restoring changelog _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 to store group-aggregate-store-ONE_MINUTE with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,413] INFO Restore completed for store [group-aggregate-store-ONE_MINUTE] and topic-partition [_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,381] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-6 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,394] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 to store MonitoringStream-THREE_HOURS with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,420] INFO Restore completed for store [MonitoringStream-THREE_HOURS] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,429] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] task [11_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:42,468] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] Restoration took 2346 ms for all tasks [10_8, 11_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,468] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,469] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-8 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,470] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,468] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,472] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,474] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] task [2_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:42,459] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,459] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,458] INFO Restore started for store [AlertHistoryStore] with topic-partition [_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,479] INFO Restore started for store [TriggerActionsStore] with topic-partition [_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,456] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 to 0 since the associated topicId changed from null to NZXeFHK0TkaUSTnQorwW-A (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,483] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 to 0 since the associated topicId changed from null to nzdecc30ShSdQcB04frvog (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,484] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,455] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] End offset for changelog _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,529] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] End offset for changelog _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,530] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] End offset for changelog _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 initialized as 0. (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,530] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-restore-consumer, groupId=null] Assigned to partition(s): _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,531] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,532] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-8 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,533] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,533] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:42,536] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-2 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,537] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,441] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,440] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,562] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-1 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,563] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,563] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:42,546] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:42,531] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:42,527] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,619] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:42,620] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-restore-consumer, groupId=null] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:42,624] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] task [7_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:42,495] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] Restoration took 2651 ms for all tasks [2_0, 10_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,627] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,628] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,629] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,495] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] task [6_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:42,645] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] Restoration took 2737 ms for all tasks [6_0, 10_4] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,646] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,647] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,647] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-4 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,484] INFO Restore started for store [TriggerEventsStore] with topic-partition [_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,484] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 to 0 since the associated topicId changed from null to 4A9-74zvQouR64zqhXh9FA (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,483] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,628] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] Restoration took 2480 ms for all tasks [10_5, 7_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,684] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,683] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,695] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,696] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-5 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,699] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,700] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-4 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,700] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,700] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:42,704] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-6 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,704] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,704] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:42,709] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,738] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-5 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,740] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,741] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:42,762] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,792] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 to 0 since the associated topicId changed from null to HSq3iWH_Q0G8WAozBnXiFQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,792] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 to 0 since the associated topicId changed from null to O3OKZPZ5RfOh-XbP8Y4yyQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,792] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-restore-consumer, groupId=null] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 to 0 since the associated topicId changed from null to No9nQpMqSCWKHXxTpgGjHA (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,798] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] Finished restoring changelog _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 to store TriggerActionsStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,800] INFO Restore completed for store [TriggerActionsStore] and topic-partition [_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,801] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] Finished restoring changelog _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 to store TriggerEventsStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,801] INFO Restore completed for store [TriggerEventsStore] and topic-partition [_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,801] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] Finished restoring changelog _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 to store AlertHistoryStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,801] INFO Restore completed for store [AlertHistoryStore] and topic-partition [_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,811] INFO Restore started for store [KSTREAM-OUTEROTHER-0000000106-store] with topic-partition [_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,811] INFO Restore started for store [KSTREAM-OUTERTHIS-0000000105-store] with topic-partition [_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,812] INFO Restore started for store [MonitoringTriggerStore] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,814] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,814] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,819] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:42,827] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-restore-consumer, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:42,833] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,848] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:42,849] INFO tocheck=[Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=AlertHistoryStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=Group, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=TriggerEventsStore, rollup=false}, Store{name=group-aggregate-store, rollup=true}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:42,851] INFO streams in state=REBALANCING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:42,917] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] Finished restoring changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 to store KSTREAM-OUTEROTHER-0000000106-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,927] INFO Restore completed for store [KSTREAM-OUTEROTHER-0000000106-store] and topic-partition [_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,961] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] Finished restoring changelog _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 to store KSTREAM-OUTERTHIS-0000000105-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,962] INFO Restore completed for store [KSTREAM-OUTERTHIS-0000000105-store] and topic-partition [_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,962] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 to store MonitoringTriggerStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:42,962] INFO Restore completed for store [MonitoringTriggerStore] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,919] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:42,973] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] task [12_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:42,974] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] Restoration took 3405 ms for all tasks [10_9, 12_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,974] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:42,974] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-9 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,976] INFO Restore started for store [MonitoringVerifierStore] with topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,977] INFO Restore started for store [aggregate-topic-partition-store] with topic-partition [_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:42,980] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:42,979] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:43,016] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:43,017] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:43,026] INFO Restore started for store [monitoring-aggregate-rekey-store] with topic-partition [_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0] of total records to restore [0]  (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:43,029] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:43,035] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:43,035] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:43,041] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] task [9_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:43,047] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] Restoration took 2714 ms for all tasks [9_0, 10_7] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:43,048] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:43,049] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:43,050] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-metrics-7 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:43,102] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:43,102] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-9 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:43,103] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:43,115] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:43,129] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] Finished restoring changelog _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 to store MonitoringVerifierStore with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:43,135] INFO Restore completed for store [MonitoringVerifierStore] and topic-partition [_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:43,135] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] Finished restoring changelog _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 to store monitoring-aggregate-rekey-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:43,135] INFO Restore completed for store [monitoring-aggregate-rekey-store] and topic-partition [_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:43,136] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] Finished restoring changelog _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 to store aggregate-topic-partition-store with a total number of 0 records (org.apache.kafka.streams.processor.internals.StoreChangelogReader)
control-center   | [2023-08-04 11:16:43,137] INFO Restore completed for store [aggregate-topic-partition-store] and topic-partition [_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0] taking [0] seconds to restore [0] records (io.confluent.controlcenter.streams.C3LoggingRestoreListener)
control-center   | [2023-08-04 11:16:43,147] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:43,148] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:43,148] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:43,152] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-metrics-7 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:43,154] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:43,158] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:43,172] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:43,186] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:43,160] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:43,205] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:43,209] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:43,217] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:43,234] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 8fgQicUSRVi7xJcKYztVRQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:43,349] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] task [1_0] Restored and ready to run (org.apache.kafka.streams.processor.internals.StreamTask)
control-center   | [2023-08-04 11:16:43,357] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] Restoration took 3215 ms for all tasks [1_0, 0_0] (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:43,358] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:43,359] INFO stream-client [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb] State transition from REBALANCING to RUNNING (org.apache.kafka.streams.KafkaStreams)
control-center   | [2023-08-04 11:16:43,360] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-monitoring-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:43,361] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:43,430] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:43,532] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Found no committed offset for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:43,533] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Setting offset for partition _confluent-monitoring-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
control-center   | [2023-08-04 11:16:43,536] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] No custom setting defined for topic '_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store' using original config 'earliest' for offset reset (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:16:43,539] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Seeking to earliest offset of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
control-center   | [2023-08-04 11:16:43,852] INFO tocheck=[Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=AlertHistoryStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=Group, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=TriggerEventsStore, rollup=false}, Store{name=group-aggregate-store, rollup=true}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:43,853] INFO streams in state=RUNNING (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:43,854] INFO tocheck=[Store{name=aggregatedTopicPartitionTableWindows, rollup=true}, Store{name=MonitoringMessageAggregatorWindows, rollup=true}, Store{name=MonitoringVerifierStore, rollup=false}, Store{name=MonitoringTriggerStore, rollup=false}, Store{name=MetricsAggregateStore, rollup=false}, Store{name=AlertHistoryStore, rollup=false}, Store{name=TriggerActionsStore, rollup=false}, Store{name=aggregate-topic-partition-store, rollup=false}, Store{name=Group, rollup=true}, Store{name=KSTREAM-OUTEROTHER-0000000106-store, rollup=false}, Store{name=monitoring-aggregate-rekey-store, rollup=false}, Store{name=KSTREAM-OUTERTHIS-0000000105-store, rollup=false}, Store{name=TriggerEventsStore, rollup=false}, Store{name=group-aggregate-store, rollup=true}, Store{name=MonitoringStream, rollup=true}] (io.confluent.controlcenter.streams.KafkaStreamsManager)
control-center   | [2023-08-04 11:16:43,948] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Requesting the log end offset for _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 in order to compute lag (org.apache.kafka.clients.consumer.KafkaConsumer)
control-center   | [2023-08-04 11:16:44,865] INFO action=started topology=monitoring (io.confluent.controlcenter.application.AllControlCenter)
control-center   | [2023-08-04 11:16:45,200] INFO AdminClientConfig values: 
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = 
control-center   | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	confluent.use.controller.listener = false
control-center   | 	connections.max.idle.ms = 300000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center   | [2023-08-04 11:16:45,277] WARN These configurations '[consumer.session.timeout.ms, producer.max.block.ms, producer.retries, upgrade.from, producer.retry.backoff.ms, producer.linger.ms, producer.delivery.timeout.ms, task.timeout.ms, cache.max.bytes.buffering, producer.compression.type, num.stream.threads]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center   | [2023-08-04 11:16:45,278] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:45,278] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:45,278] INFO Kafka startTimeMs: 1691147805277 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:45,386] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:45,416] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:16:45,416] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:16:45,417] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
control-center   | [2023-08-04 11:16:45,458] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center   | 	metrics.jmx.prefix = confluent.controlcenter
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = true
control-center   | 	port = 9021
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:16:45,525] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center   | 	metrics.jmx.prefix = confluent.controlcenter
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = true
control-center   | 	port = 9021
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:16:45,528] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center   | 	metrics.jmx.prefix = confluent.controlcenter
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = true
control-center   | 	port = 9021
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:16:45,532] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center   | 	metrics.jmx.prefix = confluent.controlcenter
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = true
control-center   | 	port = 9021
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:16:45,559] INFO EventEmitterConfig values: 
control-center   |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
control-center   | [2023-08-04 11:16:45,595] INFO EventEmitterConfig values: 
control-center   |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
control-center   | [2023-08-04 11:16:45,608] INFO ConfluentTelemetryConfig values: 
control-center   | 	confluent.telemetry.api.key = null
control-center   | 	confluent.telemetry.api.secret = null
control-center   | 	confluent.telemetry.debug.enabled = false
control-center   | 	confluent.telemetry.enabled = false
control-center   | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
control-center   | 	confluent.telemetry.events.enable = true
control-center   | 	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*
control-center   | 	confluent.telemetry.metrics.collector.interval.ms = 60000
control-center   | 	confluent.telemetry.metrics.collector.slo.enabled = false
control-center   | 	confluent.telemetry.proxy.password = null
control-center   | 	confluent.telemetry.proxy.url = null
control-center   | 	confluent.telemetry.proxy.username = null
control-center   |  (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center   | [2023-08-04 11:16:45,636] INFO VolumeMetricsCollectorConfig values: 
control-center   | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
control-center   |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
control-center   | [2023-08-04 11:16:45,638] INFO HttpExporterConfig values: 
control-center   | 	api.key = null
control-center   | 	api.secret = null
control-center   | 	buffer.batch.duration.max.ms = null
control-center   | 	buffer.batch.items.max = null
control-center   | 	buffer.inflight.submissions.max = null
control-center   | 	buffer.pending.batches.max = null
control-center   | 	client.attempts.max = null
control-center   | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center   | 	client.compression = null
control-center   | 	client.connect.timeout.ms = null
control-center   | 	client.contentType = null
control-center   | 	client.request.timeout.ms = null
control-center   | 	client.retry.delay.seconds = null
control-center   | 	enabled = false
control-center   | 	events.enabled = true
control-center   | 	metrics.enabled = true
control-center   | 	metrics.include = null
control-center   | 	proxy.password = null
control-center   | 	proxy.url = null
control-center   | 	proxy.username = null
control-center   | 	type = http
control-center   |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
control-center   | [2023-08-04 11:16:45,647] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center   | [2023-08-04 11:16:45,648] INFO RemoteConfigConfiguration values: 
control-center   | 	enabled = true
control-center   | 	polling.interval.ms = 60000
control-center   |  (io.confluent.shaded.io.confluent.telemetry.config.remote.RemoteConfigConfiguration)
control-center   | [2023-08-04 11:16:45,650] WARN Ignoring redefinition of existing telemetry label controlcenter.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
control-center   | [2023-08-04 11:16:45,710] INFO ConfluentTelemetryConfig values: 
control-center   | 	confluent.telemetry.api.key = null
control-center   | 	confluent.telemetry.api.secret = null
control-center   | 	confluent.telemetry.debug.enabled = false
control-center   | 	confluent.telemetry.enabled = false
control-center   | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
control-center   | 	confluent.telemetry.events.enable = true
control-center   | 	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.controlcenter/.*(metrics_input_topic_progress|monitoring_input_topic_progress|misconfigured_topics|missing_topic_configurations|broker_log_persistent_dir|cluster_offline|streams_status|total_lag|request_latency|response_size|response_rate).*
control-center   | 	confluent.telemetry.metrics.collector.interval.ms = 60000
control-center   | 	confluent.telemetry.metrics.collector.slo.enabled = false
control-center   | 	confluent.telemetry.proxy.password = null
control-center   | 	confluent.telemetry.proxy.url = null
control-center   | 	confluent.telemetry.proxy.username = null
control-center   |  (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center   | [2023-08-04 11:16:45,711] INFO VolumeMetricsCollectorConfig values: 
control-center   | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
control-center   |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
control-center   | [2023-08-04 11:16:45,713] INFO HttpExporterConfig values: 
control-center   | 	api.key = null
control-center   | 	api.secret = null
control-center   | 	buffer.batch.duration.max.ms = null
control-center   | 	buffer.batch.items.max = null
control-center   | 	buffer.inflight.submissions.max = null
control-center   | 	buffer.pending.batches.max = null
control-center   | 	client.attempts.max = null
control-center   | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center   | 	client.compression = null
control-center   | 	client.connect.timeout.ms = null
control-center   | 	client.contentType = null
control-center   | 	client.request.timeout.ms = null
control-center   | 	client.retry.delay.seconds = null
control-center   | 	enabled = false
control-center   | 	events.enabled = true
control-center   | 	metrics.enabled = true
control-center   | 	metrics.include = null
control-center   | 	proxy.password = null
control-center   | 	proxy.url = null
control-center   | 	proxy.username = null
control-center   | 	type = http
control-center   |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
control-center   | [2023-08-04 11:16:45,720] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
control-center   | [2023-08-04 11:16:45,723] INFO RemoteConfigConfiguration values: 
control-center   | 	enabled = true
control-center   | 	polling.interval.ms = 60000
control-center   |  (io.confluent.shaded.io.confluent.telemetry.config.remote.RemoteConfigConfiguration)
control-center   | [2023-08-04 11:16:45,724] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
control-center   | [2023-08-04 11:16:45,724] INFO EventLoggerConfig values: 
control-center   | 	event.logger.cloudevent.codec = structured
control-center   | 	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter
control-center   |  (io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig)
control-center   | [2023-08-04 11:16:45,726] INFO HttpExporterConfig values: 
control-center   | 	api.key = null
control-center   | 	api.secret = null
control-center   | 	buffer.batch.duration.max.ms = null
control-center   | 	buffer.batch.items.max = null
control-center   | 	buffer.inflight.submissions.max = null
control-center   | 	buffer.pending.batches.max = null
control-center   | 	client.attempts.max = null
control-center   | 	client.base.url = https://collector.telemetry.confluent.cloud
control-center   | 	client.compression = null
control-center   | 	client.connect.timeout.ms = null
control-center   | 	client.request.timeout.ms = null
control-center   | 	client.retry.delay.seconds = null
control-center   | 	enabled = false
control-center   | 	events.enabled = true
control-center   | 	metrics.enabled = true
control-center   | 	proxy.password = null
control-center   | 	proxy.url = null
control-center   | 	proxy.username = null
control-center   | 	type = http
control-center   |  (io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig)
control-center   | [2023-08-04 11:16:45,882] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter)
control-center   | [2023-08-04 11:16:46,714] INFO [Producer clientId=c3-command] Resetting the last seen epoch of partition _confluent-command-0 to 0 since the associated topicId changed from null to AFaY0a_lSiO7tlDgMwz2qw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:47,260] INFO KafkaRestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	advertised.listeners = []
control-center   | 	api.endpoints.allowlist = []
control-center   | 	api.endpoints.blocklist = []
control-center   | 	api.v2.enable = true
control-center   | 	api.v3.enable = true
control-center   | 	api.v3.produce.rate.limit.cache.expiry.ms = 3600000
control-center   | 	api.v3.produce.rate.limit.enabled = false
control-center   | 	api.v3.produce.rate.limit.max.bytes.global.per.sec = 10000000
control-center   | 	api.v3.produce.rate.limit.max.bytes.per.sec = 10000000
control-center   | 	api.v3.produce.rate.limit.max.requests.global.per.sec = 10000
control-center   | 	api.v3.produce.rate.limit.max.requests.per.sec = 10000
control-center   | 	api.v3.produce.response.thread.pool.size = 4
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	bootstrap.servers = broker:29092
control-center   | 	client.init.timeout.ms = 60000
control-center   | 	client.sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	client.sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	client.sasl.kerberos.service.name = 
control-center   | 	client.sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	client.sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	client.sasl.mechanism = GSSAPI
control-center   | 	client.security.protocol = PLAINTEXT
control-center   | 	client.ssl.cipher.suites = 
control-center   | 	client.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
control-center   | 	client.ssl.endpoint.identification.algorithm = 
control-center   | 	client.ssl.key.password = [hidden]
control-center   | 	client.ssl.keymanager.algorithm = SunX509
control-center   | 	client.ssl.keystore.location = 
control-center   | 	client.ssl.keystore.password = [hidden]
control-center   | 	client.ssl.keystore.type = JKS
control-center   | 	client.ssl.protocol = TLS
control-center   | 	client.ssl.provider = 
control-center   | 	client.ssl.trustmanager.algorithm = PKIX
control-center   | 	client.ssl.truststore.location = 
control-center   | 	client.ssl.truststore.password = [hidden]
control-center   | 	client.ssl.truststore.type = JKS
control-center   | 	client.timeout.ms = 500
control-center   | 	client.zk.session.timeout.ms = 30000
control-center   | 	compression.enable = true
control-center   | 	confluent.resource.name.authority = 
control-center   | 	connector.connection.limit = 0
control-center   | 	consumer.instance.timeout.ms = 300000
control-center   | 	consumer.iterator.backoff.ms = 50
control-center   | 	consumer.iterator.timeout.ms = 1
control-center   | 	consumer.request.max.bytes = 67108864
control-center   | 	consumer.request.timeout.ms = 1000
control-center   | 	consumer.threads = 50
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	fetch.min.bytes = -1
control-center   | 	host.name = 
control-center   | 	http2.enabled = true
control-center   | 	id = 
control-center   | 	idle.timeout.ms = 30000
control-center   | 	kafka.rest.resource.extension.class = [io.confluent.kafkarest.KafkaRestResourceExtension]
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = []
control-center   | 	metrics.jmx.prefix = kafka.rest
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = false
control-center   | 	port = 8082
control-center   | 	producer.threads = 5
control-center   | 	proxy.protocol.enabled = false
control-center   | 	rate.limit.backend = guava
control-center   | 	rate.limit.costs = 
control-center   | 	rate.limit.default.cost = 1
control-center   | 	rate.limit.enable = false
control-center   | 	rate.limit.per.cluster.cache.expiry.ms = 3600000
control-center   | 	rate.limit.per.cluster.permits.per.sec = 50
control-center   | 	rate.limit.permits.per.sec = 50
control-center   | 	rate.limit.timeout.ms = 0
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json, application/vnd.kafka.v2+json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	schema.registry.url = http://localhost:8081
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	simpleconsumer.pool.size.max = 25
control-center   | 	simpleconsumer.pool.timeout.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	streaming.connection.max.duration.grace.period.ms = 500
control-center   | 	streaming.connection.max.duration.ms = 86400000
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /api/kafka-rest-ws/MkU3OEVBNTcwNTJENDM2Qg
control-center   | 	websocket.servlet.initializor.classes = []
control-center   | 	zookeeper.connect = 
control-center   |  (io.confluent.kafkarest.KafkaRestConfig)
control-center   | [2023-08-04 11:16:50,529] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
control-center   | [2023-08-04 11:16:50,535] INFO SchemaRegistryConfig values: 
control-center   | 	auto.register.schemas = false
control-center   | 	basic.auth.credentials.source = URL
control-center   | 	basic.auth.user.info = [hidden]
control-center   | 	bearer.auth.cache.expiry.buffer.seconds = 300
control-center   | 	bearer.auth.client.id = null
control-center   | 	bearer.auth.client.secret = null
control-center   | 	bearer.auth.credentials.source = STATIC_TOKEN
control-center   | 	bearer.auth.custom.provider.class = null
control-center   | 	bearer.auth.identity.pool.id = null
control-center   | 	bearer.auth.issuer.endpoint.url = null
control-center   | 	bearer.auth.logical.cluster = null
control-center   | 	bearer.auth.scope = null
control-center   | 	bearer.auth.scope.claim.name = scope
control-center   | 	bearer.auth.sub.claim.name = sub
control-center   | 	bearer.auth.token = [hidden]
control-center   | 	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
control-center   | 	http.connect.timeout.ms = 60000
control-center   | 	http.read.timeout.ms = 60000
control-center   | 	id.compatibility.strict = true
control-center   | 	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
control-center   | 	latest.cache.size = 1000
control-center   | 	latest.cache.ttl.sec = -1
control-center   | 	latest.compatibility.strict = true
control-center   | 	max.schemas.per.subject = 1000
control-center   | 	normalize.schemas = false
control-center   | 	proxy.host = 
control-center   | 	proxy.port = -1
control-center   | 	rule.actions = []
control-center   | 	rule.executors = []
control-center   | 	rule.service.loader.enable = true
control-center   | 	schema.format = null
control-center   | 	schema.reflection = false
control-center   | 	schema.registry.basic.auth.user.info = [hidden]
control-center   | 	schema.registry.ssl.cipher.suites = null
control-center   | 	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	schema.registry.ssl.endpoint.identification.algorithm = https
control-center   | 	schema.registry.ssl.engine.factory.class = null
control-center   | 	schema.registry.ssl.key.password = null
control-center   | 	schema.registry.ssl.keymanager.algorithm = SunX509
control-center   | 	schema.registry.ssl.keystore.certificate.chain = null
control-center   | 	schema.registry.ssl.keystore.key = null
control-center   | 	schema.registry.ssl.keystore.location = null
control-center   | 	schema.registry.ssl.keystore.password = null
control-center   | 	schema.registry.ssl.keystore.type = JKS
control-center   | 	schema.registry.ssl.protocol = TLSv1.3
control-center   | 	schema.registry.ssl.provider = null
control-center   | 	schema.registry.ssl.secure.random.implementation = null
control-center   | 	schema.registry.ssl.trustmanager.algorithm = PKIX
control-center   | 	schema.registry.ssl.truststore.certificates = null
control-center   | 	schema.registry.ssl.truststore.location = null
control-center   | 	schema.registry.ssl.truststore.password = null
control-center   | 	schema.registry.ssl.truststore.type = JKS
control-center   | 	schema.registry.url = [http://localhost:8081]
control-center   | 	use.latest.version = false
control-center   | 	use.latest.with.metadata = null
control-center   | 	use.schema.id = -1
control-center   | 	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
control-center   |  (io.confluent.kafkarest.config.SchemaRegistryConfig)
control-center   | [2023-08-04 11:16:51,444] INFO Binding EmbeddedKafkaRestApplication to all listeners. (io.confluent.rest.Application)
control-center   | [2023-08-04 11:16:53,336] INFO Starting Health Check (io.confluent.controlcenter.application.AllControlCenter)
control-center   | [2023-08-04 11:16:53,338] INFO Starting Alert Manager (io.confluent.controlcenter.application.AllControlCenter)
control-center   | [2023-08-04 11:16:53,356] INFO current clusterId=MkU3OEVBNTcwNTJENDM2Qg (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:16:53,387] INFO Starting Consumer Offsets Fetch (io.confluent.controlcenter.application.AllControlCenter)
control-center   | [2023-08-04 11:16:53,389] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center   | 	metrics.jmx.prefix = confluent.controlcenter
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = true
control-center   | 	port = 9021
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:16:53,452] INFO Initial capacity 128, increased by 64, maximum capacity 2147483647. (io.confluent.rest.ApplicationServer)
control-center   | [2023-08-04 11:16:53,555] INFO broker id set has changed new={1=[broker:29092 (id: 1 rack: null tags: [])]} removed={} (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:16:53,565] INFO new controller=broker:29092 (id: 1 rack: null tags: []) (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:16:53,578] INFO AdminClientConfig values: 
control-center   | 	auto.include.jmx.reporter = true
control-center   | 	bootstrap.servers = [broker:29092]
control-center   | 	client.dns.lookup = use_all_dns_ips
control-center   | 	client.id = 
control-center   | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
control-center   | 	confluent.proxy.protocol.client.address = null
control-center   | 	confluent.proxy.protocol.client.port = null
control-center   | 	confluent.proxy.protocol.client.version = NONE
control-center   | 	confluent.use.controller.listener = false
control-center   | 	connections.max.idle.ms = 300000
control-center   | 	default.api.timeout.ms = 60000
control-center   | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
control-center   | 	metadata.max.age.ms = 300000
control-center   | 	metric.reporters = []
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.recording.level = INFO
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	receive.buffer.bytes = 65536
control-center   | 	reconnect.backoff.max.ms = 1000
control-center   | 	reconnect.backoff.ms = 50
control-center   | 	request.timeout.ms = 30000
control-center   | 	retries = 2147483647
control-center   | 	retry.backoff.ms = 100
control-center   | 	sasl.client.callback.handler.class = null
control-center   | 	sasl.jaas.config = null
control-center   | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
control-center   | 	sasl.kerberos.min.time.before.relogin = 60000
control-center   | 	sasl.kerberos.service.name = null
control-center   | 	sasl.kerberos.ticket.renew.jitter = 0.05
control-center   | 	sasl.kerberos.ticket.renew.window.factor = 0.8
control-center   | 	sasl.login.callback.handler.class = null
control-center   | 	sasl.login.class = null
control-center   | 	sasl.login.connect.timeout.ms = null
control-center   | 	sasl.login.read.timeout.ms = null
control-center   | 	sasl.login.refresh.buffer.seconds = 300
control-center   | 	sasl.login.refresh.min.period.seconds = 60
control-center   | 	sasl.login.refresh.window.factor = 0.8
control-center   | 	sasl.login.refresh.window.jitter = 0.05
control-center   | 	sasl.login.retry.backoff.max.ms = 10000
control-center   | 	sasl.login.retry.backoff.ms = 100
control-center   | 	sasl.mechanism = GSSAPI
control-center   | 	sasl.oauthbearer.clock.skew.seconds = 30
control-center   | 	sasl.oauthbearer.expected.audience = null
control-center   | 	sasl.oauthbearer.expected.issuer = null
control-center   | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
control-center   | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
control-center   | 	sasl.oauthbearer.jwks.endpoint.url = null
control-center   | 	sasl.oauthbearer.scope.claim.name = scope
control-center   | 	sasl.oauthbearer.sub.claim.name = sub
control-center   | 	sasl.oauthbearer.token.endpoint.url = null
control-center   | 	security.protocol = PLAINTEXT
control-center   | 	security.providers = null
control-center   | 	send.buffer.bytes = 131072
control-center   | 	socket.connection.setup.timeout.max.ms = 30000
control-center   | 	socket.connection.setup.timeout.ms = 10000
control-center   | 	ssl.cipher.suites = null
control-center   | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
control-center   | 	ssl.endpoint.identification.algorithm = https
control-center   | 	ssl.engine.factory.class = null
control-center   | 	ssl.key.password = null
control-center   | 	ssl.keymanager.algorithm = SunX509
control-center   | 	ssl.keystore.certificate.chain = null
control-center   | 	ssl.keystore.key = null
control-center   | 	ssl.keystore.location = null
control-center   | 	ssl.keystore.password = null
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.protocol = TLSv1.3
control-center   | 	ssl.provider = null
control-center   | 	ssl.secure.random.implementation = null
control-center   | 	ssl.trustmanager.algorithm = PKIX
control-center   | 	ssl.truststore.certificates = null
control-center   | 	ssl.truststore.location = null
control-center   | 	ssl.truststore.password = null
control-center   | 	ssl.truststore.type = JKS
control-center   |  (org.apache.kafka.clients.admin.AdminClientConfig)
control-center   | [2023-08-04 11:16:53,694] WARN These configurations '[consumer.session.timeout.ms, producer.max.block.ms, producer.retries, upgrade.from, producer.retry.backoff.ms, producer.linger.ms, producer.delivery.timeout.ms, task.timeout.ms, cache.max.bytes.buffering, producer.compression.type, num.stream.threads]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
control-center   | [2023-08-04 11:16:53,729] INFO Kafka version: 7.4.1-ce (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:53,739] INFO Kafka commitId: 96cc303d3f85bf31 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:53,739] INFO Kafka startTimeMs: 1691147813729 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:16:54,333] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:16:54,336] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:16:55,630] INFO Adding listener with HTTP/2: NamedURI{uri=http://0.0.0.0:9021, name='null'} (io.confluent.rest.ApplicationServer)
control-center   | [2023-08-04 11:16:56,476] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to RXByHOqASZ2CbNrZqK_AMw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:56,624] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to Y_GSqt8QROagfczJg3r40g (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:57,733] INFO name=monitoring-input-topic-progress-.count type=monitoring cluster= value=0.0 (io.confluent.controlcenter.util.StreamProgressReporter)
control-center   | [2023-08-04 11:16:57,749] INFO name=monitoring-input-topic-progress-.rate type=monitoring cluster= value=0.0 (io.confluent.controlcenter.util.StreamProgressReporter)
control-center   | [2023-08-04 11:16:57,763] INFO name=monitoring-input-topic-progress-.timestamp type=monitoring cluster= value=NaN (io.confluent.controlcenter.util.StreamProgressReporter)
control-center   | [2023-08-04 11:16:57,770] INFO name=monitoring-input-topic-progress-.min type=monitoring cluster= value=1.7976931348623157E308 (io.confluent.controlcenter.util.StreamProgressReporter)
control-center   | [2023-08-04 11:16:57,988] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to iJLqc_VsS6GtzjQhq4pkTQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:58,400] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to KabcuEMhTCOz9nkNgpIsHg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:58,577] INFO ControlCenterBoundedMemoryConfig values: 
control-center   | 	rocksdb.cache.limit.strict = false
control-center   | 	rocksdb.cache.size = 16106127360
control-center   | 	rocksdb.cache.size.limit.enabled = false
control-center   | 	rocksdb.index.filter.block.ratio = 0.0
control-center   | 	rocksdb.write.buffer.cache.use = false
control-center   | 	rocksdb.write.buffer.size = 5368709120
control-center   |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center   | [2023-08-04 11:16:58,861] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 to 0 since the associated topicId changed from null to nzdecc30ShSdQcB04frvog (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:16:59,009] INFO ControlCenterBoundedMemoryConfig values: 
control-center   | 	rocksdb.cache.limit.strict = false
control-center   | 	rocksdb.cache.size = 16106127360
control-center   | 	rocksdb.cache.size.limit.enabled = false
control-center   | 	rocksdb.index.filter.block.ratio = 0.0
control-center   | 	rocksdb.write.buffer.cache.use = false
control-center   | 	rocksdb.write.buffer.size = 5368709120
control-center   |  (io.confluent.controlcenter.ControlCenterBoundedMemoryConfig)
control-center   | [2023-08-04 11:16:59,239] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 to 0 since the associated topicId changed from null to NZXeFHK0TkaUSTnQorwW-A (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:17:00,976] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center   | 	metrics.jmx.prefix = confluent.controlcenter
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = true
control-center   | 	port = 9021
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:17:01,005] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center   | 	metrics.jmx.prefix = confluent.controlcenter
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = true
control-center   | 	port = 9021
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
connect          | [2023-08-04 11:17:04,507] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/acl/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:17:04,528] INFO Loading plugin from: /usr/share/java/kafka-serde-tools (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
control-center   | [2023-08-04 11:17:04,626] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center   | 	metrics.jmx.prefix = confluent.controlcenter
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = true
control-center   | 	port = 9021
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:17:04,943] INFO Binding ControlCenterApplication to all listeners. (io.confluent.rest.Application)
control-center   | [2023-08-04 11:17:05,297] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center   | 	metrics.jmx.prefix = confluent.controlcenter
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = true
control-center   | 	port = 9021
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:17:05,533] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center   | 	metrics.jmx.prefix = confluent.controlcenter
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = true
control-center   | 	port = 9021
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:17:07,672] INFO RestConfig values: 
control-center   | 	access.control.allow.headers = 
control-center   | 	access.control.allow.methods = 
control-center   | 	access.control.allow.origin = 
control-center   | 	access.control.skip.options = true
control-center   | 	authentication.method = NONE
control-center   | 	authentication.realm = 
control-center   | 	authentication.roles = [*]
control-center   | 	authentication.skip.paths = []
control-center   | 	compression.enable = true
control-center   | 	connector.connection.limit = 0
control-center   | 	csrf.prevention.enable = false
control-center   | 	csrf.prevention.token.endpoint = /csrf
control-center   | 	csrf.prevention.token.expiration.minutes = 30
control-center   | 	csrf.prevention.token.max.entries = 10000
control-center   | 	debug = false
control-center   | 	dos.filter.delay.ms = 100
control-center   | 	dos.filter.enabled = false
control-center   | 	dos.filter.insert.headers = true
control-center   | 	dos.filter.ip.whitelist = []
control-center   | 	dos.filter.managed.attr = false
control-center   | 	dos.filter.max.idle.tracker.ms = 30000
control-center   | 	dos.filter.max.requests.ms = 30000
control-center   | 	dos.filter.max.requests.per.connection.per.sec = 25
control-center   | 	dos.filter.max.requests.per.sec = 25
control-center   | 	dos.filter.max.wait.ms = 50
control-center   | 	dos.filter.throttle.ms = 30000
control-center   | 	dos.filter.throttled.requests = 5
control-center   | 	http2.enabled = true
control-center   | 	idle.timeout.ms = 30000
control-center   | 	listener.protocol.map = []
control-center   | 	listeners = []
control-center   | 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
control-center   | 	metrics.jmx.prefix = confluent.controlcenter
control-center   | 	metrics.num.samples = 2
control-center   | 	metrics.sample.window.ms = 30000
control-center   | 	metrics.tag.map = []
control-center   | 	nosniff.prevention.enable = true
control-center   | 	port = 9021
control-center   | 	proxy.protocol.enabled = false
control-center   | 	reject.options.request = false
control-center   | 	request.logger.name = io.confluent.rest-utils.requests
control-center   | 	request.queue.capacity = 2147483647
control-center   | 	request.queue.capacity.growby = 64
control-center   | 	request.queue.capacity.init = 128
control-center   | 	resource.extension.classes = []
control-center   | 	response.http.headers.config = 
control-center   | 	response.mediatype.default = application/json
control-center   | 	response.mediatype.preferred = [application/json]
control-center   | 	rest.servlet.initializor.classes = []
control-center   | 	server.connection.limit = 0
control-center   | 	shutdown.graceful.ms = 1000
control-center   | 	ssl.cipher.suites = []
control-center   | 	ssl.client.auth = false
control-center   | 	ssl.client.authentication = NONE
control-center   | 	ssl.enabled.protocols = []
control-center   | 	ssl.endpoint.identification.algorithm = null
control-center   | 	ssl.key.password = [hidden]
control-center   | 	ssl.keymanager.algorithm = 
control-center   | 	ssl.keystore.location = 
control-center   | 	ssl.keystore.password = [hidden]
control-center   | 	ssl.keystore.reload = false
control-center   | 	ssl.keystore.type = JKS
control-center   | 	ssl.keystore.watch.location = 
control-center   | 	ssl.protocol = TLS
control-center   | 	ssl.provider = 
control-center   | 	ssl.trustmanager.algorithm = 
control-center   | 	ssl.truststore.location = 
control-center   | 	ssl.truststore.password = [hidden]
control-center   | 	ssl.truststore.type = JKS
control-center   | 	suppress.stack.trace.response = true
control-center   | 	thread.pool.max = 200
control-center   | 	thread.pool.min = 8
control-center   | 	websocket.path.prefix = /ws
control-center   | 	websocket.servlet.initializor.classes = []
control-center   |  (io.confluent.rest.RestConfig)
control-center   | [2023-08-04 11:17:07,961] INFO adding websocket endpoint ProducerResource (io.confluent.controlcenter.rest.ControlCenterApplication)
control-center   | [2023-08-04 11:17:07,985] INFO adding websocket endpoint ConsumerResource (io.confluent.controlcenter.rest.ControlCenterApplication)
control-center   | [2023-08-04 11:17:09,172] WARN [creqId=ba3b6a6d][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T11:17:02.574Z(1691147822574000), length=0B, duration=6361ms(6361695681ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:17:09,196] WARN [creqId=ba3b6a6d][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T11:17:09.034Z(1691147829034000), length=0B, duration=0ns, totalDuration=6456ms(6456577107ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | Caused by: java.net.ConnectException: Connection refused
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center   | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | [2023-08-04 11:17:09,620] INFO jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.18+10-LTS (org.eclipse.jetty.server.Server)
control-center   | [2023-08-04 11:17:09,768] WARN [creqId=3a6627c0][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:17:04.725Z(1691147824725000), length=0B, duration=5006ms(5006415578ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:17:09,768] WARN [creqId=3a6627c0][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:17:09.732Z(1691147829732000), length=0B, duration=0ns, totalDuration=5006ms(5006685429ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:17:10,469] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
control-center   | [2023-08-04 11:17:10,469] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
control-center   | [2023-08-04 11:17:10,480] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session)
connect          | [2023-08-04 11:17:12,808] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/kafka-serde-tools/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:17:12,812] INFO Loading plugin from: /usr/share/java/confluent-hub-client (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
control-center   | [2023-08-04 11:17:14,913] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
control-center   | [2023-08-04 11:17:14,938] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
control-center   | [2023-08-04 11:17:15,082] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
control-center   | [2023-08-04 11:17:15,194] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
connect          | [2023-08-04 11:17:16,067] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-hub-client/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:17:16,080] INFO Loading plugin from: /usr/share/java/monitoring-interceptors (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:17:18,490] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/monitoring-interceptors/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:17:18,506] INFO Loading plugin from: /usr/share/java/ce-kafka-rest-servlet (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:17:18,565] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/ce-kafka-rest-servlet/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:17:18,572] INFO Loading plugin from: /usr/share/java/confluent-rebalancer (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
control-center   | [2023-08-04 11:17:30,594] WARN [creqId=7704e588][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:17:30.240Z(1691147850240000), length=0B, duration=352ms(352346102ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:17:30,594] WARN [creqId=7704e588][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:17:30.593Z(1691147850593000), length=0B, duration=0ns, totalDuration=352ms(352493379ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:17:31,536] WARN [creqId=ae9e6286][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T11:17:31.455Z(1691147851455000), length=0B, duration=79603s(79603266ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:17:31,537] WARN [creqId=ae9e6286][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T11:17:31.535Z(1691147851535000), length=0B, duration=0ns, totalDuration=79757s(79757393ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | Caused by: java.net.ConnectException: Connection refused
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center   | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | [2023-08-04 11:17:39,889] INFO Started o.e.j.s.ServletContextHandler@2d2710a8{/api/kafka-rest/MkU3OEVBNTcwNTJENDM2Qg/kafka,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
control-center   | Aug 04, 2023 11:17:41 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center   | WARNING: A provider io.confluent.controlcenter.rest.CachedConsumerOffsetsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.CachedConsumerOffsetsResource will be ignored. 
control-center   | Aug 04, 2023 11:17:41 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center   | WARNING: A provider io.confluent.controlcenter.rest.CommandResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.CommandResource will be ignored. 
control-center   | Aug 04, 2023 11:17:41 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center   | WARNING: A provider io.confluent.controlcenter.rest.MessageDeliveryResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.MessageDeliveryResource will be ignored. 
control-center   | Aug 04, 2023 11:17:41 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center   | WARNING: A provider io.confluent.controlcenter.rest.PermissionsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.PermissionsResource will be ignored. 
control-center   | Aug 04, 2023 11:17:41 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center   | WARNING: A provider io.confluent.controlcenter.rest.AuthResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.AuthResource will be ignored. 
control-center   | Aug 04, 2023 11:17:41 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center   | WARNING: A provider io.confluent.controlcenter.rest.MetricsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.MetricsResource will be ignored. 
control-center   | Aug 04, 2023 11:17:41 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center   | WARNING: A provider io.confluent.controlcenter.rest.LicenseResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.LicenseResource will be ignored. 
control-center   | Aug 04, 2023 11:17:41 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center   | WARNING: A provider io.confluent.controlcenter.rest.KafkaResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.KafkaResource will be ignored. 
control-center   | Aug 04, 2023 11:17:41 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center   | WARNING: A provider io.confluent.controlcenter.rest.ClusterResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.ClusterResource will be ignored. 
control-center   | Aug 04, 2023 11:17:41 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center   | WARNING: A provider io.confluent.controlcenter.rest.StatusResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.StatusResource will be ignored. 
control-center   | Aug 04, 2023 11:17:41 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center   | WARNING: A provider io.confluent.controlcenter.rest.AlertsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.AlertsResource will be ignored. 
control-center   | Aug 04, 2023 11:17:41 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center   | WARNING: A provider io.confluent.controlcenter.rest.ServiceHealthCheckResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.ServiceHealthCheckResource will be ignored. 
control-center   | Aug 04, 2023 11:17:41 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center   | WARNING: A provider io.confluent.controlcenter.rest.HealthCheckResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.HealthCheckResource will be ignored. 
control-center   | Aug 04, 2023 11:17:41 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
control-center   | WARNING: A provider io.confluent.controlcenter.rest.FeatureFlagResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.controlcenter.rest.FeatureFlagResource will be ignored. 
connect          | [2023-08-04 11:17:45,538] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-rebalancer/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:17:45,542] INFO Loading plugin from: /usr/share/java/confluent-security (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
control-center   | [2023-08-04 11:17:47,754] WARN [creqId=481d27df][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:17:47.623Z(1691147867623000), length=0B, duration=129ms(129128804ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:17:47,755] WARN [creqId=481d27df][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:17:47.753Z(1691147867753000), length=0B, duration=0ns, totalDuration=129ms(129424812ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:17:49,276] INFO Started o.e.j.s.ServletContextHandler@50b1dfab{/,[io.confluent.controlcenter.rest.ModifiableResource@a69c4c3],AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
control-center   | [2023-08-04 11:17:49,359] INFO Started o.e.j.s.ServletContextHandler@5c85ed05{/api/kafka-rest-ws/MkU3OEVBNTcwNTJENDM2Qg,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
control-center   | [2023-08-04 11:17:49,537] INFO Started o.e.j.s.ServletContextHandler@23657729{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
control-center   | [2023-08-04 11:17:49,853] INFO Started NetworkTrafficServerConnector@48dc06a9{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:9021} (org.eclipse.jetty.server.AbstractConnector)
control-center   | [2023-08-04 11:17:49,855] INFO Started @187305ms (org.eclipse.jetty.server.Server)
control-center   | [2023-08-04 11:17:53,512] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:17:53,554] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:17:53,624] WARN [creqId=f87cc195][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T11:17:53.536Z(1691147873536000), length=0B, duration=81912s(81912580ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:17:53,639] WARN [creqId=f87cc195][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T11:17:53.619Z(1691147873619000), length=0B, duration=0ns, totalDuration=82095s(82095682ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | Caused by: java.net.ConnectException: Connection refused
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center   | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | [2023-08-04 11:18:09,215] WARN [creqId=7a028495][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:18:09.035Z(1691147889035000), length=0B, duration=178ms(178592712ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:18:09,216] WARN [creqId=7a028495][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:18:09.214Z(1691147889214000), length=0B, duration=0ns, totalDuration=178ms(178716445ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:18:14,099] WARN [creqId=535caa6f][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T11:18:14.088Z(1691147894088000), length=0B, duration=9047s(9047848ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:18:14,100] WARN [creqId=535caa6f][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T11:18:14.098Z(1691147894098000), length=0B, duration=0ns, totalDuration=9164s(9164284ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | Caused by: java.net.ConnectException: Connection refused
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center   | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
connect          | [2023-08-04 11:18:25,474] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-security/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:25,479] INFO Loading plugin from: /usr/share/java/confluent-telemetry (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
control-center   | [2023-08-04 11:18:26,242] WARN [creqId=071efdd1][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:18:26.227Z(1691147906227000), length=0B, duration=14091s(14091855ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:18:26,243] WARN [creqId=071efdd1][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:18:26.241Z(1691147906241000), length=0B, duration=0ns, totalDuration=14162s(14162374ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
connect          | [2023-08-04 11:18:27,718] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-telemetry/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:27,723] INFO Loading plugin from: /usr/share/java/rest-utils (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
control-center   | [2023-08-04 11:18:28,300] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] Processed 1 total records, ran 0 punctuators, and committed 1 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
connect          | [2023-08-04 11:18:30,165] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/rest-utils/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:30,170] INFO Loading plugin from: /usr/share/java/confluent-common (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:30,200] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-common/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:30,203] INFO Loading plugin from: /usr/share/java/ce-kafka-rest-extensions (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:30,898] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/ce-kafka-rest-extensions/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:30,901] INFO Loading plugin from: /usr/share/java/kafka-rest-lib (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
control-center   | [2023-08-04 11:18:34,405] WARN [creqId=71162721][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T11:18:34.397Z(1691147914397000), length=0B, duration=7348s(7348123ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:18:34,406] WARN [creqId=71162721][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T11:18:34.404Z(1691147914404000), length=0B, duration=0ns, totalDuration=7602s(7602332ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | Caused by: java.net.ConnectException: Connection refused
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center   | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
connect          | [2023-08-04 11:18:35,141] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/kafka-rest-lib/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:35,143] INFO Loading plugin from: /usr/share/java/confluent-metadata-service (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
control-center   | [2023-08-04 11:18:36,925] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:18:37,027] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:18:37,027] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:18:37,079] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:18:37,219] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] Processed 14 total records, ran 0 punctuators, and committed 3 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:18:37,228] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:18:37,230] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] Processed 14 total records, ran 0 punctuators, and committed 3 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:18:37,257] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:18:37,364] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:18:37,408] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:18:37,930] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] Processed 14 total records, ran 0 punctuators, and committed 6 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:18:38,140] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
connect          | [2023-08-04 11:18:38,174] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluent-metadata-service/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:38,176] INFO Loading plugin from: /usr/share/java/ce-kafka-http-server (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:40,744] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/ce-kafka-http-server/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:40,748] INFO Loading plugin from: /usr/share/java/kafka (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
control-center   | [2023-08-04 11:18:44,405] WARN [creqId=a28ebe12][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:18:44.335Z(1691147924335000), length=0B, duration=68157s(68157058ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:18:44,412] WARN [creqId=a28ebe12][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:18:44.404Z(1691147924404000), length=0B, duration=0ns, totalDuration=68273s(68273563ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:18:53,398] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:18:53,400] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:18:55,054] WARN [creqId=7d8d2a99][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T11:18:55.012Z(1691147935012000), length=0B, duration=40266s(40266346ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:18:55,068] WARN [creqId=7d8d2a99][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T11:18:55.053Z(1691147935053000), length=0B, duration=0ns, totalDuration=40413s(40413960ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | Caused by: java.net.ConnectException: Connection refused
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center   | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
connect          | [2023-08-04 11:18:59,293] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/kafka/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:59,294] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:59,294] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:59,301] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:59,302] INFO Added plugin 'io.confluent.connect.rest.datapreview.extension.util.PreviewRecordTransformer' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:59,302] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:59,303] INFO Added plugin 'io.confluent.connect.rest.datapreview.extension.ConnectorDataPreviewRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:59,306] INFO Loading plugin from: /usr/share/java/kafka-rest-bin (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:59,351] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/kafka-rest-bin/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:18:59,355] INFO Loading plugin from: /usr/share/confluent-hub-components/confluentinc-kafka-connect-datagen (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:19:01,865] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/confluent-hub-components/confluentinc-kafka-connect-datagen/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:19:01,866] INFO Added plugin 'io.confluent.kafka.connect.datagen.DatagenConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
control-center   | [2023-08-04 11:19:04,314] WARN [creqId=2f94a877][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:19:04.282Z(1691147944282000), length=0B, duration=24898s(24898242ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:19:04,314] WARN [creqId=2f94a877][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:19:04.307Z(1691147944307000), length=0B, duration=0ns, totalDuration=25167s(25167145ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:19:17,881] WARN [creqId=738341ea][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T11:19:17.874Z(1691147957874000), length=0B, duration=5068s(5068750ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:19:17,882] WARN [creqId=738341ea][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T11:19:17.880Z(1691147957880000), length=0B, duration=0ns, totalDuration=5323s(5323159ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | Caused by: java.net.ConnectException: Connection refused
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center   | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | [2023-08-04 11:19:26,856] WARN [creqId=485bfb86][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:19:26.675Z(1691147966675000), length=0B, duration=178ms(178410432ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:19:26,857] WARN [creqId=485bfb86][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:19:26.854Z(1691147966854000), length=0B, duration=0ns, totalDuration=178ms(178636345ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:19:40,220] WARN [creqId=ae037cac][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T11:19:40.212Z(1691147980212000), length=0B, duration=7002s(7002367ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:19:40,221] WARN [creqId=ae037cac][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T11:19:40.219Z(1691147980219000), length=0B, duration=0ns, totalDuration=7122s(7122957ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | Caused by: java.net.ConnectException: Connection refused
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center   | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | [2023-08-04 11:19:48,239] WARN [creqId=2ce138ae][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:19:48.216Z(1691147988216000), length=0B, duration=21846s(21846612ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:19:48,246] WARN [creqId=2ce138ae][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:19:48.238Z(1691147988238000), length=0B, duration=0ns, totalDuration=21950s(21950428ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:19:53,389] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:19:53,390] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:19:58,415] WARN [creqId=87f1dec7][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T11:19:58.399Z(1691147998399000), length=0B, duration=14332s(14332755ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:19:58,415] WARN [creqId=87f1dec7][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T11:19:58.414Z(1691147998414000), length=0B, duration=0ns, totalDuration=14463s(14463716ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | Caused by: java.net.ConnectException: Connection refused
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center   | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | [2023-08-04 11:20:05,709] WARN [creqId=135556a1][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:20:05.679Z(1691148005679000), length=0B, duration=27264s(27264198ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:20:05,709] WARN [creqId=135556a1][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:20:05.707Z(1691148005707000), length=0B, duration=0ns, totalDuration=28010s(28010800ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
connect          | [2023-08-04 11:20:14,737] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@2c13da15 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,743] INFO Added aliases 'DatagenConnector' and 'Datagen' to plugin 'io.confluent.kafka.connect.datagen.DatagenConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,744] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,744] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,745] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,745] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,745] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,746] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,746] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,746] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,747] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,747] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,747] INFO Added aliases 'JsonSchemaConverter' and 'JsonSchema' to plugin 'io.confluent.connect.json.JsonSchemaConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,748] INFO Added aliases 'ProtobufConverter' and 'Protobuf' to plugin 'io.confluent.connect.protobuf.ProtobufConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,748] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,748] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,749] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,749] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,749] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,750] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,751] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,751] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,752] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,752] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,752] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,753] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,753] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,754] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,754] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,754] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,755] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,756] INFO Added alias 'PreviewRecordTransformer' to plugin 'io.confluent.connect.rest.datapreview.extension.util.PreviewRecordTransformer' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,756] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,757] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,757] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,758] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,759] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,760] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,760] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,761] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,761] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,761] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,761] INFO Added alias 'ConnectorDataPreviewRestExtension' to plugin 'io.confluent.connect.rest.datapreview.extension.ConnectorDataPreviewRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,761] INFO Added alias 'ConnectSecurityExtension' to plugin 'io.confluent.connect.security.ConnectSecurityExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,762] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,762] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,762] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:14,762] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
connect          | [2023-08-04 11:20:15,089] INFO DistributedConfig values: 
connect          | 	access.control.allow.methods = 
connect          | 	access.control.allow.origin = 
connect          | 	admin.listeners = null
connect          | 	bootstrap.servers = [broker:29092]
connect          | 	client.dns.lookup = use_all_dns_ips
connect          | 	client.id = 
connect          | 	config.providers = []
connect          | 	config.storage.replication.factor = 1
connect          | 	config.storage.topic = docker-connect-configs
connect          | 	confluent.connector.task.status.metrics = false
connect          | 	confluent.license = [hidden]
connect          | 	confluent.license.inject.into.connectors = true
connect          | 	confluent.topic = _confluent-command
connect          | 	confluent.topic.bootstrap.servers = []
connect          | 	confluent.topic.client.dns.lookup = use_all_dns_ips
connect          | 	confluent.topic.client.id = 
connect          | 	confluent.topic.connections.max.idle.ms = 540000
connect          | 	confluent.topic.consumer.allow.auto.create.topics = true
connect          | 	confluent.topic.consumer.auto.commit.interval.ms = 5000
connect          | 	confluent.topic.consumer.auto.offset.reset = latest
connect          | 	confluent.topic.consumer.check.crcs = true
connect          | 	confluent.topic.consumer.client.dns.lookup = use_all_dns_ips
connect          | 	confluent.topic.consumer.client.id = 
connect          | 	confluent.topic.consumer.client.rack = 
connect          | 	confluent.topic.consumer.connections.max.idle.ms = 540000
connect          | 	confluent.topic.consumer.default.api.timeout.ms = 60000
connect          | 	confluent.topic.consumer.enable.auto.commit = true
connect          | 	confluent.topic.consumer.exclude.internal.topics = true
connect          | 	confluent.topic.consumer.fetch.max.bytes = 52428800
connect          | 	confluent.topic.consumer.fetch.max.wait.ms = 500
connect          | 	confluent.topic.consumer.fetch.min.bytes = 1
connect          | 	confluent.topic.consumer.group.id = null
connect          | 	confluent.topic.consumer.group.instance.id = null
connect          | 	confluent.topic.consumer.heartbeat.interval.ms = 3000
connect          | 	confluent.topic.consumer.interceptor.classes = []
connect          | 	confluent.topic.consumer.internal.leave.group.on.close = true
connect          | 	confluent.topic.consumer.internal.throw.on.fetch.stable.offset.unsupported = false
connect          | 	confluent.topic.consumer.isolation.level = read_uncommitted
connect          | 	confluent.topic.consumer.max.partition.fetch.bytes = 1048576
connect          | 	confluent.topic.consumer.max.poll.interval.ms = 300000
connect          | 	confluent.topic.consumer.max.poll.records = 500
connect          | 	confluent.topic.consumer.metadata.max.age.ms = 300000
connect          | 	confluent.topic.consumer.metric.reporters = []
connect          | 	confluent.topic.consumer.metrics.num.samples = 2
connect          | 	confluent.topic.consumer.metrics.recording.level = INFO
connect          | 	confluent.topic.consumer.metrics.sample.window.ms = 30000
connect          | 	confluent.topic.consumer.partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
connect          | 	confluent.topic.consumer.receive.buffer.bytes = 65536
connect          | 	confluent.topic.consumer.reconnect.backoff.max.ms = 1000
connect          | 	confluent.topic.consumer.reconnect.backoff.ms = 50
connect          | 	confluent.topic.consumer.request.timeout.ms = 30000
connect          | 	confluent.topic.consumer.retry.backoff.ms = 100
connect          | 	confluent.topic.consumer.sasl.client.callback.handler.class = null
connect          | 	confluent.topic.consumer.sasl.jaas.config = null
connect          | 	confluent.topic.consumer.sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	confluent.topic.consumer.sasl.kerberos.min.time.before.relogin = 60000
connect          | 	confluent.topic.consumer.sasl.kerberos.service.name = null
connect          | 	confluent.topic.consumer.sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	confluent.topic.consumer.sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	confluent.topic.consumer.sasl.login.callback.handler.class = null
connect          | 	confluent.topic.consumer.sasl.login.class = null
connect          | 	confluent.topic.consumer.sasl.login.connect.timeout.ms = null
connect          | 	confluent.topic.consumer.sasl.login.read.timeout.ms = null
connect          | 	confluent.topic.consumer.sasl.login.refresh.buffer.seconds = 300
connect          | 	confluent.topic.consumer.sasl.login.refresh.min.period.seconds = 60
connect          | 	confluent.topic.consumer.sasl.login.refresh.window.factor = 0.8
connect          | 	confluent.topic.consumer.sasl.login.refresh.window.jitter = 0.05
connect          | 	confluent.topic.consumer.sasl.login.retry.backoff.max.ms = 10000
connect          | 	confluent.topic.consumer.sasl.login.retry.backoff.ms = 100
connect          | 	confluent.topic.consumer.sasl.mechanism = GSSAPI
connect          | 	confluent.topic.consumer.sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	confluent.topic.consumer.sasl.oauthbearer.expected.audience = null
connect          | 	confluent.topic.consumer.sasl.oauthbearer.expected.issuer = null
connect          | 	confluent.topic.consumer.sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	confluent.topic.consumer.sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	confluent.topic.consumer.sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	confluent.topic.consumer.sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	confluent.topic.consumer.sasl.oauthbearer.scope.claim.name = scope
connect          | 	confluent.topic.consumer.sasl.oauthbearer.sub.claim.name = sub
connect          | 	confluent.topic.consumer.sasl.oauthbearer.token.endpoint.url = null
connect          | 	confluent.topic.consumer.security.protocol = PLAINTEXT
connect          | 	confluent.topic.consumer.security.providers = null
connect          | 	confluent.topic.consumer.send.buffer.bytes = 131072
connect          | 	confluent.topic.consumer.session.timeout.ms = 45000
connect          | 	confluent.topic.consumer.socket.connection.setup.timeout.max.ms = 30000
connect          | 	confluent.topic.consumer.socket.connection.setup.timeout.ms = 10000
connect          | 	confluent.topic.consumer.ssl.cipher.suites = null
connect          | 	confluent.topic.consumer.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	confluent.topic.consumer.ssl.endpoint.identification.algorithm = https
connect          | 	confluent.topic.consumer.ssl.engine.factory.class = null
connect          | 	confluent.topic.consumer.ssl.key.password = null
connect          | 	confluent.topic.consumer.ssl.keymanager.algorithm = SunX509
connect          | 	confluent.topic.consumer.ssl.keystore.certificate.chain = null
connect          | 	confluent.topic.consumer.ssl.keystore.key = null
connect          | 	confluent.topic.consumer.ssl.keystore.location = null
connect          | 	confluent.topic.consumer.ssl.keystore.password = null
connect          | 	confluent.topic.consumer.ssl.keystore.type = JKS
connect          | 	confluent.topic.consumer.ssl.protocol = TLSv1.3
connect          | 	confluent.topic.consumer.ssl.provider = null
connect          | 	confluent.topic.consumer.ssl.secure.random.implementation = null
connect          | 	confluent.topic.consumer.ssl.trustmanager.algorithm = PKIX
connect          | 	confluent.topic.consumer.ssl.truststore.certificates = null
connect          | 	confluent.topic.consumer.ssl.truststore.location = null
connect          | 	confluent.topic.consumer.ssl.truststore.password = null
connect          | 	confluent.topic.consumer.ssl.truststore.type = JKS
connect          | 	confluent.topic.interceptor.classes = []
connect          | 	confluent.topic.metadata.max.age.ms = 300000
connect          | 	confluent.topic.metric.reporters = []
connect          | 	confluent.topic.metrics.num.samples = 2
connect          | 	confluent.topic.metrics.recording.level = INFO
connect          | 	confluent.topic.metrics.sample.window.ms = 30000
connect          | 	confluent.topic.producer.acks = all
connect          | 	confluent.topic.producer.batch.size = 16384
connect          | 	confluent.topic.producer.buffer.memory = 33554432
connect          | 	confluent.topic.producer.client.dns.lookup = use_all_dns_ips
connect          | 	confluent.topic.producer.client.id = 
connect          | 	confluent.topic.producer.compression.type = none
connect          | 	confluent.topic.producer.connections.max.idle.ms = 540000
connect          | 	confluent.topic.producer.delivery.timeout.ms = 120000
connect          | 	confluent.topic.producer.enable.idempotence = true
connect          | 	confluent.topic.producer.interceptor.classes = []
connect          | 	confluent.topic.producer.linger.ms = 0
connect          | 	confluent.topic.producer.max.block.ms = 60000
connect          | 	confluent.topic.producer.max.in.flight.requests.per.connection = 5
connect          | 	confluent.topic.producer.max.request.size = 1048576
connect          | 	confluent.topic.producer.metadata.max.age.ms = 300000
connect          | 	confluent.topic.producer.metadata.max.idle.ms = 300000
connect          | 	confluent.topic.producer.metric.reporters = []
connect          | 	confluent.topic.producer.metrics.num.samples = 2
connect          | 	confluent.topic.producer.metrics.recording.level = INFO
connect          | 	confluent.topic.producer.metrics.sample.window.ms = 30000
connect          | 	confluent.topic.producer.partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
connect          | 	confluent.topic.producer.receive.buffer.bytes = 32768
connect          | 	confluent.topic.producer.reconnect.backoff.max.ms = 1000
connect          | 	confluent.topic.producer.reconnect.backoff.ms = 50
connect          | 	confluent.topic.producer.request.timeout.ms = 30000
connect          | 	confluent.topic.producer.retry.backoff.ms = 100
connect          | 	confluent.topic.producer.sasl.client.callback.handler.class = null
connect          | 	confluent.topic.producer.sasl.jaas.config = null
connect          | 	confluent.topic.producer.sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	confluent.topic.producer.sasl.kerberos.min.time.before.relogin = 60000
connect          | 	confluent.topic.producer.sasl.kerberos.service.name = null
connect          | 	confluent.topic.producer.sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	confluent.topic.producer.sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	confluent.topic.producer.sasl.login.callback.handler.class 
connect          | = null
connect          | 	confluent.topic.producer.sasl.login.class = null
connect          | 	confluent.topic.producer.sasl.login.connect.timeout.ms = null
connect          | 	confluent.topic.producer.sasl.login.read.timeout.ms = null
connect          | 	confluent.topic.producer.sasl.login.refresh.buffer.seconds = 300
connect          | 	confluent.topic.producer.sasl.login.refresh.min.period.seconds = 60
connect          | 	confluent.topic.producer.sasl.login.refresh.window.factor = 0.8
connect          | 	confluent.topic.producer.sasl.login.refresh.window.jitter = 0.05
connect          | 	confluent.topic.producer.sasl.login.retry.backoff.max.ms = 10000
connect          | 	confluent.topic.producer.sasl.login.retry.backoff.ms = 100
connect          | 	confluent.topic.producer.sasl.mechanism = GSSAPI
connect          | 	confluent.topic.producer.sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	confluent.topic.producer.sasl.oauthbearer.expected.audience = null
connect          | 	confluent.topic.producer.sasl.oauthbearer.expected.issuer = null
connect          | 	confluent.topic.producer.sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	confluent.topic.producer.sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	confluent.topic.producer.sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	confluent.topic.producer.sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	confluent.topic.producer.sasl.oauthbearer.scope.claim.name = scope
connect          | 	confluent.topic.producer.sasl.oauthbearer.sub.claim.name = sub
connect          | 	confluent.topic.producer.sasl.oauthbearer.token.endpoint.url = null
connect          | 	confluent.topic.producer.security.protocol = PLAINTEXT
connect          | 	confluent.topic.producer.security.providers = null
connect          | 	confluent.topic.producer.send.buffer.bytes = 131072
connect          | 	confluent.topic.producer.socket.connection.setup.timeout.max.ms = 30000
connect          | 	confluent.topic.producer.socket.connection.setup.timeout.ms = 10000
connect          | 	confluent.topic.producer.ssl.cipher.suites = null
connect          | 	confluent.topic.producer.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	confluent.topic.producer.ssl.endpoint.identification.algorithm = https
connect          | 	confluent.topic.producer.ssl.engine.factory.class = null
connect          | 	confluent.topic.producer.ssl.key.password = null
connect          | 	confluent.topic.producer.ssl.keymanager.algorithm = SunX509
connect          | 	confluent.topic.producer.ssl.keystore.certificate.chain = null
connect          | 	confluent.topic.producer.ssl.keystore.key = null
connect          | 	confluent.topic.producer.ssl.keystore.location = null
connect          | 	confluent.topic.producer.ssl.keystore.password = null
connect          | 	confluent.topic.producer.ssl.keystore.type = JKS
connect          | 	confluent.topic.producer.ssl.protocol = TLSv1.3
connect          | 	confluent.topic.producer.ssl.provider = null
connect          | 	confluent.topic.producer.ssl.secure.random.implementation = null
connect          | 	confluent.topic.producer.ssl.trustmanager.algorithm = PKIX
connect          | 	confluent.topic.producer.ssl.truststore.certificates = null
connect          | 	confluent.topic.producer.ssl.truststore.location = null
connect          | 	confluent.topic.producer.ssl.truststore.password = null
connect          | 	confluent.topic.producer.ssl.truststore.type = JKS
connect          | 	confluent.topic.producer.transaction.timeout.ms = 60000
connect          | 	confluent.topic.producer.transactional.id = null
connect          | 	confluent.topic.receive.buffer.bytes = 32768
connect          | 	confluent.topic.reconnect.backoff.max.ms = 1000
connect          | 	confluent.topic.reconnect.backoff.ms = 50
connect          | 	confluent.topic.replication.factor = 3
connect          | 	confluent.topic.request.timeout.ms = 30000
connect          | 	confluent.topic.retry.backoff.ms = 100
connect          | 	confluent.topic.sasl.client.callback.handler.class = null
connect          | 	confluent.topic.sasl.jaas.config = null
connect          | 	confluent.topic.sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	confluent.topic.sasl.kerberos.min.time.before.relogin = 60000
connect          | 	confluent.topic.sasl.kerberos.service.name = null
connect          | 	confluent.topic.sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	confluent.topic.sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	confluent.topic.sasl.login.callback.handler.class = null
connect          | 	confluent.topic.sasl.login.class = null
connect          | 	confluent.topic.sasl.login.connect.timeout.ms = null
connect          | 	confluent.topic.sasl.login.read.timeout.ms = null
connect          | 	confluent.topic.sasl.login.refresh.buffer.seconds = 300
connect          | 	confluent.topic.sasl.login.refresh.min.period.seconds = 60
connect          | 	confluent.topic.sasl.login.refresh.window.factor = 0.8
connect          | 	confluent.topic.sasl.login.refresh.window.jitter = 0.05
connect          | 	confluent.topic.sasl.login.retry.backoff.max.ms = 10000
connect          | 	confluent.topic.sasl.login.retry.backoff.ms = 100
connect          | 	confluent.topic.sasl.mechanism = GSSAPI
connect          | 	confluent.topic.sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	confluent.topic.sasl.oauthbearer.expected.audience = null
connect          | 	confluent.topic.sasl.oauthbearer.expected.issuer = null
connect          | 	confluent.topic.sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	confluent.topic.sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	confluent.topic.sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	confluent.topic.sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	confluent.topic.sasl.oauthbearer.scope.claim.name = scope
connect          | 	confluent.topic.sasl.oauthbearer.sub.claim.name = sub
connect          | 	confluent.topic.sasl.oauthbearer.token.endpoint.url = null
connect          | 	confluent.topic.security.protocol = PLAINTEXT
connect          | 	confluent.topic.security.providers = null
connect          | 	confluent.topic.send.buffer.bytes = 131072
connect          | 	confluent.topic.socket.connection.setup.timeout.max.ms = 30000
connect          | 	confluent.topic.socket.connection.setup.timeout.ms = 10000
connect          | 	confluent.topic.ssl.cipher.suites = null
connect          | 	confluent.topic.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	confluent.topic.ssl.endpoint.identification.algorithm = https
connect          | 	confluent.topic.ssl.engine.factory.class = null
connect          | 	confluent.topic.ssl.key.password = null
connect          | 	confluent.topic.ssl.keymanager.algorithm = SunX509
connect          | 	confluent.topic.ssl.keystore.certificate.chain = null
connect          | 	confluent.topic.ssl.keystore.key = null
connect          | 	confluent.topic.ssl.keystore.location = null
connect          | 	confluent.topic.ssl.keystore.password = null
connect          | 	confluent.topic.ssl.keystore.type = JKS
connect          | 	confluent.topic.ssl.protocol = TLSv1.3
connect          | 	confluent.topic.ssl.provider = null
connect          | 	confluent.topic.ssl.secure.random.implementation = null
connect          | 	confluent.topic.ssl.trustmanager.algorithm = PKIX
connect          | 	confluent.topic.ssl.truststore.certificates = null
connect          | 	confluent.topic.ssl.truststore.location = null
connect          | 	confluent.topic.ssl.truststore.password = null
connect          | 	confluent.topic.ssl.truststore.type = JKS
connect          | 	connect.protocol = sessioned
connect          | 	connections.max.idle.ms = 540000
connect          | 	connector.client.config.override.policy = All
connect          | 	group.id = compose-connect-group
connect          | 	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
connect          | 	heartbeat.interval.ms = 3000
connect          | 	inter.worker.key.generation.algorithm = HmacSHA256
connect          | 	inter.worker.key.size = null
connect          | 	inter.worker.key.ttl.ms = 3600000
connect          | 	inter.worker.signature.algorithm = HmacSHA256
connect          | 	inter.worker.verification.algorithms = [HmacSHA256]
connect          | 	key.converter = class org.apache.kafka.connect.storage.StringConverter
connect          | 	listeners = [http://:8083]
connect          | 	metadata.max.age.ms = 300000
connect          | 	metric.reporters = []
connect          | 	metrics.num.samples = 2
connect          | 	metrics.recording.level = INFO
connect          | 	metrics.sample.window.ms = 30000
connect          | 	offset.flush.interval.ms = 10000
connect          | 	offset.flush.timeout.ms = 5000
connect          | 	offset.storage.partitions = 25
connect          | 	offset.storage.replication.factor = 1
connect          | 	offset.storage.topic = docker-connect-offsets
connect          | 	plugin.path = [/usr/share/java, /usr/share/confluent-hub-components]
connect          | 	rebalance.timeout.ms = 60000
connect          | 	receive.buffer.bytes = 32768
connect          | 	reconnect.backoff.max.ms = 1000
connect          | 	reconnect.backoff.ms = 50
connect          | 	request.timeout.ms = 40000
connect          | 	response.http.headers.config = 
connect          | 	rest.advertised.host.name = connect
connect          | 	rest.advertised.listener = null
connect          | 	rest.advertised.port = null
connect          | 	rest.extension.classes = []
connect          | 	rest.servlet.initializor.classes = []
connect          | 	retry.backoff.ms = 100
connect          | 	sasl.client.callback.handler.class = null
connect          | 	sasl.jaas.config = null
connect          | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	sasl.kerberos.min.time.before.relogin = 60000
connect          | 	sasl.kerberos.service.name = null
connect          | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	sasl.login.callback.handler.class = null
connect          | 	sasl.login.class = null
connect          | 	sasl.login.connect.timeout.ms = null
connect          | 	sasl.login.read.timeout.ms = null
connect          | 	sasl.login.refresh.buffer.seconds = 300
connect          | 	sasl.login.refresh.min.period.seconds = 60
connect          | 	sasl.login.refresh.window.factor = 0.8
connect          | 	sasl.login.refresh.window.jitter = 0.05
connect          | 	sasl.login.retry.backoff.max.ms = 10000
connect          | 	sasl.login.retry.backoff.ms = 100
connect          | 	sasl.mechanism = GSSAPI
connect          | 	sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	sasl.oauthbearer.expected.audience = null
connect          | 	sasl.oauthbearer.expected.issuer = null
connect          | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	sasl.oauthbearer.scope.claim.name = scope
connect          | 	sasl.oauthbearer.sub.claim.name = sub
connect          | 	sasl.oauthbearer.token.endpoint.url = null
connect          | 	scheduled.rebalance.max.delay.ms = 300000
connect          | 	security.protocol = PLAINTEXT
connect          | 	send.buffer.bytes = 131072
connect          | 	session.timeout.ms = 10000
connect          | 	socket.connection.setup.timeout.max.ms = 30000
connect          | 	socket.connection.setup.timeout.ms = 10000
connect          | 	ssl.cipher.suites = null
connect          | 	ssl.client.auth = none
connect          | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	ssl.endpoint.identification.algorithm = https
connect          | 	ssl.engine.factory.class = null
connect          | 	ssl.key.password = null
connect          | 	ssl.keymanager.algorithm = SunX509
connect          | 	ssl.keystore.certificate.chain = null
connect          | 	ssl.keystore.key = null
connect          | 	ssl.keystore.location = null
connect          | 	ssl.keystore.password = null
connect          | 	ssl.keystore.type = JKS
connect          | 	ssl.protocol = TLSv1.3
connect          | 	ssl.provider = null
connect          | 	ssl.secure.random.implementation = null
connect          | 	ssl.trustmanager.algorithm = PKIX
connect          | 	ssl.truststore.certificates = null
connect          | 	ssl.truststore.location = null
connect          | 	ssl.truststore.password = null
connect          | 	ssl.truststore.type = JKS
connect          | 	status.storage.partitions = 5
connect          | 	status.storage.replication.factor = 1
connect          | 	status.storage.topic = docker-connect-status
connect          | 	task.shutdown.graceful.timeout.ms = 5000
connect          | 	topic.creation.enable = true
connect          | 	topic.tracking.allow.reset = true
connect          | 	topic.tracking.enable = true
connect          | 	value.converter = class io.confluent.connect.avro.AvroConverter
connect          | 	worker.sync.timeout.ms = 3000
connect          | 	worker.unsync.backoff.ms = 300000
connect          |  (org.apache.kafka.connect.runtime.distributed.DistributedConfig)
connect          | [2023-08-04 11:20:15,115] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
connect          | [2023-08-04 11:20:15,133] INFO AdminClientConfig values: 
connect          | 	bootstrap.servers = [broker:29092]
connect          | 	client.dns.lookup = use_all_dns_ips
connect          | 	client.id = 
connect          | 	connections.max.idle.ms = 300000
connect          | 	default.api.timeout.ms = 60000
connect          | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
connect          | 	metadata.max.age.ms = 300000
connect          | 	metric.reporters = []
connect          | 	metrics.num.samples = 2
connect          | 	metrics.recording.level = INFO
connect          | 	metrics.sample.window.ms = 30000
connect          | 	receive.buffer.bytes = 65536
connect          | 	reconnect.backoff.max.ms = 1000
connect          | 	reconnect.backoff.ms = 50
connect          | 	request.timeout.ms = 30000
connect          | 	retries = 2147483647
connect          | 	retry.backoff.ms = 100
connect          | 	sasl.client.callback.handler.class = null
connect          | 	sasl.jaas.config = null
connect          | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	sasl.kerberos.min.time.before.relogin = 60000
connect          | 	sasl.kerberos.service.name = null
connect          | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	sasl.login.callback.handler.class = null
connect          | 	sasl.login.class = null
connect          | 	sasl.login.connect.timeout.ms = null
connect          | 	sasl.login.read.timeout.ms = null
connect          | 	sasl.login.refresh.buffer.seconds = 300
connect          | 	sasl.login.refresh.min.period.seconds = 60
connect          | 	sasl.login.refresh.window.factor = 0.8
connect          | 	sasl.login.refresh.window.jitter = 0.05
connect          | 	sasl.login.retry.backoff.max.ms = 10000
connect          | 	sasl.login.retry.backoff.ms = 100
connect          | 	sasl.mechanism = GSSAPI
connect          | 	sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	sasl.oauthbearer.expected.audience = null
connect          | 	sasl.oauthbearer.expected.issuer = null
connect          | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	sasl.oauthbearer.scope.claim.name = scope
connect          | 	sasl.oauthbearer.sub.claim.name = sub
connect          | 	sasl.oauthbearer.token.endpoint.url = null
connect          | 	security.protocol = PLAINTEXT
connect          | 	security.providers = null
connect          | 	send.buffer.bytes = 131072
connect          | 	socket.connection.setup.timeout.max.ms = 30000
connect          | 	socket.connection.setup.timeout.ms = 10000
connect          | 	ssl.cipher.suites = null
connect          | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	ssl.endpoint.identification.algorithm = https
connect          | 	ssl.engine.factory.class = null
connect          | 	ssl.key.password = null
connect          | 	ssl.keymanager.algorithm = SunX509
connect          | 	ssl.keystore.certificate.chain = null
connect          | 	ssl.keystore.key = null
connect          | 	ssl.keystore.location = null
connect          | 	ssl.keystore.password = null
connect          | 	ssl.keystore.type = JKS
connect          | 	ssl.protocol = TLSv1.3
connect          | 	ssl.provider = null
connect          | 	ssl.secure.random.implementation = null
connect          | 	ssl.trustmanager.algorithm = PKIX
connect          | 	ssl.truststore.certificates = null
connect          | 	ssl.truststore.location = null
connect          | 	ssl.truststore.password = null
connect          | 	ssl.truststore.type = JKS
connect          |  (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,655] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,656] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,656] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,656] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,656] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,656] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,656] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,656] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,656] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,656] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,657] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,657] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,657] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,657] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,657] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,657] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,657] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,657] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:15,660] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:15,660] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:15,660] INFO Kafka startTimeMs: 1691148015657 (org.apache.kafka.common.utils.AppInfoParser)
control-center   | [2023-08-04 11:20:16,505] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to RXByHOqASZ2CbNrZqK_AMw (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:20:16,506] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 8fgQicUSRVi7xJcKYztVRQ (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:20:16,528] INFO [AdminClient clientId=adminclient-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:20:17,548] WARN [creqId=fbd3ab50][http://connect:8083/v1/metadata/id#GET] Request: {startTime=2023-08-04T11:20:17.543Z(1691148017543000), length=0B, duration=3572s(3572624ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, scheme=none+http, name=GET, headers=[:method=GET, :path=/v1/metadata/id, :scheme=http, :authority=connect:8083]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:20:17,548] WARN [creqId=fbd3ab50][http://connect:8083/v1/metadata/id#GET] Response: {startTime=2023-08-04T11:20:17.547Z(1691148017547000), length=0B, duration=0ns, totalDuration=3689s(3689766ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083, headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.notifyConnect(HttpChannelPool.java:550)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$4(HttpChannelPool.java:378)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.HttpChannelPool.lambda$connect$5(HttpChannelPool.java:410)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: connect/192.168.224.4:8083
control-center   | Caused by: java.net.ConnectException: Connection refused
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
control-center   | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
control-center   | 	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
control-center   | 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
connect          | [2023-08-04 11:20:18,417] INFO Kafka cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.connect.util.ConnectUtils)
connect          | [2023-08-04 11:20:18,427] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:18,524] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:18,525] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:18,525] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:18,606] INFO Logging initialized @345235ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
connect          | [2023-08-04 11:20:18,874] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer)
connect          | [2023-08-04 11:20:18,875] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer)
connect          | [2023-08-04 11:20:18,913] INFO jetty-9.4.44.v20210927; built: 2021-09-27T23:02:44.612Z; git: 8da83308eeca865e495e53ef315a249d63ba9332; jvm 11.0.14.1+1-LTS (org.eclipse.jetty.server.Server)
connect          | [2023-08-04 11:20:19,028] INFO Started http_8083@3544563b{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector)
connect          | [2023-08-04 11:20:19,031] INFO Started @345658ms (org.eclipse.jetty.server.Server)
connect          | [2023-08-04 11:20:19,142] INFO Advertised URI: http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
connect          | [2023-08-04 11:20:19,142] INFO REST server listening at http://192.168.224.4:8083/, advertising URL http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
connect          | [2023-08-04 11:20:19,143] INFO Advertised URI: http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
connect          | [2023-08-04 11:20:19,143] INFO REST admin endpoints at http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
connect          | [2023-08-04 11:20:19,143] INFO Advertised URI: http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
connect          | [2023-08-04 11:20:19,163] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
connect          | [2023-08-04 11:20:19,165] INFO AdminClientConfig values: 
connect          | 	bootstrap.servers = [broker:29092]
connect          | 	client.dns.lookup = use_all_dns_ips
connect          | 	client.id = 
connect          | 	connections.max.idle.ms = 300000
connect          | 	default.api.timeout.ms = 60000
connect          | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
connect          | 	metadata.max.age.ms = 300000
connect          | 	metric.reporters = []
connect          | 	metrics.num.samples = 2
connect          | 	metrics.recording.level = INFO
connect          | 	metrics.sample.window.ms = 30000
connect          | 	receive.buffer.bytes = 65536
connect          | 	reconnect.backoff.max.ms = 1000
connect          | 	reconnect.backoff.ms = 50
connect          | 	request.timeout.ms = 30000
connect          | 	retries = 2147483647
connect          | 	retry.backoff.ms = 100
connect          | 	sasl.client.callback.handler.class = null
connect          | 	sasl.jaas.config = null
connect          | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	sasl.kerberos.min.time.before.relogin = 60000
connect          | 	sasl.kerberos.service.name = null
connect          | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	sasl.login.callback.handler.class = null
connect          | 	sasl.login.class = null
connect          | 	sasl.login.connect.timeout.ms = null
connect          | 	sasl.login.read.timeout.ms = null
connect          | 	sasl.login.refresh.buffer.seconds = 300
connect          | 	sasl.login.refresh.min.period.seconds = 60
connect          | 	sasl.login.refresh.window.factor = 0.8
connect          | 	sasl.login.refresh.window.jitter = 0.05
connect          | 	sasl.login.retry.backoff.max.ms = 10000
connect          | 	sasl.login.retry.backoff.ms = 100
connect          | 	sasl.mechanism = GSSAPI
connect          | 	sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	sasl.oauthbearer.expected.audience = null
connect          | 	sasl.oauthbearer.expected.issuer = null
connect          | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	sasl.oauthbearer.scope.claim.name = scope
connect          | 	sasl.oauthbearer.sub.claim.name = sub
connect          | 	sasl.oauthbearer.token.endpoint.url = null
connect          | 	security.protocol = PLAINTEXT
connect          | 	security.providers = null
connect          | 	send.buffer.bytes = 131072
connect          | 	socket.connection.setup.timeout.max.ms = 30000
connect          | 	socket.connection.setup.timeout.ms = 10000
connect          | 	ssl.cipher.suites = null
connect          | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	ssl.endpoint.identification.algorithm = https
connect          | 	ssl.engine.factory.class = null
connect          | 	ssl.key.password = null
connect          | 	ssl.keymanager.algorithm = SunX509
connect          | 	ssl.keystore.certificate.chain = null
connect          | 	ssl.keystore.key = null
connect          | 	ssl.keystore.location = null
connect          | 	ssl.keystore.password = null
connect          | 	ssl.keystore.type = JKS
connect          | 	ssl.protocol = TLSv1.3
connect          | 	ssl.provider = null
connect          | 	ssl.secure.random.implementation = null
connect          | 	ssl.trustmanager.algorithm = PKIX
connect          | 	ssl.truststore.certificates = null
connect          | 	ssl.truststore.location = null
connect          | 	ssl.truststore.password = null
connect          | 	ssl.truststore.type = JKS
connect          |  (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,205] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,205] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,205] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,205] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,205] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,205] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,205] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,206] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,206] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,206] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,206] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,206] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,206] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,206] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,206] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,206] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,206] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,207] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,207] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:19,207] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:19,207] INFO Kafka startTimeMs: 1691148019207 (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:19,250] INFO Kafka cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.connect.util.ConnectUtils)
connect          | [2023-08-04 11:20:19,254] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:19,274] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:19,274] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:19,275] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:19,308] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy)
connect          | [2023-08-04 11:20:19,362] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
connect          | [2023-08-04 11:20:19,364] INFO AdminClientConfig values: 
connect          | 	bootstrap.servers = [broker:29092]
connect          | 	client.dns.lookup = use_all_dns_ips
connect          | 	client.id = 
connect          | 	connections.max.idle.ms = 300000
connect          | 	default.api.timeout.ms = 60000
connect          | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
connect          | 	metadata.max.age.ms = 300000
connect          | 	metric.reporters = []
connect          | 	metrics.num.samples = 2
connect          | 	metrics.recording.level = INFO
connect          | 	metrics.sample.window.ms = 30000
connect          | 	receive.buffer.bytes = 65536
connect          | 	reconnect.backoff.max.ms = 1000
connect          | 	reconnect.backoff.ms = 50
connect          | 	request.timeout.ms = 30000
connect          | 	retries = 2147483647
connect          | 	retry.backoff.ms = 100
connect          | 	sasl.client.callback.handler.class = null
connect          | 	sasl.jaas.config = null
connect          | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	sasl.kerberos.min.time.before.relogin = 60000
connect          | 	sasl.kerberos.service.name = null
connect          | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	sasl.login.callback.handler.class = null
connect          | 	sasl.login.class = null
connect          | 	sasl.login.connect.timeout.ms = null
connect          | 	sasl.login.read.timeout.ms = null
connect          | 	sasl.login.refresh.buffer.seconds = 300
connect          | 	sasl.login.refresh.min.period.seconds = 60
connect          | 	sasl.login.refresh.window.factor = 0.8
connect          | 	sasl.login.refresh.window.jitter = 0.05
connect          | 	sasl.login.retry.backoff.max.ms = 10000
connect          | 	sasl.login.retry.backoff.ms = 100
connect          | 	sasl.mechanism = GSSAPI
connect          | 	sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	sasl.oauthbearer.expected.audience = null
connect          | 	sasl.oauthbearer.expected.issuer = null
connect          | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	sasl.oauthbearer.scope.claim.name = scope
connect          | 	sasl.oauthbearer.sub.claim.name = sub
connect          | 	sasl.oauthbearer.token.endpoint.url = null
connect          | 	security.protocol = PLAINTEXT
connect          | 	security.providers = null
connect          | 	send.buffer.bytes = 131072
connect          | 	socket.connection.setup.timeout.max.ms = 30000
connect          | 	socket.connection.setup.timeout.ms = 10000
connect          | 	ssl.cipher.suites = null
connect          | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	ssl.endpoint.identification.algorithm = https
connect          | 	ssl.engine.factory.class = null
connect          | 	ssl.key.password = null
connect          | 	ssl.keymanager.algorithm = SunX509
connect          | 	ssl.keystore.certificate.chain = null
connect          | 	ssl.keystore.key = null
connect          | 	ssl.keystore.location = null
connect          | 	ssl.keystore.password = null
connect          | 	ssl.keystore.type = JKS
connect          | 	ssl.protocol = TLSv1.3
connect          | 	ssl.provider = null
connect          | 	ssl.secure.random.implementation = null
connect          | 	ssl.trustmanager.algorithm = PKIX
connect          | 	ssl.truststore.certificates = null
connect          | 	ssl.truststore.location = null
connect          | 	ssl.truststore.password = null
connect          | 	ssl.truststore.type = JKS
connect          |  (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,388] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,388] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,389] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,389] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,389] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,389] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,389] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,389] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,389] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,389] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,389] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,389] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,389] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,389] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,390] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,390] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,390] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,390] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:19,390] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:19,390] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:19,390] INFO Kafka startTimeMs: 1691148019390 (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:19,437] INFO Kafka cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.connect.util.ConnectUtils)
connect          | [2023-08-04 11:20:19,444] INFO App info kafka.admin.client for adminclient-3 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:19,459] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:19,459] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:19,460] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:19,476] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:19,476] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:19,476] INFO Kafka startTimeMs: 1691148019476 (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,273] INFO JsonConverterConfig values: 
connect          | 	converter.type = key
connect          | 	decimal.format = BASE64
connect          | 	schemas.cache.size = 1000
connect          | 	schemas.enable = false
connect          |  (org.apache.kafka.connect.json.JsonConverterConfig)
connect          | [2023-08-04 11:20:20,278] INFO JsonConverterConfig values: 
connect          | 	converter.type = value
connect          | 	decimal.format = BASE64
connect          | 	schemas.cache.size = 1000
connect          | 	schemas.enable = false
connect          |  (org.apache.kafka.connect.json.JsonConverterConfig)
connect          | [2023-08-04 11:20:20,278] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
connect          | [2023-08-04 11:20:20,280] INFO AdminClientConfig values: 
connect          | 	bootstrap.servers = [broker:29092]
connect          | 	client.dns.lookup = use_all_dns_ips
connect          | 	client.id = 
connect          | 	connections.max.idle.ms = 300000
connect          | 	default.api.timeout.ms = 60000
connect          | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
connect          | 	metadata.max.age.ms = 300000
connect          | 	metric.reporters = []
connect          | 	metrics.num.samples = 2
connect          | 	metrics.recording.level = INFO
connect          | 	metrics.sample.window.ms = 30000
connect          | 	receive.buffer.bytes = 65536
connect          | 	reconnect.backoff.max.ms = 1000
connect          | 	reconnect.backoff.ms = 50
connect          | 	request.timeout.ms = 30000
connect          | 	retries = 2147483647
connect          | 	retry.backoff.ms = 100
connect          | 	sasl.client.callback.handler.class = null
connect          | 	sasl.jaas.config = null
connect          | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	sasl.kerberos.min.time.before.relogin = 60000
connect          | 	sasl.kerberos.service.name = null
connect          | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	sasl.login.callback.handler.class = null
connect          | 	sasl.login.class = null
connect          | 	sasl.login.connect.timeout.ms = null
connect          | 	sasl.login.read.timeout.ms = null
connect          | 	sasl.login.refresh.buffer.seconds = 300
connect          | 	sasl.login.refresh.min.period.seconds = 60
connect          | 	sasl.login.refresh.window.factor = 0.8
connect          | 	sasl.login.refresh.window.jitter = 0.05
connect          | 	sasl.login.retry.backoff.max.ms = 10000
connect          | 	sasl.login.retry.backoff.ms = 100
connect          | 	sasl.mechanism = GSSAPI
connect          | 	sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	sasl.oauthbearer.expected.audience = null
connect          | 	sasl.oauthbearer.expected.issuer = null
connect          | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	sasl.oauthbearer.scope.claim.name = scope
connect          | 	sasl.oauthbearer.sub.claim.name = sub
connect          | 	sasl.oauthbearer.token.endpoint.url = null
connect          | 	security.protocol = PLAINTEXT
connect          | 	security.providers = null
connect          | 	send.buffer.bytes = 131072
connect          | 	socket.connection.setup.timeout.max.ms = 30000
connect          | 	socket.connection.setup.timeout.ms = 10000
connect          | 	ssl.cipher.suites = null
connect          | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	ssl.endpoint.identification.algorithm = https
connect          | 	ssl.engine.factory.class = null
connect          | 	ssl.key.password = null
connect          | 	ssl.keymanager.algorithm = SunX509
connect          | 	ssl.keystore.certificate.chain = null
connect          | 	ssl.keystore.key = null
connect          | 	ssl.keystore.location = null
connect          | 	ssl.keystore.password = null
connect          | 	ssl.keystore.type = JKS
connect          | 	ssl.protocol = TLSv1.3
connect          | 	ssl.provider = null
connect          | 	ssl.secure.random.implementation = null
connect          | 	ssl.trustmanager.algorithm = PKIX
connect          | 	ssl.truststore.certificates = null
connect          | 	ssl.truststore.location = null
connect          | 	ssl.truststore.password = null
connect          | 	ssl.truststore.type = JKS
connect          |  (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,300] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,300] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,300] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,300] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,300] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,301] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,301] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,301] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,301] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,301] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,301] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,301] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,301] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,302] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,302] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,302] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,302] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,302] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,302] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,303] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,303] INFO Kafka startTimeMs: 1691148020302 (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,349] INFO Kafka cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.connect.util.ConnectUtils)
connect          | [2023-08-04 11:20:20,352] INFO App info kafka.admin.client for adminclient-4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,363] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:20,365] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:20,366] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:20,415] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
connect          | [2023-08-04 11:20:20,417] INFO AdminClientConfig values: 
connect          | 	bootstrap.servers = [broker:29092]
connect          | 	client.dns.lookup = use_all_dns_ips
connect          | 	client.id = 
connect          | 	connections.max.idle.ms = 300000
connect          | 	default.api.timeout.ms = 60000
connect          | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
connect          | 	metadata.max.age.ms = 300000
connect          | 	metric.reporters = []
connect          | 	metrics.num.samples = 2
connect          | 	metrics.recording.level = INFO
connect          | 	metrics.sample.window.ms = 30000
connect          | 	receive.buffer.bytes = 65536
connect          | 	reconnect.backoff.max.ms = 1000
connect          | 	reconnect.backoff.ms = 50
connect          | 	request.timeout.ms = 30000
connect          | 	retries = 2147483647
connect          | 	retry.backoff.ms = 100
connect          | 	sasl.client.callback.handler.class = null
connect          | 	sasl.jaas.config = null
connect          | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	sasl.kerberos.min.time.before.relogin = 60000
connect          | 	sasl.kerberos.service.name = null
connect          | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	sasl.login.callback.handler.class = null
connect          | 	sasl.login.class = null
connect          | 	sasl.login.connect.timeout.ms = null
connect          | 	sasl.login.read.timeout.ms = null
connect          | 	sasl.login.refresh.buffer.seconds = 300
connect          | 	sasl.login.refresh.min.period.seconds = 60
connect          | 	sasl.login.refresh.window.factor = 0.8
connect          | 	sasl.login.refresh.window.jitter = 0.05
connect          | 	sasl.login.retry.backoff.max.ms = 10000
connect          | 	sasl.login.retry.backoff.ms = 100
connect          | 	sasl.mechanism = GSSAPI
connect          | 	sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	sasl.oauthbearer.expected.audience = null
connect          | 	sasl.oauthbearer.expected.issuer = null
connect          | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	sasl.oauthbearer.scope.claim.name = scope
connect          | 	sasl.oauthbearer.sub.claim.name = sub
connect          | 	sasl.oauthbearer.token.endpoint.url = null
connect          | 	security.protocol = PLAINTEXT
connect          | 	security.providers = null
connect          | 	send.buffer.bytes = 131072
connect          | 	socket.connection.setup.timeout.max.ms = 30000
connect          | 	socket.connection.setup.timeout.ms = 10000
connect          | 	ssl.cipher.suites = null
connect          | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	ssl.endpoint.identification.algorithm = https
connect          | 	ssl.engine.factory.class = null
connect          | 	ssl.key.password = null
connect          | 	ssl.keymanager.algorithm = SunX509
connect          | 	ssl.keystore.certificate.chain = null
connect          | 	ssl.keystore.key = null
connect          | 	ssl.keystore.location = null
connect          | 	ssl.keystore.password = null
connect          | 	ssl.keystore.type = JKS
connect          | 	ssl.protocol = TLSv1.3
connect          | 	ssl.provider = null
connect          | 	ssl.secure.random.implementation = null
connect          | 	ssl.trustmanager.algorithm = PKIX
connect          | 	ssl.truststore.certificates = null
connect          | 	ssl.truststore.location = null
connect          | 	ssl.truststore.password = null
connect          | 	ssl.truststore.type = JKS
connect          |  (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,439] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,440] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,440] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,441] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,441] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,442] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,443] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,443] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,444] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,445] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,445] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,446] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,447] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,448] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,449] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,450] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,452] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,453] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,454] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,454] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,455] INFO Kafka startTimeMs: 1691148020454 (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,493] INFO Kafka cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.connect.util.ConnectUtils)
connect          | [2023-08-04 11:20:20,495] INFO App info kafka.admin.client for adminclient-5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,504] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:20,504] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:20,504] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:20,519] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
connect          | [2023-08-04 11:20:20,520] INFO AdminClientConfig values: 
connect          | 	bootstrap.servers = [broker:29092]
connect          | 	client.dns.lookup = use_all_dns_ips
connect          | 	client.id = 
connect          | 	connections.max.idle.ms = 300000
connect          | 	default.api.timeout.ms = 60000
connect          | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
connect          | 	metadata.max.age.ms = 300000
connect          | 	metric.reporters = []
connect          | 	metrics.num.samples = 2
connect          | 	metrics.recording.level = INFO
connect          | 	metrics.sample.window.ms = 30000
connect          | 	receive.buffer.bytes = 65536
connect          | 	reconnect.backoff.max.ms = 1000
connect          | 	reconnect.backoff.ms = 50
connect          | 	request.timeout.ms = 30000
connect          | 	retries = 2147483647
connect          | 	retry.backoff.ms = 100
connect          | 	sasl.client.callback.handler.class = null
connect          | 	sasl.jaas.config = null
connect          | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	sasl.kerberos.min.time.before.relogin = 60000
connect          | 	sasl.kerberos.service.name = null
connect          | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	sasl.login.callback.handler.class = null
connect          | 	sasl.login.class = null
connect          | 	sasl.login.connect.timeout.ms = null
connect          | 	sasl.login.read.timeout.ms = null
connect          | 	sasl.login.refresh.buffer.seconds = 300
connect          | 	sasl.login.refresh.min.period.seconds = 60
connect          | 	sasl.login.refresh.window.factor = 0.8
connect          | 	sasl.login.refresh.window.jitter = 0.05
connect          | 	sasl.login.retry.backoff.max.ms = 10000
connect          | 	sasl.login.retry.backoff.ms = 100
connect          | 	sasl.mechanism = GSSAPI
connect          | 	sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	sasl.oauthbearer.expected.audience = null
connect          | 	sasl.oauthbearer.expected.issuer = null
connect          | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	sasl.oauthbearer.scope.claim.name = scope
connect          | 	sasl.oauthbearer.sub.claim.name = sub
connect          | 	sasl.oauthbearer.token.endpoint.url = null
connect          | 	security.protocol = PLAINTEXT
connect          | 	security.providers = null
connect          | 	send.buffer.bytes = 131072
connect          | 	socket.connection.setup.timeout.max.ms = 30000
connect          | 	socket.connection.setup.timeout.ms = 10000
connect          | 	ssl.cipher.suites = null
connect          | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	ssl.endpoint.identification.algorithm = https
connect          | 	ssl.engine.factory.class = null
connect          | 	ssl.key.password = null
connect          | 	ssl.keymanager.algorithm = SunX509
connect          | 	ssl.keystore.certificate.chain = null
connect          | 	ssl.keystore.key = null
connect          | 	ssl.keystore.location = null
connect          | 	ssl.keystore.password = null
connect          | 	ssl.keystore.type = JKS
connect          | 	ssl.protocol = TLSv1.3
connect          | 	ssl.provider = null
connect          | 	ssl.secure.random.implementation = null
connect          | 	ssl.trustmanager.algorithm = PKIX
connect          | 	ssl.truststore.certificates = null
connect          | 	ssl.truststore.location = null
connect          | 	ssl.truststore.password = null
connect          | 	ssl.truststore.type = JKS
connect          |  (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,561] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,561] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,561] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,562] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,562] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,562] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,562] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,563] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,563] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,563] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,563] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,563] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,563] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,563] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,563] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,564] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,564] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,564] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,564] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,564] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,564] INFO Kafka startTimeMs: 1691148020564 (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,592] INFO Kafka cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.connect.util.ConnectUtils)
connect          | [2023-08-04 11:20:20,594] INFO App info kafka.admin.client for adminclient-6 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,601] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:20,601] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:20,602] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:20,663] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
connect          | [2023-08-04 11:20:20,664] INFO AdminClientConfig values: 
connect          | 	bootstrap.servers = [broker:29092]
connect          | 	client.dns.lookup = use_all_dns_ips
connect          | 	client.id = 
connect          | 	connections.max.idle.ms = 300000
connect          | 	default.api.timeout.ms = 60000
connect          | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
connect          | 	metadata.max.age.ms = 300000
connect          | 	metric.reporters = []
connect          | 	metrics.num.samples = 2
connect          | 	metrics.recording.level = INFO
connect          | 	metrics.sample.window.ms = 30000
connect          | 	receive.buffer.bytes = 65536
connect          | 	reconnect.backoff.max.ms = 1000
connect          | 	reconnect.backoff.ms = 50
connect          | 	request.timeout.ms = 30000
connect          | 	retries = 2147483647
connect          | 	retry.backoff.ms = 100
connect          | 	sasl.client.callback.handler.class = null
connect          | 	sasl.jaas.config = null
connect          | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	sasl.kerberos.min.time.before.relogin = 60000
connect          | 	sasl.kerberos.service.name = null
connect          | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	sasl.login.callback.handler.class = null
connect          | 	sasl.login.class = null
connect          | 	sasl.login.connect.timeout.ms = null
connect          | 	sasl.login.read.timeout.ms = null
connect          | 	sasl.login.refresh.buffer.seconds = 300
connect          | 	sasl.login.refresh.min.period.seconds = 60
connect          | 	sasl.login.refresh.window.factor = 0.8
connect          | 	sasl.login.refresh.window.jitter = 0.05
connect          | 	sasl.login.retry.backoff.max.ms = 10000
connect          | 	sasl.login.retry.backoff.ms = 100
connect          | 	sasl.mechanism = GSSAPI
connect          | 	sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	sasl.oauthbearer.expected.audience = null
connect          | 	sasl.oauthbearer.expected.issuer = null
connect          | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	sasl.oauthbearer.scope.claim.name = scope
connect          | 	sasl.oauthbearer.sub.claim.name = sub
connect          | 	sasl.oauthbearer.token.endpoint.url = null
connect          | 	security.protocol = PLAINTEXT
connect          | 	security.providers = null
connect          | 	send.buffer.bytes = 131072
connect          | 	socket.connection.setup.timeout.max.ms = 30000
connect          | 	socket.connection.setup.timeout.ms = 10000
connect          | 	ssl.cipher.suites = null
connect          | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	ssl.endpoint.identification.algorithm = https
connect          | 	ssl.engine.factory.class = null
connect          | 	ssl.key.password = null
connect          | 	ssl.keymanager.algorithm = SunX509
connect          | 	ssl.keystore.certificate.chain = null
connect          | 	ssl.keystore.key = null
connect          | 	ssl.keystore.location = null
connect          | 	ssl.keystore.password = null
connect          | 	ssl.keystore.type = JKS
connect          | 	ssl.protocol = TLSv1.3
connect          | 	ssl.provider = null
connect          | 	ssl.secure.random.implementation = null
connect          | 	ssl.trustmanager.algorithm = PKIX
connect          | 	ssl.truststore.certificates = null
connect          | 	ssl.truststore.location = null
connect          | 	ssl.truststore.password = null
connect          | 	ssl.truststore.type = JKS
connect          |  (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,678] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,678] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,678] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,678] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,678] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,679] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,679] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,679] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,679] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,679] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,679] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,679] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,679] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,679] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,680] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,680] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,680] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,680] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,680] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,680] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,680] INFO Kafka startTimeMs: 1691148020680 (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,711] INFO Kafka cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.connect.util.ConnectUtils)
connect          | [2023-08-04 11:20:20,712] INFO App info kafka.admin.client for adminclient-7 unregistered (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,720] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:20,720] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:20,720] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
connect          | [2023-08-04 11:20:20,827] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,827] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,827] INFO Kafka startTimeMs: 1691148020826 (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,846] INFO Kafka Connect distributed worker initialization took 335302ms (org.apache.kafka.connect.cli.ConnectDistributed)
connect          | [2023-08-04 11:20:20,846] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect)
connect          | [2023-08-04 11:20:20,856] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer)
connect          | [2023-08-04 11:20:20,857] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
connect          | [2023-08-04 11:20:20,858] INFO Worker starting (org.apache.kafka.connect.runtime.Worker)
connect          | [2023-08-04 11:20:20,858] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore)
connect          | [2023-08-04 11:20:20,858] INFO Starting KafkaBasedLog with topic docker-connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
connect          | [2023-08-04 11:20:20,861] INFO AdminClientConfig values: 
connect          | 	bootstrap.servers = [broker:29092]
connect          | 	client.dns.lookup = use_all_dns_ips
connect          | 	client.id = 
connect          | 	connections.max.idle.ms = 300000
connect          | 	default.api.timeout.ms = 60000
connect          | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
connect          | 	metadata.max.age.ms = 300000
connect          | 	metric.reporters = []
connect          | 	metrics.num.samples = 2
connect          | 	metrics.recording.level = INFO
connect          | 	metrics.sample.window.ms = 30000
connect          | 	receive.buffer.bytes = 65536
connect          | 	reconnect.backoff.max.ms = 1000
connect          | 	reconnect.backoff.ms = 50
connect          | 	request.timeout.ms = 30000
connect          | 	retries = 2147483647
connect          | 	retry.backoff.ms = 100
connect          | 	sasl.client.callback.handler.class = null
connect          | 	sasl.jaas.config = null
connect          | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	sasl.kerberos.min.time.before.relogin = 60000
connect          | 	sasl.kerberos.service.name = null
connect          | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	sasl.login.callback.handler.class = null
connect          | 	sasl.login.class = null
connect          | 	sasl.login.connect.timeout.ms = null
connect          | 	sasl.login.read.timeout.ms = null
connect          | 	sasl.login.refresh.buffer.seconds = 300
connect          | 	sasl.login.refresh.min.period.seconds = 60
connect          | 	sasl.login.refresh.window.factor = 0.8
connect          | 	sasl.login.refresh.window.jitter = 0.05
connect          | 	sasl.login.retry.backoff.max.ms = 10000
connect          | 	sasl.login.retry.backoff.ms = 100
connect          | 	sasl.mechanism = GSSAPI
connect          | 	sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	sasl.oauthbearer.expected.audience = null
connect          | 	sasl.oauthbearer.expected.issuer = null
connect          | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	sasl.oauthbearer.scope.claim.name = scope
connect          | 	sasl.oauthbearer.sub.claim.name = sub
connect          | 	sasl.oauthbearer.token.endpoint.url = null
connect          | 	security.protocol = PLAINTEXT
connect          | 	security.providers = null
connect          | 	send.buffer.bytes = 131072
connect          | 	socket.connection.setup.timeout.max.ms = 30000
connect          | 	socket.connection.setup.timeout.ms = 10000
connect          | 	ssl.cipher.suites = null
connect          | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	ssl.endpoint.identification.algorithm = https
connect          | 	ssl.engine.factory.class = null
connect          | 	ssl.key.password = null
connect          | 	ssl.keymanager.algorithm = SunX509
connect          | 	ssl.keystore.certificate.chain = null
connect          | 	ssl.keystore.key = null
connect          | 	ssl.keystore.location = null
connect          | 	ssl.keystore.password = null
connect          | 	ssl.keystore.type = JKS
connect          | 	ssl.protocol = TLSv1.3
connect          | 	ssl.provider = null
connect          | 	ssl.secure.random.implementation = null
connect          | 	ssl.trustmanager.algorithm = PKIX
connect          | 	ssl.truststore.certificates = null
connect          | 	ssl.truststore.location = null
connect          | 	ssl.truststore.password = null
connect          | 	ssl.truststore.type = JKS
connect          |  (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,877] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,877] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,877] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,878] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,878] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,878] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,878] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,879] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,879] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,879] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,879] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,880] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,880] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,880] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,881] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,881] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,882] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,882] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,882] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,883] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
connect          | [2023-08-04 11:20:20,884] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,884] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:20,885] INFO Kafka startTimeMs: 1691148020883 (org.apache.kafka.common.utils.AppInfoParser)
broker           | [2023-08-04 11:20:20,959] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='docker-connect-offsets', numPartitions=25, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,960] INFO [Controller 1] Created topic docker-connect-offsets with topic ID wQ6UKZG2TGmde1svteq6Gg. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,960] INFO [Controller 1] ConfigResource(type=TOPIC, name='docker-connect-offsets'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:20:20,960] INFO [Controller 1] Created partition docker-connect-offsets-0 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,960] INFO [Controller 1] Created partition docker-connect-offsets-1 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,961] INFO [Controller 1] Created partition docker-connect-offsets-2 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,961] INFO [Controller 1] Created partition docker-connect-offsets-3 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,961] INFO [Controller 1] Created partition docker-connect-offsets-4 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,969] INFO [Controller 1] Created partition docker-connect-offsets-5 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,970] INFO [Controller 1] Created partition docker-connect-offsets-6 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,970] INFO [Controller 1] Created partition docker-connect-offsets-7 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,970] INFO [Controller 1] Created partition docker-connect-offsets-8 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,971] INFO [Controller 1] Created partition docker-connect-offsets-9 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,971] INFO [Controller 1] Created partition docker-connect-offsets-10 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,971] INFO [Controller 1] Created partition docker-connect-offsets-11 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,971] INFO [Controller 1] Created partition docker-connect-offsets-12 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,972] INFO [Controller 1] Created partition docker-connect-offsets-13 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,972] INFO [Controller 1] Created partition docker-connect-offsets-14 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,972] INFO [Controller 1] Created partition docker-connect-offsets-15 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,972] INFO [Controller 1] Created partition docker-connect-offsets-16 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,973] INFO [Controller 1] Created partition docker-connect-offsets-17 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,973] INFO [Controller 1] Created partition docker-connect-offsets-18 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,973] INFO [Controller 1] Created partition docker-connect-offsets-19 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,973] INFO [Controller 1] Created partition docker-connect-offsets-20 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,974] INFO [Controller 1] Created partition docker-connect-offsets-21 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,974] INFO [Controller 1] Created partition docker-connect-offsets-22 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,974] INFO [Controller 1] Created partition docker-connect-offsets-23 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:20,975] INFO [Controller 1] Created partition docker-connect-offsets-24 with topic ID wQ6UKZG2TGmde1svteq6Gg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:21,005] INFO [Broker id=1] Transitioning 25 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:20:21,005] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(docker-connect-offsets-11, docker-connect-offsets-13, docker-connect-offsets-15, docker-connect-offsets-17, docker-connect-offsets-3, docker-connect-offsets-5, docker-connect-offsets-7, docker-connect-offsets-9, docker-connect-offsets-1, docker-connect-offsets-18, docker-connect-offsets-20, docker-connect-offsets-22, docker-connect-offsets-24, docker-connect-offsets-10, docker-connect-offsets-12, docker-connect-offsets-14, docker-connect-offsets-16, docker-connect-offsets-2, docker-connect-offsets-4, docker-connect-offsets-6, docker-connect-offsets-8, docker-connect-offsets-0, docker-connect-offsets-19, docker-connect-offsets-21, docker-connect-offsets-23) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:20:21,006] INFO [Broker id=1] Creating new partition docker-connect-offsets-11 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,023] INFO [LogLoader partition=docker-connect-offsets-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,037] INFO Created log for partition docker-connect-offsets-11 in /tmp/kraft-combined-logs/docker-connect-offsets-11 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,057] INFO [Partition docker-connect-offsets-11 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-11 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,062] INFO [Partition docker-connect-offsets-11 broker=1] Log loaded for partition docker-connect-offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,063] INFO [Broker id=1] Leader docker-connect-offsets-11 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect          | [2023-08-04 11:20:21,076] INFO Created topic (name=docker-connect-offsets, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at broker:29092 (org.apache.kafka.connect.util.TopicAdmin)
broker           | [2023-08-04 11:20:21,077] INFO [Broker id=1] Creating new partition docker-connect-offsets-13 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
connect          | [2023-08-04 11:20:21,125] INFO ProducerConfig values: 
connect          | 	acks = -1
connect          | 	batch.size = 16384
connect          | 	bootstrap.servers = [broker:29092]
connect          | 	buffer.memory = 33554432
connect          | 	client.dns.lookup = use_all_dns_ips
connect          | 	client.id = producer-1
connect          | 	compression.type = none
connect          | 	connections.max.idle.ms = 540000
connect          | 	delivery.timeout.ms = 2147483647
connect          | 	enable.idempotence = false
connect          | 	interceptor.classes = []
connect          | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
connect          | 	linger.ms = 0
connect          | 	max.block.ms = 60000
connect          | 	max.in.flight.requests.per.connection = 1
connect          | 	max.request.size = 1048576
connect          | 	metadata.max.age.ms = 300000
connect          | 	metadata.max.idle.ms = 300000
connect          | 	metric.reporters = []
connect          | 	metrics.num.samples = 2
connect          | 	metrics.recording.level = INFO
connect          | 	metrics.sample.window.ms = 30000
connect          | 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
connect          | 	receive.buffer.bytes = 32768
connect          | 	reconnect.backoff.max.ms = 1000
connect          | 	reconnect.backoff.ms = 50
connect          | 	request.timeout.ms = 30000
connect          | 	retries = 2147483647
connect          | 	retry.backoff.ms = 100
connect          | 	sasl.client.callback.handler.class = null
connect          | 	sasl.jaas.config = null
connect          | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	sasl.kerberos.min.time.before.relogin = 60000
connect          | 	sasl.kerberos.service.name = null
connect          | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	sasl.login.callback.handler.class = null
connect          | 	sasl.login.class = null
connect          | 	sasl.login.connect.timeout.ms = null
connect          | 	sasl.login.read.timeout.ms = null
connect          | 	sasl.login.refresh.buffer.seconds = 300
connect          | 	sasl.login.refresh.min.period.seconds = 60
connect          | 	sasl.login.refresh.window.factor = 0.8
connect          | 	sasl.login.refresh.window.jitter = 0.05
connect          | 	sasl.login.retry.backoff.max.ms = 10000
connect          | 	sasl.login.retry.backoff.ms = 100
connect          | 	sasl.mechanism = GSSAPI
connect          | 	sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	sasl.oauthbearer.expected.audience = null
connect          | 	sasl.oauthbearer.expected.issuer = null
connect          | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	sasl.oauthbearer.scope.claim.name = scope
connect          | 	sasl.oauthbearer.sub.claim.name = sub
connect          | 	sasl.oauthbearer.token.endpoint.url = null
connect          | 	security.protocol = PLAINTEXT
connect          | 	security.providers = null
connect          | 	send.buffer.bytes = 131072
connect          | 	socket.connection.setup.timeout.max.ms = 30000
connect          | 	socket.connection.setup.timeout.ms = 10000
connect          | 	ssl.cipher.suites = null
connect          | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	ssl.endpoint.identification.algorithm = https
connect          | 	ssl.engine.factory.class = null
connect          | 	ssl.key.password = null
connect          | 	ssl.keymanager.algorithm = SunX509
connect          | 	ssl.keystore.certificate.chain = null
connect          | 	ssl.keystore.key = null
connect          | 	ssl.keystore.location = null
connect          | 	ssl.keystore.password = null
connect          | 	ssl.keystore.type = JKS
connect          | 	ssl.protocol = TLSv1.3
connect          | 	ssl.provider = null
connect          | 	ssl.secure.random.implementation = null
connect          | 	ssl.trustmanager.algorithm = PKIX
connect          | 	ssl.truststore.certificates = null
connect          | 	ssl.truststore.location = null
connect          | 	ssl.truststore.password = null
connect          | 	ssl.truststore.type = JKS
connect          | 	transaction.timeout.ms = 60000
connect          | 	transactional.id = null
connect          | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
connect          |  (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,141] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer)
broker           | [2023-08-04 11:20:21,186] INFO [LogLoader partition=docker-connect-offsets-13, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,203] INFO Created log for partition docker-connect-offsets-13 in /tmp/kraft-combined-logs/docker-connect-offsets-13 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,204] INFO [Partition docker-connect-offsets-13 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-13 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,204] INFO [Partition docker-connect-offsets-13 broker=1] Log loaded for partition docker-connect-offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,205] INFO [Broker id=1] Leader docker-connect-offsets-13 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:21,219] INFO [Broker id=1] Creating new partition docker-connect-offsets-15 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,227] INFO [LogLoader partition=docker-connect-offsets-15, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,229] INFO Created log for partition docker-connect-offsets-15 in /tmp/kraft-combined-logs/docker-connect-offsets-15 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,229] INFO [Partition docker-connect-offsets-15 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-15 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,230] INFO [Partition docker-connect-offsets-15 broker=1] Log loaded for partition docker-connect-offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,230] INFO [Broker id=1] Leader docker-connect-offsets-15 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:21,245] INFO [Broker id=1] Creating new partition docker-connect-offsets-17 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,259] INFO [LogLoader partition=docker-connect-offsets-17, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,265] INFO Created log for partition docker-connect-offsets-17 in /tmp/kraft-combined-logs/docker-connect-offsets-17 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,266] INFO [Partition docker-connect-offsets-17 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-17 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,266] INFO [Partition docker-connect-offsets-17 broker=1] Log loaded for partition docker-connect-offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,267] INFO [Broker id=1] Leader docker-connect-offsets-17 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:21,274] INFO [Broker id=1] Creating new partition docker-connect-offsets-3 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,285] INFO [LogLoader partition=docker-connect-offsets-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,288] INFO Created log for partition docker-connect-offsets-3 in /tmp/kraft-combined-logs/docker-connect-offsets-3 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,289] INFO [Partition docker-connect-offsets-3 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-3 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,290] INFO [Partition docker-connect-offsets-3 broker=1] Log loaded for partition docker-connect-offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,290] INFO [Broker id=1] Leader docker-connect-offsets-3 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:21,299] INFO [Broker id=1] Creating new partition docker-connect-offsets-5 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,309] INFO [LogLoader partition=docker-connect-offsets-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,312] INFO Created log for partition docker-connect-offsets-5 in /tmp/kraft-combined-logs/docker-connect-offsets-5 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,312] INFO [Partition docker-connect-offsets-5 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-5 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,312] INFO [Partition docker-connect-offsets-5 broker=1] Log loaded for partition docker-connect-offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,312] INFO [Broker id=1] Leader docker-connect-offsets-5 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:21,326] INFO [Broker id=1] Creating new partition docker-connect-offsets-7 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
connect          | [2023-08-04 11:20:21,325] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,327] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,327] WARN The configuration 'metrics.context.resource.version' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,327] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,329] WARN The configuration 'metrics.context.resource.type' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,329] WARN The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,329] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,329] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,329] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,329] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,329] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,329] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,329] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,329] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,329] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,329] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,330] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,330] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,330] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,336] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,337] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,337] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
broker           | [2023-08-04 11:20:21,337] INFO [LogLoader partition=docker-connect-offsets-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,340] INFO Created log for partition docker-connect-offsets-7 in /tmp/kraft-combined-logs/docker-connect-offsets-7 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,341] INFO [Partition docker-connect-offsets-7 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-7 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,342] INFO [Partition docker-connect-offsets-7 broker=1] Log loaded for partition docker-connect-offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,343] INFO [Broker id=1] Leader docker-connect-offsets-7 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect          | [2023-08-04 11:20:21,343] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:21,344] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:21,344] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:21,345] INFO Kafka startTimeMs: 1691148021343 (org.apache.kafka.common.utils.AppInfoParser)
broker           | [2023-08-04 11:20:21,358] INFO [Broker id=1] Creating new partition docker-connect-offsets-9 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,369] INFO [LogLoader partition=docker-connect-offsets-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,375] INFO Created log for partition docker-connect-offsets-9 in /tmp/kraft-combined-logs/docker-connect-offsets-9 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,375] INFO [Partition docker-connect-offsets-9 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-9 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,375] INFO [Partition docker-connect-offsets-9 broker=1] Log loaded for partition docker-connect-offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,376] INFO [Broker id=1] Leader docker-connect-offsets-9 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:21,384] INFO [Broker id=1] Creating new partition docker-connect-offsets-1 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
connect          | [2023-08-04 11:20:21,394] INFO ConsumerConfig values: 
connect          | 	allow.auto.create.topics = true
connect          | 	auto.commit.interval.ms = 5000
connect          | 	auto.offset.reset = earliest
connect          | 	bootstrap.servers = [broker:29092]
connect          | 	check.crcs = true
connect          | 	client.dns.lookup = use_all_dns_ips
connect          | 	client.id = consumer-compose-connect-group-1
connect          | 	client.rack = 
connect          | 	connections.max.idle.ms = 540000
connect          | 	default.api.timeout.ms = 60000
connect          | 	enable.auto.commit = false
connect          | 	exclude.internal.topics = true
connect          | 	fetch.max.bytes = 52428800
connect          | 	fetch.max.wait.ms = 500
connect          | 	fetch.min.bytes = 1
connect          | 	group.id = compose-connect-group
connect          | 	group.instance.id = null
connect          | 	heartbeat.interval.ms = 3000
connect          | 	interceptor.classes = []
connect          | 	internal.leave.group.on.close = true
connect          | 	internal.throw.on.fetch.stable.offset.unsupported = false
connect          | 	isolation.level = read_uncommitted
connect          | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
connect          | 	max.partition.fetch.bytes = 1048576
connect          | 	max.poll.interval.ms = 300000
connect          | 	max.poll.records = 500
connect          | 	metadata.max.age.ms = 300000
connect          | 	metric.reporters = []
connect          | 	metrics.num.samples = 2
connect          | 	metrics.recording.level = INFO
connect          | 	metrics.sample.window.ms = 30000
connect          | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
connect          | 	receive.buffer.bytes = 65536
connect          | 	reconnect.backoff.max.ms = 1000
connect          | 	reconnect.backoff.ms = 50
connect          | 	request.timeout.ms = 30000
connect          | 	retry.backoff.ms = 100
connect          | 	sasl.client.callback.handler.class = null
connect          | 	sasl.jaas.config = null
connect          | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	sasl.kerberos.min.time.before.relogin = 60000
connect          | 	sasl.kerberos.service.name = null
connect          | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	sasl.login.callback.handler.class = null
connect          | 	sasl.login.class = null
connect          | 	sasl.login.connect.timeout.ms = null
connect          | 	sasl.login.read.timeout.ms = null
connect          | 	sasl.login.refresh.buffer.seconds = 300
connect          | 	sasl.login.refresh.min.period.seconds = 60
connect          | 	sasl.login.refresh.window.factor = 0.8
connect          | 	sasl.login.refresh.window.jitter = 0.05
connect          | 	sasl.login.retry.backoff.max.ms = 10000
connect          | 	sasl.login.retry.backoff.ms = 100
connect          | 	sasl.mechanism = GSSAPI
connect          | 	sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	sasl.oauthbearer.expected.audience = null
connect          | 	sasl.oauthbearer.expected.issuer = null
connect          | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	sasl.oauthbearer.scope.claim.name = scope
connect          | 	sasl.oauthbearer.sub.claim.name = sub
connect          | 	sasl.oauthbearer.token.endpoint.url = null
connect          | 	security.protocol = PLAINTEXT
connect          | 	security.providers = null
connect          | 	send.buffer.bytes = 131072
connect          | 	session.timeout.ms = 45000
connect          | 	socket.connection.setup.timeout.max.ms = 30000
connect          | 	socket.connection.setup.timeout.ms = 10000
connect          | 	ssl.cipher.suites = null
connect          | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	ssl.endpoint.identification.algorithm = https
connect          | 	ssl.engine.factory.class = null
connect          | 	ssl.key.password = null
connect          | 	ssl.keymanager.algorithm = SunX509
connect          | 	ssl.keystore.certificate.chain = null
connect          | 	ssl.keystore.key = null
connect          | 	ssl.keystore.location = null
connect          | 	ssl.keystore.password = null
connect          | 	ssl.keystore.type = JKS
connect          | 	ssl.protocol = TLSv1.3
connect          | 	ssl.provider = null
connect          | 	ssl.secure.random.implementation = null
connect          | 	ssl.trustmanager.algorithm = PKIX
connect          | 	ssl.truststore.certificates = null
connect          | 	ssl.truststore.location = null
connect          | 	ssl.truststore.password = null
connect          | 	ssl.truststore.type = JKS
connect          | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
connect          |  (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,396] INFO [Producer clientId=producer-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
broker           | [2023-08-04 11:20:21,402] INFO [LogLoader partition=docker-connect-offsets-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,405] INFO Created log for partition docker-connect-offsets-1 in /tmp/kraft-combined-logs/docker-connect-offsets-1 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,406] INFO [Partition docker-connect-offsets-1 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-1 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,407] INFO [Partition docker-connect-offsets-1 broker=1] Log loaded for partition docker-connect-offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,408] INFO [Broker id=1] Leader docker-connect-offsets-1 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:21,430] INFO [Broker id=1] Creating new partition docker-connect-offsets-18 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,440] INFO [LogLoader partition=docker-connect-offsets-18, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,446] INFO Created log for partition docker-connect-offsets-18 in /tmp/kraft-combined-logs/docker-connect-offsets-18 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,446] INFO [Partition docker-connect-offsets-18 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-18 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,446] INFO [Partition docker-connect-offsets-18 broker=1] Log loaded for partition docker-connect-offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,457] INFO [Broker id=1] Leader docker-connect-offsets-18 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:21,474] INFO [Broker id=1] Creating new partition docker-connect-offsets-20 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,493] INFO [LogLoader partition=docker-connect-offsets-20, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,496] INFO Created log for partition docker-connect-offsets-20 in /tmp/kraft-combined-logs/docker-connect-offsets-20 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,496] INFO [Partition docker-connect-offsets-20 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-20 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,496] INFO [Partition docker-connect-offsets-20 broker=1] Log loaded for partition docker-connect-offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,508] INFO [Broker id=1] Leader docker-connect-offsets-20 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:21,543] INFO [Broker id=1] Creating new partition docker-connect-offsets-22 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,593] INFO [LogLoader partition=docker-connect-offsets-22, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
connect          | [2023-08-04 11:20:21,587] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,602] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,602] WARN The configuration 'metrics.context.resource.version' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,603] WARN The configuration 'metrics.context.resource.type' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,603] WARN The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,603] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,604] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,604] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,604] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,605] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,605] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,605] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,605] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,605] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,605] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,606] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,606] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,606] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,606] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,607] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,607] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,607] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:21,608] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:21,608] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:21,608] INFO Kafka startTimeMs: 1691148021607 (org.apache.kafka.common.utils.AppInfoParser)
broker           | [2023-08-04 11:20:21,609] INFO Created log for partition docker-connect-offsets-22 in /tmp/kraft-combined-logs/docker-connect-offsets-22 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,610] INFO [Partition docker-connect-offsets-22 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-22 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,610] INFO [Partition docker-connect-offsets-22 broker=1] Log loaded for partition docker-connect-offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,610] INFO [Broker id=1] Leader docker-connect-offsets-22 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:21,624] INFO [Broker id=1] Creating new partition docker-connect-offsets-24 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,657] INFO [LogLoader partition=docker-connect-offsets-24, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,661] INFO Created log for partition docker-connect-offsets-24 in /tmp/kraft-combined-logs/docker-connect-offsets-24 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,661] INFO [Partition docker-connect-offsets-24 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-24 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,661] INFO [Partition docker-connect-offsets-24 broker=1] Log loaded for partition docker-connect-offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,662] INFO [Broker id=1] Leader docker-connect-offsets-24 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect          | [2023-08-04 11:20:21,665] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
broker           | [2023-08-04 11:20:21,677] INFO [Broker id=1] Creating new partition docker-connect-offsets-10 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,688] INFO [LogLoader partition=docker-connect-offsets-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,700] INFO Created log for partition docker-connect-offsets-10 in /tmp/kraft-combined-logs/docker-connect-offsets-10 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,707] INFO [Partition docker-connect-offsets-10 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-10 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,708] INFO [Partition docker-connect-offsets-10 broker=1] Log loaded for partition docker-connect-offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,709] INFO [Broker id=1] Leader docker-connect-offsets-10 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect          | [2023-08-04 11:20:21,711] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Subscribed to partition(s): docker-connect-offsets-17, docker-connect-offsets-20, docker-connect-offsets-11, docker-connect-offsets-23, docker-connect-offsets-14, docker-connect-offsets-5, docker-connect-offsets-0, docker-connect-offsets-8, docker-connect-offsets-7, docker-connect-offsets-4, docker-connect-offsets-1, docker-connect-offsets-10, docker-connect-offsets-13, docker-connect-offsets-24, docker-connect-offsets-21, docker-connect-offsets-16, docker-connect-offsets-3, docker-connect-offsets-9, docker-connect-offsets-15, docker-connect-offsets-18, docker-connect-offsets-19, docker-connect-offsets-22, docker-connect-offsets-6, docker-connect-offsets-2, docker-connect-offsets-12 (org.apache.kafka.clients.consumer.KafkaConsumer)
broker           | [2023-08-04 11:20:21,717] INFO [Broker id=1] Creating new partition docker-connect-offsets-12 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,726] INFO [LogLoader partition=docker-connect-offsets-12, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,729] INFO Created log for partition docker-connect-offsets-12 in /tmp/kraft-combined-logs/docker-connect-offsets-12 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,730] INFO [Partition docker-connect-offsets-12 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-12 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,731] INFO [Partition docker-connect-offsets-12 broker=1] Log loaded for partition docker-connect-offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,734] INFO [Broker id=1] Leader docker-connect-offsets-12 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:21,748] INFO [Broker id=1] Creating new partition docker-connect-offsets-14 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
connect          | [2023-08-04 11:20:21,765] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,775] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,776] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,776] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,777] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,783] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,785] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,786] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,787] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,788] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,789] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,790] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,791] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,792] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
broker           | [2023-08-04 11:20:21,792] INFO [LogLoader partition=docker-connect-offsets-14, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
connect          | [2023-08-04 11:20:21,794] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
broker           | [2023-08-04 11:20:21,794] INFO Created log for partition docker-connect-offsets-14 in /tmp/kraft-combined-logs/docker-connect-offsets-14 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,794] INFO [Partition docker-connect-offsets-14 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-14 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,794] INFO [Partition docker-connect-offsets-14 broker=1] Log loaded for partition docker-connect-offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,795] INFO [Broker id=1] Leader docker-connect-offsets-14 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect          | [2023-08-04 11:20:21,796] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,798] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,799] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,801] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,803] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,804] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,805] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,805] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,806] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:21,807] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
broker           | [2023-08-04 11:20:21,816] INFO [Broker id=1] Creating new partition docker-connect-offsets-16 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,843] INFO [LogLoader partition=docker-connect-offsets-16, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,845] INFO Created log for partition docker-connect-offsets-16 in /tmp/kraft-combined-logs/docker-connect-offsets-16 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,846] INFO [Partition docker-connect-offsets-16 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-16 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,847] INFO [Partition docker-connect-offsets-16 broker=1] Log loaded for partition docker-connect-offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,848] INFO [Broker id=1] Leader docker-connect-offsets-16 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:21,861] INFO [Broker id=1] Creating new partition docker-connect-offsets-2 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,873] INFO [LogLoader partition=docker-connect-offsets-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,878] INFO Created log for partition docker-connect-offsets-2 in /tmp/kraft-combined-logs/docker-connect-offsets-2 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,879] INFO [Partition docker-connect-offsets-2 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-2 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,879] INFO [Partition docker-connect-offsets-2 broker=1] Log loaded for partition docker-connect-offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,880] INFO [Broker id=1] Leader docker-connect-offsets-2 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:21,890] INFO [Broker id=1] Creating new partition docker-connect-offsets-4 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,899] INFO [LogLoader partition=docker-connect-offsets-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,904] INFO Created log for partition docker-connect-offsets-4 in /tmp/kraft-combined-logs/docker-connect-offsets-4 with properties {cleanup.policy=compact} (kafka.log.LogManager)
connect          | [2023-08-04 11:20:21,904] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
connect          | [2023-08-04 11:20:21,905] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
broker           | [2023-08-04 11:20:21,906] INFO [Partition docker-connect-offsets-4 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-4 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,907] INFO [Partition docker-connect-offsets-4 broker=1] Log loaded for partition docker-connect-offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,907] INFO [Broker id=1] Leader docker-connect-offsets-4 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect          | [2023-08-04 11:20:21,910] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session)
broker           | [2023-08-04 11:20:21,920] INFO [Broker id=1] Creating new partition docker-connect-offsets-6 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,928] INFO [LogLoader partition=docker-connect-offsets-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,931] INFO Created log for partition docker-connect-offsets-6 in /tmp/kraft-combined-logs/docker-connect-offsets-6 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,934] INFO [Partition docker-connect-offsets-6 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-6 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,935] INFO [Partition docker-connect-offsets-6 broker=1] Log loaded for partition docker-connect-offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,936] INFO [Broker id=1] Leader docker-connect-offsets-6 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect          | [2023-08-04 11:20:21,943] INFO [AdminClient clientId=adminclient-8] Retrying to fetch metadata. (org.apache.kafka.clients.admin.KafkaAdminClient)
broker           | [2023-08-04 11:20:21,958] INFO [Broker id=1] Creating new partition docker-connect-offsets-8 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:21,976] INFO [LogLoader partition=docker-connect-offsets-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:21,978] INFO Created log for partition docker-connect-offsets-8 in /tmp/kraft-combined-logs/docker-connect-offsets-8 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:21,978] INFO [Partition docker-connect-offsets-8 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-8 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,978] INFO [Partition docker-connect-offsets-8 broker=1] Log loaded for partition docker-connect-offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:21,978] INFO [Broker id=1] Leader docker-connect-offsets-8 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:22,000] INFO [Broker id=1] Creating new partition docker-connect-offsets-0 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
connect          | [2023-08-04 11:20:22,008] INFO [AdminClient clientId=adminclient-8] Retrying to fetch metadata. (org.apache.kafka.clients.admin.KafkaAdminClient)
broker           | [2023-08-04 11:20:22,020] INFO [LogLoader partition=docker-connect-offsets-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:22,028] INFO Created log for partition docker-connect-offsets-0 in /tmp/kraft-combined-logs/docker-connect-offsets-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:22,029] INFO [Partition docker-connect-offsets-0 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,029] INFO [Partition docker-connect-offsets-0 broker=1] Log loaded for partition docker-connect-offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,031] INFO [Broker id=1] Leader docker-connect-offsets-0 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:22,041] INFO [Broker id=1] Creating new partition docker-connect-offsets-19 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:22,056] INFO [LogLoader partition=docker-connect-offsets-19, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:22,060] INFO Created log for partition docker-connect-offsets-19 in /tmp/kraft-combined-logs/docker-connect-offsets-19 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:22,067] INFO [Partition docker-connect-offsets-19 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-19 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,067] INFO [Partition docker-connect-offsets-19 broker=1] Log loaded for partition docker-connect-offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,067] INFO [Broker id=1] Leader docker-connect-offsets-19 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:22,075] INFO [Broker id=1] Creating new partition docker-connect-offsets-21 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
connect          | [2023-08-04 11:20:22,077] INFO [AdminClient clientId=adminclient-8] Retrying to fetch metadata. (org.apache.kafka.clients.admin.KafkaAdminClient)
broker           | [2023-08-04 11:20:22,088] INFO [LogLoader partition=docker-connect-offsets-21, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:22,091] INFO Created log for partition docker-connect-offsets-21 in /tmp/kraft-combined-logs/docker-connect-offsets-21 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:22,091] INFO [Partition docker-connect-offsets-21 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-21 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,091] INFO [Partition docker-connect-offsets-21 broker=1] Log loaded for partition docker-connect-offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,095] INFO [Broker id=1] Leader docker-connect-offsets-21 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:22,105] INFO [Broker id=1] Creating new partition docker-connect-offsets-23 with topic id wQ6UKZG2TGmde1svteq6Gg. (state.change.logger)
broker           | [2023-08-04 11:20:22,112] INFO [LogLoader partition=docker-connect-offsets-23, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:22,115] INFO Created log for partition docker-connect-offsets-23 in /tmp/kraft-combined-logs/docker-connect-offsets-23 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:22,116] INFO [Partition docker-connect-offsets-23 broker=1] No checkpointed highwatermark is found for partition docker-connect-offsets-23 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,116] INFO [Partition docker-connect-offsets-23 broker=1] Log loaded for partition docker-connect-offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,117] INFO [Broker id=1] Leader docker-connect-offsets-23 with topic id Some(wQ6UKZG2TGmde1svteq6Gg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:22,125] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic docker-connect-offsets with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
connect          | [2023-08-04 11:20:22,222] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-17 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,223] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-20 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,224] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-11 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,224] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-23 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,224] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-14 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,224] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-5 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,225] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-0 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,225] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-8 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,225] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-7 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,225] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-4 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,226] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-1 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,226] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-10 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,226] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-13 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,226] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-24 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,227] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-21 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,227] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-16 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,227] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-3 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,228] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-9 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,228] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-15 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,228] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-18 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,228] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-19 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,229] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-22 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,229] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-6 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,230] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-2 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,230] INFO [Consumer clientId=consumer-compose-connect-group-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-12 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,290] INFO Finished reading KafkaBasedLog for topic docker-connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
connect          | [2023-08-04 11:20:22,290] INFO Started KafkaBasedLog for topic docker-connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
connect          | [2023-08-04 11:20:22,290] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore)
connect          | [2023-08-04 11:20:22,417] INFO LogEventsConfig values: 
connect          | 	confluent.event.logger.cloudevent.codec = binary
connect          | 	confluent.event.logger.enable = false
connect          | 	confluent.event.logger.exporter.class = class io.confluent.telemetry.events.exporter.kafka.EventKafkaExporter
connect          | 	confluent.event.logger.exporter.kafka.producer.bootstrap.servers = 
connect          | 	confluent.event.logger.exporter.kafka.producer.client.id = confluent-connect-log-events-emitter-compose-connect-group
connect          | 	confluent.event.logger.exporter.kafka.topic.create = true
connect          | 	confluent.event.logger.exporter.kafka.topic.name = confluent-connect-log-events
connect          | 	confluent.event.logger.exporter.kafka.type = kafka
connect          |  (io.confluent.logevents.connect.LogEventsConfig)
connect          | [2023-08-04 11:20:22,419] INFO Connect Log Events aren't enabled. (io.confluent.logevents.connect.LogEventsKafkaEmitter)
connect          | [2023-08-04 11:20:22,420] INFO Worker started (org.apache.kafka.connect.runtime.Worker)
connect          | [2023-08-04 11:20:22,420] INFO Starting KafkaBasedLog with topic docker-connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
broker           | [2023-08-04 11:20:22,427] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='docker-connect-status', numPartitions=5, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:22,427] INFO [Controller 1] Created topic docker-connect-status with topic ID QJz7Q4ODTTa4GNlYOt6wNQ. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:22,428] INFO [Controller 1] ConfigResource(type=TOPIC, name='docker-connect-status'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:20:22,428] INFO [Controller 1] Created partition docker-connect-status-0 with topic ID QJz7Q4ODTTa4GNlYOt6wNQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:22,428] INFO [Controller 1] Created partition docker-connect-status-1 with topic ID QJz7Q4ODTTa4GNlYOt6wNQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:22,428] INFO [Controller 1] Created partition docker-connect-status-2 with topic ID QJz7Q4ODTTa4GNlYOt6wNQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:22,428] INFO [Controller 1] Created partition docker-connect-status-3 with topic ID QJz7Q4ODTTa4GNlYOt6wNQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:22,428] INFO [Controller 1] Created partition docker-connect-status-4 with topic ID QJz7Q4ODTTa4GNlYOt6wNQ and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:22,466] INFO [Broker id=1] Transitioning 5 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:20:22,476] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(docker-connect-status-1, docker-connect-status-0, docker-connect-status-3, docker-connect-status-2, docker-connect-status-4) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:20:22,476] INFO [Broker id=1] Creating new partition docker-connect-status-1 with topic id QJz7Q4ODTTa4GNlYOt6wNQ. (state.change.logger)
connect          | [2023-08-04 11:20:22,482] INFO Created topic (name=docker-connect-status, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at broker:29092 (org.apache.kafka.connect.util.TopicAdmin)
connect          | [2023-08-04 11:20:22,485] INFO ProducerConfig values: 
connect          | 	acks = -1
connect          | 	batch.size = 16384
connect          | 	bootstrap.servers = [broker:29092]
connect          | 	buffer.memory = 33554432
connect          | 	client.dns.lookup = use_all_dns_ips
connect          | 	client.id = producer-2
connect          | 	compression.type = none
connect          | 	connections.max.idle.ms = 540000
connect          | 	delivery.timeout.ms = 120000
connect          | 	enable.idempotence = false
connect          | 	interceptor.classes = []
connect          | 	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
connect          | 	linger.ms = 0
connect          | 	max.block.ms = 60000
connect          | 	max.in.flight.requests.per.connection = 1
connect          | 	max.request.size = 1048576
connect          | 	metadata.max.age.ms = 300000
connect          | 	metadata.max.idle.ms = 300000
connect          | 	metric.reporters = []
connect          | 	metrics.num.samples = 2
connect          | 	metrics.recording.level = INFO
connect          | 	metrics.sample.window.ms = 30000
connect          | 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
connect          | 	receive.buffer.bytes = 32768
connect          | 	reconnect.backoff.max.ms = 1000
connect          | 	reconnect.backoff.ms = 50
connect          | 	request.timeout.ms = 30000
connect          | 	retries = 0
connect          | 	retry.backoff.ms = 100
connect          | 	sasl.client.callback.handler.class = null
connect          | 	sasl.jaas.config = null
connect          | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	sasl.kerberos.min.time.before.relogin = 60000
connect          | 	sasl.kerberos.service.name = null
connect          | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	sasl.login.callback.handler.class = null
connect          | 	sasl.login.class = null
connect          | 	sasl.login.connect.timeout.ms = null
connect          | 	sasl.login.read.timeout.ms = null
connect          | 	sasl.login.refresh.buffer.seconds = 300
connect          | 	sasl.login.refresh.min.period.seconds = 60
connect          | 	sasl.login.refresh.window.factor = 0.8
connect          | 	sasl.login.refresh.window.jitter = 0.05
connect          | 	sasl.login.retry.backoff.max.ms = 10000
connect          | 	sasl.login.retry.backoff.ms = 100
connect          | 	sasl.mechanism = GSSAPI
connect          | 	sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	sasl.oauthbearer.expected.audience = null
connect          | 	sasl.oauthbearer.expected.issuer = null
connect          | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	sasl.oauthbearer.scope.claim.name = scope
connect          | 	sasl.oauthbearer.sub.claim.name = sub
connect          | 	sasl.oauthbearer.token.endpoint.url = null
connect          | 	security.protocol = PLAINTEXT
connect          | 	security.providers = null
connect          | 	send.buffer.bytes = 131072
connect          | 	socket.connection.setup.timeout.max.ms = 30000
connect          | 	socket.connection.setup.timeout.ms = 10000
connect          | 	ssl.cipher.suites = null
connect          | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	ssl.endpoint.identification.algorithm = https
connect          | 	ssl.engine.factory.class = null
connect          | 	ssl.key.password = null
connect          | 	ssl.keymanager.algorithm = SunX509
connect          | 	ssl.keystore.certificate.chain = null
connect          | 	ssl.keystore.key = null
connect          | 	ssl.keystore.location = null
connect          | 	ssl.keystore.password = null
connect          | 	ssl.keystore.type = JKS
connect          | 	ssl.protocol = TLSv1.3
connect          | 	ssl.provider = null
connect          | 	ssl.secure.random.implementation = null
connect          | 	ssl.trustmanager.algorithm = PKIX
connect          | 	ssl.truststore.certificates = null
connect          | 	ssl.truststore.location = null
connect          | 	ssl.truststore.password = null
connect          | 	ssl.truststore.type = JKS
connect          | 	transaction.timeout.ms = 60000
connect          | 	transactional.id = null
connect          | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
connect          |  (org.apache.kafka.clients.producer.ProducerConfig)
broker           | [2023-08-04 11:20:22,492] INFO [LogLoader partition=docker-connect-status-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:22,494] INFO Created log for partition docker-connect-status-1 in /tmp/kraft-combined-logs/docker-connect-status-1 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:22,499] INFO [Partition docker-connect-status-1 broker=1] No checkpointed highwatermark is found for partition docker-connect-status-1 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,499] INFO [Partition docker-connect-status-1 broker=1] Log loaded for partition docker-connect-status-1 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,500] INFO [Broker id=1] Leader docker-connect-status-1 with topic id Some(QJz7Q4ODTTa4GNlYOt6wNQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:22,514] INFO [Broker id=1] Creating new partition docker-connect-status-0 with topic id QJz7Q4ODTTa4GNlYOt6wNQ. (state.change.logger)
broker           | [2023-08-04 11:20:22,524] INFO [LogLoader partition=docker-connect-status-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:22,526] INFO Created log for partition docker-connect-status-0 in /tmp/kraft-combined-logs/docker-connect-status-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:22,526] INFO [Partition docker-connect-status-0 broker=1] No checkpointed highwatermark is found for partition docker-connect-status-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,529] INFO [Partition docker-connect-status-0 broker=1] Log loaded for partition docker-connect-status-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,530] INFO [Broker id=1] Leader docker-connect-status-0 with topic id Some(QJz7Q4ODTTa4GNlYOt6wNQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect          | [2023-08-04 11:20:22,531] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,532] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,532] WARN The configuration 'metrics.context.resource.version' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,532] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,532] WARN The configuration 'metrics.context.resource.type' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,532] WARN The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,532] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,532] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,533] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,533] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,533] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,533] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,533] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,533] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,533] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,533] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,533] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,533] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,533] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,533] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,533] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,534] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,534] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,534] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:22,534] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:22,534] INFO Kafka startTimeMs: 1691148022534 (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:22,541] INFO ConsumerConfig values: 
connect          | 	allow.auto.create.topics = true
connect          | 	auto.commit.interval.ms = 5000
connect          | 	auto.offset.reset = earliest
connect          | 	bootstrap.servers = [broker:29092]
connect          | 	check.crcs = true
connect          | 	client.dns.lookup = use_all_dns_ips
connect          | 	client.id = consumer-compose-connect-group-2
connect          | 	client.rack = 
connect          | 	connections.max.idle.ms = 540000
connect          | 	default.api.timeout.ms = 60000
connect          | 	enable.auto.commit = false
connect          | 	exclude.internal.topics = true
connect          | 	fetch.max.bytes = 52428800
connect          | 	fetch.max.wait.ms = 500
connect          | 	fetch.min.bytes = 1
connect          | 	group.id = compose-connect-group
connect          | 	group.instance.id = null
connect          | 	heartbeat.interval.ms = 3000
connect          | 	interceptor.classes = []
connect          | 	internal.leave.group.on.close = true
connect          | 	internal.throw.on.fetch.stable.offset.unsupported = false
connect          | 	isolation.level = read_uncommitted
connect          | 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
connect          | 	max.partition.fetch.bytes = 1048576
connect          | 	max.poll.interval.ms = 300000
connect          | 	max.poll.records = 500
connect          | 	metadata.max.age.ms = 300000
connect          | 	metric.reporters = []
connect          | 	metrics.num.samples = 2
connect          | 	metrics.recording.level = INFO
connect          | 	metrics.sample.window.ms = 30000
connect          | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
connect          | 	receive.buffer.bytes = 65536
connect          | 	reconnect.backoff.max.ms = 1000
connect          | 	reconnect.backoff.ms = 50
connect          | 	request.timeout.ms = 30000
connect          | 	retry.backoff.ms = 100
connect          | 	sasl.client.callback.handler.class = null
connect          | 	sasl.jaas.config = null
connect          | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	sasl.kerberos.min.time.before.relogin = 60000
connect          | 	sasl.kerberos.service.name = null
connect          | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	sasl.login.callback.handler.class = null
connect          | 	sasl.login.class = null
connect          | 	sasl.login.connect.timeout.ms = null
connect          | 	sasl.login.read.timeout.ms = null
connect          | 	sasl.login.refresh.buffer.seconds = 300
connect          | 	sasl.login.refresh.min.period.seconds = 60
connect          | 	sasl.login.refresh.window.factor = 0.8
connect          | 	sasl.login.refresh.window.jitter = 0.05
connect          | 	sasl.login.retry.backoff.max.ms = 10000
connect          | 	sasl.login.retry.backoff.ms = 100
connect          | 	sasl.mechanism = GSSAPI
connect          | 	sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	sasl.oauthbearer.expected.audience = null
connect          | 	sasl.oauthbearer.expected.issuer = null
connect          | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	sasl.oauthbearer.scope.claim.name = scope
connect          | 	sasl.oauthbearer.sub.claim.name = sub
connect          | 	sasl.oauthbearer.token.endpoint.url = null
connect          | 	security.protocol = PLAINTEXT
connect          | 	security.providers = null
connect          | 	send.buffer.bytes = 131072
connect          | 	session.timeout.ms = 45000
connect          | 	socket.connection.setup.timeout.max.ms = 30000
connect          | 	socket.connection.setup.timeout.ms = 10000
connect          | 	ssl.cipher.suites = null
connect          | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	ssl.endpoint.identification.algorithm = https
connect          | 	ssl.engine.factory.class = null
connect          | 	ssl.key.password = null
connect          | 	ssl.keymanager.algorithm = SunX509
connect          | 	ssl.keystore.certificate.chain = null
connect          | 	ssl.keystore.key = null
connect          | 	ssl.keystore.location = null
connect          | 	ssl.keystore.password = null
connect          | 	ssl.keystore.type = JKS
connect          | 	ssl.protocol = TLSv1.3
connect          | 	ssl.provider = null
connect          | 	ssl.secure.random.implementation = null
connect          | 	ssl.trustmanager.algorithm = PKIX
connect          | 	ssl.truststore.certificates = null
connect          | 	ssl.truststore.location = null
connect          | 	ssl.truststore.password = null
connect          | 	ssl.truststore.type = JKS
connect          | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
connect          |  (org.apache.kafka.clients.consumer.ConsumerConfig)
broker           | [2023-08-04 11:20:22,544] INFO [Broker id=1] Creating new partition docker-connect-status-3 with topic id QJz7Q4ODTTa4GNlYOt6wNQ. (state.change.logger)
broker           | [2023-08-04 11:20:22,559] INFO [LogLoader partition=docker-connect-status-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:22,564] INFO Created log for partition docker-connect-status-3 in /tmp/kraft-combined-logs/docker-connect-status-3 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:22,565] INFO [Partition docker-connect-status-3 broker=1] No checkpointed highwatermark is found for partition docker-connect-status-3 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,566] INFO [Partition docker-connect-status-3 broker=1] Log loaded for partition docker-connect-status-3 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,567] INFO [Broker id=1] Leader docker-connect-status-3 with topic id Some(QJz7Q4ODTTa4GNlYOt6wNQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2023-08-04 11:20:22,580] INFO [Broker id=1] Creating new partition docker-connect-status-2 with topic id QJz7Q4ODTTa4GNlYOt6wNQ. (state.change.logger)
broker           | [2023-08-04 11:20:22,592] INFO [LogLoader partition=docker-connect-status-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:22,593] INFO Created log for partition docker-connect-status-2 in /tmp/kraft-combined-logs/docker-connect-status-2 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:22,594] INFO [Partition docker-connect-status-2 broker=1] No checkpointed highwatermark is found for partition docker-connect-status-2 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,594] INFO [Partition docker-connect-status-2 broker=1] Log loaded for partition docker-connect-status-2 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,594] INFO [Broker id=1] Leader docker-connect-status-2 with topic id Some(QJz7Q4ODTTa4GNlYOt6wNQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect          | [2023-08-04 11:20:22,588] INFO [Producer clientId=producer-2] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,604] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,609] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,609] WARN The configuration 'metrics.context.resource.version' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,609] WARN The configuration 'metrics.context.resource.type' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,609] WARN The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,609] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,609] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,610] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,610] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,610] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,610] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,610] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,610] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,610] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,610] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,610] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,610] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,610] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,610] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,610] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,610] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,611] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,611] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:22,611] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:22,611] INFO Kafka startTimeMs: 1691148022611 (org.apache.kafka.common.utils.AppInfoParser)
broker           | [2023-08-04 11:20:22,609] INFO [Broker id=1] Creating new partition docker-connect-status-4 with topic id QJz7Q4ODTTa4GNlYOt6wNQ. (state.change.logger)
broker           | [2023-08-04 11:20:22,626] INFO [LogLoader partition=docker-connect-status-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:22,629] INFO Created log for partition docker-connect-status-4 in /tmp/kraft-combined-logs/docker-connect-status-4 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2023-08-04 11:20:22,629] INFO [Partition docker-connect-status-4 broker=1] No checkpointed highwatermark is found for partition docker-connect-status-4 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,629] INFO [Partition docker-connect-status-4 broker=1] Log loaded for partition docker-connect-status-4 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,632] INFO [Broker id=1] Leader docker-connect-status-4 with topic id Some(QJz7Q4ODTTa4GNlYOt6wNQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect          | [2023-08-04 11:20:22,643] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,646] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Subscribed to partition(s): docker-connect-status-1, docker-connect-status-3, docker-connect-status-2, docker-connect-status-0, docker-connect-status-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
connect          | [2023-08-04 11:20:22,646] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:22,646] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:22,647] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:22,647] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:22,648] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
broker           | [2023-08-04 11:20:22,650] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic docker-connect-status with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
connect          | [2023-08-04 11:20:22,676] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-1 to 0 since the associated topicId changed from null to QJz7Q4ODTTa4GNlYOt6wNQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,677] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-3 to 0 since the associated topicId changed from null to QJz7Q4ODTTa4GNlYOt6wNQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,678] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-2 to 0 since the associated topicId changed from null to QJz7Q4ODTTa4GNlYOt6wNQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,679] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-0 to 0 since the associated topicId changed from null to QJz7Q4ODTTa4GNlYOt6wNQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,679] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-4 to 0 since the associated topicId changed from null to QJz7Q4ODTTa4GNlYOt6wNQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,692] INFO Finished reading KafkaBasedLog for topic docker-connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
connect          | [2023-08-04 11:20:22,692] INFO Started KafkaBasedLog for topic docker-connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
connect          | [2023-08-04 11:20:22,743] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
connect          | [2023-08-04 11:20:22,743] INFO Starting KafkaBasedLog with topic docker-connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)
broker           | [2023-08-04 11:20:22,754] INFO [Controller 1] CreateTopics result(s): CreatableTopic(name='docker-connect-configs', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact')]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:22,755] INFO [Controller 1] Created topic docker-connect-configs with topic ID ISAKcDcaSuSPu9aTa_qbYg. (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:22,756] INFO [Controller 1] ConfigResource(type=TOPIC, name='docker-connect-configs'): set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
broker           | [2023-08-04 11:20:22,757] INFO [Controller 1] Created partition docker-connect-configs-0 with topic ID ISAKcDcaSuSPu9aTa_qbYg and PartitionRegistration(replicas=[1], isr=[1], removingReplicas=[], addingReplicas=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager)
broker           | [2023-08-04 11:20:22,791] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
broker           | [2023-08-04 11:20:22,791] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(docker-connect-configs-0) (kafka.server.ReplicaFetcherManager)
broker           | [2023-08-04 11:20:22,791] INFO [Broker id=1] Creating new partition docker-connect-configs-0 with topic id ISAKcDcaSuSPu9aTa_qbYg. (state.change.logger)
connect          | [2023-08-04 11:20:22,804] INFO Created topic (name=docker-connect-configs, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at broker:29092 (org.apache.kafka.connect.util.TopicAdmin)
connect          | [2023-08-04 11:20:22,806] INFO ProducerConfig values: 
connect          | 	acks = -1
connect          | 	batch.size = 16384
connect          | 	bootstrap.servers = [broker:29092]
connect          | 	buffer.memory = 33554432
connect          | 	client.dns.lookup = use_all_dns_ips
connect          | 	client.id = producer-3
connect          | 	compression.type = none
connect          | 	connections.max.idle.ms = 540000
connect          | 	delivery.timeout.ms = 2147483647
connect          | 	enable.idempotence = false
connect          | 	interceptor.classes = []
connect          | 	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
connect          | 	linger.ms = 0
connect          | 	max.block.ms = 60000
connect          | 	max.in.flight.requests.per.connection = 1
connect          | 	max.request.size = 1048576
connect          | 	metadata.max.age.ms = 300000
connect          | 	metadata.max.idle.ms = 300000
connect          | 	metric.reporters = []
connect          | 	metrics.num.samples = 2
connect          | 	metrics.recording.level = INFO
connect          | 	metrics.sample.window.ms = 30000
connect          | 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
connect          | 	receive.buffer.bytes = 32768
connect          | 	reconnect.backoff.max.ms = 1000
connect          | 	reconnect.backoff.ms = 50
connect          | 	request.timeout.ms = 30000
connect          | 	retries = 2147483647
connect          | 	retry.backoff.ms = 100
connect          | 	sasl.client.callback.handler.class = null
connect          | 	sasl.jaas.config = null
connect          | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	sasl.kerberos.min.time.before.relogin = 60000
connect          | 	sasl.kerberos.service.name = null
connect          | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	sasl.login.callback.handler.class = null
connect          | 	sasl.login.class = null
connect          | 	sasl.login.connect.timeout.ms = null
connect          | 	sasl.login.read.timeout.ms = null
connect          | 	sasl.login.refresh.buffer.seconds = 300
connect          | 	sasl.login.refresh.min.period.seconds = 60
connect          | 	sasl.login.refresh.window.factor = 0.8
connect          | 	sasl.login.refresh.window.jitter = 0.05
connect          | 	sasl.login.retry.backoff.max.ms = 10000
connect          | 	sasl.login.retry.backoff.ms = 100
connect          | 	sasl.mechanism = GSSAPI
connect          | 	sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	sasl.oauthbearer.expected.audience = null
connect          | 	sasl.oauthbearer.expected.issuer = null
connect          | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	sasl.oauthbearer.scope.claim.name = scope
connect          | 	sasl.oauthbearer.sub.claim.name = sub
connect          | 	sasl.oauthbearer.token.endpoint.url = null
connect          | 	security.protocol = PLAINTEXT
connect          | 	security.providers = null
connect          | 	send.buffer.bytes = 131072
connect          | 	socket.connection.setup.timeout.max.ms = 30000
connect          | 	socket.connection.setup.timeout.ms = 10000
connect          | 	ssl.cipher.suites = null
connect          | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	ssl.endpoint.identification.algorithm = https
connect          | 	ssl.engine.factory.class = null
connect          | 	ssl.key.password = null
connect          | 	ssl.keymanager.algorithm = SunX509
connect          | 	ssl.keystore.certificate.chain = null
connect          | 	ssl.keystore.key = null
connect          | 	ssl.keystore.location = null
connect          | 	ssl.keystore.password = null
connect          | 	ssl.keystore.type = JKS
connect          | 	ssl.protocol = TLSv1.3
connect          | 	ssl.provider = null
connect          | 	ssl.secure.random.implementation = null
connect          | 	ssl.trustmanager.algorithm = PKIX
connect          | 	ssl.truststore.certificates = null
connect          | 	ssl.truststore.location = null
connect          | 	ssl.truststore.password = null
connect          | 	ssl.truststore.type = JKS
connect          | 	transaction.timeout.ms = 60000
connect          | 	transactional.id = null
connect          | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
connect          |  (org.apache.kafka.clients.producer.ProducerConfig)
broker           | [2023-08-04 11:20:22,822] INFO [LogLoader partition=docker-connect-configs-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2023-08-04 11:20:22,828] INFO Created log for partition docker-connect-configs-0 in /tmp/kraft-combined-logs/docker-connect-configs-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
connect          | [2023-08-04 11:20:22,842] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,842] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,842] WARN The configuration 'metrics.context.resource.version' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,842] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,842] WARN The configuration 'metrics.context.resource.type' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,843] WARN The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,843] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,843] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,843] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,843] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,844] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,844] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,844] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,844] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,844] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,844] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,844] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,844] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,844] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,845] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,845] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,845] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,845] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
connect          | [2023-08-04 11:20:22,845] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:22,845] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:22,845] INFO Kafka startTimeMs: 1691148022845 (org.apache.kafka.common.utils.AppInfoParser)
broker           | [2023-08-04 11:20:22,852] INFO [Partition docker-connect-configs-0 broker=1] No checkpointed highwatermark is found for partition docker-connect-configs-0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,854] INFO [Partition docker-connect-configs-0 broker=1] Log loaded for partition docker-connect-configs-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2023-08-04 11:20:22,855] INFO [Broker id=1] Leader docker-connect-configs-0 with topic id Some(ISAKcDcaSuSPu9aTa_qbYg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
connect          | [2023-08-04 11:20:22,857] INFO ConsumerConfig values: 
connect          | 	allow.auto.create.topics = true
connect          | 	auto.commit.interval.ms = 5000
connect          | 	auto.offset.reset = earliest
connect          | 	bootstrap.servers = [broker:29092]
connect          | 	check.crcs = true
connect          | 	client.dns.lookup = use_all_dns_ips
connect          | 	client.id = consumer-compose-connect-group-3
connect          | 	client.rack = 
connect          | 	connections.max.idle.ms = 540000
connect          | 	default.api.timeout.ms = 60000
connect          | 	enable.auto.commit = false
connect          | 	exclude.internal.topics = true
connect          | 	fetch.max.bytes = 52428800
connect          | 	fetch.max.wait.ms = 500
connect          | 	fetch.min.bytes = 1
connect          | 	group.id = compose-connect-group
connect          | 	group.instance.id = null
connect          | 	heartbeat.interval.ms = 3000
connect          | 	interceptor.classes = []
connect          | 	internal.leave.group.on.close = true
connect          | 	internal.throw.on.fetch.stable.offset.unsupported = false
connect          | 	isolation.level = read_uncommitted
connect          | 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
connect          | 	max.partition.fetch.bytes = 1048576
connect          | 	max.poll.interval.ms = 300000
connect          | 	max.poll.records = 500
connect          | 	metadata.max.age.ms = 300000
connect          | 	metric.reporters = []
connect          | 	metrics.num.samples = 2
connect          | 	metrics.recording.level = INFO
connect          | 	metrics.sample.window.ms = 30000
connect          | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
connect          | 	receive.buffer.bytes = 65536
connect          | 	reconnect.backoff.max.ms = 1000
connect          | 	reconnect.backoff.ms = 50
connect          | 	request.timeout.ms = 30000
connect          | 	retry.backoff.ms = 100
connect          | 	sasl.client.callback.handler.class = null
connect          | 	sasl.jaas.config = null
connect          | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
connect          | 	sasl.kerberos.min.time.before.relogin = 60000
connect          | 	sasl.kerberos.service.name = null
connect          | 	sasl.kerberos.ticket.renew.jitter = 0.05
connect          | 	sasl.kerberos.ticket.renew.window.factor = 0.8
connect          | 	sasl.login.callback.handler.class = null
connect          | 	sasl.login.class = null
connect          | 	sasl.login.connect.timeout.ms = null
connect          | 	sasl.login.read.timeout.ms = null
connect          | 	sasl.login.refresh.buffer.seconds = 300
connect          | 	sasl.login.refresh.min.period.seconds = 60
connect          | 	sasl.login.refresh.window.factor = 0.8
connect          | 	sasl.login.refresh.window.jitter = 0.05
connect          | 	sasl.login.retry.backoff.max.ms = 10000
connect          | 	sasl.login.retry.backoff.ms = 100
connect          | 	sasl.mechanism = GSSAPI
connect          | 	sasl.oauthbearer.clock.skew.seconds = 30
connect          | 	sasl.oauthbearer.expected.audience = null
connect          | 	sasl.oauthbearer.expected.issuer = null
connect          | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
connect          | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
connect          | 	sasl.oauthbearer.jwks.endpoint.url = null
connect          | 	sasl.oauthbearer.scope.claim.name = scope
connect          | 	sasl.oauthbearer.sub.claim.name = sub
connect          | 	sasl.oauthbearer.token.endpoint.url = null
connect          | 	security.protocol = PLAINTEXT
connect          | 	security.providers = null
connect          | 	send.buffer.bytes = 131072
connect          | 	session.timeout.ms = 45000
connect          | 	socket.connection.setup.timeout.max.ms = 30000
connect          | 	socket.connection.setup.timeout.ms = 10000
connect          | 	ssl.cipher.suites = null
connect          | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
connect          | 	ssl.endpoint.identification.algorithm = https
connect          | 	ssl.engine.factory.class = null
connect          | 	ssl.key.password = null
connect          | 	ssl.keymanager.algorithm = SunX509
connect          | 	ssl.keystore.certificate.chain = null
connect          | 	ssl.keystore.key = null
connect          | 	ssl.keystore.location = null
connect          | 	ssl.keystore.password = null
connect          | 	ssl.keystore.type = JKS
connect          | 	ssl.protocol = TLSv1.3
connect          | 	ssl.provider = null
connect          | 	ssl.secure.random.implementation = null
connect          | 	ssl.trustmanager.algorithm = PKIX
connect          | 	ssl.truststore.certificates = null
connect          | 	ssl.truststore.location = null
connect          | 	ssl.truststore.password = null
connect          | 	ssl.truststore.type = JKS
connect          | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
connect          |  (org.apache.kafka.clients.consumer.ConsumerConfig)
broker           | [2023-08-04 11:20:22,868] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic docker-connect-configs with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
connect          | [2023-08-04 11:20:22,901] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,905] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,905] WARN The configuration 'metrics.context.resource.version' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,905] WARN The configuration 'metrics.context.resource.type' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,905] WARN The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,906] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,906] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,906] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,906] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,906] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,907] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,907] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,907] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,907] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,907] INFO [Producer clientId=producer-3] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,908] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,910] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,910] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,911] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,914] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,917] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,917] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,918] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
connect          | [2023-08-04 11:20:22,918] INFO Kafka version: 7.1.0-ce (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:22,919] INFO Kafka commitId: 5c05312ab63acecf (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:22,922] INFO Kafka startTimeMs: 1691148022918 (org.apache.kafka.common.utils.AppInfoParser)
connect          | [2023-08-04 11:20:22,949] INFO [Consumer clientId=consumer-compose-connect-group-3, groupId=compose-connect-group] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:22,971] INFO [Consumer clientId=consumer-compose-connect-group-3, groupId=compose-connect-group] Subscribed to partition(s): docker-connect-configs-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
connect          | [2023-08-04 11:20:22,971] INFO [Consumer clientId=consumer-compose-connect-group-3, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
connect          | [2023-08-04 11:20:23,004] INFO [Consumer clientId=consumer-compose-connect-group-3, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-configs-0 to 0 since the associated topicId changed from null to ISAKcDcaSuSPu9aTa_qbYg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,011] INFO Finished reading KafkaBasedLog for topic docker-connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)
connect          | [2023-08-04 11:20:23,011] INFO Started KafkaBasedLog for topic docker-connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)
connect          | [2023-08-04 11:20:23,012] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
connect          | [2023-08-04 11:20:23,013] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
connect          | [2023-08-04 11:20:23,079] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 to 0 since the associated topicId changed from null to FBg1W8OSQXWAXSTUaAxC7Q (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,079] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to 1llU1igJTl2WtB6Ombk9uQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,080] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 to 0 since the associated topicId changed from null to S495LVuvRsuHe_-TfpTCVA (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,081] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to 9SrPkJvdQRKHmFqHX1yx4w (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,081] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to aFs6ffa2RL65YNQHKdNm3A (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,081] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 to 0 since the associated topicId changed from null to SXqs9z_IT0ycq4w-So2PtQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,081] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to HoP6Lp2hR0mutBINDVd0SA (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,082] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 to 0 since the associated topicId changed from null to RkotBavzTyeTSSFDELtuFA (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,082] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 to 0 since the associated topicId changed from null to iJLqc_VsS6GtzjQhq4pkTQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,082] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 to 0 since the associated topicId changed from null to NZXeFHK0TkaUSTnQorwW-A (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,082] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 to 0 since the associated topicId changed from null to O3OKZPZ5RfOh-XbP8Y4yyQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,089] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 to 0 since the associated topicId changed from null to pssqfmbBQmKndkFz4zchXw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,090] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-1 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,090] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-11 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,090] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-10 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,090] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-5 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,090] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-0 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,090] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-8 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,090] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-3 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,091] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-9 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,091] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-6 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,091] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-2 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,091] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-7 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,092] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-metrics-4 to 0 since the associated topicId changed from null to YkgFeJSTQIKRJZkpXXhRjg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,092] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to qP4lTE8ATsmQfTcF8slryw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,092] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 to 0 since the associated topicId changed from null to RNee_9E6QoWuyPfJUZqYdg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,092] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-configs-0 to 0 since the associated topicId changed from null to ISAKcDcaSuSPu9aTa_qbYg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,092] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 to 0 since the associated topicId changed from null to sAkAJV7KS4-GAs2bkdIKRA (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,092] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 to 0 since the associated topicId changed from null to nzdecc30ShSdQcB04frvog (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,092] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-command-0 to 0 since the associated topicId changed from null to AFaY0a_lSiO7tlDgMwz2qw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,093] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 to 0 since the associated topicId changed from null to UygM9SIITY-KoEx4I2w6zw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,093] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 to 0 since the associated topicId changed from null to 4A9-74zvQouR64zqhXh9FA (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,093] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to gFRdYcjWQsGSJ47CHoXyqw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,093] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to pAGnLBBpRCSB7OJ5_G2CJA (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,093] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 to 0 since the associated topicId changed from null to SR4XL91XSTmhEPn2HtkHYQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,093] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to yh8_-dziTp-s7IDqy8w3Rw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,093] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 to 0 since the associated topicId changed from null to Y_GSqt8QROagfczJg3r40g (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,093] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 to 0 since the associated topicId changed from null to f6T0fWnJTtClUuyvYTjZBw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,094] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to JdPg8CHIR2eSEEpP-4pgKA (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,094] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 to 0 since the associated topicId changed from null to Pmp7XBFVTWm7zuSANCQ_8g (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,094] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-1 to 0 since the associated topicId changed from null to QJz7Q4ODTTa4GNlYOt6wNQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,094] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-3 to 0 since the associated topicId changed from null to QJz7Q4ODTTa4GNlYOt6wNQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,094] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-2 to 0 since the associated topicId changed from null to QJz7Q4ODTTa4GNlYOt6wNQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,094] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-0 to 0 since the associated topicId changed from null to QJz7Q4ODTTa4GNlYOt6wNQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,094] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-status-4 to 0 since the associated topicId changed from null to QJz7Q4ODTTa4GNlYOt6wNQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,095] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to RifsNDOQSCG4j2fz_vLz5g (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,095] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 to 0 since the associated topicId changed from null to oBNDr9MwSwyfz3UNlKj0Aw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,095] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to WdjVN2t-SM21TyDTgxyLhw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,095] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to 44Yp-2aPRK-xKM9zKUCEJw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,095] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to I-AspoMlSU-I5nUZzPsT2Q (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,095] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 to 0 since the associated topicId changed from null to HSq3iWH_Q0G8WAozBnXiFQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,095] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 to 0 since the associated topicId changed from null to xpuJp62xRjKbInyyPCX1lQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,096] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 to 0 since the associated topicId changed from null to RXByHOqASZ2CbNrZqK_AMw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,096] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 3eA5D2OKTTmHRKZTjiQWtA (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,096] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to KabcuEMhTCOz9nkNgpIsHg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,096] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 to 0 since the associated topicId changed from null to dlujEO5uTCeuzLFQQNtU0A (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,096] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to e66bfUqWTZWX8U4X4Ipjsw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,098] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to 88agc7ovSmm9apNE4HN69Q (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,098] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 to 0 since the associated topicId changed from null to xT2oJSg6R6ONg7UwEIUFtA (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,098] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 to 0 since the associated topicId changed from null to Trp1P4PASmedQMblOoysPQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,098] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 to 0 since the associated topicId changed from null to VHDKH6yPRKekarvjYHIjyw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,099] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-17 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,099] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-20 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,107] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-11 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,107] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-23 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,107] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-14 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,107] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-5 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,108] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-0 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,108] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-8 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,108] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-7 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,108] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-4 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,108] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-1 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,108] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-10 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,108] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-13 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,108] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-24 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,109] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-21 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,109] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-16 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,109] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-3 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,109] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-9 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,109] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-15 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,109] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-18 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,109] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-19 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,109] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-22 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,110] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-6 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,110] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-2 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,110] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition docker-connect-offsets-12 to 0 since the associated topicId changed from null to wQ6UKZG2TGmde1svteq6Gg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,110] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 to 0 since the associated topicId changed from null to No9nQpMqSCWKHXxTpgGjHA (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,110] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-monitoring-0 to 0 since the associated topicId changed from null to 8fgQicUSRVi7xJcKYztVRQ (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,110] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 to 0 since the associated topicId changed from null to w47q9kvPRoGucrcFjBQ9fw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,110] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 to 0 since the associated topicId changed from null to 7J5ZVEciQBa64RcXLSLhrw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,110] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 to 0 since the associated topicId changed from null to 6s9RSwQ_RiqRp339_OPjrw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,111] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-17 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,111] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-11 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,111] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-23 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,111] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-40 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,111] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-5 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,111] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-0 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,111] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-29 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,112] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-46 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,112] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-30 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,112] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-4 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,112] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-39 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,112] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-42 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,112] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-36 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,112] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-48 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,112] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-10 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,113] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-13 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,113] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-45 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,113] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-16 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,113] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-28 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,114] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-34 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,115] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-19 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,115] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-22 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,115] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-31 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,115] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-2 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,116] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-25 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,123] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-20 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,123] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-26 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,123] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-14 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,123] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-32 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,123] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-37 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,124] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-8 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,124] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-43 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,124] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-7 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,124] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-49 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,124] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-33 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,124] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-1 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,124] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-27 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,124] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-24 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,125] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-21 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,125] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-47 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,125] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-3 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,125] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-9 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,125] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-15 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,125] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-18 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,125] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-44 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,125] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-6 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,125] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-35 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,126] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-38 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,126] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-41 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,126] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition __consumer_offsets-12 to 0 since the associated topicId changed from null to 6FR4V_fAS7Gu-9aw7Irmcg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,126] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 to 0 since the associated topicId changed from null to I_0cUFAkR3umlIgUR1kaaw (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,133] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Cluster ID: MkU3OEVBNTcwNTJENDM2Qg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,137] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
connect          | [2023-08-04 11:20:23,143] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
connect          | [2023-08-04 11:20:23,143] INFO [Worker clientId=connect-1, groupId=compose-connect-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
broker           | [2023-08-04 11:20:23,190] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group compose-connect-group in Empty state. Created a new member id connect-1-c3b4f443-0240-44ad-97fd-d305853fc174 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
connect          | [2023-08-04 11:20:23,193] INFO [Worker clientId=connect-1, groupId=compose-connect-group] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
broker           | [2023-08-04 11:20:23,197] INFO [GroupCoordinator 1]: Preparing to rebalance group compose-connect-group in state PreparingRebalance with old generation 0 (__consumer_offsets-37) (reason: Adding new member connect-1-c3b4f443-0240-44ad-97fd-d305853fc174 with group instance id None; client reason: not provided) (kafka.coordinator.group.GroupCoordinator)
broker           | [2023-08-04 11:20:23,213] INFO [GroupCoordinator 1]: Stabilized group compose-connect-group generation 1 (__consumer_offsets-37) with 1 members (kafka.coordinator.group.GroupCoordinator)
connect          | [2023-08-04 11:20:23,226] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Successfully joined group with generation Generation{generationId=1, memberId='connect-1-c3b4f443-0240-44ad-97fd-d305853fc174', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
broker           | [2023-08-04 11:20:23,354] INFO [GroupCoordinator 1]: Assignment received from leader connect-1-c3b4f443-0240-44ad-97fd-d305853fc174 for group compose-connect-group for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
connect          | [2023-08-04 11:20:23,361] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Successfully synced group in generation Generation{generationId=1, memberId='connect-1-c3b4f443-0240-44ad-97fd-d305853fc174', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
connect          | [2023-08-04 11:20:23,363] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-c3b4f443-0240-44ad-97fd-d305853fc174', leaderUrl='http://connect:8083/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
connect          | [2023-08-04 11:20:23,369] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Starting connectors and tasks using config offset -1 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
connect          | [2023-08-04 11:20:23,370] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
connect          | [2023-08-04 11:20:23,507] INFO [Producer clientId=producer-3] Resetting the last seen epoch of partition docker-connect-configs-0 to 0 since the associated topicId changed from null to ISAKcDcaSuSPu9aTa_qbYg (org.apache.kafka.clients.Metadata)
connect          | [2023-08-04 11:20:23,774] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
connect          | Aug 04, 2023 11:20:24 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
connect          | WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
connect          | Aug 04, 2023 11:20:24 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
connect          | WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
connect          | Aug 04, 2023 11:20:24 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
connect          | WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
connect          | Aug 04, 2023 11:20:24 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
connect          | WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConfluentV1MetadataResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConfluentV1MetadataResource will be ignored. 
connect          | Aug 04, 2023 11:20:24 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
connect          | WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
connect          | [2023-08-04 11:20:25,018] INFO HV000001: Hibernate Validator 6.1.7.Final (org.hibernate.validator.internal.util.Version)
connect          | Aug 04, 2023 11:20:26 AM org.glassfish.jersey.internal.Errors logErrors
connect          | WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
connect          | WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
connect          | WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
connect          | WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
connect          | WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.
connect          | 
connect          | 
connect          | [2023-08-04 11:20:26,198] INFO Started o.e.j.s.ServletContextHandler@7a6ba56{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
connect          | [2023-08-04 11:20:26,198] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer)
connect          | [2023-08-04 11:20:26,198] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect)
control-center   | [2023-08-04 11:20:28,397] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:20:33,427] WARN [creqId=c9857bd1][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:20:29.416Z(1691148029416000), length=0B, duration=4009ms(4009809233ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:20:33,428] WARN [creqId=c9857bd1][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:20:33.426Z(1691148033426000), length=0B, duration=0ns, totalDuration=4009ms(4009931393ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:605)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:20:36,977] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:20:37,052] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:20:37,062] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:20:37,165] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:20:37,229] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:20:37,293] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:20:37,300] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:20:37,344] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:20:37,457] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:20:37,459] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:20:37,833] INFO [AdminClient clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-admin] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:20:37,996] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] Processed 16 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:20:38,205] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:20:51,333] WARN [creqId=294ac8e7][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:20:51.267Z(1691148051267000), length=0B, duration=63420s(63420273ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:20:51,334] WARN [creqId=294ac8e7][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:20:51.331Z(1691148051331000), length=0B, duration=0ns, totalDuration=63698s(63698461ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:20:53,357] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:20:53,358] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:21:14,317] WARN [creqId=32d1b8ec][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:21:14.300Z(1691148074300000), length=0B, duration=15317s(15317416ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:21:14,317] WARN [creqId=32d1b8ec][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:21:14.315Z(1691148074315000), length=0B, duration=0ns, totalDuration=15503s(15503066ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:21:32,825] WARN [creqId=9bac81c0][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:21:32.803Z(1691148092803000), length=0B, duration=20172s(20172912ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:21:32,825] WARN [creqId=9bac81c0][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:21:32.824Z(1691148092824000), length=0B, duration=0ns, totalDuration=20359s(20359227ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:21:33,498] INFO [AdminClient clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-admin] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:21:34,553] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 to 0 since the associated topicId changed from null to nzdecc30ShSdQcB04frvog (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:21:34,553] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-producer] Resetting the last seen epoch of partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 to 0 since the associated topicId changed from null to KabcuEMhTCOz9nkNgpIsHg (org.apache.kafka.clients.Metadata)
control-center   | [2023-08-04 11:21:53,350] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:21:53,351] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:21:54,024] INFO [AdminClient clientId=adminclient-3] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:21:56,629] WARN [creqId=d15ec0b2][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:21:56.611Z(1691148116611000), length=0B, duration=16625s(16625407ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:21:56,629] WARN [creqId=d15ec0b2][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:21:56.628Z(1691148116628000), length=0B, duration=0ns, totalDuration=16770s(16770803ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:22:17,402] WARN [creqId=c9389bb5][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:22:17.383Z(1691148137383000), length=0B, duration=18043s(18043321ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:22:17,403] WARN [creqId=c9389bb5][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:22:17.401Z(1691148137401000), length=0B, duration=0ns, totalDuration=18135s(18135503ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fire
control-center   | ChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:22:28,480] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:22:35,480] WARN [creqId=8f6c0729][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:22:35.463Z(1691148155463000), length=0B, duration=15111s(15111511ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:22:35,480] WARN [creqId=8f6c0729][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:22:35.479Z(1691148155479000), length=0B, duration=0ns, totalDuration=15237s(15237418ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:22:37,022] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:22:37,073] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:22:37,075] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:22:37,265] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:22:37,324] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] Processed 14 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:22:37,326] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:22:37,328] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] Processed 14 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:22:37,423] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:22:37,469] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:22:37,475] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:22:38,029] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] Processed 14 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:22:38,228] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:22:53,345] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:22:53,345] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:22:56,159] WARN [creqId=9af173e6][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:22:56.139Z(1691148176139000), length=0B, duration=19203s(19203985ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:22:56,160] WARN [creqId=9af173e6][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:22:56.158Z(1691148176158000), length=0B, duration=0ns, totalDuration=19279s(19279955ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fire
control-center   | ChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:23:16,683] WARN [creqId=6119c8e9][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:23:16.624Z(1691148196624000), length=0B, duration=58002s(58002775ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:23:16,683] WARN [creqId=6119c8e9][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:23:16.682Z(1691148196682000), length=0B, duration=0ns, totalDuration=58115s(58115603ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:23:33,860] WARN [creqId=ef06780e][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:23:33.775Z(1691148213775000), length=0B, duration=78880s(78880824ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:23:33,860] WARN [creqId=ef06780e][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:23:33.855Z(1691148213855000), length=0B, duration=0ns, totalDuration=79133s(79133842ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:23:53,352] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:23:53,353] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:23:55,756] WARN [creqId=47b3e07b][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:23:55.739Z(1691148235739000), length=0B, duration=15606s(15606444ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:23:55,756] WARN [creqId=47b3e07b][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:23:55.755Z(1691148235755000), length=0B, duration=0ns, totalDuration=15673s(15673231ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:24:14,078] WARN [creqId=9625f892][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:24:14.015Z(1691148254015000), length=0B, duration=61592s(61592510ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:24:14,079] WARN [creqId=9625f892][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:24:14.077Z(1691148254077000), length=0B, duration=0ns, totalDuration=61693s(61693725ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:24:26,826] INFO [Producer clientId=confluent-control-center-heartbeat-sender-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:24:28,564] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:24:36,703] WARN [creqId=1c1e1f7d][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:24:36.643Z(1691148276643000), length=0B, duration=58576s(58576532ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:24:36,704] WARN [creqId=1c1e1f7d][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:24:36.702Z(1691148276702000), length=0B, duration=0ns, totalDuration=58692s(58692586ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:24:37,050] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:24:37,165] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:24:37,170] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:24:37,266] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:24:37,330] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:24:37,404] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:24:37,422] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:24:37,473] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:24:37,512] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:24:37,515] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:24:38,109] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] Processed 16 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:24:38,304] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:24:39,404] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:24:40,167] INFO [Producer clientId=c3-command] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:24:45,356] INFO [Consumer clientId=_confluent-controlcenter-license-manager-7-4-1-1-global-consumer, groupId=null] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:24:45,443] INFO [Producer clientId=_confluent-controlcenter-license-manager-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:24:52,936] WARN [creqId=5efd13c2][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:24:52.876Z(1691148292876000), length=0B, duration=59093s(59093104ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:24:52,936] WARN [creqId=5efd13c2][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:24:52.935Z(1691148292935000), length=0B, duration=0ns, totalDuration=59214s(59214847ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:24:53,351] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:24:53,351] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:25:11,054] WARN [creqId=6906ab12][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:25:11.010Z(1691148311010000), length=0B, duration=43041s(43041002ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:25:11,054] WARN [creqId=6906ab12][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:25:11.053Z(1691148311053000), length=0B, duration=0ns, totalDuration=43146s(43146432ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
connect          | [2023-08-04 11:25:21,029] INFO [AdminClient clientId=adminclient-8] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:27,411] WARN [creqId=ee52c6e5][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:25:27.355Z(1691148327355000), length=0B, duration=54419s(54419542ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:25:27,411] WARN [creqId=ee52c6e5][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:25:27.410Z(1691148327410000), length=0B, duration=0ns, totalDuration=54492s(54492342ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:25:28,179] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1-command] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:34,049] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:34,219] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:34,443] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:34,599] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:35,117] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:35,628] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:35,987] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:36,251] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:36,587] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:37,204] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:37,290] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:37,472] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:37,569] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:37,583] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:37,641] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:37,875] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:37,875] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:37,918] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:37,925] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:37,940] INFO [AdminClient clientId=_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-admin] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:38,096] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:38,140] INFO [Consumer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-consumer, groupId=_confluent-controlcenter-7-4-1-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:43,936] WARN [creqId=48ac7d5f][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:25:43.883Z(1691148343883000), length=0B, duration=52039s(52039886ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:25:43,936] WARN [creqId=48ac7d5f][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:25:43.935Z(1691148343935000), length=0B, duration=0ns, totalDuration=52118s(52118804ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:25:44,103] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:44,165] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:44,365] INFO [Producer clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:25:53,348] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:25:53,348] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:26:05,498] WARN [creqId=12f78d95][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:26:05.399Z(1691148365399000), length=0B, duration=94326s(94326657ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:26:05,498] WARN [creqId=12f78d95][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:26:05.494Z(1691148365494000), length=0B, duration=0ns, totalDuration=94501s(94501986ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:26:11,480] INFO 192.168.224.1 - - [04/Aug/2023:11:26:10 +0000] "GET / HTTP/1.1" 200 1151 "-" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 511 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:11,635] INFO 192.168.224.1 - - [04/Aug/2023:11:26:11 +0000] "GET /dist/bootstrap-local.76a65c8.js HTTP/1.1" 200 61728 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 146 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:12,041] INFO 192.168.224.1 - - [04/Aug/2023:11:26:12 +0000] "GET /dist/favicon.ico HTTP/1.1" 200 33310 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 22 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:12,034] INFO 192.168.224.1 - - [04/Aug/2023:11:26:12 +0000] "GET /dist/manifest.json HTTP/1.1" 200 389 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 17 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:12,150] INFO 192.168.224.1 - - [04/Aug/2023:11:26:12 +0000] "GET /dist/dist/android-chrome-144x144.png HTTP/1.1" 404 399 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 63 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:12,513] INFO 192.168.224.1 - - [04/Aug/2023:11:26:12 +0000] "GET /3.0/license/payload HTTP/1.1" 200 170 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 498 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:12,545] INFO 192.168.224.1 - - [04/Aug/2023:11:26:11 +0000] "GET /2.0/feature/flags HTTP/1.1" 200 429 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 555 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:23,765] INFO 192.168.224.1 - - [04/Aug/2023:11:26:22 +0000] "GET /dist/c3.chunk-e71fc146570941633bfc.js HTTP/1.1" 200 1077929 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 1191 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:23,876] INFO 192.168.224.1 - - [04/Aug/2023:11:26:22 +0000] "GET /dist/c3.chunk-71c32f43e29ac8156892.js HTTP/1.1" 200 1116319 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 1294 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:26,664] INFO 192.168.224.1 - - [04/Aug/2023:11:26:26 +0000] "GET /2.0/feature/flags HTTP/1.1" 200 429 "http://localhost:9021/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 53 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:26,989] WARN [creqId=d7ec86e7][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:26:26.920Z(1691148386920000), length=0B, duration=67709s(67709844ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:26:26,991] WARN [creqId=d7ec86e7][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:26:26.988Z(1691148386988000), length=0B, duration=0ns, totalDuration=67903s(67903298ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:629)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:26:27,168] INFO 192.168.224.1 - - [04/Aug/2023:11:26:27 +0000] "GET /3.0/license HTTP/1.1" 200 446 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 85 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:27,945] INFO 192.168.224.1 - - [04/Aug/2023:11:26:27 +0000] "GET /3.0/auth/principal HTTP/1.1" 200 30 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 336 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:27,978] INFO 192.168.224.1 - - [04/Aug/2023:11:26:27 +0000] "GET /2.0/clusters/kafka/display/CLUSTER_MANAGEMENT HTTP/1.1" 200 110 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 229 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:27,988] INFO 192.168.224.1 - - [04/Aug/2023:11:26:27 +0000] "GET /2.0/clusters/kafka/display/stream-monitoring HTTP/1.1" 200 110 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 386 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:28,028] INFO 192.168.224.1 - - [04/Aug/2023:11:26:27 +0000] "GET /2.0/health/status HTTP/1.1" 200 148 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 523 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:28,057] INFO 192.168.224.1 - - [04/Aug/2023:11:26:27 +0000] "GET /2.0/clusters/kafka/display/cluster_management HTTP/1.1" 200 110 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 387 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:28,188] INFO 192.168.224.1 - - [04/Aug/2023:11:26:27 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 612 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:28,201] INFO 192.168.224.1 - - [04/Aug/2023:11:26:28 +0000] "GET /2.0/clusters/kafka HTTP/1.1" 200 143 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 27 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:28,206] INFO 192.168.224.1 - - [04/Aug/2023:11:26:28 +0000] "GET /2.0/clusters/ksql HTTP/1.1" 200 2 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 117 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:28,248] INFO 192.168.224.1 - - [04/Aug/2023:11:26:28 +0000] "GET /2.0/clusters/schema-registry HTTP/1.1" 200 144 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 168 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:28,249] INFO 192.168.224.1 - - [04/Aug/2023:11:26:28 +0000] "GET /2.0/clusters/connect HTTP/1.1" 200 89 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 175 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:28,621] INFO stream-thread [_confluent-controlcenter-7-4-1-1-command-131145ad-beaf-40ab-9f2f-4f67571c7c4c-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:26:29,364] INFO 192.168.224.1 - - [04/Aug/2023:11:26:28 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/controller HTTP/1.1" 200 67 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 561 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:29,970] INFO 192.168.224.1 - - [04/Aug/2023:11:26:28 +0000] "GET /2.0/metrics/maxtime HTTP/1.1" 200 2 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 1196 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:30,124] INFO 192.168.224.1 - - [04/Aug/2023:11:26:29 +0000] "GET /api/connect/connect-default/ HTTP/1.1" 200 110 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 785 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:30,206] INFO 192.168.224.1 - - [04/Aug/2023:11:26:28 +0000] "GET /2.0/metrics/clusters/status HTTP/1.1" 200 216 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 1424 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:31,592] INFO 192.168.224.1 - - [04/Aug/2023:11:26:31 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/brokers/config HTTP/1.1" 200 4496 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 273 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:31,754] INFO 192.168.224.1 - - [04/Aug/2023:11:26:31 +0000] "GET /2.0/metrics/MkU3OEVBNTcwNTJENDM2Qg/broker/status HTTP/1.1" 200 162 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 85 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:33,599] INFO [AdminClient clientId=_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-admin] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
control-center   | [2023-08-04 11:26:37,135] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-7] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:26:37,216] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-10] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:26:37,217] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-4] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:26:37,316] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-5] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:26:37,336] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-11] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:26:37,411] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-6] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:26:37,458] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-8] Processed 16 total records, ran 0 punctuators, and committed 4 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:26:37,503] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-2] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:26:37,517] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-12] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:26:37,601] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-3] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:26:38,152] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-1] Processed 16 total records, ran 0 punctuators, and committed 8 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:26:38,341] INFO stream-thread [_confluent-controlcenter-7-4-1-1-8bb38cc0-eb32-4be5-be61-027cb1582feb-StreamThread-9] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update (org.apache.kafka.streams.processor.internals.StreamThread)
control-center   | [2023-08-04 11:26:50,094] WARN [creqId=4f1d77f8][http://schema-registry:8081/#GET] Request: {startTime=2023-08-04T11:26:46.076Z(1691148406076000), length=0B, duration=4017ms(4017201961ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , scheme=none+http, name=GET, headers=[:method=GET, :path=/, :scheme=http, :authority=schema-registry:8081]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | [2023-08-04 11:26:50,095] WARN [creqId=4f1d77f8][http://schema-registry:8081/#GET] Response: {startTime=2023-08-04T11:26:50.093Z(1691148410093000), length=0B, duration=0ns, totalDuration=4017ms(4017307920ns), cause=com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries , headers=[:status=0]} (com.linecorp.armeria.client.logging.LoggingClient)
control-center   | com.linecorp.armeria.client.UnprocessedRequestException: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at com.linecorp.armeria.client.UnprocessedRequestException.of(UnprocessedRequestException.java:45)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$execute$0(HttpClientDelegate.java:102)
control-center   | 	at com.linecorp.armeria.client.HttpClientDelegate.lambda$resolveAddress$1(HttpClientDelegate.java:131)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$doResolve$5(RefreshingAddressResolver.java:132)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
control-center   | 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
control-center   | 	at com.linecorp.armeria.client.RefreshingAddressResolver.lambda$sendQueries$6(RefreshingAddressResolver.java:160)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1055)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1000)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:418)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:605)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.access$400(DnsResolveContext.java:66)
control-center   | 	at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:462)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)
control-center   | 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:232)
control-center   | 	at io.netty.resolver.dns.DnsQueryContext.finish(DnsQueryContext.java:224)
control-center   | 	at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1314)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
control-center   | 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
control-center   | 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
control-center   | 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
control-center   | 	at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:97)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
control-center   | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
control-center   | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
control-center   | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
control-center   | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
control-center   | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
control-center   | 	at java.base/java.lang.Thread.run(Thread.java:829)
control-center   | Caused by: java.net.UnknownHostException: Failed to resolve 'schema-registry' after 2 queries 
control-center   | 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1047)
control-center   | 	... 35 more
control-center   | [2023-08-04 11:26:53,350] WARN broker=1 is not instrumented with ConfluentMetricsReporter (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:26:53,350] ERROR broker=1 is storing logs in /tmp/kraft-combined-logs, Kafka expects to store log data in a persistent location (io.confluent.controlcenter.healthcheck.AllHealthCheck)
control-center   | [2023-08-04 11:26:57,707] INFO 192.168.224.1 - - [04/Aug/2023:11:26:57 +0000] "GET /2.0/metrics/maxtime HTTP/1.1" 200 2 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 40 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:57,732] INFO 192.168.224.1 - - [04/Aug/2023:11:26:57 +0000] "GET /2.0/kafka/MkU3OEVBNTcwNTJENDM2Qg/controller HTTP/1.1" 200 67 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 26 (io.confluent.rest-utils.requests)
control-center   | [2023-08-04 11:26:57,843] INFO 192.168.224.1 - - [04/Aug/2023:11:26:57 +0000] "GET /2.0/metrics/clusters/status HTTP/1.1" 200 216 "http://localhost:9021/clusters" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 153 (io.confluent.rest-utils.requests)
Gracefully stopping... (press Ctrl+C again to force)
Aborting on container exit...
 Container rest-proxy  Stopping
 Container control-center  Stopping
 Container rest-proxy  Stopped
 Container control-center  Stopped
 Container connect  Stopping
 Container connect  Stopped
 Container schema-registry  Stopping
 Container schema-registry  Stopped
 Container broker  Stopping
 Container broker  Stopped
canceled
