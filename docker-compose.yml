version: "3"
services:

  crowler-producer:
    build:
      context: ./crawler-kafka/
      dockerfile: Dockerfile
    image: crawler-kafka:latest
    ports:
      - "9092:9092"

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka1
    hostname: kafka1
    ports:
      - "9092:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT'
      KAFKA_LISTENERS: 'INTERNAL://kafka1:29092,CONTROLLER://kafka1:29093,EXTERNAL://0.0.0.0:9092'
      KAFKA_ADVERTISED_LISTENERS: 'INTERNAL://kafka1:29092,EXTERNAL://localhost:9092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'INTERNAL'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka1:29093'
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_CREATE_TOPICS: "gate_of_word:1:1"
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
#      CLUSTER_ID: 'ciWo7IWazngRchmPES6q5A=='
    volumes:
      - ./scripts/update_run.sh:/tmp/update_run.sh	
      - ./clusterID:/tmp/clusterID
    command: "bash -c '/tmp/update_run.sh && /etc/confluent/docker/run'"    
      
  kafka-gen:
    image: confluentinc/cp-kafka:7.3.3
    hostname: kafka-gen
    container_name: kafka-gen
    volumes:
      - ./scripts/create_cluster_id.sh:/tmp/create_cluster_id.sh
      - ./clusterID:/tmp/clusterID
    command: "bash -c '/tmp/create_cluster_id.sh'"
      
  schema-registry:
    image: confluentinc/cp-schema-registry
    container_name: schema-registry
    hostname: schema-registry
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka1:29092'
      SCHEMA_REGISTRY_LISTENERS: 'http://0.0.0.0:8081'
    depends_on:
      - kafka
      
#  kafka-ui:
#    image: provectuslabs/kafka-ui
#    container_name: kafka-ui
#    ports:
#      - 8090:8080
#    restart: always
#    environment:
#      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka1:29092
#      - KAFKA_CLUSTERS_0_NAME=kraft
#    links:
#      - kafka

  jobmanager:
    image: flink:1.17-scala_2.12
    ports:
      - "8081:8081"
    command: standalone-job --job-classname com.example.sharov.anatoliy.DataStreamJob 
    volumes:
      - /home/anatolii/work/projects/data-agrigator/kafka-flink-postgresql-gradle/build/libs/kafka-flink-postgresql-gradle-0.1-SNAPSHOT-all.jar:/opt/flink/usrlib/my_api.jar
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: 192.168.1.100  # Замените на IP-адрес вашего хоста
        parallelism.default: 2        

  taskmanager:
    image: flink:1.17-scala_2.12
    depends_on:
      - jobmanager
    command: taskmanager
    scale: 1
    volumes:
      - /home/anatolii/work/projects/data-agrigator/kafka-flink-postgresql-gradle/build/libs/kafka-flink-postgresql-gradle-0.1-SNAPSHOT-all.jar:/opt/flink/usrlib/my_api.jar
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: 192.168.1.100  # Замените на IP-адрес вашего хоста
        taskmanager.numberOfTaskSlots: 2
        parallelism.default: 2 
        
  postgres:
    image: postgres:15.3-alpine
    container_name: postgres_db
    restart: always
    ports:
      - 5432:5432
    environment:
      POSTGRES_DB: counted_words
      POSTGRES_USER: agregator
      POSTGRES_PASSWORD: 1111
    volumes:
      - my_postgres_data:/var/lib/postgresql/data
      - /home/anatolii/work/projects/data-agrigator/database/db_scripts/create_table.sql:/docker-entrypoint-initdb.d/create_tables.sql
      
volumes:
    my_postgres_data:
